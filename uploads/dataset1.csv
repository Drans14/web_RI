Title,Abstract,Abstrak_bersih
Statements on Open Science for Sustainable Development Goals,"This article attempts to practicalise Open Science (OS) to promote ideas and enhance efforts for the Sustainable Development Goals (SDGs). It delineates General Statements (n = 20) as guiding beacons and the Specific Statements (n = 70) that act as precision tools in OS orientated policymaking, research, innovations, and public engagement, and access to scientific knowledge. The authors hope to draw kindled and educated attention to OS besides underscoring the need for unbiased, inclusive, and diligent execution of the SDGs. By adopting these Statements accordingly and in appropriate stages within national strategies and ensuring transparent reporting of the progress, the authors envision a transformed world by 2030. With this appeal, scientific endeavours could be more effectively directed and optimised with OS, significantly advancing progress toward the SDGs. Â© 2024 The Author(s).","This article attempts to practicalise Open Science (OS) to promote ideas and enhance efforts for the Sustainable Development Goals (SDGs). It delineates General Statements (n = 20) as guiding beacons and the Specific Statements (n = 70) that act as precision tools in OS orientated policymaking, research, innovations, and public engagement, and access to scientific knowledge. The authors hope to draw kindled and educated attention to OS besides underscoring the need for unbiased, inclusive, and diligent execution of the SDGs. By adopting these Statements accordingly and in appropriate stages within national strategies and ensuring transparent reporting of the progress, the authors envision a transformed world by 2030. With this appeal, scientific endeavours could be more effectively directed and optimised with OS, significantly advancing progress toward the SDGs."
KadiStudio Use-Case Workflow: Automation of Data Processing for in Situ Micropillar Compression Tests,"Scientific processes produce huge amounts of data that are usually acquired, transformed and analyzed on a regular basis. Translating these processes into automatable and reproducible workflows is considered to be an efficient way to support scientists in performing repeated processes that would otherwise be time-consuming and error-prone tasks. Consequently, the quality of scientific research can be accelerated and enhanced. In this article, we present for the first time a use-case of KadiStudio as a tool to automate analysis procedures of scientific data that are repeatedly acquired from in situ scanning electron microscope (SEM) micromechanical testing. KadiStudio provides a desktop-based workflow editor as part of the ecosystem of Kadi4Mat: Karlsruhe Data Infrastructure for material Science. The presented workflow includes nodes for processing and analysis of different types of data, namely mechanical response in text format and a series of SEM images in video file format acquired during in situ SEM deformation tests. In addition, the raw and analyzed data are automatically uploaded to the KadiWeb repository via nodes based on the kadi-apy library. Â© 2023 The Author(s).","Scientific processes produce huge amounts of data that are usually acquired, transformed and analyzed on a regular basis. Translating these processes into automatable and reproducible workflows is considered to be an efficient way to support scientists in performing repeated processes that would otherwise be time-consuming and error-prone tasks. Consequently, the quality of scientific research can be accelerated and enhanced. In this article, we present for the first time a use-case of KadiStudio as a tool to automate analysis procedures of scientific data that are repeatedly acquired from in situ scanning electron microscope (SEM) micromechanical testing. KadiStudio provides a desktop-based workflow editor as part of the ecosystem of Kadi4Mat: Karlsruhe Data Infrastructure for material Science. The presented workflow includes nodes for processing and analysis of different types of data, namely mechanical response in text format and a series of SEM images in video file format acquired during in situ SEM deformation tests. In addition, the raw and analyzed data are automatically uploaded to the KadiWeb repository via nodes based on the kadi-apy library."
Umbrella Data Management Plans to Integrate FAIR Data : Lessons From the ISIDORe and BY-COVID Consortia for Pandemic Preparedness,"The Horizon Europe project ISIDORe is dedicated to pandemic preparedness and responsiveness research. It brings together 17 research infrastructures (RIs) and networks to provide a broad range of services to infectious disease researchers. An efficient and structured treatment of data is central to ISIDOReâs aim to furnish seamless access to its multidisciplinary catalogue of services, and to ensure that usersâ results are treated FAIRly. ISIDORe therefore requires a data management plan (DMP) covering both access management and research outputs, applicable over a broad range of disciplines, and compatible with the constraints and existing practices of its diverse partners. Here, we describe how, to achieve that aim, we undertook an iterative, step-by-step, process to build a community-approved living document, identifying good practices and processes, on the basis of use cases, presented as proof of concepts. International fora such as the RDA and EOSC, and primarily the BY-COVID project, furnished registries, tools and online data platforms, as well as standards, and the support of data scientists. Together, these elements provide a path for building an umbrella, FAIR-compliant DMP, aligned as fully as possible with FAIR principles, which could also be applied as a framework for data management harmonisation in other large-scale, challenge-driven projects. Finally, we discuss how data management and reuse can be further improved through the use of knowledge models when writing DMPs and, how, in the future, an inter-RI network of data stewards could contribute to the establishment of a community of practice, to be integrated subsequently into planned trans-RI competence centres. Â© 2023 The Author(s).","The Horizon Europe project ISIDORe is dedicated to pandemic preparedness and responsiveness research. It brings together 17 research infrastructures (RIs) and networks to provide a broad range of services to infectious disease researchers. An efficient and structured treatment of data is central to ISIDORes aim to furnish seamless access to its multidisciplinary catalogue of services, and to ensure that users results are treated FAIRly. ISIDORe therefore requires a data management plan (DMP) covering both access management and research outputs, applicable over a broad range of disciplines, and compatible with the constraints and existing practices of its diverse partners. Here, we describe how, to achieve that aim, we undertook an iterative, step-by-step, process to build a community-approved living document, identifying good practices and processes, on the basis of use cases, presented as proof of concepts. International fora such as the RDA and EOSC, and primarily the BY-COVID project, furnished registries, tools and online data platforms, as well as standards, and the support of data scientists. Together, these elements provide a path for building an umbrella, FAIR-compliant DMP, aligned as fully as possible with FAIR principles, which could also be applied as a framework for data management harmonisation in other large-scale, challenge-driven projects. Finally, we discuss how data management and reuse can be further improved through the use of knowledge models when writing DMPs and, how, in the future, an inter-RI network of data stewards could contribute to the establishment of a community of practice, to be integrated subsequently into planned trans-RI competence centres."
What are Researchersâ Needs in Data Discovery? Analysis and Ranking of a Large-Scale Collection of Crowdsourced Use Cases,"Data discovery is important to facilitate data re-use. In order to help frame the development and improvement of data discovery tools, we collected a list of requirements and usersâ wishes. This paper presents the analysis of these 101 use cases to examine data discovery requirements; these cases were collected between 2019 and 2020. We categorized the information across 12 âtopicsâ and eight types of users. While the availability of metadata was an expected topic of importance, users were also keen on receiving more information on data citation and a better overview of their field. We conducted and analysed a survey among data infrastructure specialists in a first attempt at ranking the requirements. Between these data professionals, these rankings were very different, excepting the availability of metadata and data quality assessment. Â© 2023 The Author(s).","Data discovery is important to facilitate data re-use. In order to help frame the development and improvement of data discovery tools, we collected a list of requirements and users wishes. This paper presents the analysis of these 101 use cases to examine data discovery requirements; these cases were collected between 2019 and 2020. We categorized the information across 12 topics and eight types of users. While the availability of metadata was an expected topic of importance, users were also keen on receiving more information on data citation and a better overview of their field. We conducted and analysed a survey among data infrastructure specialists in a first attempt at ranking the requirements. Between these data professionals, these rankings were very different, excepting the availability of metadata and data quality assessment."
Organizing Scientific Knowledge from Engineering Sciences Using the Open Research Knowledge Graph: The Tailored Forming Process Chain Use Case,"Background: Engineering sciences are essential for addressing contemporary technical, environmental, and economic challenges. Despite its data-intensive and interdisciplinary nature, the organization of Findable, Accessible, Interoperable, and Reusable (FAIR) scientific knowledge and data in this research field remains understudied. Engineers need infrastructures with services that support them in organizing FAIR scientific knowledge and data for communication and (re-)use. Aim: We explore the use of the Open Research Knowledge Graph (ORKG) as such an infrastructure by demonstrating how engineers can utilize the ORKG in innovative ways for communication and (re-)use. Method: For a use case from the Collaborative Research Center 1153 âTailored Formingâ, we collect, extract, and analyze scientific knowledge on 10 Tailored Forming Process Chains (TFPCs) from five publications in the ORKG. In particular, we semantically describe the TFPCs, i.a., regarding their steps, manufacturing methods, measurements, and results. The usefulness of the data extraction topics, their organization, and the relevance of the knowledge described is examined by an expert consultation with 21 experts. Results: Based on the described knowledge, we build and publish an ORKG comparison as a detailed overview for communication. Furthermore, we (re-)use the knowledge and answer eight competency questions asked by two domain experts. The validation shows a clear agreement of the 21 experts regarding the examined usefulness and relevance. Conclusions: Our use case shows that the ORKG as a ready-to-use infrastructure with services supports researchers, including engineers, in sustainably organizing FAIR scientific knowledge. The direct use of the ORKG by engineers is feasible, so the ORKG is a promising infrastructure for innovative ways of communicating and (re-)using FAIR scientific knowledge in engineering sciences, thus advancing this research field. Â© 2024, Ubiquity Press. All rights reserved.","Background: Engineering sciences are essential for addressing contemporary technical, environmental, and economic challenges. Despite its data-intensive and interdisciplinary nature, the organization of Findable, Accessible, Interoperable, and Reusable (FAIR) scientific knowledge and data in this research field remains understudied. Engineers need infrastructures with services that support them in organizing FAIR scientific knowledge and data for communication and (re-)use. Aim: We explore the use of the Open Research Knowledge Graph (ORKG) as such an infrastructure by demonstrating how engineers can utilize the ORKG in innovative ways for communication and (re-)use. Method: For a use case from the Collaborative Research Center 1153 Tailored Forming, we collect, extract, and analyze scientific knowledge on 10 Tailored Forming Process Chains (TFPCs) from five publications in the ORKG. In particular, we semantically describe the TFPCs, , regarding their steps, manufacturing methods, measurements, and results. The usefulness of the data extraction topics, their organization, and the relevance of the knowledge described is examined by an expert consultation with 21 experts. Results: Based on the described knowledge, we build and publish an ORKG comparison as a detailed overview for communication. Furthermore, we (re-)use the knowledge and answer eight competency questions asked by two domain experts. The validation shows a clear agreement of the 21 experts regarding the examined usefulness and relevance. Conclusions: Our use case shows that the ORKG as a ready-to-use infrastructure with services supports researchers, including engineers, in sustainably organizing FAIR scientific knowledge. The direct use of the ORKG by engineers is feasible, so the ORKG is a promising infrastructure for innovative ways of communicating and (re-)using FAIR scientific knowledge in engineering sciences, thus advancing this research field."
Development of a Job Advertisement Analysis for Assessing Data Science Competencies,"Data science competencies receive rising attention. How can competency profiles be built for data scientists? This question is encountered by synthesizing various competency frameworks from the literature and by conducting a job advertisement analysis. The âSkills and Recruitment Ontologyâ is used as the underlying ontology for the job advertisement analysis. Therefore, over 5000 job postings were crawled from the job platform âJoobleâ and the results evaluated in a focus group. This work provides a competency data set for data science jobs. It points out newly found competencies and also provides design principles for competency frameworks. Â© 2023 The Author(s).","Data science competencies receive rising attention. How can competency profiles be built for data scientists? This question is encountered by synthesizing various competency frameworks from the literature and by conducting a job advertisement analysis. The Skills and Recruitment Ontology is used as the underlying ontology for the job advertisement analysis. Therefore, over 5000 job postings were crawled from the job platform Jooble and the results evaluated in a focus group. This work provides a competency data set for data science jobs. It points out newly found competencies and also provides design principles for competency frameworks."
Decentralised Semantics: A Semantic Engine User Perspective,"The Findable, Accessible, Interoperable and Reusable (FAIR) data principles were created to guide the improvement of research data (Wilkinson et al., 2016). As data curators and educators, we often see individual research groups and researchers establish their own unique data collection process, resulting in poor and inconsistent data documentation. At the conclusion of the project, while the data may be accessible and understood by members within the team, it is often not readily usable to anyone outside of those most closely associated with data collection and analysis. The root cause of this is the difficulty to document the pertinent information required to capture the context in which data was captured, processed, and presented. And even when this is attempted it tends to be static and non-machine actionable. As a result, the project data might be FAIR but it is not visible and the cost of re-use is too high as currently few protocols are machine actionable. The availability of context documentation will help other researchers understand and facilitate the re-use the data. Agri-Food Data Canada operates across multiple projects in different fields and run by different institutions. It is a natural environment to recognize the need of decentralized semantic definitions where each research group can influence, modify, or adjust the definition of the data while maintaining integrity of data objects (e.g., schema, data sets, catalogues) across the ecosystem. This practice paper describes the release of the first version of the Semantic Engine leveraging OCA, an architecture to document schemas optimized for decentralized collaboration and reproducibility. OCA leverages new technologies on self-addressing identifiers and enables content-based authority vs. location-based authority. We present here the first results of the Semantic Engine development and the future application. Â© 2024 The Author(s).","The Findable, Accessible, Interoperable and Reusable (FAIR) data principles were created to guide the improvement of research data (Wilkinson et al., 2016). As data curators and educators, we often see individual research groups and researchers establish their own unique data collection process, resulting in poor and inconsistent data documentation. At the conclusion of the project, while the data may be accessible and understood by members within the team, it is often not readily usable to anyone outside of those most closely associated with data collection and analysis. The root cause of this is the difficulty to document the pertinent information required to capture the context in which data was captured, processed, and presented. And even when this is attempted it tends to be static and non-machine actionable. As a result, the project data might be FAIR but it is not visible and the cost of re-use is too high as currently few protocols are machine actionable. The availability of context documentation will help other researchers understand and facilitate the re-use the data. Agri-Food Data Canada operates across multiple projects in different fields and run by different institutions. It is a natural environment to recognize the need of decentralized semantic definitions where each research group can influence, modify, or adjust the definition of the data while maintaining integrity of data objects (, schema, data sets, catalogues) across the ecosystem. This practice paper describes the release of the first version of the Semantic Engine leveraging OCA, an architecture to document schemas optimized for decentralized collaboration and reproducibility. OCA leverages new technologies on self-addressing identifiers and enables content-based authority vs. location-based authority. We present here the first results of the Semantic Engine development and the future application."
Bridging the Gap: Enhancing Prominence and Provenance of NASA Datasets in Research Publications,"Attribution of datasets that were used to generate research results described in peer-reviewed publications to the original source of these datasets (which are often archived at NASA Earth Science data centers) has been very challenging. Even though the data citation standard of citing datasets as research artifacts and citing them with Digital Object Identifiers (DOIs) was introduced over a decade ago, most authors do not properly reference the data used in their studies and merely mention them in the text. The lack of proper citations of datasets makes the peer-reviewed publication less transparent, imperils reproducibility, and impedes open science. We offer an open-source publication management methodology and a tool that can help to enhance usage-based data discovery, prominence, and provenance of the data; reproducibility of the research results; and potentially increase the return on investment on NASA-funded research. Â© 2024 The Author(s).","Attribution of datasets that were used to generate research results described in peer-reviewed publications to the original source of these datasets (which are often archived at NASA Earth Science data centers) has been very challenging. Even though the data citation standard of citing datasets as research artifacts and citing them with Digital Object Identifiers (DOIs) was introduced over a decade ago, most authors do not properly reference the data used in their studies and merely mention them in the text. The lack of proper citations of datasets makes the peer-reviewed publication less transparent, imperils reproducibility, and impedes open science. We offer an open-source publication management methodology and a tool that can help to enhance usage-based data discovery, prominence, and provenance of the data; reproducibility of the research results; and potentially increase the return on investment on NASA-funded research."
Enhancing Privacy-Preserving Intrusion Detection in Blockchain-Based Networks with Deep Learning,"Data transfer in sensitive industries such as healthcare presents significant challenges due to privacy issues, which makes it difficult to collaborate and use machine learning effectively. These issues are explored in this study by looking at how hybrid learning approaches can be used to move models between users and consumers as well as within organizations. Blockchain technology is used, compensating participants with tokens, to provide privacy-preserving data collection and safe model transfer. The proposed approach combines Long Short-Term Memory (LSTM) and Gated Recurrent Units (GRU) to create a privacy-preserving secure framework for predictive analytics. LSTM-GRU-based federated learning techniques are used for local model training. The approach uses blockchain to securely transmit data to a distributed, decentralised cloud server, guaranteeing data confidentiality and privacy using a variety of storage techniques. This architecture addresses privacy issues and encourages seamless cooperation by utilising hybrid learning, federated learning, and blockchain technology. The study contributes to bridging the gap between secure data transfer and effective deep learning, specifically within sensitive domains. Experimental results demonstrate an impressive accuracy rate of 99.01%. Â© 2023 The Author(s).","Data transfer in sensitive industries such as healthcare presents significant challenges due to privacy issues, which makes it difficult to collaborate and use machine learning effectively. These issues are explored in this study by looking at how hybrid learning approaches can be used to move models between users and consumers as well as within organizations. Blockchain technology is used, compensating participants with tokens, to provide privacy-preserving data collection and safe model transfer. The proposed approach combines Long Short-Term Memory (LSTM) and Gated Recurrent Units (GRU) to create a privacy-preserving secure framework for predictive analytics. LSTM-GRU-based federated learning techniques are used for local model training. The approach uses blockchain to securely transmit data to a distributed, decentralised cloud server, guaranteeing data confidentiality and privacy using a variety of storage techniques. This architecture addresses privacy issues and encourages seamless cooperation by utilising hybrid learning, federated learning, and blockchain technology. The study contributes to bridging the gap between secure data transfer and effective deep learning, specifically within sensitive domains. Experimental results demonstrate an impressive accuracy rate of 99.01%."
ENEA PAES: A Web Platform for Supporting Italian Municipalities in Sustainable Energy Action Plan,"The Covenant of Mayors promotes the Sustainable Energy Action Plan (SEAP), aiming to mitigate greenhouse gas (GHG) emissions in line with the European Unionâs 2030 and 2050 targets. The Covenant signatories could take enormous advantage from a digital platform that allows SEAP drafting also to no technically skilled users, like majority of them are. The Italian National Agency for New Technologies, Energy and Sustainable Economic Development (ENEA) has developed the PAES platform in order to provide digital support to public administrations (PA) adhering to the Covenant of Mayors. The platform exploits open data and it is fed by energetic data aggregated on a municipal level. The platform offers appropriate functionalities for baseline CO2 emissions inventory (BEI) filling out and a best practice (BP) simulation tool. The latter allows to contextualize each BP and to estimate its effects in terms of the main GHG emission. The BP showing the best estimation results can then be converted into concrete adaptation actions. So, this digital system facilitates local Italian municipalities in the strategic planning and monitoring of adaptation actions taken over time. Â© 2023 The Author(s).","The Covenant of Mayors promotes the Sustainable Energy Action Plan (SEAP), aiming to mitigate greenhouse gas (GHG) emissions in line with the European Unions 2030 and 2050 targets. The Covenant signatories could take enormous advantage from a digital platform that allows SEAP drafting also to no technically skilled users, like majority of them are. The Italian National Agency for New Technologies, Energy and Sustainable Economic Development (ENEA) has developed the PAES platform in order to provide digital support to public administrations (PA) adhering to the Covenant of Mayors. The platform exploits open data and it is fed by energetic data aggregated on a municipal level. The platform offers appropriate functionalities for baseline CO2 emissions inventory (BEI) filling out and a best practice (BP) simulation tool. The latter allows to contextualize each BP and to estimate its effects in terms of the main GHG emission. The BP showing the best estimation results can then be converted into concrete adaptation actions. So, this digital system facilitates local Italian municipalities in the strategic planning and monitoring of adaptation actions taken over time."
A Notion of Feature Importance by Decorrelation and Detection of Trends by Random Forest Regression,"In many studies, we want to determine the influence of certain features on a dependent variable. More specifically, we are interested in the strength of the influence â i.e., is the feature relevant? And, if so, how the feature influences the dependent variable. Recently, data-driven approaches such as random forest regression have found their way into applications (Boulesteix et al. 2012). These models allow researchers to directly derive measures of feature importance, which are a natural indicator of the strength of the influence. For the relevant features, the correlation or rank correlation between the feature and the dependent variable has typically been used to determine the nature of the influence. More recent methods, some of which can also measure interactions between features, are based on a modeling approach. In particular, when machine learning models are used, SHAP scores are a recent and prominent method to determine these trends (Lundberg et al. 2017). In this paper, we introduce a novel notion of feature importance based on the well-studied Gram-Schmidt decorrelation method. Furthermore, we propose two estimators for identifying trends in the data using random forest regression, the so-called absolute and relative traversal rate. We empirically compare the properties of our estimators with those of well-established estimators on a variety of synthetic and real-world datasets. Â© 2023 The Author(s).","In many studies, we want to determine the influence of certain features on a dependent variable. More specifically, we are interested in the strength of the influence , is the feature relevant? And, if so, how the feature influences the dependent variable. Recently, data-driven approaches such as random forest regression have found their way into applications (Boulesteix et al. 2012). These models allow researchers to directly derive measures of feature importance, which are a natural indicator of the strength of the influence. For the relevant features, the correlation or rank correlation between the feature and the dependent variable has typically been used to determine the nature of the influence. More recent methods, some of which can also measure interactions between features, are based on a modeling approach. In particular, when machine learning models are used, SHAP scores are a recent and prominent method to determine these trends (Lundberg et al. 2017). In this paper, we introduce a novel notion of feature importance based on the well-studied Gram-Schmidt decorrelation method. Furthermore, we propose two estimators for identifying trends in the data using random forest regression, the so-called absolute and relative traversal rate. We empirically compare the properties of our estimators with those of well-established estimators on a variety of synthetic and real-world datasets."
Knowledge Infrastructures Are Growing Up: The Case for Institutional (Data) Repositories 10 Years After the Holdren Memo,"Institutional data repositories are uniquely positioned to support researchers in sharing scholarly outputs. As funding agencies develop and institute policies for research data access and sharing, institutional data repositories have emerged as a critical feature in ecosystems for data stewardship and sharing. We show that institutional data repositories can meet and exceed the requirements and recommendations of federal data policy, thereby maximizing the benefits of data sharing. We present results of a mixed-method study which explores the adoption and usage of institutional repositories to share data from 2017 to 2023. Data from two previous studies were combined with data collected in 2023 on the data sharing solutions of Association of Research Libraries member institutions in the United States and Canada. The analysis of the aggregated data indicates that data stewardship has increased in both institutional repositories and institutional data repositories with an increase in complementary infrastructure to support data sharing. We then conduct an âinfrastructural inversionâ (Bowker & Star, 1999) to âsurface invisible workâ of making data repositories function well, and demonstrate that institutional data repositories have advantages for providing sustainable stewardship, curation, and sharing of research data. Finally, we show that institutional data repositories may produce additional benefits through established infrastructure, local interoperability, and control. Â© 2024 The Author(s).","Institutional data repositories are uniquely positioned to support researchers in sharing scholarly outputs. As funding agencies develop and institute policies for research data access and sharing, institutional data repositories have emerged as a critical feature in ecosystems for data stewardship and sharing. We show that institutional data repositories can meet and exceed the requirements and recommendations of federal data policy, thereby maximizing the benefits of data sharing. We present results of a mixed-method study which explores the adoption and usage of institutional repositories to share data from 2017 to 2023. Data from two previous studies were combined with data collected in 2023 on the data sharing solutions of Association of Research Libraries member institutions in the United States and Canada. The analysis of the aggregated data indicates that data stewardship has increased in both institutional repositories and institutional data repositories with an increase in complementary infrastructure to support data sharing. We then conduct an infrastructural inversion (Bowker & Star, 1999) to surface invisible work of making data repositories function well, and demonstrate that institutional data repositories have advantages for providing sustainable stewardship, curation, and sharing of research data. Finally, we show that institutional data repositories may produce additional benefits through established infrastructure, local interoperability, and control."
A Framework for Active DMPs in Photon and Neutron Science Large-Scale Facilities,"In this paper, a framework and a system architecture are presented to support researchers in DMP creation and execution, with a focus on the generation of FAIR data. Using the research data lifecycle within Photon and Neutron analytical facilities as a detailed exemplar of this approach in practice, it shows how combining the creation of the DMP with the project management framework PMBOK makes it easier to integrate DMP creation within the researchersâ workflow and reuse pre-existing information within the research infrastructure and related project roles. The paper identifies requirements and introduces a lifecycle for pre-existing information that helps in automatic population of the DMP. This paper also discusses a data model for the reuse of pre-existing information. It shows possible approaches to support scientists through the (semi-)automation of the creation, execution, and use of a DMP and knowledge transfer. The approach is based on work within the PaNData ODI, ExPaNDS, and PaNOSC projects. Â© 2024 The Author(s).","In this paper, a framework and a system architecture are presented to support researchers in DMP creation and execution, with a focus on the generation of FAIR data. Using the research data lifecycle within Photon and Neutron analytical facilities as a detailed exemplar of this approach in practice, it shows how combining the creation of the DMP with the project management framework PMBOK makes it easier to integrate DMP creation within the researchers workflow and reuse pre-existing information within the research infrastructure and related project roles. The paper identifies requirements and introduces a lifecycle for pre-existing information that helps in automatic population of the DMP. This paper also discusses a data model for the reuse of pre-existing information. It shows possible approaches to support scientists through the (semi-)automation of the creation, execution, and use of a DMP and knowledge transfer. The approach is based on work within the PaNData ODI, ExPaNDS, and PaNOSC projects."
"Development of the NIST X-ray Photoelectron Spectroscopy (XPS) Database, Version 5","It has been over 20 years since the National Institute of Standards and Technology (NIST) launched the first web version of its X-ray Photoelectron Spectroscopy (XPS) database (Lee et al., 2002; Wagner, 1991) which has approximately 1000 active users every day. This database was recently redesigned to meet NIST security requirements and to include new features and enhancements. The new application is built upon a relational database using a cross-platform and open-source framework (Henning et al., 2002). In addition to a modern design interface, the new features include custom-built components for displaying formatted molecular formulas and spectral lines, for sorting spectral lines, and for graphical display of chemical shifts of binding energies, Auger-electron kinetic energies, and Auger parameters for elements in different compounds. Â© 2024 The Author(s).","It has been over 20 years since the National Institute of Standards and Technology (NIST) launched the first web version of its X-ray Photoelectron Spectroscopy (XPS) database (Lee et al., 2002; Wagner, 1991) which has approximately 1000 active users every day. This database was recently redesigned to meet NIST security requirements and to include new features and enhancements. The new application is built upon a relational database using a cross-platform and open-source framework (Henning et al., 2002). In addition to a modern design interface, the new features include custom-built components for displaying formatted molecular formulas and spectral lines, for sorting spectral lines, and for graphical display of chemical shifts of binding energies, Auger-electron kinetic energies, and Auger parameters for elements in different compounds."
Implementing Informatics Tools with Data Management Plans for Disease Area Research,"Data Management Plans (DMPs) are essential to a research data life cycle. The DMPs should be developed as part of the research programs to be effective. For disease area research, integrating research community-recommended data standards during collection can enhance the likelihood of data reuse. Informatics tools are required as part of DMPs with the aim of data being findable, accessible, interoperable, and reusable. The US National Institutes of Health supports various disease area research programs and has recently finalized the Data Management and Sharing Policy. The policy highlights the importance of sharing data and metadata, including information on various elements such as data types, standards, storage repositories, access, services, and tools used for a proposed research project. The present paper provides Traumatic Brain Injury (TBI) and Parkinsonâs Disease (PD) research as examples of where the elements of the policy are being supported. The software tools that have been developed for the TBI and PD plans are available through the Biomedical Research Informatics Computing System. A Protocol and Form Research Management System (ProFoRMS) facilitates researchers to manage research protocols when collecting clinical data. The ProFoRMS also supports automatic validation with the data dictionaries for TBI and Parkinsonâs disease. Detailed information on the functionality of the software tools used for preserving data within TBI and PD repositories is openly available on their respective websites. Â© 2023 The Author(s).","Data Management Plans (DMPs) are essential to a research data life cycle. The DMPs should be developed as part of the research programs to be effective. For disease area research, integrating research community-recommended data standards during collection can enhance the likelihood of data reuse. Informatics tools are required as part of DMPs with the aim of data being findable, accessible, interoperable, and reusable. The US National Institutes of Health supports various disease area research programs and has recently finalized the Data Management and Sharing Policy. The policy highlights the importance of sharing data and metadata, including information on various elements such as data types, standards, storage repositories, access, services, and tools used for a proposed research project. The present paper provides Traumatic Brain Injury (TBI) and Parkinsons Disease (PD) research as examples of where the elements of the policy are being supported. The software tools that have been developed for the TBI and PD plans are available through the Biomedical Research Informatics Computing System. A Protocol and Form Research Management System (ProFoRMS) facilitates researchers to manage research protocols when collecting clinical data. The ProFoRMS also supports automatic validation with the data dictionaries for TBI and Parkinsons disease. Detailed information on the functionality of the software tools used for preserving data within TBI and PD repositories is openly available on their respective websites."
The Nansen Legacy Template Generator for Darwin Core and CF-NetCDF,"The scientific community is growing increasingly aware of the importance of the FAIR data management principles for publishing scientific data. For data providers, this includes creating machine-understandable file structures such as CF-NetCDF files and Darwin Core Archives. To support scientists in creating these, we present significant upgrades to the Nansen Legacy template generator, an open-source tool for generating standardised spreadsheet templates. The new version of the template generator includes an intuitive graphical interface that encourages users to select data and metadata terms from well-established controlled vocabularies like the CF standard names, Darwin Core terms and the Attribute Convention for Data Discovery. The spreadsheet templates created include descriptions for each term and cell restrictions to prevent users from entering invalid values. While the templates generated are not inherently compliant with the FAIR principles, this paper provides suggestions for how users can transform them into FAIR-compliant data structures. The Nansen Legacy template generator helps scientists follow the FAIR data principles, facilitating transparent and reproducible sharing of scientific data. The template generator is hosted at https://www.nordatanet.no/aen/template-generator. Â© 2024 The Author(s).","The scientific community is growing increasingly aware of the importance of the FAIR data management principles for publishing scientific data. For data providers, this includes creating machine-understandable file structures such as CF-NetCDF files and Darwin Core Archives. To support scientists in creating these, we present significant upgrades to the Nansen Legacy template generator, an open-source tool for generating standardised spreadsheet templates. The new version of the template generator includes an intuitive graphical interface that encourages users to select data and metadata terms from well-established controlled vocabularies like the CF standard names, Darwin Core terms and the Attribute Convention for Data Discovery. The spreadsheet templates created include descriptions for each term and cell restrictions to prevent users from entering invalid values. While the templates generated are not inherently compliant with the FAIR principles, this paper provides suggestions for how users can transform them into FAIR-compliant data structures. The Nansen Legacy template generator helps scientists follow the FAIR data principles, facilitating transparent and reproducible sharing of scientific data. The template generator is hosted at https://www.nordatanet.no/aen/template-generator."
Data Management for PalMod-II â A FAIR-Based Strategy for Data Handling in Large Climate Modeling Projects,"PalMod-II was a multi-institutional research project in Germany focusing on enabling and performing global numerical climate simulations with state-of-theart coupled Earth System Models spanning a full glacial cycle from 130 000 years in the past to the present and beyond. The main project goal was the dataset resulting from these simulations and making it available for reuse by the climate science community in-line with the FAIR data principles. In this paper, we present the research data management (RDM) approach developed and employed in PalMod-II to progress towards that project goal. The RDM approach was implemented by RDM professionals specifically funded by PalMod-II, which made it possible to provide RDM services tailored specifically to the project needs. The compilation and maintenance of a project-wide data management plan (DMP) has proven essential for keeping the project on track and serving as a central focal point of any data-related aspects. These include the specification of data responsible scientists, allocation of storage and computaional resources on a high-performance computing system, documentation of simulation output requirements, definition of data standardisation, and publication workflows in-line with the FAIR data principles. Since the RDM approach executed in PalMod-II was first-of-its-kind for all project partners, exhaustive communication at par with the scientists was required to create trust and a collaborative atmosphere within the project. Finally, the RDM approach implemented in PalMod-II facilitated the publication of a flagship dataset for global reuse, and will also be implemented in the follow-up project: PalMod-III. Â© 2023, Ubiquity Press. All rights reserved.","PalMod-II was a multi-institutional research project in Germany focusing on enabling and performing global numerical climate simulations with state-of-theart coupled Earth System Models spanning a full glacial cycle from 130 000 years in the past to the present and beyond. The main project goal was the dataset resulting from these simulations and making it available for reuse by the climate science community in-line with the FAIR data principles. In this paper, we present the research data management (RDM) approach developed and employed in PalMod-II to progress towards that project goal. The RDM approach was implemented by RDM professionals specifically funded by PalMod-II, which made it possible to provide RDM services tailored specifically to the project needs. The compilation and maintenance of a project-wide data management plan (DMP) has proven essential for keeping the project on track and serving as a central focal point of any data-related aspects. These include the specification of data responsible scientists, allocation of storage and computaional resources on a high-performance computing system, documentation of simulation output requirements, definition of data standardisation, and publication workflows in-line with the FAIR data principles. Since the RDM approach executed in PalMod-II was first-of-its-kind for all project partners, exhaustive communication at par with the scientists was required to create trust and a collaborative atmosphere within the project. Finally, the RDM approach implemented in PalMod-II facilitated the publication of a flagship dataset for global reuse, and will also be implemented in the follow-up project: PalMod-"
The Optimization of n-Gram Feature Extraction Based on Term Occurrence for Cyberbullying Classification,"Cyberbullied communications should be bundled since online harassment is growing and has serious implications. High cyberbullying requires strong text classification algorithms to safeguard persons and communities. The n-Gram models language by collecting ânâ components, usually words or characters, from a text and detecting how words relate and if major items or sentences are cyberbullying document types. The research improves term value generation and text classification accuracy by extracting features using TF-IDF and n-Gram. The optimum TF-IDF feature extraction pattern demonstrated the usefulness of n-Gram in cyberbullying document classification. This field demands good categorization and feature extraction. Because cyberbullying takes numerous forms and venues, broad classification is essential. To test unigram, bigram, and trigram approaches across text lengths and frequencies, this study uses several parameter values. The research also shows the limitations and gaps in earlier methods and underscores the necessity for various n-Gram parameter values to overcome cyberbullying text complexity. Short-sentence articles, fluctuating data frequencies, and dynamic online interactions necessitate complex solutions. Ideal n-Gram patterns increase cyberbullying text categorization and give context to the field. This research acknowledges cyberbullyingâs prevalence and effects, the necessity for effective categorization methods, and current techniquesâ limitations, opening the way for more comprehensive and adaptive online harassment combating strategies. Â© 2024 The Author(s).","Cyberbullied communications should be bundled since online harassment is growing and has serious implications. High cyberbullying requires strong text classification algorithms to safeguard persons and communities. The n-Gram models language by collecting n components, usually words or characters, from a text and detecting how words relate and if major items or sentences are cyberbullying document types. The research improves term value generation and text classification accuracy by extracting features using TF-IDF and n-Gram. The optimum TF-IDF feature extraction pattern demonstrated the usefulness of n-Gram in cyberbullying document classification. This field demands good categorization and feature extraction. Because cyberbullying takes numerous forms and venues, broad classification is essential. To test unigram, bigram, and trigram approaches across text lengths and frequencies, this study uses several parameter values. The research also shows the limitations and gaps in earlier methods and underscores the necessity for various n-Gram parameter values to overcome cyberbullying text complexity. Short-sentence articles, fluctuating data frequencies, and dynamic online interactions necessitate complex solutions. Ideal n-Gram patterns increase cyberbullying text categorization and give context to the field. This research acknowledges cyberbullyings prevalence and effects, the necessity for effective categorization methods, and current techniques limitations, opening the way for more comprehensive and adaptive online harassment combating strategies."
The Value of a Data and Digital Object Management Plan (D(DO)MP) in Fostering Sharing Practices in a Multidisciplinary Multinational Project,"Data Management Plans (DMP) are now a routine part of research proposals but are generally not referred to after funding is granted. The Belmont Forum requires an extensive document, a âData and Digital Object Management Planâ (D(DO)MP) for its awarded projects that is expected to be kept current over the life of the project. The D(DO)MP is intended to record team decisions about major tools and practices to be used over the life of the project for data and software stewardship, and for preservation of data and software products, aligned with the desired Open Science outcomes relevant to the project. Here we present one of the first instances of the use of Belmontâs D(DO)MP through a case study of the PARSEC project, a multinational and multidisciplinary investigation of the socioeconomic impacts of protected areas. We describe the development and revision of our interpretation of the D(DO)MP and discuss its adoption and acceptance by our research group. We periodically assessed the data management sophistication of team members and their use of the various nominated tools and practices. As a result, for example, we included summaries to enable the key components of the D(DO)MP to be readily viewed by the researcher. To meet the Open Science outcomes in a complex project like PARSEC, a comprehensive and appropriately structured D(DO)MP helps project leaders (a) ensure that team members are committed to the collaboration goals of the project, (b) that there is regular and effective feedback within the team, (c) training in new tools is provided as and when needed, and (d) there is easy access to a short reference to the tools and descriptions of the nominated practices. Â© 2023 The Author(s).","Data Management Plans (DMP) are now a routine part of research proposals but are generally not referred to after funding is granted. The Belmont Forum requires an extensive document, a Data and Digital Object Management Plan (D(DO)MP) for its awarded projects that is expected to be kept current over the life of the project. The D(DO)MP is intended to record team decisions about major tools and practices to be used over the life of the project for data and software stewardship, and for preservation of data and software products, aligned with the desired Open Science outcomes relevant to the project. Here we present one of the first instances of the use of Belmonts D(DO)MP through a case study of the PARSEC project, a multinational and multidisciplinary investigation of the socioeconomic impacts of protected areas. We describe the development and revision of our interpretation of the D(DO)MP and discuss its adoption and acceptance by our research group. We periodically assessed the data management sophistication of team members and their use of the various nominated tools and practices. As a result, for example, we included summaries to enable the key components of the D(DO)MP to be readily viewed by the researcher. To meet the Open Science outcomes in a complex project like PARSEC, a comprehensive and appropriately structured D(DO)MP helps project leaders (a) ensure that team members are committed to the collaboration goals of the project, (b) that there is regular and effective feedback within the team, training in new tools is provided as and when needed, and there is easy access to a short reference to the tools and descriptions of the nominated practices."
"Correction: Social Media Impact on the âCosmosâ Blockchain Ecosystem: State and Prospect (Data Science Journal, 23(1): 8. https://doi.org/10.5334/dsj-2024-008)","Pavlyshyn et al. (2024) excluded Sergey Vasylchuk who was intended to be listed as a co-author at the time of publication. This error occurred when the article was being formatted by one member of the authorial team. All authors acknowledge missing this detail during proofing, as the content and materials of the article and the accuracy of the calculations were prioritized. Sergey Vasylchuk was a permanent member of the working group, contributing to the collection of research data, the sampling the main parameters, the introduction of the idea of correlations and tracking dependencies between publications on social networks, and changing the distribution of stake in the Cosmos blockchain. He contributed to weekly meetings with all authors for over a year after which the results were composed for publication. Sergeyâs credentials and metadata have been included as a co-author in this correction. Â© 2024 The Author(s).","Pavlyshyn et al. excluded Sergey Vasylchuk who was intended to be listed as a co-author at the time of publication. This error occurred when the article was being formatted by one member of the authorial team. All authors acknowledge missing this detail during proofing, as the content and materials of the article and the accuracy of the calculations were prioritized. Sergey Vasylchuk was a permanent member of the working group, contributing to the collection of research data, the sampling the main parameters, the introduction of the idea of correlations and tracking dependencies between publications on social networks, and changing the distribution of stake in the Cosmos blockchain. He contributed to weekly meetings with all authors for over a year after which the results were composed for publication. Sergeys credentials and metadata have been included as a co-author in this correction."
"Improving NASAâs Earth Satellite and Model Data Discoverability for Interdisciplinary Research, Applications, and Education","Since the Internet era began, numerous earth science data services have been developed to facilitate data discovery (e.g., data sources, documents, facts, visualization, opinions) and data access for research and application activities. For example, a large collection of NASAâs earth science data has been made searchable and freely downloadable over the Internet. Some value-added services even allow users to analyze and visualize many variables online (e.g., 2,000+ in NASA Giovanni) without downloading data and software. However, finding and discovering suitable datasets and information for interdisciplinary research (involving two or more scientific disciplines), applications, education, and other emerging activities (e.g., water, food, energy nexus) has been a challenge not only for users, especially for those who are unfamiliar with scientific disciplines, measurements, or models, but also for data producers and service developers who want their data to be discoverable. NASA earth science data are currently archived and distributed at twelve NASA discipline-oriented Distributed Active Archive Centers (DAACs). Even though most datasets are online, search results often contain many similar datasets with limited information for self-guided or heuristic dataset discovery, so some users simply send an inquiry to DAAC support staff for advice. Conducting interdisciplinary research often requires multiple datasets from different data repositories, which can make it even harder to find and discover suitable datasets without additional data services to accommodate these emerging user needs. In this article, we assess current data discovery practices and publications (e.g., reports from working groups) to identify challenges and make actionable recommendations for improving earth science data discoverability and facilitating interdisciplinary activities. Highlights: â¢ Status review of earth science data discoverability for interdisciplinary research and applications at NASA GES DISC. â¢ Review of data discovery research and working group activity. â¢ Discuss challenges and opportunities for interdisciplinary data discovery with recommendations for practitioners. Â© 2023 The Author(s).","Since the Internet era began, numerous earth science data services have been developed to facilitate data discovery (, data sources, documents, facts, visualization, opinions) and data access for research and application activities. For example, a large collection of NASAs earth science data has been made searchable and freely downloadable over the Internet. Some value-added services even allow users to analyze and visualize many variables online (, 2,000+ in NASA Giovanni) without downloading data and software. However, finding and discovering suitable datasets and information for interdisciplinary research (involving two or more scientific disciplines), applications, education, and other emerging activities (, water, food, energy nexus) has been a challenge not only for users, especially for those who are unfamiliar with scientific disciplines, measurements, or models, but also for data producers and service developers who want their data to be discoverable. NASA earth science data are currently archived and distributed at twelve NASA discipline-oriented Distributed Active Archive Centers (DAACs). Even though most datasets are online, search results often contain many similar datasets with limited information for self-guided or heuristic dataset discovery, so some users simply send an inquiry to DAAC support staff for advice. Conducting interdisciplinary research often requires multiple datasets from different data repositories, which can make it even harder to find and discover suitable datasets without additional data services to accommodate these emerging user needs. In this article, we assess current data discovery practices and publications (, reports from working groups) to identify challenges and make actionable recommendations for improving earth science data discoverability and facilitating interdisciplinary activities. Highlights: Status review of earth science data discoverability for interdisciplinary research and applications at NASA GES DISC. Review of data discovery research and working group activity. Discuss challenges and opportunities for interdisciplinary data discovery with recommendations for practitioners."
Supporting Data Discovery: Comparing Perspectives of Support Specialists and Researchers,"Purpose: Much of the research in data discovery is centered on the usersâ viewpoint, frequently overlooking the perspective of those who develop and maintain the discovery infrastructure. Our goal is to conduct a comparative study on research data discovery, examining both support specialistsâ and researchersâ views by merging new analysis with prior research insights. Methods: This work summarizes the studies the authors have conducted over the last seven years investigating the data discovery practices of support specialists from different disciplines. Although support specialists were not the main target of some of these studies, data about their perspectives was collected. Our corpus comprises in-depth interviews with 6 social science support specialists, interviews with 19 researchers and 3 support specialists from multiple disciplines, a global survey with 1630 researchers and 47 support specialists, and a use case analysis of 25 support specialists. In the analysis section, we juxtapose the fresh insights on support specialistsâ views with the already documented perspectives of researchers for a holistic understanding. The latter is primarily discussed in the literature review, with references made in the analysis section to draw comparisons. Results: We found that support specialistsâ views on data discovery are not entirely different from those of the researchers. There are, however, some differences that we have identified, most notably the interconnection of data discovery with general web search, literature search, and social networks. Conclusion: We conclude by proposing recommendations for different types of support work to better support researchersâ data discovery practices. Â© 2024 The Author(s).","Much of the research in data discovery is centered on the users viewpoint, frequently overlooking the perspective of those who develop and maintain the discovery infrastructure. Our goal is to conduct a comparative study on research data discovery, examining both support specialists and researchers views by merging new analysis with prior research insights. Methods: This work summarizes the studies the authors have conducted over the last seven years investigating the data discovery practices of support specialists from different disciplines. Although support specialists were not the main target of some of these studies, data about their perspectives was collected. Our corpus comprises in-depth interviews with 6 social science support specialists, interviews with 19 researchers and 3 support specialists from multiple disciplines, a global survey with 1630 researchers and 47 support specialists, and a use case analysis of 25 support specialists. In the analysis section, we juxtapose the fresh insights on support specialists views with the already documented perspectives of researchers for a holistic understanding. The latter is primarily discussed in the literature review, with references made in the analysis section to draw comparisons. Results: We found that support specialists views on data discovery are not entirely different from those of the researchers. There are, however, some differences that we have identified, most notably the interconnection of data discovery with general web search, literature search, and social networks. Conclusion: We conclude by proposing recommendations for different types of support work to better support researchers data discovery practices."
Attending to the Cultures of Data Science Work,"This essay reflects on the shifting attention to the âsocialâ and the âculturalâ in data science communities. While recently the âsocialâ and the âculturalâ have been prioritized in data science discourse, social and cultural concerns that get raised in data science are almost always outwardly focused â applying to the communities that data scientists seek to support more so than more computationally-focused data science communities. I argue that data science communities have a responsibility to attend not only to the cultures that orient the work of domain communities, but also to the cultures that orient their own work. I describe how ethnographic frameworks such as thick description can be enlisted to encourage more reflexive data science work, and I conclude with recommendations for documenting the cultural provenance of data policy and infrastructure. Â© 2023 The Author(s).","This essay reflects on the shifting attention to the social and the cultural in data science communities. While recently the social and the cultural have been prioritized in data science discourse, social and cultural concerns that get raised in data science are almost always outwardly focused applying to the communities that data scientists seek to support more so than more computationally-focused data science communities. I argue that data science communities have a responsibility to attend not only to the cultures that orient the work of domain communities, but also to the cultures that orient their own work. I describe how ethnographic frameworks such as thick description can be enlisted to encourage more reflexive data science work, and I conclude with recommendations for documenting the cultural provenance of data policy and infrastructure."
StampâStandardized Data Management Plan for Educational Research: A Blueprint to Improve Data Management across Disciplines,"To provide more tailored, discipline-specific guidance on data management, Science Europe suggested the concept of domain data protocols. Based on this concept, the project Domain Data Protocols for Educational Research developed a first domain data protocol for educational research, titled Standardized Data Management Plan for Educational Research (Stamp). Its multi-level approach includes minimal conditions on managing data according to the FAIR Data Principles and checklists with concrete activities to reach each minimal condition; also included are auxiliary materials to support researchers in educational research in planning, implementing, and realizing different data management activities. Although we developed the Stamp for educational research, its design and flexible structure enables transferring it to other (research) domains and communities. To investigate this flexibility, we organized two workshops, discussing to what extent the Stamp can be used beyond educational research, with representatives from other social science domains as well as from research domains beyond the social sciences. In sum, there was consensus among participants of both workshops on the usability of the Stamp outside educational research, at least if the same types of data are processed and analyzed with similar methods. For other types of data, the Stamp serves as a blueprint to develop further domain data protocols, in terms of standardized data management plans, according to the specific needs of the respective domain. Â© 2024 The Author(s).","To provide more tailored, discipline-specific guidance on data management, Science Europe suggested the concept of domain data protocols. Based on this concept, the project Domain Data Protocols for Educational Research developed a first domain data protocol for educational research, titled Standardized Data Management Plan for Educational Research (Stamp). Its multi-level approach includes minimal conditions on managing data according to the FAIR Data Principles and checklists with concrete activities to reach each minimal condition; also included are auxiliary materials to support researchers in educational research in planning, implementing, and realizing different data management activities. Although we developed the Stamp for educational research, its design and flexible structure enables transferring it to other (research) domains and communities. To investigate this flexibility, we organized two workshops, discussing to what extent the Stamp can be used beyond educational research, with representatives from other social science domains as well as from research domains beyond the social sciences. In sum, there was consensus among participants of both workshops on the usability of the Stamp outside educational research, at least if the same types of data are processed and analyzed with similar methods. For other types of data, the Stamp serves as a blueprint to develop further domain data protocols, in terms of standardized data management plans, according to the specific needs of the respective domain."
The âPROTECTâ Essential Elements in Managing Crisis Data Policies,"Based on a literature review, policy study, and a conference session discussion, this paper systematically analyzed seven predominant elements in crisis data policies, namely âpeople, resources, operation, technology, ethics, communication, and trust,â abbreviated as the âPROTECTâ essentials. As a guiding checklist for crisis data policy, implementing the âPROTECTâ essentials should follow the guiding principles, in which people should be united to understand each other better and get prepared for intelligible data resources; the operation of crisis data work should be uniquely tailored to scenarios, with promising IT adoption guided by utilitarian ethics; crisis communication should be prompt and unambiguous; trust between people and machines should be sustainable. These can be summarized as the âUPsâ principles. All these efforts together contribute to a sustainable crisis data ecosystem. Selected case studies on the COVID-19 pandemic, and the February 6, 2023 TÃ¼rkiye-Syria earthquake, validate how the âPROTECTâ essential framework helps guide crisis data work. We hope the âPROTECTâ essentials and the implementation guidelines could provide insights into future crisis data policies. Â© 2024 The Author(s).","Based on a literature review, policy study, and a conference session discussion, this paper systematically analyzed seven predominant elements in crisis data policies, namely people, resources, operation, technology, ethics, communication, and trust, abbreviated as the PROTECT essentials. As a guiding checklist for crisis data policy, implementing the PROTECT essentials should follow the guiding principles, in which people should be united to understand each other better and get prepared for intelligible data resources; the operation of crisis data work should be uniquely tailored to scenarios, with promising IT adoption guided by utilitarian ethics; crisis communication should be prompt and unambiguous; trust between people and machines should be sustainable. These can be summarized as the UPs principles. All these efforts together contribute to a sustainable crisis data ecosystem. Selected case studies on the COVID-19 pandemic, and the February 6, 2023 Trkiye-Syria earthquake, validate how the PROTECT essential framework helps guide crisis data work. We hope the PROTECT essentials and the implementation guidelines could provide insights into future crisis data policies."
Data Science in a Pandemic,"Data Science has the potential to provide humanity with critical insight into the massive data being collected during a pandemic. The COVID-19 pandemic presented that opportunity, and Data Science supported an international audience promptly, reliably, effectively, and frequently during that difficult time. The most significant contributions were data visualizations and data dashboards, however, other tools, such as predictive and prescriptive analytics, were equally critical to the effort. The urgency at the start of the pandemic was to quickly communicate information to citizens, governments, and institutions. The change in modality from traditional statistical metrics and tables to data visualizations was extremely significant and helpful to so many. This paper reviews these contributions by demonstrating how the COVID-19 story unfolded through author-generated data visualizations and dashboards, and by providing the community with open-source access to the scripts that generated these visualizations. The open-source access to the (R language) scripts reflects this articleâs novelty in the literature. Using publicly available datasets from multiple sources, and employing R toolkits, the author validates the role that Data Science can play in a pandemic, and that can be implemented by anyone with some basic knowledge of scripting languages, like R. The intent is to provide these valuable tools to the community and to demonstrate their effectiveness in the likely event when there is another crisis. Â© 2023 The Author(s).","Data Science has the potential to provide humanity with critical insight into the massive data being collected during a pandemic. The COVID-19 pandemic presented that opportunity, and Data Science supported an international audience promptly, reliably, effectively, and frequently during that difficult time. The most significant contributions were data visualizations and data dashboards, however, other tools, such as predictive and prescriptive analytics, were equally critical to the effort. The urgency at the start of the pandemic was to quickly communicate information to citizens, governments, and institutions. The change in modality from traditional statistical metrics and tables to data visualizations was extremely significant and helpful to so many. This paper reviews these contributions by demonstrating how the COVID-19 story unfolded through author-generated data visualizations and dashboards, and by providing the community with open-source access to the scripts that generated these visualizations. The open-source access to the (R language) scripts reflects this articles novelty in the literature. Using publicly available datasets from multiple sources, and employing R toolkits, the author validates the role that Data Science can play in a pandemic, and that can be implemented by anyone with some basic knowledge of scripting languages, like The intent is to provide these valuable tools to the community and to demonstrate their effectiveness in the likely event when there is another crisis."
Cloud-Based Machine Learning Service for Astronomical Sub-Object Classification: Case Study On the First Byurakan Survey Spectra,"The classification of astronomical objects in the Digitized First Byurakan Survey (DFBS), comprising low-dispersion spectra for approximately twenty million objects, presents challenges regarding performance and computational resources. However, considering the distinct spectral characteristics within subgroups, sub-object classification becomes crucial for a more detailed understanding of the dataset. The article addresses these challenges by proposing a comprehensive cloud-based service for classifying objects into spectral classes and subtypes, with a focus on carbon stars, white dwarfs / subdwarfs, and Markarian (UV-excess) galaxies, which are the primary objects in DFBS. By leveraging the power of cloud computing, it effectively handles the computational requirements associated with analyzing the extensive DFBS dataset. The service employs advanced machine learning algorithms trained on labeled data to classify objects into their respective spectral types and subtypes. The service can be accessed and utilized through a user-friendly interface, making it accessible to a wide range of users in the astronomical community. Â© 2024 The Author(s).","The classification of astronomical objects in the Digitized First Byurakan Survey (DFBS), comprising low-dispersion spectra for approximately twenty million objects, presents challenges regarding performance and computational resources. However, considering the distinct spectral characteristics within subgroups, sub-object classification becomes crucial for a more detailed understanding of the dataset. The article addresses these challenges by proposing a comprehensive cloud-based service for classifying objects into spectral classes and subtypes, with a focus on carbon stars, white dwarfs / subdwarfs, and Markarian (UV-excess) galaxies, which are the primary objects in DFBS. By leveraging the power of cloud computing, it effectively handles the computational requirements associated with analyzing the extensive DFBS dataset. The service employs advanced machine learning algorithms trained on labeled data to classify objects into their respective spectral types and subtypes. The service can be accessed and utilized through a user-friendly interface, making it accessible to a wide range of users in the astronomical community."
Advancing FAIR Agricultural Data: The AgReFed FAIR Assessment Tool,"The FAIR principles provide guidance for improving the findability, accessibility, interoperability and reuse of research data and other digital objects. The Agricultural Research Federation (AgReFed) has developed a FAIR implementation consisting of policies, resources and tools to enable FAIR agricultural data in Australia. It prescribes minimum acceptable data standards and an associated set of FAIR metrics for agricultural research data. Existing FAIR assessment tools were examined and found to have limitations in serving the purposes of AgReFed. The AgReFed FAIR assessment tool addresses these needs with novel features to help improve FAIRness of datasets and other digital resources. By providing the ability to assess and subsequently re-assess datasets while also enabling users to add comments, it facilitates building work lists that support and document the improvement in FAIRness. Other innovative features include customisable FAIR metrics, storage of assessment results, a versioning system and inbuilt help resources. In addition to user-friendly reporting of AgReFed standards compliance, the integrated automated F-UJI FAIR assessment API provides a supplementary set of machine-readable FAIR scores. The AgReFed FAIR Assessment Tool has been deployed for public use and released as open source to help data custodians enact FAIR principles in domains and data communities within and beyond Australian agriculture. Â© 2024 The Author(s).","The FAIR principles provide guidance for improving the findability, accessibility, interoperability and reuse of research data and other digital objects. The Agricultural Research Federation (AgReFed) has developed a FAIR implementation consisting of policies, resources and tools to enable FAIR agricultural data in Australia. It prescribes minimum acceptable data standards and an associated set of FAIR metrics for agricultural research data. Existing FAIR assessment tools were examined and found to have limitations in serving the purposes of AgReFed. The AgReFed FAIR assessment tool addresses these needs with novel features to help improve FAIRness of datasets and other digital resources. By providing the ability to assess and subsequently re-assess datasets while also enabling users to add comments, it facilitates building work lists that support and document the improvement in FAIRness. Other innovative features include customisable FAIR metrics, storage of assessment results, a versioning system and inbuilt help resources. In addition to user-friendly reporting of AgReFed standards compliance, the integrated automated F-UJI FAIR assessment API provides a supplementary set of machine-readable FAIR scores. The AgReFed FAIR Assessment Tool has been deployed for public use and released as open source to help data custodians enact FAIR principles in domains and data communities within and beyond Australian agriculture."
Enhancing Findability and Searchability of Research Data: Metadata Conversion and Registration in Institutional Repositories,"This paper outlines our practice to enhance the findability and searchability of research data through metadata conversion from the Space Physics Archive Search and Extract (SPASE) schema to a more generic schema, the Japan Consortium for Open Access Repository (JPCOAR) schema, and registration of converted metadata in institutional repositories. Traditionally, earth and space science research data have been organized using the SPASE schema. Although the SPASE schema is comprehensive, its usage has been restricted to highly professional databases, limiting broader accessibility and impeding cross-disciplinary research. We discuss a case study at Nagoya University where 284 metadata records were converted from SPASE to JPCOAR, and illustrate the process and benefits of this conversion. This approach significantly improved the visibility and usability of metadata across various platforms like the Institutional Repositories DataBase (IRDB), the Data Catalog Cross-Search System, and Googleâs Dataset Search, extending access to a wider range of users beyond the highly professional scientific community. This approach also aligns with national policies in Japan on research data management and simplifies metadata handling for researchers. Future direction includes expanding this conversion and registration model to other universities and institutions by leveraging the ubiquity of the SPASE schema in the earth and space science fields. Our practice may be useful in other research fields. This initiative aims to improve the overall findability of research data, to foster cross-disciplinary collaboration, and to enhance the value of research data itself and of creators and managers of the data. Â© 2024 The Author(s).","This paper outlines our practice to enhance the findability and searchability of research data through metadata conversion from the Space Physics Archive Search and Extract (SPASE) schema to a more generic schema, the Japan Consortium for Open Access Repository (JPCOAR) schema, and registration of converted metadata in institutional repositories. Traditionally, earth and space science research data have been organized using the SPASE schema. Although the SPASE schema is comprehensive, its usage has been restricted to highly professional databases, limiting broader accessibility and impeding cross-disciplinary research. We discuss a case study at Nagoya University where 284 metadata records were converted from SPASE to JPCOAR, and illustrate the process and benefits of this conversion. This approach significantly improved the visibility and usability of metadata across various platforms like the Institutional Repositories DataBase (IRDB), the Data Catalog Cross-Search System, and Googles Dataset Search, extending access to a wider range of users beyond the highly professional scientific community. This approach also aligns with national policies in Japan on research data management and simplifies metadata handling for researchers. Future direction includes expanding this conversion and registration model to other universities and institutions by leveraging the ubiquity of the SPASE schema in the earth and space science fields. Our practice may be useful in other research fields. This initiative aims to improve the overall findability of research data, to foster cross-disciplinary collaboration, and to enhance the value of research data itself and of creators and managers of the data."
"Identifying Inconsistencies in Data Quality Between FAOSTAT, WOAH, UN Agriculture Census, and National Data","With the growth of AI and data modelling, the old saying by George Fuechsel regarding data quality âGarbage in, garbage outâ holds more truth than ever. Data Scientists are learning the quality of their models depends on the quality of data. Data used by the Global Burden of Animal Diseases (GBADs) is available to modellers around the world, and the quality of the data provided is important as it is used in modelling disease, greenhouse gas emissions, and more. These are important topics, so the data given to the modellers must be investigated and checked for internal and external inconsistencies. The goal of this paper is to investigate data provided by GBADs to find inconsistencies in the data. Data quality was analysed using a five-year trailing average comparison, the interquartile range for the yearly rates of change, and observing outliers on a normal distribution for the yearly rates of change for livestock populations over time. The normal distribution and interquartile range analysis is an internal data analysis that can find outliers that indicate possible data inconsistencies. The five-year trailing average helps identify external data inconsistencies between sources. Using purpose-built data analysis tools and performing analysis on the data shows there are inconsistencies in the data. The consequences of these findings show that researchers need to be cognisant of the data they are using and need to perform their own analysis before they use it in their models as the data can show incorrect results. Â© 2024 The Author(s).","With the growth of AI and data modelling, the old saying by George Fuechsel regarding data quality Garbage in, garbage out holds more truth than ever. Data Scientists are learning the quality of their models depends on the quality of data. Data used by the Global Burden of Animal Diseases (GBADs) is available to modellers around the world, and the quality of the data provided is important as it is used in modelling disease, greenhouse gas emissions, and more. These are important topics, so the data given to the modellers must be investigated and checked for internal and external inconsistencies. The goal of this paper is to investigate data provided by GBADs to find inconsistencies in the data. Data quality was analysed using a five-year trailing average comparison, the interquartile range for the yearly rates of change, and observing outliers on a normal distribution for the yearly rates of change for livestock populations over time. The normal distribution and interquartile range analysis is an internal data analysis that can find outliers that indicate possible data inconsistencies. The five-year trailing average helps identify external data inconsistencies between sources. Using purpose-built data analysis tools and performing analysis on the data shows there are inconsistencies in the data. The consequences of these findings show that researchers need to be cognisant of the data they are using and need to perform their own analysis before they use it in their models as the data can show incorrect results."
Conversion of XYZ Tile Data into Grid Square Data and Their Application to Tsunami Risk Assessment,"This paper proposes a novel algorithm for converting XYZ Tiles (Slippy Maps) to World Grid Square tile for a given Z value by merging several XYZ Tiles and cutting them into several pieces of World Grid Square tiles. We propose a framework to analyze XYZ Tiles with World Grid Square statistics beyond different scientific disciplines. In other words, we can perform statistical analysis on cartographic images. This paper describes the conversion of XYZ-based map tiles into World Grid Square tiles. We will examine the size of the actual World Grid Square area and the World Grid Square tile made from XYZ Tiles. As an example of analysis, we calculate the number of people affected by the tsunami by combining Grid Square statistics created from the XYZ Tiles published using the Web API with existing Grid Square statistics. Finally, as a case study, we will calculate the number of people affected by the tsunami by combining the World Grid Square tile converted from the XYZ Tiles published using the API with existing World Grid Square statistics. Â© 2024 The Author(s).","This paper proposes a novel algorithm for converting XYZ Tiles (Slippy Maps) to World Grid Square tile for a given Z value by merging several XYZ Tiles and cutting them into several pieces of World Grid Square tiles. We propose a framework to analyze XYZ Tiles with World Grid Square statistics beyond different scientific disciplines. In other words, we can perform statistical analysis on cartographic images. This paper describes the conversion of XYZ-based map tiles into World Grid Square tiles. We will examine the size of the actual World Grid Square area and the World Grid Square tile made from XYZ Tiles. As an example of analysis, we calculate the number of people affected by the tsunami by combining Grid Square statistics created from the XYZ Tiles published using the Web API with existing Grid Square statistics. Finally, as a case study, we will calculate the number of people affected by the tsunami by combining the World Grid Square tile converted from the XYZ Tiles published using the API with existing World Grid Square statistics."
Thoughts on Starting the CODATA Data Science Journal,"This essay discusses some of the considerations that led to the founding of the [CODATA] Data Science Journal. Three factors were most relevant to the founding. First, there was a need to have a more formal publication mechanism for the papers given at the biennial CODATA International Conferences. Second, there was a pressing need for data science advancements made in one area of scientific data work to be shared with other scientific disciplines. Lastly the increasing number of scientists interested in data, throughout science, and throughout the world, required a more convenient publication outlet. Thus arose the Data Science Journal. Â© 2023 The Author(s).","This essay discusses some of the considerations that led to the founding of the [CODATA] Data Science Journal. Three factors were most relevant to the founding. First, there was a need to have a more formal publication mechanism for the papers given at the biennial CODATA International Conferences. Second, there was a pressing need for data science advancements made in one area of scientific data work to be shared with other scientific disciplines. Lastly the increasing number of scientists interested in data, throughout science, and throughout the world, required a more convenient publication outlet. Thus arose the Data Science Journal."
A Data-Driven Approach to Monitor and Improve Open and FAIR Research Data in a Federated Research Ecosystem,"In this contribution we present a data-driven approach to monitoring and assessing the state of open and FAIR data in an interdisciplinary, federated research ecosystem. The project is part of a multi-method approach by the Helmholtz Metadata Collaboration (HMC) to monitor and assess the state of open and FAIR data practices in the Helmholtz Association of German research centers, Germanyâs largest non-university research organization. The approach consists of two parts: a modular data harvesting and assessment pipeline, and an openly accessible dashboard with interactive statistics about the data publications identified with the pipeline. The dashboard provides insight into which data repositories research communities use to publish research data and it allows for assessing systematic gaps of this data with respect to the FAIR data guidelines. We illustrate how the approach can be used to engage communities in FAIR data practices and to counsel data infrastructure towards improving FAIR data across a federated research organization. All software and data discussed here are published under an open license and reusable by data professionals at other research performing organizations. Â© 2024 The Author(s).","In this contribution we present a data-driven approach to monitoring and assessing the state of open and FAIR data in an interdisciplinary, federated research ecosystem. The project is part of a multi-method approach by the Helmholtz Metadata Collaboration (HMC) to monitor and assess the state of open and FAIR data practices in the Helmholtz Association of German research centers, Germanys largest non-university research organization. The approach consists of two parts: a modular data harvesting and assessment pipeline, and an openly accessible dashboard with interactive statistics about the data publications identified with the pipeline. The dashboard provides insight into which data repositories research communities use to publish research data and it allows for assessing systematic gaps of this data with respect to the FAIR data guidelines. We illustrate how the approach can be used to engage communities in FAIR data practices and to counsel data infrastructure towards improving FAIR data across a federated research organization. All software and data discussed here are published under an open license and reusable by data professionals at other research performing organizations."
Social Media Impact on the âCosmosâ Blockchain Ecosystem: State and Prospect,"The proliferation of blockchain technology heralds transformative impacts across various sectors, offering decentralization, transparency, and enhanced security. This paper explores the unique case of Cosmos, a scalable blockchain ecosystem designed to address the challenges of isolation and interoperability among existing blockchains. With its implementation of Tendermint consensus and the Inter-Blockchain Communication protocol, Cosmos stands out in facilitating seamless cross-blockchain interactions. The ATOM token serves a dual role as the networkâs currency and a governance tool, empowering stakeholders in decision-making processes. Significantly, this study investigates the intricate relationship between Cosmos and social media platforms, examining how online sentiment influences voting on governance proposals, with a detailed analysis of two specific proposals. Furthermore, the paper delves into Cosmosâ integral role in the burgeoning Decentralized Finance sector, underscoring how its modular architecture fosters financial innovation. In the broader context, there are numerous PoS (Proof of Stake) networks. Cosmos, one of the foundational and longstanding projects, exemplifies a classic blockchain economic model, making it an ideal subject for this analysis. Finally, the paper assesses Cosmosâ contribution to the overarching Web3 vision, asserting its significance as a foundational element for a decentralized, user-oriented digital framework. Our findings illuminate Cosmosâ multifaceted impact, from technological innovation to reshaping societal structures, reaffirming blockchainâs potential in redefining modern paradigms. Â© 2024 The Author(s).","The proliferation of blockchain technology heralds transformative impacts across various sectors, offering decentralization, transparency, and enhanced security. This paper explores the unique case of Cosmos, a scalable blockchain ecosystem designed to address the challenges of isolation and interoperability among existing blockchains. With its implementation of Tendermint consensus and the Inter-Blockchain Communication protocol, Cosmos stands out in facilitating seamless cross-blockchain interactions. The ATOM token serves a dual role as the networks currency and a governance tool, empowering stakeholders in decision-making processes. Significantly, this study investigates the intricate relationship between Cosmos and social media platforms, examining how online sentiment influences voting on governance proposals, with a detailed analysis of two specific proposals. Furthermore, the paper delves into Cosmos integral role in the burgeoning Decentralized Finance sector, underscoring how its modular architecture fosters financial innovation. In the broader context, there are numerous PoS (Proof of Stake) networks. Cosmos, one of the foundational and longstanding projects, exemplifies a classic blockchain economic model, making it an ideal subject for this analysis. Finally, the paper assesses Cosmos contribution to the overarching Web3 vision, asserting its significance as a foundational element for a decentralized, user-oriented digital framework. Our findings illuminate Cosmos multifaceted impact, from technological innovation to reshaping societal structures, reaffirming blockchains potential in redefining modern paradigms."
Looking Back to the Future: A Glimpse at Twenty Years of Data Science,"This paper carries out a lightweight review to explore the potentials of data science in the last two decades and especially focuses on the four essential components: data resources, technologies, data infrastructures, and data education. Considering the barriers of data science, the analysis has been mapped into four essential components, highlighting priorities and challenges in social and cultural, epistemological, scientific and technical, economic, legal, and ethical aspects. As a result, the future development of data science tends to shift toward datafication, data technicity, infrastructuralism, and data literacy empowerment. The data ecosystem, at the macro level, has also been analyzed under the open science umbrella, providing a snapshot for the future development of data science. Â© 2023 The Author(s).","This paper carries out a lightweight review to explore the potentials of data science in the last two decades and especially focuses on the four essential components: data resources, technologies, data infrastructures, and data education. Considering the barriers of data science, the analysis has been mapped into four essential components, highlighting priorities and challenges in social and cultural, epistemological, scientific and technical, economic, legal, and ethical aspects. As a result, the future development of data science tends to shift toward datafication, data technicity, infrastructuralism, and data literacy empowerment. The data ecosystem, at the macro level, has also been analyzed under the open science umbrella, providing a snapshot for the future development of data science."
Polar data forum IV â an ocean of opportunities,"This paper reports on the Hackathon Sessions organised at the Polar Data Forum IV (PDF IV) (20â24 September 2021), during which 351 participants from 50 different countries discussed collaboratively about the latest developments in polar data management. The 4th edition of the PDF hosted lively discussions on (i) best practices for polar data management, (ii) data policy, (ii) documenting data flows into aggregators, (iv) data interoperability, (v) polar federated search, (vi) semantics and vocabularies, (vii) Virtual Research Environments (VREs), and (viii) new polar technologies. This paper provides an overview of the organisational aspects of PDF IV and summarises the polar data objectives and outcomes by describing the conclusions drawn from the Hackathon Sessions. Â© 2023 The Author(s).","This paper reports on the Hackathon Sessions organised at the Polar Data Forum IV (PDF IV) (2024 September 2021), during which 351 participants from 50 different countries discussed collaboratively about the latest developments in polar data management. The 4th edition of the PDF hosted lively discussions on best practices for polar data management, data policy, documenting data flows into aggregators, data interoperability, polar federated search, semantics and vocabularies, Virtual Research Environments (VREs), and new polar technologies. This paper provides an overview of the organisational aspects of PDF IV and summarises the polar data objectives and outcomes by describing the conclusions drawn from the Hackathon Sessions."
"Data Management in Distributed, Federated Research Infrastructures: The Case of EPOS","Data management is a key activity when Open Data stewardship through services complying with the FAIR principles is required, as it happens in many National and European initiatives. Existing guidelines and tools facilitate the drafting of Data Management Plans by focusing on a set of common parameters or questions. In this paper we describe how data management is carried out in EPOS, the European Research Infrastructure for providing access to integrated data and services in the solid Earth domain. EPOS relies on a federated model and is committed to remain operational in the long term. In EPOS, five key dimensions were identified for the Federated Data Management, namely the management of: thematic data; e-infrastructure for data integration; community of data providers committed to data provision processes; sustainability; and policies. On the basis of the EPOS experience, which is to some extent applicable to other research infrastructures, we propose additional components that may extend the EU Horizon 2020 Data Management Guidelines template, thus comprehensively addressing the Federated Data Management in the context of distributed Research Infrastructures. Â© 2024 The Author(s).","Data management is a key activity when Open Data stewardship through services complying with the FAIR principles is required, as it happens in many National and European initiatives. Existing guidelines and tools facilitate the drafting of Data Management Plans by focusing on a set of common parameters or questions. In this paper we describe how data management is carried out in EPOS, the European Research Infrastructure for providing access to integrated data and services in the solid Earth domain. EPOS relies on a federated model and is committed to remain operational in the long term. In EPOS, five key dimensions were identified for the Federated Data Management, namely the management of: thematic data; e-infrastructure for data integration; community of data providers committed to data provision processes; sustainability; and policies. On the basis of the EPOS experience, which is to some extent applicable to other research infrastructures, we propose additional components that may extend the EU Horizon 2020 Data Management Guidelines template, thus comprehensively addressing the Federated Data Management in the context of distributed Research Infrastructures."
A Programmatic and Scalable Approach to Making Data Management Machine-Actionable,"The challenge of tracking research productivity and impact is compounded by the fragmented ecosystem in which research data outputs are managed, with stakeholders such as funders, research centers, government agencies, and academic institutions struggling to ensure compliance with mandates for data sharing and other regulatory requirements. While data management plans (DMPs) are crucial for effective research data management (RDM), their inadequate integration exacerbates fragmentation. This paper presents a robust, scalable, persistent identifier (PID)âenabled infrastructure that interlinks metadata and updates DOI records, effectively incorporating DMPs to bolster data discoverability, reusability, and compliance. Â© 2023 The Author(s).","The challenge of tracking research productivity and impact is compounded by the fragmented ecosystem in which research data outputs are managed, with stakeholders such as funders, research centers, government agencies, and academic institutions struggling to ensure compliance with mandates for data sharing and other regulatory requirements. While data management plans (DMPs) are crucial for effective research data management (RDM), their inadequate integration exacerbates fragmentation. This paper presents a robust, scalable, persistent identifier (PID)enabled infrastructure that interlinks metadata and updates DOI records, effectively incorporating DMPs to bolster data discoverability, reusability, and compliance."
Harvestable Metadata Services Development: Analysis of Use Cases from the World Data System,"Minimally, a research data repository exists to make a collection of data assets available to potential users. If a dataset cannot be discovered and found, it cannot be reused (Garnett et al. 2017). Harvestable metadata catalogues are a key strategy for achieving greater global findability of data assets, as they create a surveyable access point to discover data products within large data collections. Such catalogues can be especially effective if they are tailored for interoperability with feature-rich infrastructures (e.g. meta-catalogues, see Kapiszewski & Karcher 2020; CRFCB 2014) that are highly visible and widely used, and also themselves integrated within the larger ecosystem of research infrastructures. This study offers insight into a set of World Data System (WDS) research data repositories ongoing and successful implementations of harvestable metadata services, which apply established and emerging research data standards and practices to fit global, local and domain-specific interoperability contexts. Establishing a harvestable metadata service involves making choices in a space where standards and technologies are continuously evolving. The repositories in this study leverage the resources they have, within the policy and funding constraints of their institution, to serve the changing needs of heterogeneous user groups. This document encapsulates and completes the work that was carried out by the WDS International Technology Office (ITO) Harvestable Metadata Services Working Group (HMetS-WG). Â© 2023 The Author(s).","Minimally, a research data repository exists to make a collection of data assets available to potential users. If a dataset cannot be discovered and found, it cannot be reused (Garnett et al. 2017). Harvestable metadata catalogues are a key strategy for achieving greater global findability of data assets, as they create a surveyable access point to discover data products within large data collections. Such catalogues can be especially effective if they are tailored for interoperability with feature-rich infrastructures ( meta-catalogues, see Kapiszewski & Karcher 2020; CRFCB 2014) that are highly visible and widely used, and also themselves integrated within the larger ecosystem of research infrastructures. This study offers insight into a set of World Data System (WDS) research data repositories ongoing and successful implementations of harvestable metadata services, which apply established and emerging research data standards and practices to fit global, local and domain-specific interoperability contexts. Establishing a harvestable metadata service involves making choices in a space where standards and technologies are continuously evolving. The repositories in this study leverage the resources they have, within the policy and funding constraints of their institution, to serve the changing needs of heterogeneous user groups. This document encapsulates and completes the work that was carried out by the WDS International Technology Office (ITO) Harvestable Metadata Services Working Group (HMetS-WG)."
Correction to: 39 Hints to Facilitate the Use of Semantics for Data on Agriculture and Nutrition (Data Science Journal),"This article details a correction to the article: Caracciolo, C., Aubin, S., Jonquet, C., Amdouni, E., David, R., Garcia, L., Whitehead, B., Roussey, C., Stellato, A. and Villa, F., 2020. 39 Hints to Facilitate the Use of Semantics for Data on Agriculture and Nutrition. Data Science Journal, 19(1), p.47. DOI: http://doi.org/10.5334/dsj-2020-047. Â© 2023 The Author(s).","This article details a correction to the article: Caracciolo, , Aubin, , Jonquet, , Amdouni, , David, , Garcia, , Whitehead, , Roussey, , Stellato, and Villa, , 2020. 39 Hints to Facilitate the Use of Semantics for Data on Agriculture and Nutrition. Data Science Journal, 19, 47. DOI: http://doi.org/10.5334/dsj-2020-047."
Benefits and Challenges: Data Management Plans in Two Collaborative Projects,"The data-driven shift in the science research leads to a wider range of research data. To manage this data in a sustainable and adequate way, data management plans (DMPs) were established as a method. However, some researchers still do not create DMPs due to lack of time, resources and understanding of the needs. Furthermore, most of the existing templates and tools are largely unknown. In this article, we investigated the benefits and challenges of DMPs in two joint research projects of several academic institutions. For this, we described the process during the DMP creation, potential challenges and benefits experienced. We showed that a DMP with completely uniform content among the partner institutions was not possible due to individual and subject differences (e.g., in storage and policies). Instead, individual texts had to be formulated in some cases to overcome the diversity. This complexity could not be handled with the existing tools. Therefore, both projects created an own adapted template with some generic contents. Existing guidelines and internal project policies helped during the generation. We experienced that fewer people work more efficiently on a DMP than many and that all researchers within the project can profit from every individual DMP. Although we were not required to produce one, we recognised the associated benefits as a guide during the research process in joint projects. Â© 2023 The Author(s).","The data-driven shift in the science research leads to a wider range of research data. To manage this data in a sustainable and adequate way, data management plans (DMPs) were established as a method. However, some researchers still do not create DMPs due to lack of time, resources and understanding of the needs. Furthermore, most of the existing templates and tools are largely unknown. In this article, we investigated the benefits and challenges of DMPs in two joint research projects of several academic institutions. For this, we described the process during the DMP creation, potential challenges and benefits experienced. We showed that a DMP with completely uniform content among the partner institutions was not possible due to individual and subject differences (, in storage and policies). Instead, individual texts had to be formulated in some cases to overcome the diversity. This complexity could not be handled with the existing tools. Therefore, both projects created an own adapted template with some generic contents. Existing guidelines and internal project policies helped during the generation. We experienced that fewer people work more efficiently on a DMP than many and that all researchers within the project can profit from every individual DMP. Although we were not required to produce one, we recognised the associated benefits as a guide during the research process in joint projects."
Supporting FAIR Data Management Planning Across Different Disciplines at the University of Sheffield,"Recognition is growing sector-wide of the importance of FAIR data management planning in facilitating the sharing and reuse of research outputs. Indeed, research funders increasingly mandate practices such as data sharing or inclusion of a data availability statement in publications issuing from funded projects, practices which need to be anticipated at the data management planning stage. Nevertheless, there is a shortage of discipline-specific guidance to support researchers seeking to make data and software FAIR within a given domain and to a meaningful degree. This article outlines a project which explored ways to address this shortage. In Spring 2022, the University of Sheffield Library worked with researchers in seven disciplines to develop subject-specific FAIR checklists for the use of colleagues before, during and at the end of their research project. We chart the outcomes and implications of the project and the ways this will inform future support for researchers in planning for FAIR data at this institution. Â© 2023, Ubiquity Press. All rights reserved.","Recognition is growing sector-wide of the importance of FAIR data management planning in facilitating the sharing and reuse of research outputs. Indeed, research funders increasingly mandate practices such as data sharing or inclusion of a data availability statement in publications issuing from funded projects, practices which need to be anticipated at the data management planning stage. Nevertheless, there is a shortage of discipline-specific guidance to support researchers seeking to make data and software FAIR within a given domain and to a meaningful degree. This article outlines a project which explored ways to address this shortage. In Spring 2022, the University of Sheffield Library worked with researchers in seven disciplines to develop subject-specific FAIR checklists for the use of colleagues before, during and at the end of their research project. We chart the outcomes and implications of the project and the ways this will inform future support for researchers in planning for FAIR data at this institution."
"Data Management Plan Implementation, Assessments, and Evaluations: Implications and Recommendations","Data management plans (DMPs) have become nearly a worldwide requirement for research funding. To meet these new funding agency expectations, information professionals across domains and the world have worked to create resources and services to successfully implement and sometimes assess DMPs. This essay presents a series of case studies from different institutions across the globe to highlight current practices and share recommendations for future work. A summary of various projects related to DMP implementation, assessment, and evaluation in different contexts provides a useful overview of current practices. The essay concludes with recommendations for practical oversight and scoring to improve DMPsâ utility in enabling the sharing of data. Â© 2023 The Author(s).","Data management plans (DMPs) have become nearly a worldwide requirement for research funding. To meet these new funding agency expectations, information professionals across domains and the world have worked to create resources and services to successfully implement and sometimes assess DMPs. This essay presents a series of case studies from different institutions across the globe to highlight current practices and share recommendations for future work. A summary of various projects related to DMP implementation, assessment, and evaluation in different contexts provides a useful overview of current practices. The essay concludes with recommendations for practical oversight and scoring to improve DMPs utility in enabling the sharing of data."
"Software Management Plans - Current Concepts, Tools, and Application","The present article is a review of the state of the art about software management plans (SMPs). It provides a selection of questionnaires, tools and application cases for SMPs from a European (German) point of view, and discusses the possible connections of SMPs to other aspects of software sustainability, such as metadata, FAIR4RS principles or machine-actionable SMPs. The aim of our publication is to provide basic knowledge to start diving into the subject and a handout for infrastructure providers who are about to establish/develop a SMP service in oneâs own institution. Â© 2024, Ubiquity Press. All rights reserved.","The present article is a review of the state of the art about software management plans (SMPs). It provides a selection of questionnaires, tools and application cases for SMPs from a European (German) point of view, and discusses the possible connections of SMPs to other aspects of software sustainability, such as metadata, FAIR4RS principles or machine-actionable SMPs. The aim of our publication is to provide basic knowledge to start diving into the subject and a handout for infrastructure providers who are about to establish/develop a SMP service in ones own institution."
Making Data Management Plans Machine Actionable: Templates and Tools,"Since September of 2019, a task group within the European Open Science Cloud-EOSC Nordic Project, work-package 5 (T5.3.2), has focused its attention on machine-actionable Data Management Plans (maDMPs). A delivery working-paper from the group (Hasan et al. 2021) concluded in summary that extracting useful information from traditional free-text based DMPs is problematic. While maDMPs are generally more FAIR compliant, and as such accessible to both humans and machines, more interoperable with other systems, and serving different stakeholders for processing, sharing, evaluation and reuse. Different DMP tools and templates have developed independently, to a varying degree, allowing for the creation of genuinely machine actionable DMPs. Here we will describe the first three tools or projects for creating maDMPs that were central parts of the original task group mission. We will get into a more detailed account of one of these, specifically the Stockholm University â EOSC Nordic maDMP project using the DMP Online tool, as described by Philipson (2021). We will also briefly touch upon some other current tools and projects for creating maDMPs that are compliant with the RDA DMP Common Standard (RDCS), aiming for integration with other research information systems or research data repositories. A possible conclusion from this overview is that the development of tools for maDMPs is progressing fast and seems to converge towards a common standard. Nonetheless, there remains an immense amount of work to get there. Â© 2023, Ubiquity Press. All rights reserved.","Since September of 2019, a task group within the European Open Science Cloud-EOSC Nordic Project, work-package 5 (T5.3.2), has focused its attention on machine-actionable Data Management Plans (maDMPs). A delivery working-paper from the group (Hasan et al. 2021) concluded in summary that extracting useful information from traditional free-text based DMPs is problematic. While maDMPs are generally more FAIR compliant, and as such accessible to both humans and machines, more interoperable with other systems, and serving different stakeholders for processing, sharing, evaluation and reuse. Different DMP tools and templates have developed independently, to a varying degree, allowing for the creation of genuinely machine actionable DMPs. Here we will describe the first three tools or projects for creating maDMPs that were central parts of the original task group mission. We will get into a more detailed account of one of these, specifically the Stockholm University EOSC Nordic maDMP project using the DMP Online tool, as described by Philipson . We will also briefly touch upon some other current tools and projects for creating maDMPs that are compliant with the RDA DMP Common Standard (RDCS), aiming for integration with other research information systems or research data repositories. A possible conclusion from this overview is that the development of tools for maDMPs is progressing fast and seems to converge towards a common standard. Nonetheless, there remains an immense amount of work to get there."
Using OpenBIS as Virtual Research Environment: An ELN-LIMS Open-Source Database Tool as a Framework within the CRC 1411 Design of Particulate Products,"The digital transformation and consequent use of new digital technologies not only have a substantial impact on society and companies, but also on science. Analog documentation as we have known it for centuries will eventually be replaced by intelligent and FAIR (Findable, Accessible, Interoperable, and Reusable) systems. In addition to the actual research data and results, metadata now plays an important role not only for individual, independently existing projects, but for future scientific use and interdisciplinary research groups and disciplines as well. The solution presented here, consisting of an electronic laboratory notebook and laboratory information management system (ELN-LIMS) based on the openBIS (open Biology Information System) environment, offers interesting features and advantages, especially for interdisciplinary work. The Collaborative Research Centre (CRC) 1411 âDesign of Particulate Productsâ of the German Research Foundation is characterized by the cooperation of different working groups of synthesis, characterization, and simulation, and therefore serves as a model environment to present the implementation of openBIS. OpenBIS, as an open source ELN-LIMS solution following FAIR principles, provides a common set of general entries with the possibility of sharing and linking (meta-)data to improve the scientific exchange between all users. Â© 2023 The Author(s).","The digital transformation and consequent use of new digital technologies not only have a substantial impact on society and companies, but also on science. Analog documentation as we have known it for centuries will eventually be replaced by intelligent and FAIR (Findable, Accessible, Interoperable, and Reusable) systems. In addition to the actual research data and results, metadata now plays an important role not only for individual, independently existing projects, but for future scientific use and interdisciplinary research groups and disciplines as well. The solution presented here, consisting of an electronic laboratory notebook and laboratory information management system (ELN-LIMS) based on the openBIS (open Biology Information System) environment, offers interesting features and advantages, especially for interdisciplinary work. The Collaborative Research Centre (CRC) 1411 Design of Particulate Products of the German Research Foundation is characterized by the cooperation of different working groups of synthesis, characterization, and simulation, and therefore serves as a model environment to present the implementation of openBIS. OpenBIS, as an open source ELN-LIMS solution following FAIR principles, provides a common set of general entries with the possibility of sharing and linking (meta-)data to improve the scientific exchange between all users."
"Correction: The âPROTECTâ Essential Elements in Managing Crisis Data Policies (Data Science Journal, 23(1), p. 12. Available at: https://doi.org/10.5334/dsj-2024-012)","Consistent with the PROTECT essentials, the third element in Figure 1 of Zhang et al. (2024) should read âoperationâ (not âgovernanceâ). Below is the corrected Figure 1. Further explanation of the figure is in the published paper. (Figure presented) Â© 2024 The Author(s).","Consistent with the PROTECT essentials, the third element in Figure 1 of Zhang et al. should read operation (not governance). Below is the corrected Figure 1. Further explanation of the figure is in the published paper. (Figure presented)"
Rethinking Data Management Planning: Introducing Research Output Management Planning (ROMPi) Approach,"Data management plans (DMPs), designed to adhere to Findable, Accessible, Interoperable, Reusable (FAIR) principles, were introduced to enhance research data management (RDM) but have encountered challenges in implementation. This essay calls for a paradigm shift by introducing the âResearch Output Management Planning (ROMPi)â approach, aiming to integrate traditional research project management practices promoting a holistic perspective of RDM. In its essence, ROMPi reframes the DMP in the conventional project management work breakdown structure in work packages (WPs), with research outputs going through their lifecycle. It also advocates reimagining the concept of data into research outputs, acknowledging a holistic perspective of the research outcomes. We demonstrated that the research project management perspective at the early implementation stage could ultimately align DMP within the research process. ROMPi offers a practical research output management approach, fostering a holistic project-researcher-centric perspective. Â© 2024 The Author(s).","Data management plans (DMPs), designed to adhere to Findable, Accessible, Interoperable, Reusable (FAIR) principles, were introduced to enhance research data management (RDM) but have encountered challenges in implementation. This essay calls for a paradigm shift by introducing the Research Output Management Planning (ROMPi) approach, aiming to integrate traditional research project management practices promoting a holistic perspective of RDM. In its essence, ROMPi reframes the DMP in the conventional project management work breakdown structure in work packages (WPs), with research outputs going through their lifecycle. It also advocates reimagining the concept of data into research outputs, acknowledging a holistic perspective of the research outcomes. We demonstrated that the research project management perspective at the early implementation stage could ultimately align DMP within the research process. ROMPi offers a practical research output management approach, fostering a holistic project-researcher-centric perspective."
Data Law Companion: Enhancing Data Protection Law Compliance in the Digital Age,"The rapid expansion and use of digital data in the era of generative Artificial Intelligence raises significant challenges in legal compliance and ethical data handling. The Data Law Companion (DLC) emerges as a pivotal tool, blending advanced Large Language Models like ChatGPT-4 with expert legal insights to democratize understanding of data protection laws. This paper explores DLCâs role in simplifying complex legal frameworks for diverse stakeholders through a mixed-method approach, leveraging primary data from East African Data Protection Acts and comprehensive online research. Our findings underscore DLCâs effectiveness in enhancing legal comprehension and compliance in the digital era. Â© 2024 The Author(s).","The rapid expansion and use of digital data in the era of generative Artificial Intelligence raises significant challenges in legal compliance and ethical data handling. The Data Law Companion emerges as a pivotal tool, blending advanced Large Language Models like ChatGPT-4 with expert legal insights to democratize understanding of data protection laws. This paper explores DLCs role in simplifying complex legal frameworks for diverse stakeholders through a mixed-method approach, leveraging primary data from East African Data Protection Acts and comprehensive online research. Our findings underscore DLCs effectiveness in enhancing legal comprehension and compliance in the digital era."
A Resource for Guiding Data Stewards to Make European Rare Disease Patient Registries FAIR,"Objective: This paper reports on the development of a dynamic data management planning questionnaire to guide data stewards of the European Reference Network (ERN) rare disease patient registries to make their data findable, accessible, interoperable, and reusable (FAIR). As part of this work, the questionnaire was validated through expert review and aligned with existing resources on rare diseases and FAIR data management. Materials and Methods: The questionnaire was developed for the Data Stewardship Wizard, a tool for data management planning. Knowledge sources on FAIR data, ERN patient registries, and data management were used to compose questions. Ten domain experts validated the questionnaire. The topics in the questionnaire were aligned with existing knowledge bases. Results: A total of 57 questions were included in the questionnaire. Twenty-three references to the FAIR Cookbook and Research Data Management toolkit for Life Sciences were added. Expert validation provided a total of 166 comments on content, structure, and software-related issues. A public instance of the Data Stewardship Wizard was deployed for use by data stewards of ERN patient registries. Discussion: The questionnaire addresses issues that ERNs encounter when making their registries FAIR and follows the implementation choices made by the European rare disease community. A challenging task for future research is to extend the questionnaire to other types of registries and to validate with users. Conclusion: This smart questionnaire is the first model created for the Data Stewardship Wizard that helps ERN patient registries with making their data FAIR. It will assist data stewards in aligning their efforts and providing guidance on FAIR data. Â© 2023 The Author(s).","Objective: This paper reports on the development of a dynamic data management planning questionnaire to guide data stewards of the European Reference Network (ERN) rare disease patient registries to make their data findable, accessible, interoperable, and reusable (FAIR). As part of this work, the questionnaire was validated through expert review and aligned with existing resources on rare diseases and FAIR data management. Materials and Methods: The questionnaire was developed for the Data Stewardship Wizard, a tool for data management planning. Knowledge sources on FAIR data, ERN patient registries, and data management were used to compose questions. Ten domain experts validated the questionnaire. The topics in the questionnaire were aligned with existing knowledge bases. Results: A total of 57 questions were included in the questionnaire. Twenty-three references to the FAIR Cookbook and Research Data Management toolkit for Life Sciences were added. Expert validation provided a total of 166 comments on content, structure, and software-related issues. A public instance of the Data Stewardship Wizard was deployed for use by data stewards of ERN patient registries. Discussion: The questionnaire addresses issues that ERNs encounter when making their registries FAIR and follows the implementation choices made by the European rare disease community. A challenging task for future research is to extend the questionnaire to other types of registries and to validate with users. Conclusion: This smart questionnaire is the first model created for the Data Stewardship Wizard that helps ERN patient registries with making their data FAIR. It will assist data stewards in aligning their efforts and providing guidance on FAIR data."
Ontology-Driven Semantic Enrichment Framework for Open Data Value Creation,"The reviewed semantic enrichment frameworks lack mechanisms to assess the degree of semantic value added to flat-text resources in terms of knowledge and semantic capabilities. This complicates the tasks of driving the semantic value creation process toward the specific enrichment output and evaluating the output. In addressing this gap, we propose the semantic value creation solution, which converts flat-text resources to knowledge resources. Namely, we propose the ontology-driven semantic enrichment (ODSE) framework, with a mechanism for semantic valuation. The frameworkâs development involved adopting the design science research methodology for information systems. The developed framework leverages linked data principles for knowledge creation. This framework was demonstrated to determine the semantic capabilities enabled by the syntax additions, as well as the knowledge enabled by the semantic additions to flat-text resources, along with its potential impact on knowledge creation, mining, and resource-usability effectiveness. The ODSE framework is reusable in semantic value creation implementations that transform a flat text to semantic formats. Â© 2023 The Author(s).","The reviewed semantic enrichment frameworks lack mechanisms to assess the degree of semantic value added to flat-text resources in terms of knowledge and semantic capabilities. This complicates the tasks of driving the semantic value creation process toward the specific enrichment output and evaluating the output. In addressing this gap, we propose the semantic value creation solution, which converts flat-text resources to knowledge resources. Namely, we propose the ontology-driven semantic enrichment (ODSE) framework, with a mechanism for semantic valuation. The frameworks development involved adopting the design science research methodology for information systems. The developed framework leverages linked data principles for knowledge creation. This framework was demonstrated to determine the semantic capabilities enabled by the syntax additions, as well as the knowledge enabled by the semantic additions to flat-text resources, along with its potential impact on knowledge creation, mining, and resource-usability effectiveness. The ODSE framework is reusable in semantic value creation implementations that transform a flat text to semantic formats."
Engaging with Researchers and Raising Awareness of FAIR and Open Science through the FAIR+ Implementation Survey Tool (FAIRIST),"Seven years after the seminal paper on FAIR was published, that introduced the concept of making research outputs Findable, Accessible, Interoperable, and Reusable, researchers still struggle to understand how to implement the principles. For many researchers, FAIR promises long-term benefits for near-term effort, requires skills not yet acquired, and is one more thing in a long list of unfunded mandates and onerous requirements for scientists. Even for those required to, or who are convinced that they must make time for FAIR research practices, their preference is for just-in-time advice properly sized to the scientific artifacts and process. Because of the generality of most FAIR implementation guidance, it is difficult for a researcher to adjust to the advice according to their situation. Technological advances, especially in the area of artificial intelligence (AI) and machine learning (ML), complicate FAIR adoption, as researchers and data stewards ponder how to make software, workflows, and models FAIR and reproducible. The FAIR+ Implementation Survey Tool (FAIRIST) mitigates the problem by integrating research requirements with research proposals in a systematic way. FAIRIST factors in new scholarly outputs, such as nanopublications and notebooks, and the various research artifacts related to AI research (data, models, workflows, and benchmarks). Researchers step through a self-serve survey process and receive a table ready for use in their data management plan (DMP) and/or work plan. while gaining awareness of the FAIR Principles and Open Science concepts. FAIRIST is a model that uses part of the proposal process as a way to do outreach, raise awareness of FAIR dimensions and considerations, while providing timely assistance for competitive proposals. Â© 2023, Ubiquity Press. All rights reserved.","Seven years after the seminal paper on FAIR was published, that introduced the concept of making research outputs Findable, Accessible, Interoperable, and Reusable, researchers still struggle to understand how to implement the principles. For many researchers, FAIR promises long-term benefits for near-term effort, requires skills not yet acquired, and is one more thing in a long list of unfunded mandates and onerous requirements for scientists. Even for those required to, or who are convinced that they must make time for FAIR research practices, their preference is for just-in-time advice properly sized to the scientific artifacts and process. Because of the generality of most FAIR implementation guidance, it is difficult for a researcher to adjust to the advice according to their situation. Technological advances, especially in the area of artificial intelligence (AI) and machine learning , complicate FAIR adoption, as researchers and data stewards ponder how to make software, workflows, and models FAIR and reproducible. The FAIR+ Implementation Survey Tool (FAIRIST) mitigates the problem by integrating research requirements with research proposals in a systematic way. FAIRIST factors in new scholarly outputs, such as nanopublications and notebooks, and the various research artifacts related to AI research (data, models, workflows, and benchmarks). Researchers step through a self-serve survey process and receive a table ready for use in their data management plan (DMP) and/or work plan. while gaining awareness of the FAIR Principles and Open Science concepts. FAIRIST is a model that uses part of the proposal process as a way to do outreach, raise awareness of FAIR dimensions and considerations, while providing timely assistance for competitive proposals."
Data Management Planning across Disciplines and Infrastructures. Introduction to the Special Collection,"The Special Collection Data Management Planning across Disciplines and Infrastructures of the Data Science Journal consists of papers describing practical experiences, concepts, and future directions on the design and deployment of effective data management plans and associated tools. Papers contain practical examples on managing and sharing data, consider the integration of data management plans into infrastructures and reflect innovative research into new directions for disciplinary and cross-disciplinary data management planning. Â© 2024 The Author(s).","The Special Collection Data Management Planning across Disciplines and Infrastructures of the Data Science Journal consists of papers describing practical experiences, concepts, and future directions on the design and deployment of effective data management plans and associated tools. Papers contain practical examples on managing and sharing data, consider the integration of data management plans into infrastructures and reflect innovative research into new directions for disciplinary and cross-disciplinary data management planning."
Black Hole Clustering: Gravity-Based Approach with No Predetermined Parameters,"Clustering is a fundamental technique in data mining and machine learning, aiming to group data elements into related clusters. However, traditional clustering algorithms, such as K-means, suffer from limitations such as the need for user-defined parameters and sensitivity to initial conditions. This paper introduces a novel clustering algorithm called Black Hole Clustering (BHC), which leverages the concept of gravity to identify clusters. Inspired by the behavior of masses in the physical world, gravity-based clustering treats data points as mass points that attract each other based on distance. This approach enables the detection of high-density clusters of arbitrary shapes and sizes without the need for predefined parameters. We extensively evaluate BHC on synthetic and real-world datasets, demonstrating its effectiveness in handling complex data structures and varying point densities. Notably, BHC excels in accurate prediction of the number of clusters and achieves competitive clustering accuracy rates. Moreover, its parameter-free nature enhances clustering accuracy, robustness, and scalability. These findings represent a significant contribution to advanced clustering techniques and pave the way for further research and application of gravity-based clustering in diverse fields. BHC offers a promising approach to addressing clustering challenges in complex datasets, opening up new possibilities for improved data analysis and pattern discovery. Â© 2024 The Author(s).","Clustering is a fundamental technique in data mining and machine learning, aiming to group data elements into related clusters. However, traditional clustering algorithms, such as K-means, suffer from limitations such as the need for user-defined parameters and sensitivity to initial conditions. This paper introduces a novel clustering algorithm called Black Hole Clustering (BHC), which leverages the concept of gravity to identify clusters. Inspired by the behavior of masses in the physical world, gravity-based clustering treats data points as mass points that attract each other based on distance. This approach enables the detection of high-density clusters of arbitrary shapes and sizes without the need for predefined parameters. We extensively evaluate BHC on synthetic and real-world datasets, demonstrating its effectiveness in handling complex data structures and varying point densities. Notably, BHC excels in accurate prediction of the number of clusters and achieves competitive clustering accuracy rates. Moreover, its parameter-free nature enhances clustering accuracy, robustness, and scalability. These findings represent a significant contribution to advanced clustering techniques and pave the way for further research and application of gravity-based clustering in diverse fields. BHC offers a promising approach to addressing clustering challenges in complex datasets, opening up new possibilities for improved data analysis and pattern discovery."
KBJNet: Kinematic Bi-Joint Temporal Convolutional Network Attention for Anomaly Detection in Multivariate Time Series Data,"Detecting anomalies in multivariate time series data is crucial to ensure the security and stability of industrial processes. Yet, it remains challenging due to the absence of labeled anomaly data, the complexity of time series data, and the large dataset size. We propose KBJNet, an innovative model incorporating Transformer and Dilated Temporal Convolutional Network (TCN) techniques to address these obstacles. Our model employs a Single TCN-Attention Network, utilizing a single layer of Transformer encoder, making it highly efficient for inference. To further enhance its robustness, we introduce a novel adaptive attention mechanism that dynamically weights temporal context, enabling KBJNet to capture long-range dependencies in time series data effectively. The evaluation of KBJNet on eight publicly available datasets revealed that KBJNet considerably outperforms the most recent methods, enhancing F1 scores by as much as 6%. This result represents a significant contribution to anomaly detection, and we anticipate that our approach will have practical implications for developing next-generation anomaly detection systems in various industrial applications. Â© 2024 The Author(s).","Detecting anomalies in multivariate time series data is crucial to ensure the security and stability of industrial processes. Yet, it remains challenging due to the absence of labeled anomaly data, the complexity of time series data, and the large dataset size. We propose KBJNet, an innovative model incorporating Transformer and Dilated Temporal Convolutional Network (TCN) techniques to address these obstacles. Our model employs a Single TCN-Attention Network, utilizing a single layer of Transformer encoder, making it highly efficient for inference. To further enhance its robustness, we introduce a novel adaptive attention mechanism that dynamically weights temporal context, enabling KBJNet to capture long-range dependencies in time series data effectively. The evaluation of KBJNet on eight publicly available datasets revealed that KBJNet considerably outperforms the most recent methods, enhancing F1 scores by as much as 6%. This result represents a significant contribution to anomaly detection, and we anticipate that our approach will have practical implications for developing next-generation anomaly detection systems in various industrial applications."
Scaling Identifiers and their Metadata to Gigascale: An Architecture to Tackle the Challenges of Volume and Variety,"Persistent identifiers are applied to an ever-increasing variety of research objects, including software, samples, models, people, instruments, grants, and projects, and there is a growing need to apply identifiers at a finer and finer granularity. Unfortunately, the systems developed over two decades ago to manage identifiers and the metadata describing the identified objects no longer scale. Communities working with physical samples have grappled with these three challenges of the increasing volume, variety, and variability of identified objects for many years. To address this dual challenge, the IGSN 2040 project explored how metadata and catalogues for physical samples could be shared at the scale of billions of samples across an ever-growing variety of users and disciplines. In this paper, we focus on how we scale identifiers and their describing metadata to billions of objects and who the actors involved with this system are. Our analysis of these requirements resulted in the definition of a minimum viable product and the design of an architecture that not only addresses the challenges of increasing volume and variety but, more importantly, is easy to implement because it reuses commonly used Web components. Our solution is based on a Web architectural model that utilises Schema.org, JSON-LD, and sitemaps. Applying these commonly used architectural patterns on the internet allows us to not only handle increasing variety but also enable better compliance with the FAIR Guiding Principles. Â© 2023 The Author(s).","Persistent identifiers are applied to an ever-increasing variety of research objects, including software, samples, models, people, instruments, grants, and projects, and there is a growing need to apply identifiers at a finer and finer granularity. Unfortunately, the systems developed over two decades ago to manage identifiers and the metadata describing the identified objects no longer scale. Communities working with physical samples have grappled with these three challenges of the increasing volume, variety, and variability of identified objects for many years. To address this dual challenge, the IGSN 2040 project explored how metadata and catalogues for physical samples could be shared at the scale of billions of samples across an ever-growing variety of users and disciplines. In this paper, we focus on how we scale identifiers and their describing metadata to billions of objects and who the actors involved with this system are. Our analysis of these requirements resulted in the definition of a minimum viable product and the design of an architecture that not only addresses the challenges of increasing volume and variety but, more importantly, is easy to implement because it reuses commonly used Web components. Our solution is based on a Web architectural model that utilises Schema.org, JSON-LD, and sitemaps. Applying these commonly used architectural patterns on the internet allows us to not only handle increasing variety but also enable better compliance with the FAIR Guiding Principles."
"MÄori Algorithmic Sovereignty: Idea, Principles, and Use","With the emergence of data technologies and algorithms in Aotearoa New Zealand that are used for decision-making and support, there is a need for frameworks to guide how we maximise the opportunities these technologies create and minimise the risks they may impose. For algorithms that use MÄori data, these require extra considerations due to the heightened risks MÄori endure due to systemic biases inherent within data and the processes that underlie algorithm development. Algorithms can be framed as a particular use of data, therefore data frameworks that currently exist can be extended to include algorithms. MÄori data sovereignty principles are well-known and are used by researchers and government agencies to guide the culturally appropriate use of MÄori data. Extending these principles to fit the context of algorithms, and re-working the underlying sub-principles to address issues related to responsible algorithms from a MÄori perspective leads to the MÄori algorithmic sovereignty principles. We define this idea, present the updated principles and sub-principles, and highlight a strategy for the detection and minimisation of bias within the algorithm development process. Â© 2024 The Author(s).","With the emergence of data technologies and algorithms in Aotearoa New Zealand that are used for decision-making and support, there is a need for frameworks to guide how we maximise the opportunities these technologies create and minimise the risks they may impose. For algorithms that use Mori data, these require extra considerations due to the heightened risks Mori endure due to systemic biases inherent within data and the processes that underlie algorithm development. Algorithms can be framed as a particular use of data, therefore data frameworks that currently exist can be extended to include algorithms. Mori data sovereignty principles are well-known and are used by researchers and government agencies to guide the culturally appropriate use of Mori data. Extending these principles to fit the context of algorithms, and re-working the underlying sub-principles to address issues related to responsible algorithms from a Mori perspective leads to the Mori algorithmic sovereignty principles. We define this idea, present the updated principles and sub-principles, and highlight a strategy for the detection and minimisation of bias within the algorithm development process."
From Meaningful Data Science to Impactful Decisions: The Importance of Being Causally Prescriptive,"This article proposes a framework for transition from traditional data science, where the focus is on extracting value from available data, to goal-driven analytical decision-making, where the business objective is defined first, through integration of various analytical techniques in a common setting. We discuss the link between predictive analytics and prescriptive analytics in the context of formulating the problem and assert that all prescriptive analytics problem formulations assume a causal link between decisions and outcomes. We emphasize the role of predictive analytics and causal inference in specifying the causal link between decisions and outcomes accurately and ultimately in aligning the analysis with the business objectives. We offer practical examples that integrate various required analytics tasks and describe scenarios where causal inference is required versus not required. Â© 2023 The Author(s).","This article proposes a framework for transition from traditional data science, where the focus is on extracting value from available data, to goal-driven analytical decision-making, where the business objective is defined first, through integration of various analytical techniques in a common setting. We discuss the link between predictive analytics and prescriptive analytics in the context of formulating the problem and assert that all prescriptive analytics problem formulations assume a causal link between decisions and outcomes. We emphasize the role of predictive analytics and causal inference in specifying the causal link between decisions and outcomes accurately and ultimately in aligning the analysis with the business objectives. We offer practical examples that integrate various required analytics tasks and describe scenarios where causal inference is required versus not required."
The Research Data Management Organiser (RDMO) â a Strong Community Behind an Established Software for DMPs and Much More,"This practice paper provides an overview of the Research Data Management Organiser (RDMO) software for data management planning and the RDMO community. It covers the background and history of RDMO as a funded project, its current status as a consortium and an open source software and provides insights into the organisation of the vibrant RDMO community. Furthermore, we introduce RDMO from a software developerâs perspective and outline, in detail, the current work in the different sub-working groups committed to developing DMP templates and related guidance materials. Â© 2024 The Author(s).","This practice paper provides an overview of the Research Data Management Organiser (RDMO) software for data management planning and the RDMO community. It covers the background and history of RDMO as a funded project, its current status as a consortium and an open source software and provides insights into the organisation of the vibrant RDMO community. Furthermore, we introduce RDMO from a software developers perspective and outline, in detail, the current work in the different sub-working groups committed to developing DMP templates and related guidance materials."
Data Science as an Interdiscipline: Historical Parallels from Information Science,"Considerable debate exists today on almost every facet of what data science entails. Almost all commentators agree, however, that data science must be characterized as having an interdisciplinary or metadisciplinary nature. There is interest from many stakeholders in formalizing the emerging discipline of data science by defining boundaries and core concepts for the field. This paper presents a comparison between the data science of today and the development and evolution of information science over the past century. Data science and information science present a number of similarities: diverse participants and institutions, contested disciplinary boundaries, and diffuse core concepts. This comparison is used to discuss three questions about data science going forward: (1) What will be the focal points around which data science and its stakeholders coalesce? (2) Can data science stakeholders use the lack of disciplinary clarity as a strength? (3) Can data science feed into an âempowering professionâ? The historical comparison to information science suggests that the boundaries of data science will be a source of contestation and debate for the foreseeable future. Stakeholders face many questions as data science evolves with the inevitable societal and technological changes of the next few decades. Â© 2023 The Author(s).","Considerable debate exists today on almost every facet of what data science entails. Almost all commentators agree, however, that data science must be characterized as having an interdisciplinary or metadisciplinary nature. There is interest from many stakeholders in formalizing the emerging discipline of data science by defining boundaries and core concepts for the field. This paper presents a comparison between the data science of today and the development and evolution of information science over the past century. Data science and information science present a number of similarities: diverse participants and institutions, contested disciplinary boundaries, and diffuse core concepts. This comparison is used to discuss three questions about data science going forward: What will be the focal points around which data science and its stakeholders coalesce? Can data science stakeholders use the lack of disciplinary clarity as a strength? Can data science feed into an empowering profession? The historical comparison to information science suggests that the boundaries of data science will be a source of contestation and debate for the foreseeable future. Stakeholders face many questions as data science evolves with the inevitable societal and technological changes of the next few decades."
Harmonizing GCW Cryosphere Vocabularies with ENVO and SWEET. Towards a General Model for Semantic Harmonization,"This paper presents the specific process used by members of the Earth Science Information Partners (ESIP) Semantic Harmonization Cluster, to harmonize cryospheric terms gathered by the Global Cryosphere Watch (GCW) with two leading semantic resources used in the Earth and Environmental science communitiesâthe Semantic Web for Earth and Environmental Terminology (SWEET) and the Environment Ontology (ENVO). This process led to updates to both ENVO and SWEET as well as the development of an alignment file relating cryospheric terms in ENVO to those in SWEET. In addition, we summarize several leading practices which may be applied to other projects/realms within Earth and Environmental science and perhaps beyond, as well as suggest a generalized process for doing so. This paper describes the history of the effort, the technical and decision-making processes used to resolve differences between semantic resources, and describes several issues encountered, with a focus on those that were addressed during the effort. Lessons learned, examples of the problems encountered and a summary of resulting leading practices growing out of this work is provided. Â© 2024 The Author(s).","This paper presents the specific process used by members of the Earth Science Information Partners (ESIP) Semantic Harmonization Cluster, to harmonize cryospheric terms gathered by the Global Cryosphere Watch (GCW) with two leading semantic resources used in the Earth and Environmental science communitiesthe Semantic Web for Earth and Environmental Terminology (SWEET) and the Environment Ontology (ENVO). This process led to updates to both ENVO and SWEET as well as the development of an alignment file relating cryospheric terms in ENVO to those in SWEET. In addition, we summarize several leading practices which may be applied to other projects/realms within Earth and Environmental science and perhaps beyond, as well as suggest a generalized process for doing so. This paper describes the history of the effort, the technical and decision-making processes used to resolve differences between semantic resources, and describes several issues encountered, with a focus on those that were addressed during the effort. Lessons learned, examples of the problems encountered and a summary of resulting leading practices growing out of this work is provided."
Secured and Modular Data Portal: Database System to Manage Broadly Classified and Large-Scale Data,"Using various types of broadly classified and large-scale data, the Ethiopian Construction Design and Supervision Works Corporation (ECDSWC) provides professional services such as engineering studies and design. Storing, managing, and sharing datasets within workgroups and research teams was not an easy task before the corporation implemented the data portal. To resolve the issues related to data management, this study provides a secured and modular data portal by implementing REST API principles. The data portal is used to manage a wide variety of datasets, such as spatial data and their attributes, along with metadata and other serialized documents that support the business operation of the corporation. On the top of data management strategies, the Security in Depth (SiD) mechanism is developed by implementing the Unified Identity Authentication Service, which provides a multilayered security scheme. The applicability of the data portal is demonstrated using datasets obtained from ECDSWC. Further, the accuracy, integrity and privacy of the data are evaluated, and system performance is weighed through a prepared test case. Â© 2024 The Author(s).","Using various types of broadly classified and large-scale data, the Ethiopian Construction Design and Supervision Works Corporation (ECDSWC) provides professional services such as engineering studies and design. Storing, managing, and sharing datasets within workgroups and research teams was not an easy task before the corporation implemented the data portal. To resolve the issues related to data management, this study provides a secured and modular data portal by implementing REST API principles. The data portal is used to manage a wide variety of datasets, such as spatial data and their attributes, along with metadata and other serialized documents that support the business operation of the corporation. On the top of data management strategies, the Security in Depth (SiD) mechanism is developed by implementing the Unified Identity Authentication Service, which provides a multilayered security scheme. The applicability of the data portal is demonstrated using datasets obtained from ECDSWC. Further, the accuracy, integrity and privacy of the data are evaluated, and system performance is weighed through a prepared test case."
The Dataset Finder: A Tool Utilizing Data Management Plans as a Key to Data Discoverability,"In the past years, there has been an increased interest in sharing and reusing research data. While the importance of sharing data is urgent for enabling collaboration, many research projects are currently struggling with setting up a strategy and the right infrastructure for enabling such data-driven collaboration among the projectâs researchers. Through an analysis of the Cluster of Excellence project Internet of Production as a use case, we have found that to enable researchers to share and find research data, a suitable platform is needed, as well as processes that smoothly blend into existing research data management practices. We argue that leveraging data management plans from a medium of documentation to a dynamic knowledge source enhances overview and discoverability of data, while integrating easily into day-to-day workflows of researchers. We present a tool, the Dataset Finder, which is built on the basis of data management plans, and allows users to intuitively query available datasets. The current functionalities of the tools are discussed, results of a preliminary evaluation, as well as potential future features. Â© 2024 The Author(s).","In the past years, there has been an increased interest in sharing and reusing research data. While the importance of sharing data is urgent for enabling collaboration, many research projects are currently struggling with setting up a strategy and the right infrastructure for enabling such data-driven collaboration among the projects researchers. Through an analysis of the Cluster of Excellence project Internet of Production as a use case, we have found that to enable researchers to share and find research data, a suitable platform is needed, as well as processes that smoothly blend into existing research data management practices. We argue that leveraging data management plans from a medium of documentation to a dynamic knowledge source enhances overview and discoverability of data, while integrating easily into day-to-day workflows of researchers. We present a tool, the Dataset Finder, which is built on the basis of data management plans, and allows users to intuitively query available datasets. The current functionalities of the tools are discussed, results of a preliminary evaluation, as well as potential future features."
Enhancing the FAIRness of Arctic Research Data Through Semantic Annotation,"The National Science Foundationâs Arctic Data Center is the primary data repository for NSF-funded research conducted in the Arctic. There are major challenges in discovering and interpreting resources in a repository containing data as heterogeneous and interdisciplinary as those in the Arctic Data Center. This paper reports on advances in cyberinfrastructure at the Arctic Data Center that help address these issues by leveraging semantic technologies that enhance the repositoryâs adherence to the FAIR data principles and improve the Findability, Accessibility, Interoperability, and Reusability of digital resources in the repository. We describe the Arctic Data Centerâs improvements. We use semantic annotation to bind metadata about Arctic data sets with concepts in web-accessible ontologies. The Arctic Data Centerâs implementation of a semantic annotation mechanism is accompanied by the development of an extended search interface that increases the findability of data by allowing users to search for specific, broader, and narrower meanings of measurement descriptions, as well as through their potential synonyms. Based on research carried out by the DataONE project, we evaluated the potential impact of this approach, regarding the accessibility, interoperability, and reusability of measurement data. Arctic research often benefits from having additional data, typically from multiple, heterogeneous sources, that complement and extend the bases â spatially, temporally, or thematically â for understanding Arctic phenomena. These relevant data resources must be âfoundâ, and âharmonizedâ prior to integration and analysis. The findings of a case study indicated that the semantic annotation of measurement data enhances the capabilities of researchers to accomplish these tasks. Â© 2024 The Author(s).","The National Science Foundations Arctic Data Center is the primary data repository for NSF-funded research conducted in the Arctic. There are major challenges in discovering and interpreting resources in a repository containing data as heterogeneous and interdisciplinary as those in the Arctic Data Center. This paper reports on advances in cyberinfrastructure at the Arctic Data Center that help address these issues by leveraging semantic technologies that enhance the repositorys adherence to the FAIR data principles and improve the Findability, Accessibility, Interoperability, and Reusability of digital resources in the repository. We describe the Arctic Data Centers improvements. We use semantic annotation to bind metadata about Arctic data sets with concepts in web-accessible ontologies. The Arctic Data Centers implementation of a semantic annotation mechanism is accompanied by the development of an extended search interface that increases the findability of data by allowing users to search for specific, broader, and narrower meanings of measurement descriptions, as well as through their potential synonyms. Based on research carried out by the DataONE project, we evaluated the potential impact of this approach, regarding the accessibility, interoperability, and reusability of measurement data. Arctic research often benefits from having additional data, typically from multiple, heterogeneous sources, that complement and extend the bases spatially, temporally, or thematically for understanding Arctic phenomena. These relevant data resources must be found, and harmonized prior to integration and analysis. The findings of a case study indicated that the semantic annotation of measurement data enhances the capabilities of researchers to accomplish these tasks."
Five Suggestions Towards User-Centred Data Repositories in the Social Sciences,"Data repositories for the social sciences are facing some discipline-specific challenges. This essay provides an overview of the four key challenges. In addition, five suggestions are made to strengthen domain-specific online data repositories in the social sciences, supported by good practices. Using a user-centred approach, these suggestions aim to further open up the social sciences data landscape. The essay first addresses insights regarding the sharing of quantitative and qualitative data in light of the specific needs and issues regarding these two types of data. Thereafter, we suggest (i) âdata labsâ for novice data re-users, (ii) advocate for the repository as an online meeting place for researchers, (iii&iv) argue for two concrete metadata approaches for quantitative and qualitative data separately, (v) provide ideas for handling epistemological challenges of data sharing, and (vi) introduce the data repository as a possible contact medium for research participants. Â© 2024 The Author(s).","Data repositories for the social sciences are facing some discipline-specific challenges. This essay provides an overview of the four key challenges. In addition, five suggestions are made to strengthen domain-specific online data repositories in the social sciences, supported by good practices. Using a user-centred approach, these suggestions aim to further open up the social sciences data landscape. The essay first addresses insights regarding the sharing of quantitative and qualitative data in light of the specific needs and issues regarding these two types of data. Thereafter, we suggest data labs for novice data re-users, advocate for the repository as an online meeting place for researchers, (iii&iv) argue for two concrete metadata approaches for quantitative and qualitative data separately, provide ideas for handling epistemological challenges of data sharing, and introduce the data repository as a possible contact medium for research participants."
"The Need for Data Policy in Times of Crisis An IDPC CODATA Report Following a Scientific Workshop Held on 22 October 2022 in Leiden, The Netherlands","Recent events of global impact have led the scientific community to re-evaluate and re-affirm the role of science in crisis situations. In particular, the COVID-19 health emergency required the abrupt (re-)allocation of scientific resources to address a pandemic which had demonstrated the vulnerability of science as well as the essential role data science has in responding to a pandemic. In a digital world, data collection, data processing, and data reuse are critical to science and its use for critical decision-making in societyâs response. This paper reports a workshop undertaken by the CODATA International Data Policy Committee (IDPC), together with leading international partners, on the role of data policy in times of crisis. A critical investigation of the role of data policy in crisis situations was engaged by a leading group of scientists and data experts representing a wide audience of various scientific disciplines and expertise from inter-governmental organisations. This paper presents preliminary results as to how data policy, specifically designed to address the need for science in crisis situations, can contribute to building a more robust scientific enterprise that is appropriately prepared for and capable of acting with confidence in the urgencies of crises. The workshop identified a need for establishing principles for data policy in times of crisis as developing concrete recommendations to support engagement with the international scientific community and inter-governmental organisations. Â© 2023 The Author(s).","Recent events of global impact have led the scientific community to re-evaluate and re-affirm the role of science in crisis situations. In particular, the COVID-19 health emergency required the abrupt (re-)allocation of scientific resources to address a pandemic which had demonstrated the vulnerability of science as well as the essential role data science has in responding to a pandemic. In a digital world, data collection, data processing, and data reuse are critical to science and its use for critical decision-making in societys response. This paper reports a workshop undertaken by the CODATA International Data Policy Committee (IDPC), together with leading international partners, on the role of data policy in times of crisis. A critical investigation of the role of data policy in crisis situations was engaged by a leading group of scientists and data experts representing a wide audience of various scientific disciplines and expertise from inter-governmental organisations. This paper presents preliminary results as to how data policy, specifically designed to address the need for science in crisis situations, can contribute to building a more robust scientific enterprise that is appropriately prepared for and capable of acting with confidence in the urgencies of crises. The workshop identified a need for establishing principles for data policy in times of crisis as developing concrete recommendations to support engagement with the international scientific community and inter-governmental organisations."
Legal Regulation of State Electronic Services: Relevant Issues and Ways of Improvement,"The emergence of new platforms to promote concepts such as e-government and open data, which are currently being actively implemented in many countries around the world, and, more importantly, the need to promote civic participation and engagement in this regard, which are perhaps two key components for the successful implementation of any modern e-government project, provide both new opportunities and challenges for policy makers in implementing this idea in the Republic of Kazakhstan, which is actively trying to technologically reform the public sector. The result of the policy of implementing the e-government in the Republic of Kazakhstan was the creation of a single e-government portal with unified databases and unified electronic services for the entire country, which were integrated into a single area of the concept in both the political and technological meaning. At present, public services are provided by personal contact through the offices of the Public Service Centre and online through the e-government portal, whose projects include dozens of different information systems, registers, and state databases, and hundreds of applications and services. In modern realities in the Republic of Kazakhstan, it is necessary to conduct a survey to measure the effectiveness of public services, similar to Citizens First in Canada, in order to determine the quality and comparison in the survey, a Common Measurement Tool can be used. As a result of the study, it was also concluded that the following aspects of legal regulation need to be improved in the Republic of Kazakhstan: the establishment of a body for monitoring and protecting information data, as well as the consideration of complaints regarding the violations of the right to protect information data; the need to consolidate national legislation in the field of e-government into a single legal act; the establishment of an interdepartmental state body in the field of e-government. Â© 2023 The Author(s).","The emergence of new platforms to promote concepts such as e-government and open data, which are currently being actively implemented in many countries around the world, and, more importantly, the need to promote civic participation and engagement in this regard, which are perhaps two key components for the successful implementation of any modern e-government project, provide both new opportunities and challenges for policy makers in implementing this idea in the Republic of Kazakhstan, which is actively trying to technologically reform the public sector. The result of the policy of implementing the e-government in the Republic of Kazakhstan was the creation of a single e-government portal with unified databases and unified electronic services for the entire country, which were integrated into a single area of the concept in both the political and technological meaning. At present, public services are provided by personal contact through the offices of the Public Service Centre and online through the e-government portal, whose projects include dozens of different information systems, registers, and state databases, and hundreds of applications and services. In modern realities in the Republic of Kazakhstan, it is necessary to conduct a survey to measure the effectiveness of public services, similar to Citizens First in Canada, in order to determine the quality and comparison in the survey, a Common Measurement Tool can be used. As a result of the study, it was also concluded that the following aspects of legal regulation need to be improved in the Republic of Kazakhstan: the establishment of a body for monitoring and protecting information data, as well as the consideration of complaints regarding the violations of the right to protect information data; the need to consolidate national legislation in the field of e-government into a single legal act; the establishment of an interdepartmental state body in the field of e-government."
Using ELN Functionality of Kadi4Mat (KadiWeb) in a Materials Science Case Study of a User Facility,"In this paper, we introduce our approach in using the web-based application Kadi4Mat (KadiWeb) as an electronic laboratory notebook (ELN) combined with an integrated instrument database to facilitate Findable-Accessible-Interoperable-Reusabe (FAIR) research data. Facing transmission electron microscopy (TEM), focused ion beam (FIB), atom probe tomography (APT), or scanning electron microscopy (SEM) tasks, including sample preparation challenges, we developed a strategy to document the complex processes in our user facility KNMFi. To create appropriate records in Kadi4Mat we are comprising one central record for the material/sample to be investigated, a record for the sample preparation, a record for the investigation/experiment, and a record for the data evaluation. Therefore, a set of appropriate templates for the categories âsample preparation general,â âsample preparation for TEM,â âFocused Ion Beam and Scanning Electron Microscopy,â âTransmission Electron Microscopy,â âAtom Probe Tomography,â and âData Evaluationâ was developed in âatomistic units.â The templates can be combined easily and have been designed to be user-friendly, but at the same time requesting the relevant metadata in a structured and standardized way. The documentation process, including MaTeLiS-instrument database, is demonstrated in a use-case with several sample preparation steps and different investigation methods. The developed templates can be exported in JSON-format and might be used as models for other tasks. Â© 2024 The Author(s).","In this paper, we introduce our approach in using the web-based application Kadi4Mat (KadiWeb) as an electronic laboratory notebook (ELN) combined with an integrated instrument database to facilitate Findable-Accessible-Interoperable-Reusabe (FAIR) research data. Facing transmission electron microscopy (TEM), focused ion beam (FIB), atom probe tomography (APT), or scanning electron microscopy (SEM) tasks, including sample preparation challenges, we developed a strategy to document the complex processes in our user facility KNMFi. To create appropriate records in Kadi4Mat we are comprising one central record for the material/sample to be investigated, a record for the sample preparation, a record for the investigation/experiment, and a record for the data evaluation. Therefore, a set of appropriate templates for the categories sample preparation general, sample preparation for TEM, Focused Ion Beam and Scanning Electron Microscopy, Transmission Electron Microscopy, Atom Probe Tomography, and Data Evaluation was developed in atomistic units. The templates can be combined easily and have been designed to be user-friendly, but at the same time requesting the relevant metadata in a structured and standardized way. The documentation process, including MaTeLiS-instrument database, is demonstrated in a use-case with several sample preparation steps and different investigation methods. The developed templates can be exported in JSON-format and might be used as models for other tasks."
A Study on the Application of Data Mining Techniques in the Management of Sustainable Education for Employment,"With the gradual advancement of education management towards data and informationisation, how to establish a perfect employment education management system has become an important element of current student work. Data mining technology can extract valuable implicit information from the database, and is widely used in the practical processing of massive data. Based on the analysis of the characteristics of employment education management in universities, the study first improved the K-means algorithm by adding splitting and aggregation operations to it, used the improved K-means algorithm to cluster and analyse the employment education data, and then combined it with the optimised Apriori algorithm to further mine the useful information in the employment education data. The experimental outcomes demonstrate that the error rate of the improved K-means algorithm is stable at around 10%, with a high accuracy rate and strong stability; combined with the optimised Apriori algorithm applied to the employment education management system, the accuracy rate is basically maintained at over 96%, and the scores of studentsâ employment knowledge and employment practice are all over 90, indicating that the method can provide effective guidance for studentsâ employment and give a guideline for the sustainable education management of employment. Â© 2023 The Author(s).","With the gradual advancement of education management towards data and informationisation, how to establish a perfect employment education management system has become an important element of current student work. Data mining technology can extract valuable implicit information from the database, and is widely used in the practical processing of massive data. Based on the analysis of the characteristics of employment education management in universities, the study first improved the K-means algorithm by adding splitting and aggregation operations to it, used the improved K-means algorithm to cluster and analyse the employment education data, and then combined it with the optimised Apriori algorithm to further mine the useful information in the employment education data. The experimental outcomes demonstrate that the error rate of the improved K-means algorithm is stable at around 10%, with a high accuracy rate and strong stability; combined with the optimised Apriori algorithm applied to the employment education management system, the accuracy rate is basically maintained at over 96%, and the scores of students employment knowledge and employment practice are all over 90, indicating that the method can provide effective guidance for students employment and give a guideline for the sustainable education management of employment."
The FAIR Assessment Conundrum: Reflections on Tools and Metrics,"Several tools for assessing FAIRness have been developed. Although their purpose is common, they use different assessment techniques, they are designed to work with diverse research products, and they are applied in specific scientific disciplines. It is thus inevitable that they perform the assessment using different metrics. This paper provides an overview of the actual FAIR assessment tools and metrics landscape to highlight the challenges characterising this task. In particular, 20 relevant FAIR assessment tools and 1180 relevant metrics were identified and analysed concerning (i) the toolâs distinguishing aspects and their trends, (ii) the gaps between the metric intents and the FAIR principles, (iii) the discrepancies between the declared intent of the metrics and the actual aspects assessed, including the most recurring issues, (iv) the technologies used or mentioned the most in the assessment metrics. The findings highlight (a) the distinguishing characteristics of the tools and the emergence of trends over time concerning those characteristics, (b) the identification of gaps at both metric and tool levels, (c) discrepancies observed in 345 metrics between their declared intent and the actual aspects assessed, pointing at several recurring issues, and (d) the variety in the technology used for the assessments, the majority of which can be ascribed to linked data solutions. This work also highlights some open issues that FAIR assessment still needs to address. Â© 2024 The Author(s).","Several tools for assessing FAIRness have been developed. Although their purpose is common, they use different assessment techniques, they are designed to work with diverse research products, and they are applied in specific scientific disciplines. It is thus inevitable that they perform the assessment using different metrics. This paper provides an overview of the actual FAIR assessment tools and metrics landscape to highlight the challenges characterising this task. In particular, 20 relevant FAIR assessment tools and 1180 relevant metrics were identified and analysed concerning the tools distinguishing aspects and their trends, the gaps between the metric intents and the FAIR principles, the discrepancies between the declared intent of the metrics and the actual aspects assessed, including the most recurring issues, the technologies used or mentioned the most in the assessment metrics. The findings highlight (a) the distinguishing characteristics of the tools and the emergence of trends over time concerning those characteristics, (b) the identification of gaps at both metric and tool levels, discrepancies observed in 345 metrics between their declared intent and the actual aspects assessed, pointing at several recurring issues, and the variety in the technology used for the assessments, the majority of which can be ascribed to linked data solutions. This work also highlights some open issues that FAIR assessment still needs to address."
Detailed Implementation of a Reproducible Machine Learning-Enabled Workflow,"Machine learning (ML) and advanced computational methods are powerful tools for processing and deriving value from large data volumes. These methods are being developed and deployed rapidly, but best practices are still evolving regarding code and data standards, leading to irreproducibility of ML-enabled research. In this Practice Paper, we describe our efforts to make a ML-enabled research project to create a global inventory of biodata resources open and reproducible. To contribute to community conversations on evolving norms and expectations, we present our experiences as a practical, real-world case study that includes the implementation details as well as our overall approach and subsequent decisions. Our goal in openly sharing this experience is to provide a concrete example that others may consider as they look to vet, adapt, and adopt similar strategies to make their own work open and reproducible. Â© 2024 The Author(s).","Machine learning and advanced computational methods are powerful tools for processing and deriving value from large data volumes. These methods are being developed and deployed rapidly, but best practices are still evolving regarding code and data standards, leading to irreproducibility of ML-enabled research. In this Practice Paper, we describe our efforts to make a ML-enabled research project to create a global inventory of biodata resources open and reproducible. To contribute to community conversations on evolving norms and expectations, we present our experiences as a practical, real-world case study that includes the implementation details as well as our overall approach and subsequent decisions. Our goal in openly sharing this experience is to provide a concrete example that others may consider as they look to vet, adapt, and adopt similar strategies to make their own work open and reproducible."
Strategies in the Quality Assurance of Geomagnetic Observation Data in China,"Geomagnetic observation is an important approach to explore the variations of the Earthâs magnetic field. However, factors such as observation environment, observation facilities and observation equipment may affect the quality of observation data. This paper deeply analyzes the factors that may affect the quality of data in Chinaâs current geomagnetic observations, and discusses in detail the strategies we have implemented in this field. These strategies include the selection of observation sites, the protection and monitoring of the observation environment, the maintenance and improvement of observation facilities, the maintenance and monitoring of observation equipment, the analysis and processing of interference information, and effective measures in quality control. Â© 2024 The Author(s).","Geomagnetic observation is an important approach to explore the variations of the Earths magnetic field. However, factors such as observation environment, observation facilities and observation equipment may affect the quality of observation data. This paper deeply analyzes the factors that may affect the quality of data in Chinas current geomagnetic observations, and discusses in detail the strategies we have implemented in this field. These strategies include the selection of observation sites, the protection and monitoring of the observation environment, the maintenance and improvement of observation facilities, the maintenance and monitoring of observation equipment, the analysis and processing of interference information, and effective measures in quality control."
An Unsupervised Learning Approach to Evaluate Questionnaire Dataâ What One Can Learn from Violations of Measurement Invariance,"In several branches of the social sciences and humanities, surveys based on standardized questionnaires are a prominent research tool. While there are a variety of ways to analyze the data, some standard procedures have become established. When those surveys want to analyze differences in the answer patterns of different groups (e.g., countries, gender, age), these procedures can only be carried out in a meaningful way if there is measurement invariance; i.e., the measured construct has psychometric equivalence across groups. As recently raised as an open problem by Sauerwein et al. (2021), new evaluation methods that work in the absence of measurement invariance are needed. This paper promotes an unsupervised learning-based approach to such research data by proposing a procedure that works in three phases: data preparation, clustering of questionnaires, and measuring similarity based on the obtained clustering and the properties of each group. We generate synthetic data in three data sets, which allows us to compare our approach with the PCA approach under measurement invariance and under violated measurement invariance. As a main result, we obtain that the approach provides a natural comparison between groups and a natural description of the response patterns of the groups. Moreover, it can be safely applied to a wide variety of data sets, even in the absence of measurement invariance. Finally, this approach allows us to translate (violations of) measurement invariance into a meaningful measure of similarity. Â© 2024 The Author(s).","In several branches of the social sciences and humanities, surveys based on standardized questionnaires are a prominent research tool. While there are a variety of ways to analyze the data, some standard procedures have become established. When those surveys want to analyze differences in the answer patterns of different groups (, countries, gender, age), these procedures can only be carried out in a meaningful way if there is measurement invariance; , the measured construct has psychometric equivalence across groups. As recently raised as an open problem by Sauerwein et al. , new evaluation methods that work in the absence of measurement invariance are needed. This paper promotes an unsupervised learning-based approach to such research data by proposing a procedure that works in three phases: data preparation, clustering of questionnaires, and measuring similarity based on the obtained clustering and the properties of each group. We generate synthetic data in three data sets, which allows us to compare our approach with the PCA approach under measurement invariance and under violated measurement invariance. As a main result, we obtain that the approach provides a natural comparison between groups and a natural description of the response patterns of the groups. Moreover, it can be safely applied to a wide variety of data sets, even in the absence of measurement invariance. Finally, this approach allows us to translate (violations of) measurement invariance into a meaningful measure of similarity."
Data Sharing and Use in Cybersecurity Research,"Data sharing is crucial for strengthening research integrity and outcomes and for addressing complex problems. In cybersecurity research, data sharing can enable the development of new security measures, prediction of malicious attacks, and increased privacy. Understanding the landscape of data sharing and use in cybersecurity research can help to improve both the existing practices of data management and use and the outcomes of cybersecurity research. To this end, this study used methods of qualitative analysis and descriptive statistics to analyze 171 papers published between 2015 and 2019, their authorsâ characteristics, such as gender and professional title, and datasetsâ attributes, including their origin and public availability. The study found that more than half of the datasets in the sample (58%) and an even larger percentage of code in the papers (89%) were not publicly available. By offering an updated in-depth perspective on data practices in cybersecurity, including the role of authors, research methods, data sharing, and code availability, this study calls for the improvement of data management in cybersecurity research and for further collaboration in addressing the issues of cyberinfrastructure, policies, and citation and attribution standards in order to advance the quality and availability of data in this field. Â© 2024 The Author(s).","Data sharing is crucial for strengthening research integrity and outcomes and for addressing complex problems. In cybersecurity research, data sharing can enable the development of new security measures, prediction of malicious attacks, and increased privacy. Understanding the landscape of data sharing and use in cybersecurity research can help to improve both the existing practices of data management and use and the outcomes of cybersecurity research. To this end, this study used methods of qualitative analysis and descriptive statistics to analyze 171 papers published between 2015 and 2019, their authors characteristics, such as gender and professional title, and datasets attributes, including their origin and public availability. The study found that more than half of the datasets in the sample (58%) and an even larger percentage of code in the papers (89%) were not publicly available. By offering an updated in-depth perspective on data practices in cybersecurity, including the role of authors, research methods, data sharing, and code availability, this study calls for the improvement of data management in cybersecurity research and for further collaboration in addressing the issues of cyberinfrastructure, policies, and citation and attribution standards in order to advance the quality and availability of data in this field."
Time-Series Mining Approaches for Malaria Vector Prediction On Mid-Infrared Spectroscopy Data,"Malaria is an infectious disease caused by the Plasmodium parasite transmitted to humans by the bite of infected female Anopheles mosquitoes. The disease remains a major cause of child mortality globally and caused more than 600,000 deaths in 85 countries only in 2022, predominantly affecting the African region. Recent discussions point out that climate change is expanding the geographical distribution of mosquitoes, accelerating the malaria burst in areas free from outbreaks. Traditional vector control relies on chemical methods (e.g., insecticides), but effective control implementation requires accurate and cheap mosquito population monitoring and longevity estimates. This study investigates using mid-infrared spectroscopy (MIRS) data as input for efficient time-series classification methods to predict the species and age of malarial mosquitoes. Unlike previous studies using traditional machine learning, our comprehensive evaluation includes 14 algorithms from four time-series mining approaches, such as feature-based, interval-based, convolutional-based, and deep learning methods. These methods consider the particularities of time series, such as temporal dependencies and correlations between observations. Results indicate that the deep learning algorithm InceptionTime achieves 97% species identification accuracy and 83% age prediction accuracy, outperforming the traditional methods. This research contributes to the field by highlighting the effectiveness of time-series mining approaches for malaria vector control using spectroscopy. As malaria continues to pose a significant threat, these advancements contribute to developing innovative and efficient tools for malaria control strategies. Â© 2024 The Author(s).","Malaria is an infectious disease caused by the Plasmodium parasite transmitted to humans by the bite of infected female Anopheles mosquitoes. The disease remains a major cause of child mortality globally and caused more than 600,000 deaths in 85 countries only in 2022, predominantly affecting the African region. Recent discussions point out that climate change is expanding the geographical distribution of mosquitoes, accelerating the malaria burst in areas free from outbreaks. Traditional vector control relies on chemical methods (, insecticides), but effective control implementation requires accurate and cheap mosquito population monitoring and longevity estimates. This study investigates using mid-infrared spectroscopy (MIRS) data as input for efficient time-series classification methods to predict the species and age of malarial mosquitoes. Unlike previous studies using traditional machine learning, our comprehensive evaluation includes 14 algorithms from four time-series mining approaches, such as feature-based, interval-based, convolutional-based, and deep learning methods. These methods consider the particularities of time series, such as temporal dependencies and correlations between observations. Results indicate that the deep learning algorithm InceptionTime achieves 97% species identification accuracy and 83% age prediction accuracy, outperforming the traditional methods. This research contributes to the field by highlighting the effectiveness of time-series mining approaches for malaria vector control using spectroscopy. As malaria continues to pose a significant threat, these advancements contribute to developing innovative and efficient tools for malaria control strategies."
Earth Science Data Repositories: Implementing the CARE Principles,"Datasets carry cultural and political context at all parts of the data life cycle. Historically, Earth science data repositories have taken their guidance and policies as a combination of mandates from their funding agencies and the needs of their user communities, typically universities, agencies, and researchers. Consequently, repository practices have rarely taken into consideration the needs of other communities such as the Indigenous Peoples on whose lands data are often acquired. In recent years, a number of global efforts have worked to improve the conduct of research as well as data policy and practices by the repositories that hold and disseminate it. One of these established the CARE Principles for Indigenous Data Governance (Carroll et al. 2020), representing âCollective Benefitâ, âAuthority to Controlâ, âResponsibilityâ, and âEthicsââ hosted by the Global Indigenous Data Alliance (GIDA 2023a). In order to align to the CARE Principles, repositories may need to update their policies, architecture, service offerings, and their collaboration models. The question is how? Operationalizing principles into active repositories is generally a fraught process. This paper captures perspectives and recommendations from many of the repositories that are members of the Earth Science Information Partners (ESIPFed, n.d.) in conjunction with members of the Collaboratory for Indigenous Data Governance (Collaboratory for Indigenous Data Governance n.d.) and GIDA, defines and prioritizes the set of activities Earth and Environmental repositories can take to better adhere to CARE Principles in the hopes that this will help implementation in repositories globally. Â© 2024 The Author(s).","Datasets carry cultural and political context at all parts of the data life cycle. Historically, Earth science data repositories have taken their guidance and policies as a combination of mandates from their funding agencies and the needs of their user communities, typically universities, agencies, and researchers. Consequently, repository practices have rarely taken into consideration the needs of other communities such as the Indigenous Peoples on whose lands data are often acquired. In recent years, a number of global efforts have worked to improve the conduct of research as well as data policy and practices by the repositories that hold and disseminate it. One of these established the CARE Principles for Indigenous Data Governance (Carroll et al. 2020), representing Collective Benefit, Authority to Control, Responsibility, and Ethics hosted by the Global Indigenous Data Alliance (GIDA 2023a). In order to align to the CARE Principles, repositories may need to update their policies, architecture, service offerings, and their collaboration models. The question is how? Operationalizing principles into active repositories is generally a fraught process. This paper captures perspectives and recommendations from many of the repositories that are members of the Earth Science Information Partners (ESIPFed, ) in conjunction with members of the Collaboratory for Indigenous Data Governance (Collaboratory for Indigenous Data Governance ) and GIDA, defines and prioritizes the set of activities Earth and Environmental repositories can take to better adhere to CARE Principles in the hopes that this will help implementation in repositories globally."
Road to a Chemistry-Specific Data Management Plan,"In order to develop a discipline-specific data management plan (DMP) template, it is important to obtain information from researchers. For a chemistry-specific template, NFDI4Chem conducted a series of interviews with 27 participants and used data from the RDA WG Discipline-specific Guidance for DMP online survey. The interviews showed that the implementation of research data management in everyday work is a big challenge. Key findings from the interview series highlight challenges in implementing FAIR principles, with a focus on âFindabilityâ and âReusability.â The importance of linking physical samples and data in chemistry is emphasised, with discussions on storage, archiving, and the use of tools like electronic lab notebooks and repositories. However, documentation methods, software tools, and naming conventions commonly used in chemical research are also addressed. Overall, the study underscores the need for improved resources and strategies to enhance data management practices in the field of chemistry. All the gathered information and examples will be used to develop a DMP template in line with chemistry-specific requirements. The results provide a comprehensive outlook on the future developments of research data management (RDM) in chemistry. Â© 2024 The Author(s).","In order to develop a discipline-specific data management plan (DMP) template, it is important to obtain information from researchers. For a chemistry-specific template, NFDI4Chem conducted a series of interviews with 27 participants and used data from the RDA WG Discipline-specific Guidance for DMP online survey. The interviews showed that the implementation of research data management in everyday work is a big challenge. Key findings from the interview series highlight challenges in implementing FAIR principles, with a focus on Findability and Reusability. The importance of linking physical samples and data in chemistry is emphasised, with discussions on storage, archiving, and the use of tools like electronic lab notebooks and repositories. However, documentation methods, software tools, and naming conventions commonly used in chemical research are also addressed. Overall, the study underscores the need for improved resources and strategies to enhance data management practices in the field of chemistry. All the gathered information and examples will be used to develop a DMP template in line with chemistry-specific requirements. The results provide a comprehensive outlook on the future developments of research data management (RDM) in chemistry."
The Launch of the Data Science Journal in 2002,"The Committee on Data for Science and Technology of the International Council of Scientific Unions (CODATA) decided in February, 2001 to publish a new on-line peer reviewed journal, freely available to all. It would be called the Data Science Journal. The subsequent steps taken to launch this journal are described, leading to the publication of the first issue of 10 papers in April 2002 and of the first volume of 19 papers by December 2002. Some necessary corrections and improvements before the publication of a second volume of 19 papers in 2003 are also described. Â© 2023 The Author(s).","The Committee on Data for Science and Technology of the International Council of Scientific Unions (CODATA) decided in February, 2001 to publish a new on-line peer reviewed journal, freely available to all. It would be called the Data Science Journal. The subsequent steps taken to launch this journal are described, leading to the publication of the first issue of 10 papers in April 2002 and of the first volume of 19 papers by December 2002. Some necessary corrections and improvements before the publication of a second volume of 19 papers in 2003 are also described."
The Role of Open Science and Geoinformatics in Advancing Sustainable Development Goals in Africa: A Strategic Framework and Action Plan,"Open Science (OS) encourages research accessibility, openness, reusability and reproducibility, which are all vital for the growth of the scientific community worldwide. Open data, in general, and remote sensing/geospatial data, in particular, offer significant economic and societal benefits that can be of great contribution to achieving the African Agenda 2063. This article focuses on examining the importance and contribution of OS and Geoinformatics to accomplish the African Agenda 2063 and Sustainable Development Goals (SDGs) in Africa, and how OS principles can enhance accessibility, collaboration, reproducibility, and innovation in geosciences. It also explores the potential of artificial intelligence on augmenting OS and Geoinformatics and proposes solutions to address the implementation challenges. It also evaluates the relationships, international financing, and cooperative efforts required to fully realize the potential of open research and geoinformatics in Africa compared to developed countries. The results revealed that the promotion of OS aims to make geospatial data and tools more accessible to researchers and policymakers, addressing critical challenges such as climate change and crisis management. The results highlighted the importance of international collaboration through the African Open Science Platform. By presenting a comprehensive framework and an action plan, this study offers a roadmap for African governments to leverage OS and Geoinformatics to advance Agenda-2063 and the SDGs, bridging the gap between theoretical knowledge and practical application. Â© 2024 The Author(s).","Open Science (OS) encourages research accessibility, openness, reusability and reproducibility, which are all vital for the growth of the scientific community worldwide. Open data, in general, and remote sensing/geospatial data, in particular, offer significant economic and societal benefits that can be of great contribution to achieving the African Agenda 2063. This article focuses on examining the importance and contribution of OS and Geoinformatics to accomplish the African Agenda 2063 and Sustainable Development Goals (SDGs) in Africa, and how OS principles can enhance accessibility, collaboration, reproducibility, and innovation in geosciences. It also explores the potential of artificial intelligence on augmenting OS and Geoinformatics and proposes solutions to address the implementation challenges. It also evaluates the relationships, international financing, and cooperative efforts required to fully realize the potential of open research and geoinformatics in Africa compared to developed countries. The results revealed that the promotion of OS aims to make geospatial data and tools more accessible to researchers and policymakers, addressing critical challenges such as climate change and crisis management. The results highlighted the importance of international collaboration through the African Open Science Platform. By presenting a comprehensive framework and an action plan, this study offers a roadmap for African governments to leverage OS and Geoinformatics to advance Agenda-2063 and the SDGs, bridging the gap between theoretical knowledge and practical application."
Assessment of Personal Values for Data-Driven Human Resource Management,"Business organizations have introduced data analytics to human resource management (HRM) to predict employee behavior in recent years. This practice is called HR analytics, and uses the various properties of employees, including their personal values, as inputs. Personal values are conceptualized as stable personal traits that represent the relevant employee. Previous studies in the area have shown that the personal values of employees influence their attitudes and behaviors as well as the performance of their group. Assessing these values is thus important in HRM. Although most previous studies have measured personal values based on self-reported questionnaires, this method encounters many problems, such as the social desirability bias. Accordingly, more recent studies have proposed alternative methods that apply machine learning algorithms to linguistic or visual data. This study reviews research on personal values by focusing on the methods used to measure them, and discusses the usefulness of and the challenges to the measurement of personal values. We also discuss future directions of research on the assessment of personal values. Â© 2023 The Author(s).","Business organizations have introduced data analytics to human resource management (HRM) to predict employee behavior in recent years. This practice is called HR analytics, and uses the various properties of employees, including their personal values, as inputs. Personal values are conceptualized as stable personal traits that represent the relevant employee. Previous studies in the area have shown that the personal values of employees influence their attitudes and behaviors as well as the performance of their group. Assessing these values is thus important in HRM. Although most previous studies have measured personal values based on self-reported questionnaires, this method encounters many problems, such as the social desirability bias. Accordingly, more recent studies have proposed alternative methods that apply machine learning algorithms to linguistic or visual data. This study reviews research on personal values by focusing on the methods used to measure them, and discusses the usefulness of and the challenges to the measurement of personal values. We also discuss future directions of research on the assessment of personal values."
Advancing Global Resilience Through Open Data Resources and Services in Disaster Risk Reduction,"Open Science, particularly open data and services, is critical for achieving global goals on sustainable development and disaster risk reduction. Through selected reports, this paper summarizes the 2023 International Data Week session on âOpen Data and Open Services for Disaster Risk Reduction.â Key highlights identified the importance of advancing global resilience in data preparedness throughout crises, adopting robust digital technologies, and implementing trustworthy and interoperable data infrastructures. The session also underscored the necessity for community-centered approaches to address data challenges and mitigate the impacts of disasters. Coordinated efforts are needed at the local, national, regional, and international levels to depict the crisis data landscape, utilize advanced technologies, and co-build robust Open Science Infrastructures that prioritize interconnectivity, cross-sectoral interoperability and intelligibility for open data and open service delivery in times of crisis. Â© 2024 The Author(s).","Open Science, particularly open data and services, is critical for achieving global goals on sustainable development and disaster risk reduction. Through selected reports, this paper summarizes the 2023 International Data Week session on Open Data and Open Services for Disaster Risk Reduction. Key highlights identified the importance of advancing global resilience in data preparedness throughout crises, adopting robust digital technologies, and implementing trustworthy and interoperable data infrastructures. The session also underscored the necessity for community-centered approaches to address data challenges and mitigate the impacts of disasters. Coordinated efforts are needed at the local, national, regional, and international levels to depict the crisis data landscape, utilize advanced technologies, and co-build robust Open Science Infrastructures that prioritize interconnectivity, cross-sectoral interoperability and intelligibility for open data and open service delivery in times of crisis."
Implementation of a Federated Information System by Means of Reuse of Research Data Archived in Research Data Repositories,"At universities, research data is increasingly stored in research data repositories according to a data management plan (DMP) and thus made available for further use. The challenge of reusing hundreds, thousands, or millions of data sets is to obtain an overview of the data in a short period of time and to search through all the data. The high variability of the formats used to store research data requires a new approach to data reusability that focuses on the visualisation and searchability of archived research data, which can also be combined with each other. In this article, we present a practical DMP that describes how information systems can be created on demand by reusing research data archived in research data repositories and how these systems can be merged into a federated information system. As a result, in our projects, information systems have been created in minutes or a couple of hours with few resources. The initial effort to create a federated system remains; however, this allows federated searches to be performed. Extending a federated system to include other information systems can then be accomplished by making a few configurations and manageable adjustments to the source code. Â© 2023 The Author(s).","At universities, research data is increasingly stored in research data repositories according to a data management plan (DMP) and thus made available for further use. The challenge of reusing hundreds, thousands, or millions of data sets is to obtain an overview of the data in a short period of time and to search through all the data. The high variability of the formats used to store research data requires a new approach to data reusability that focuses on the visualisation and searchability of archived research data, which can also be combined with each other. In this article, we present a practical DMP that describes how information systems can be created on demand by reusing research data archived in research data repositories and how these systems can be merged into a federated information system. As a result, in our projects, information systems have been created in minutes or a couple of hours with few resources. The initial effort to create a federated system remains; however, this allows federated searches to be performed. Extending a federated system to include other information systems can then be accomplished by making a few configurations and manageable adjustments to the source code."
Semantic Schema Extraction in NoSQL Databases using BERT Embeddings,"NoSQL databases, valued for flexibility and scalability, pose analytics challenges due to their schema-less nature. Automatic schema extraction is crucial, with existing techniques limited in handling nested structures. Leveraging Natural Language Processing (NLP) advancements, this paper introduces a novel BERT Embeddings-Based approach for extracting schemas from NoSQL databases. The method analyzes semantic relationships within triplets from JSON documents through four stages: triplet extraction, preprocessing, BERT Embedding generation, and similarity analysis. Evaluation on real datasets demonstrates over 83% accuracy in extracting valid nested schema components. The study reveals interdisciplinary intersections, using NLP to unveil structures in scenarios lacking explicit schemas, showcasing significant potential for autonomous schema extraction from raw, unstructured data formats. Â© 2024 The Author(s).","NoSQL databases, valued for flexibility and scalability, pose analytics challenges due to their schema-less nature. Automatic schema extraction is crucial, with existing techniques limited in handling nested structures. Leveraging Natural Language Processing (NLP) advancements, this paper introduces a novel BERT Embeddings-Based approach for extracting schemas from NoSQL databases. The method analyzes semantic relationships within triplets from JSON documents through four stages: triplet extraction, preprocessing, BERT Embedding generation, and similarity analysis. Evaluation on real datasets demonstrates over 83% accuracy in extracting valid nested schema components. The study reveals interdisciplinary intersections, using NLP to unveil structures in scenarios lacking explicit schemas, showcasing significant potential for autonomous schema extraction from raw, unstructured data formats."
Centering Relationality and CARE for Stewardship of Indigenous Research Data,"The CARE Principles for Indigenous Data Governance are a seminal advance in the stewardship of Indigenous data. The Data Services for Indigenous Scholarship and Sovereignty (DSISS) project is working to guide how research libraries and data repositories can apply the CARE principles to support scholars of Indigenous culture and language. Building on a set of foundational case studies of Indigenous scholarship, this paper reports on analysis of formal engagement activities with scholars, Indigenous community members, and information and data professionals. We discuss three prominent themesâownership, trust, and relational accountabilityâand their implications for concrete steps toward implementation of the CARE principles in research data services (RDS). The results show that sustaining and furthering Indigenous scholarship and data sovereignty in alignment with CARE requires infrastructure and services that attend to a mix of interrelated, and potentially divergent, interests of scholars, Indigenous communities, and institutions. RDS professionals need to build expertise in Indigenous research methods and the sensitivities and distinctiveness inherent in Indigenous ways of knowing. Stewarding institutions will need to make significant investments in restoring trust as genuine extensions of relational accountability. Â© 2024 The Author(s).","The CARE Principles for Indigenous Data Governance are a seminal advance in the stewardship of Indigenous data. The Data Services for Indigenous Scholarship and Sovereignty (DSISS) project is working to guide how research libraries and data repositories can apply the CARE principles to support scholars of Indigenous culture and language. Building on a set of foundational case studies of Indigenous scholarship, this paper reports on analysis of formal engagement activities with scholars, Indigenous community members, and information and data professionals. We discuss three prominent themesownership, trust, and relational accountabilityand their implications for concrete steps toward implementation of the CARE principles in research data services (RDS). The results show that sustaining and furthering Indigenous scholarship and data sovereignty in alignment with CARE requires infrastructure and services that attend to a mix of interrelated, and potentially divergent, interests of scholars, Indigenous communities, and institutions. RDS professionals need to build expertise in Indigenous research methods and the sensitivities and distinctiveness inherent in Indigenous ways of knowing. Stewarding institutions will need to make significant investments in restoring trust as genuine extensions of relational accountability."
FAIRness Along the Machine Learning Lifecycle Using Dataverse in Combination with MLflow,"Typical Machine Learning (ML) approaches are characterized by their iterative and exploratory nature: continuously refining and adapting not only code but also ML models to optimize the results and the performance on new data. This poses novel challenges related to keeping the trained model Findable, Accessible, Interoperable and Reusable (FAIR), especially for the automation of the entire machine learning lifecycle within the concept of Machine Learning Operations (MLOps). The article introduces a comprehensive integration of a data repository (based on the software Dataverse) and an ML platform (based on the MLflow framework) that enables seamless sharing and publishing of data, experiments and models, ensuring FAIRness. The presented solution is evaluated using an ML use case scenario with model training, hyper-parameter optimization, and model sharing via the data platform. Â© 2024 The Author(s).","Typical Machine Learning approaches are characterized by their iterative and exploratory nature: continuously refining and adapting not only code but also ML models to optimize the results and the performance on new data. This poses novel challenges related to keeping the trained model Findable, Accessible, Interoperable and Reusable (FAIR), especially for the automation of the entire machine learning lifecycle within the concept of Machine Learning Operations (MLOps). The article introduces a comprehensive integration of a data repository (based on the software Dataverse) and an ML platform (based on the MLflow framework) that enables seamless sharing and publishing of data, experiments and models, ensuring FAIRness. The presented solution is evaluated using an ML use case scenario with model training, hyper-parameter optimization, and model sharing via the data platform."
RDM+PM Checklist: Towards a Measure of Your Institutionâs Preparedness for the Effective Planning of Research Data Management,"A review at our institution and a number of other Australian universities was conducted to identify an optimal institutional-wide approach to Research Data Management (RDM). We found, with a few notable exceptions, a lack of clear policies and processes across institutes and no harmonisation in the approaches taken. We identified limited methods in place to cater for the development of Research Data Management Plans (RDMPs) across different disciplines, project types and no identifiable business intelligence (BI) for auditing or oversight. When interviewed, many researchers were not aware of their institutionâs RDM policy, whilst others did not understand how it was relevant to their research. It was also discovered that primary materials (PM), which are often directly linked to the effective management of research data, were not well covered. Additionally, it was unclear in understanding who was the data custodian responsible for overall oversight, and there was a lack of clear guidance on the roles and responsibilities of researchers and their supervisors. These findings indicate that institutions are at risk in terms of meeting regulatory requirements and managing data effectively and safely. In this paper, we outline an alternative approach focusing on RDM âPlanningâ rather than on RDMPs themselves. We developed simple-to-understand guidance for researchers on the redeveloped RDM policy, which was implemented via an online âRDM+PM Checklistâ tool that guides researchers and students. Moreover, as it is a structured tool, it provides real-time business intelligence that can be used to measure how compliant the organisation is and ideally identify opportunities for continuous improvement. Â© 2023 The Author(s).","A review at our institution and a number of other Australian universities was conducted to identify an optimal institutional-wide approach to Research Data Management (RDM). We found, with a few notable exceptions, a lack of clear policies and processes across institutes and no harmonisation in the approaches taken. We identified limited methods in place to cater for the development of Research Data Management Plans (RDMPs) across different disciplines, project types and no identifiable business intelligence (BI) for auditing or oversight. When interviewed, many researchers were not aware of their institutions RDM policy, whilst others did not understand how it was relevant to their research. It was also discovered that primary materials (PM), which are often directly linked to the effective management of research data, were not well covered. Additionally, it was unclear in understanding who was the data custodian responsible for overall oversight, and there was a lack of clear guidance on the roles and responsibilities of researchers and their supervisors. These findings indicate that institutions are at risk in terms of meeting regulatory requirements and managing data effectively and safely. In this paper, we outline an alternative approach focusing on RDM Planning rather than on RDMPs themselves. We developed simple-to-understand guidance for researchers on the redeveloped RDM policy, which was implemented via an online RDM+PM Checklist tool that guides researchers and students. Moreover, as it is a structured tool, it provides real-time business intelligence that can be used to measure how compliant the organisation is and ideally identify opportunities for continuous improvement."
Are Researchers Citing Their Data? A Case Study from The U.S. Geological Survey,"Data citation promotes accessibility and discoverability of data through measures carried out by researchers, publishers, repositories, and the scientific community. This paper examines how a data citation workflow has been implemented by the U.S. Geological Survey (USGS) by evaluating publication and data linkages. Two different methods were used to identify data citations: examining publication structural metadata and examining the full text of the publication. A growing number of USGS researchers are complying with publisher data sharing policies aimed to capture data citation information in a standardized way within associated publications. However, inconsistencies in how data citation information is documented in publications has limited the accessibility and discoverability of the data. This paper demonstrates how organizational evaluations of publication and data linkages can be used to identify obstacles in advancing data citation efforts and improve data citation workflows. Â© 2024 The Author(s).","Data citation promotes accessibility and discoverability of data through measures carried out by researchers, publishers, repositories, and the scientific community. This paper examines how a data citation workflow has been implemented by the Geological Survey (USGS) by evaluating publication and data linkages. Two different methods were used to identify data citations: examining publication structural metadata and examining the full text of the publication. A growing number of USGS researchers are complying with publisher data sharing policies aimed to capture data citation information in a standardized way within associated publications. However, inconsistencies in how data citation information is documented in publications has limited the accessibility and discoverability of the data. This paper demonstrates how organizational evaluations of publication and data linkages can be used to identify obstacles in advancing data citation efforts and improve data citation workflows."
Unrestricted Versus Regulated Open Data Governance: A Bibliometric Comparison of SARS-CoV-2 Nucleotide Sequence Databases,"Two distinct modes of data governance have emerged in accessing and reusing viral data pertaining to COVID-19: an unrestricted model, espoused by data repositories part of the International Nucleotide Sequence Database Collaboration and a regulated model promoted by the Global Initiative on Sharing All Influenza data. In this paper, we focus on publications mentioning either infrastructure in the period between January 2020 and January 2023, thus capturing a period of acute response to the COVID-19 pandemic. Through a variety of bibliometric and network science methods, we compare the extent to which either data infrastructure facilitated collaboration from different countries around the globe to understand how data reuse can enhance forms of diversity between institutions, countries, and funding groups. Our findings reveal disparities in representation and usage between the two data infrastructures. We conclude that both approaches offer useful lessons, with the unrestricted model providing insights into complex data linkage and the regulated model demonstrating the importance of global representation. Â© 2024 The Author(s).","Two distinct modes of data governance have emerged in accessing and reusing viral data pertaining to COVID-19: an unrestricted model, espoused by data repositories part of the International Nucleotide Sequence Database Collaboration and a regulated model promoted by the Global Initiative on Sharing All Influenza data. In this paper, we focus on publications mentioning either infrastructure in the period between January 2020 and January 2023, thus capturing a period of acute response to the COVID-19 pandemic. Through a variety of bibliometric and network science methods, we compare the extent to which either data infrastructure facilitated collaboration from different countries around the globe to understand how data reuse can enhance forms of diversity between institutions, countries, and funding groups. Our findings reveal disparities in representation and usage between the two data infrastructures. We conclude that both approaches offer useful lessons, with the unrestricted model providing insights into complex data linkage and the regulated model demonstrating the importance of global representation."
How and Why Do Researchers Reference Data? A Study of Rhetorical Features and Functions of Data References in Academic Articles,"Data reuse is a common practice in the social sciences. While published data play an essential role in the production of social science research, they are not consistently cited, which makes it difficult to assess their full scholarly impact and give credit to the original data producers. Furthermore, it can be challenging to understand researchersâ motivations for referencing data. Like references to academic literature, data references perform various rhetorical functions, such as paying homage, signaling disagreement, or drawing comparisons. This paper studies how and why researchers reference social science data in their academic writing. We develop a typology to model relationships between the entities that anchor data references, along with their features (access, actions, locations, styles, types) and functions (critique, describe, illustrate, interact, legitimize). We illustrate the use of the typology by coding multidisciplinary research articles (n = 30) referencing social science data archived at the Inter-university Consortium for Political and Social Research (ICPSR). We show how our typology captures researchersâ interactions with data and purposes for referencing data. Our typology provides a systematic way to document and analyze researchersâ narratives about data use, extending our ability to give credit to data that support research. Â© 2023 The Author(s).","Data reuse is a common practice in the social sciences. While published data play an essential role in the production of social science research, they are not consistently cited, which makes it difficult to assess their full scholarly impact and give credit to the original data producers. Furthermore, it can be challenging to understand researchers motivations for referencing data. Like references to academic literature, data references perform various rhetorical functions, such as paying homage, signaling disagreement, or drawing comparisons. This paper studies how and why researchers reference social science data in their academic writing. We develop a typology to model relationships between the entities that anchor data references, along with their features (access, actions, locations, styles, types) and functions (critique, describe, illustrate, interact, legitimize). We illustrate the use of the typology by coding multidisciplinary research articles (n = 30) referencing social science data archived at the Inter-university Consortium for Political and Social Research (ICPSR). We show how our typology captures researchers interactions with data and purposes for referencing data. Our typology provides a systematic way to document and analyze researchers narratives about data use, extending our ability to give credit to data that support research."
Two Journals and a Pandemic: Reflections on Being a Data Science Editor-in-Chief,"This essay is a collection of reflections from my experiences in the past few years, first as editor-in-chief (EiC) of the Data Science Journal and then launching Patterns as its first EiC during the global COVID-19 pandemic. I discuss what I learned and how I worked with these journals, and I close with some hopes for the next 20 years of data science. Â© 2023 The Author(s).","This essay is a collection of reflections from my experiences in the past few years, first as editor-in-chief (EiC) of the Data Science Journal and then launching Patterns as its first EiC during the global COVID-19 pandemic. I discuss what I learned and how I worked with these journals, and I close with some hopes for the next 20 years of data science."
Towards a Toolbox for Automated Assessment of Machine-Actionable Data Management Plans,"Most research funders require Data Management Plans (DMPs). The review process can be time consuming, since reviewers read text documents submitted by researchers and provide their feedback. Moreover, it requires specific expert knowledge in data stewardship, which is scarce. Machine-actionable Data Management Plans (maDMPs) and semantic technologies increase the potential for automatic assessment of information contained in DMPs. However, the level of automation and new possibilities are still not well-explored and leveraged. This paper discusses methods for the automation of DMP assessment. It goes beyond generating human-readable reports. It explores how the information contained in maDMPs can be used to provide automated pre-assessment or to fetch further information, allowing reviewers to better judge the content. We map the identified methods to various reviewer goals. Â© 2023 The Author(s).","Most research funders require Data Management Plans (DMPs). The review process can be time consuming, since reviewers read text documents submitted by researchers and provide their feedback. Moreover, it requires specific expert knowledge in data stewardship, which is scarce. Machine-actionable Data Management Plans (maDMPs) and semantic technologies increase the potential for automatic assessment of information contained in DMPs. However, the level of automation and new possibilities are still not well-explored and leveraged. This paper discusses methods for the automation of DMP assessment. It goes beyond generating human-readable reports. It explores how the information contained in maDMPs can be used to provide automated pre-assessment or to fetch further information, allowing reviewers to better judge the content. We map the identified methods to various reviewer goals."
Data Management Plans for the Photon and Neutron Communities,"This paper presents the interdisciplinary work undertaken by six photon and neutron research institutes to develop a common approach and implementation to data management plans in the context of the Photon and Neutron Open Science Cloud project. The paper introduces the context of the project and then describes the approach based on the concept of active data management plans and a common template of questions. The comparison of and choice of a common tool, the Data Stewardship Wizard, is then described. Finally, the deployment of the tool and its integration in the different facilitiesâ data workflows is described in more detail. The paper concludes with a summary of the achievements, the lessons learned, and which issues still need to be addressed. Â© 2023 The Author(s).","This paper presents the interdisciplinary work undertaken by six photon and neutron research institutes to develop a common approach and implementation to data management plans in the context of the Photon and Neutron Open Science Cloud project. The paper introduces the context of the project and then describes the approach based on the concept of active data management plans and a common template of questions. The comparison of and choice of a common tool, the Data Stewardship Wizard, is then described. Finally, the deployment of the tool and its integration in the different facilities data workflows is described in more detail. The paper concludes with a summary of the achievements, the lessons learned, and which issues still need to be addressed."
Development and Governance of FAIR Thresholds for a Data Federation,"The FAIR (findable, accessible, interoperable, and re-usable) principles and practice recommendations provide high level guidance and recommendations that are not research-domain specific in nature. There remains a gap in practice at the data provider and domain scientist level demonstrating how the FAIR principles can be applied beyond a set of generalist guidelines to meet the needs of a specific domain community. We present our insights developing FAIR thresholds in a domain specific context for self-governance by a community (agricultural research). âMinimum thresholdsâ for FAIR data are required to align expectations for data delivered from providersâ distributed data stores through a community-governed federation (the Agricultural Research Federation, AgReFed). Data providers were supported to make data holdings more FAIR. There was a range of different FAIR starting points, organisational goals, and end user needs, solutions, and capabilities. This informed the distilling of a set of FAIR criteria ranging from âMinimum thresholdsâ to âStretch targetsâ. These were operationalised through consensus into a framework for governance and implementation by the agricultural research domain community. Improving the FAIR maturity of data took resourcing and incentive to do so, highlighting the challenge for data federations to generate value whilst reducing costs of participation. Our experience showed a role for supporting collective advocacy, relationship brokering, tailored support, and low-bar tooling access particularly across the areas of data structure, access and semantics that were challenging to domain researchers. Active democratic participation supported by a governance framework like AgReFedâs will ensure participants have a say in how federations can deliver individual and collective benefits for members. Â© 2022 The Author(s).","The FAIR (findable, accessible, interoperable, and re-usable) principles and practice recommendations provide high level guidance and recommendations that are not research-domain specific in nature. There remains a gap in practice at the data provider and domain scientist level demonstrating how the FAIR principles can be applied beyond a set of generalist guidelines to meet the needs of a specific domain community. We present our insights developing FAIR thresholds in a domain specific context for self-governance by a community (agricultural research). Minimum thresholds for FAIR data are required to align expectations for data delivered from providers distributed data stores through a community-governed federation (the Agricultural Research Federation, AgReFed). Data providers were supported to make data holdings more FAIR. There was a range of different FAIR starting points, organisational goals, and end user needs, solutions, and capabilities. This informed the distilling of a set of FAIR criteria ranging from Minimum thresholds to Stretch targets. These were operationalised through consensus into a framework for governance and implementation by the agricultural research domain community. Improving the FAIR maturity of data took resourcing and incentive to do so, highlighting the challenge for data federations to generate value whilst reducing costs of participation. Our experience showed a role for supporting collective advocacy, relationship brokering, tailored support, and low-bar tooling access particularly across the areas of data structure, access and semantics that were challenging to domain researchers. Active democratic participation supported by a governance framework like AgReFeds will ensure participants have a say in how federations can deliver individual and collective benefits for members."
Impact of the protein data bank across scientific disciplines,"The Protein Data Bank archive (PDB) was established in 1971 as the 1st open access digital data resource for biology and medicine. Today, the PDB contains >160,000 atomic-level, experimentally-determined 3D biomolecular structures. PDB data are freely and publicly available for download, without restrictions. Each entry contains summary information about the structure and experiment, atomic coordinates, and in most cases, a citation to a corresponding scientific publication. Individually and in bulk, PDB structures can be downloaded and/or analyzed and visualized online using tools at RCSB.org. As such, it is challenging to understand and monitor reuse of data. Citations of the scientific publications describing PDB structures provide one way of understanding which structures are being used, and in which research areas. Our analysis highlights frequently-cited structures and identifies milestone structures that have demonstrated impact across scientific fields. Â© 2020 The Author(s).","The Protein Data Bank archive (PDB) was established in 1971 as the 1st open access digital data resource for biology and medicine. Today, the PDB contains >160,000 atomic-level, experimentally-determined 3D biomolecular structures. PDB data are freely and publicly available for download, without restrictions. Each entry contains summary information about the structure and experiment, atomic coordinates, and in most cases, a citation to a corresponding scientific publication. Individually and in bulk, PDB structures can be downloaded and/or analyzed and visualized online using tools at RCSB.org. As such, it is challenging to understand and monitor reuse of data. Citations of the scientific publications describing PDB structures provide one way of understanding which structures are being used, and in which research areas. Our analysis highlights frequently-cited structures and identifies milestone structures that have demonstrated impact across scientific fields."
Persistent identification of instrument,"Instruments play an essential role in creating research data. Given the importance of instruments and associated metadata to the assessment of data quality and data reuse, globally unique, persistent and resolvable identification of instruments is crucial. The Research Data Alliance Working Group Persistent Identification of Instruments (PIDINST) developed a community-driven solution for persistent identification of instruments which we present and discuss in this paper. Based on an analysis of 10 use cases, PIDINST developed a metadata schema and prototyped schema implementation with DataCite and ePIC as representative persistent identifier infrastructures and with HZB (Helmholtz-Zentrum Berlin fÃ¼r Materialien und Energie) and BODC (British Oceanographic Data Centre) as representative institutional instrument providers. These implementations demonstrate the viability of the proposed solution in practice. Moving forward, PIDINST will further catalyse adoption and consolidate the schema by addressing new stakeholder requirements. Â© 2020 The Author(s).","Instruments play an essential role in creating research data. Given the importance of instruments and associated metadata to the assessment of data quality and data reuse, globally unique, persistent and resolvable identification of instruments is crucial. The Research Data Alliance Working Group Persistent Identification of Instruments (PIDINST) developed a community-driven solution for persistent identification of instruments which we present and discuss in this paper. Based on an analysis of 10 use cases, PIDINST developed a metadata schema and prototyped schema implementation with DataCite and ePIC as representative persistent identifier infrastructures and with HZB (Helmholtz-Zentrum Berlin fr Materialien und Energie) and BODC (British Oceanographic Data Centre) as representative institutional instrument providers. These implementations demonstrate the viability of the proposed solution in practice. Moving forward, PIDINST will further catalyse adoption and consolidate the schema by addressing new stakeholder requirements."
Progress in activities of WDS-China data centers,"The World Data System (WDS) plays an important role in promoting global scientific data man-agement, exchange, and sharing. There are 8 WDS data centers in mainland China including study areas of astronomy, space science, global change, renewable resources and environmental, cold and dry regions, microbiology, geophysics, and the ocean. This paper summarizes the current status of the WDS China data centers, along with their major progress in recent years. This progress includes a clearinghouse for metadata exchange, research data archival, historical data saving, international data exchange, data publishing models, CoreTrustSeal certification, open repositories for the scientific community, science popularization services, international training workshops, and awards. We conclude with a discussion of the opportunities and challenges faced by WDS China data centers. Â© 2020 The Author(s).","The World Data System (WDS) plays an important role in promoting global scientific data man-agement, exchange, and sharing. There are 8 WDS data centers in mainland China including study areas of astronomy, space science, global change, renewable resources and environmental, cold and dry regions, microbiology, geophysics, and the ocean. This paper summarizes the current status of the WDS China data centers, along with their major progress in recent years. This progress includes a clearinghouse for metadata exchange, research data archival, historical data saving, international data exchange, data publishing models, CoreTrustSeal certification, open repositories for the scientific community, science popularization services, international training workshops, and awards. We conclude with a discussion of the opportunities and challenges faced by WDS China data centers."
A survey of researchersâ needs and priorities for data sharing,"One of the ways in which the publisher PLOS supports open science is via a stringent data availability policy established in 2014. Despite this policy, and more data sharing policies being introduced by other organizations, best practices for data sharing are adopted by a minority of researchers in their publications. Problems with effective research data sharing persist and these problems have been quantified by previous research as a lack of time, resources, incentives, and/or skills to share data. In this study we built on this research by investigating the importance of tasks associated with data sharing, and researchersâ satisfaction with their ability to complete these tasks. By investigating these factors we aimed to better understand opportunities for new or improved solutions for sharing data. In May-June 2020 we surveyed researchers from Europe and North America to rate tasks associated with data sharing on (i) their importance and (ii) their satisfaction with their ability to complete them. We received 617 completed responses. We calculated mean importance and satisfaction scores to highlight potential opportunities for new solutions to and compare different cohorts. Tasks relating to research impact, funder compliance, and credit had the highest importance scores. 52% of respondents reuse research data but the average satisfaction score for obtaining data for reuse was relatively low. Tasks associated with sharing data were rated somewhat important and respondents were reasonably well satisfied in their ability to accomplish them. Notably, this included tasks associated with best data sharing practice, such as use of data repositories. However, the most common method for sharing data was in fact via supplemental files with articles, which is not considered to be best practice. We presume that researchers are unlikely to seek new solutions to a problem or task that they are satisfied in their ability to accomplish, even if many do not attempt this task. This implies there are few opportunities for new solutions or tools to meet these researcher needs. Publishers can likely meet these needs for data sharing by working to seamlessly integrate existing solutions that reduce the effort or behaviour change involved in some tasks, and focusing on advocacy and education around the benefits of sharing data. There may however be opportunities â unmet researcher needs â in relation to better supporting data reuse, which could be met in part by strengthening data sharing policies of journals and publishers, and improving the discoverability of data associated with published articles. Â© 2021 The Author(s).","One of the ways in which the publisher PLOS supports open science is via a stringent data availability policy established in 2014. Despite this policy, and more data sharing policies being introduced by other organizations, best practices for data sharing are adopted by a minority of researchers in their publications. Problems with effective research data sharing persist and these problems have been quantified by previous research as a lack of time, resources, incentives, and/or skills to share data. In this study we built on this research by investigating the importance of tasks associated with data sharing, and researchers satisfaction with their ability to complete these tasks. By investigating these factors we aimed to better understand opportunities for new or improved solutions for sharing data. In May-June 2020 we surveyed researchers from Europe and North America to rate tasks associated with data sharing on their importance and their satisfaction with their ability to complete them. We received 617 completed responses. We calculated mean importance and satisfaction scores to highlight potential opportunities for new solutions to and compare different cohorts. Tasks relating to research impact, funder compliance, and credit had the highest importance scores. 52% of respondents reuse research data but the average satisfaction score for obtaining data for reuse was relatively low. Tasks associated with sharing data were rated somewhat important and respondents were reasonably well satisfied in their ability to accomplish them. Notably, this included tasks associated with best data sharing practice, such as use of data repositories. However, the most common method for sharing data was in fact via supplemental files with articles, which is not considered to be best practice. We presume that researchers are unlikely to seek new solutions to a problem or task that they are satisfied in their ability to accomplish, even if many do not attempt this task. This implies there are few opportunities for new solutions or tools to meet these researcher needs. Publishers can likely meet these needs for data sharing by working to seamlessly integrate existing solutions that reduce the effort or behaviour change involved in some tasks, and focusing on advocacy and education around the benefits of sharing data. There may however be opportunities unmet researcher needs in relation to better supporting data reuse, which could be met in part by strengthening data sharing policies of journals and publishers, and improving the discoverability of data associated with published articles."
"Correction: Developing a research data policy framework for all journals and publishers (Data Science Journal, (2020), 19, 1, (5), 10.5334/dsj-2020-005)","This article details a correction to the article: Hrynaszkiewicz, I., Simons, N., Hussain, A., Grant, R. and Goudie, S., 2020. Developing a Research Data Policy Framework for All Journals and Publishers. Data Science Journal, 19(1), p. 5. DOI: http://doi.org/10.5334/dsj-2020-005. Correction- Shortly after publication of âDeveloping a Research Data Policy Framework for All Journals and Publishersâ (Hrynaszkiewicz et al 2020), it was brought to our attention that important information regarding funding was not included: âThis paper was supported by the RDA Europe 4.0 project that has received funding from the European Unionâs Horizon 2020 research and innovation programme under grant agreement No 777388.â It was also found that the articleâs DOI was stated incorrectly in the supplementary file. The supplementary file has now been corrected. Competing Interests At the time of writing the original manuscript Iain Hrynaszkiewicz and Rebecca Grant were employees of the publisher Springer Nature and Simon Goudie an employee of the publisher Wiley. At the time of submission to the journal Iain Hrynaszkiewicz is an employee of PLOS. None of these employers required approval or review of the text before publication. Â© 2020 The Author(s).","This article details a correction to the article: Hrynaszkiewicz, , Simons, , Hussain, , Grant, and Goudie, , 2020. Developing a Research Data Policy Framework for All Journals and Publishers. Data Science Journal, 19, 5. DOI: http://doi.org/10.5334/dsj-2020-005. Correction- Shortly after publication of Developing a Research Data Policy Framework for All Journals and Publishers (Hrynaszkiewicz et al 2020), it was brought to our attention that important information regarding funding was not included: This paper was supported by the RDA Europe 4.0 project that has received funding from the European Unions Horizon 2020 research and innovation programme under grant agreement No 777388. It was also found that the articles DOI was stated incorrectly in the supplementary file. The supplementary file has now been corrected. Competing Interests At the time of writing the original manuscript Iain Hrynaszkiewicz and Rebecca Grant were employees of the publisher Springer Nature and Simon Goudie an employee of the publisher Wiley. At the time of submission to the journal Iain Hrynaszkiewicz is an employee of PLOS. None of these employers required approval or review of the text before publication."
Kadi4mat: A research data infrastructure for materials science,"The concepts and current developments of a research data infrastructure for materials science are presented, extending and combining the features of an electronic lab notebook and a repository. The objective of this infrastructure is to incorporate the possibility of structured data storage and data exchange with documented and reproducible data analysis and visualization, which finally leads to the publication of the data. This way, researchers can be supported throughout the entire research process. The software is being developed as a web-based and desktop-based system, offering both a graphical user interface and a programmatic interface. The focus of the development is on the integration of technologies and systems based on both established as well as new concepts. Due to the heterogeneous nature of materials science data, the current features are kept mostly generic, and the structuring of the data is largely left to the users. As a result, an extension of the research data infrastructure to other disciplines is possible in the future. The source code of the project is publicly available under a permissive Apache 2.0 license. Â© 2021 The Author(s).","The concepts and current developments of a research data infrastructure for materials science are presented, extending and combining the features of an electronic lab notebook and a repository. The objective of this infrastructure is to incorporate the possibility of structured data storage and data exchange with documented and reproducible data analysis and visualization, which finally leads to the publication of the data. This way, researchers can be supported throughout the entire research process. The software is being developed as a web-based and desktop-based system, offering both a graphical user interface and a programmatic interface. The focus of the development is on the integration of technologies and systems based on both established as well as new concepts. Due to the heterogeneous nature of materials science data, the current features are kept mostly generic, and the structuring of the data is largely left to the users. As a result, an extension of the research data infrastructure to other disciplines is possible in the future. The source code of the project is publicly available under a permissive Apache 2.0 license."
Earth science and biodiversity journals can improve support for data sharing,This study reviews research data policies and author instructions of 31 journals from the Earth sciences and from Biodiversity that are published by German learned societies or research institutions. 12 journals donât address data sharing at all. The statements on data sharing of the journalâs data policies/author guidelines were matched to 14 defined features of journal research data policies. A brief discussion on quality of data policies is presented to raise awareness of German learned societies/research institutions and to guide them towards improved data policies of their journals. Â© 2020 The Author(s).,This study reviews research data policies and author instructions of 31 journals from the Earth sciences and from Biodiversity that are published by German learned societies or research institutions. 12 journals dont address data sharing at all. The statements on data sharing of the journals data policies/author guidelines were matched to 14 defined features of journal research data policies. A brief discussion on quality of data policies is presented to raise awareness of German learned societies/research institutions and to guide them towards improved data policies of their journals.
Towards globally unique identification of physical samples: Governance and technical implementation of the igsn global sample number,"Persistent unique identifiers (PID) are a critical element in digital research data infrastructure to unambiguously identify, locate, and cite digital representations of a growing range of entities â publications, data, instruments, organizations, funding awards, field programs, and others. The IGSN was developed as the International Geo Sample Number to provide a persistent, globally unique, web resolvable identifier for physical samples. IGSN is both a governance and technical system for assigning globally unique persistent identifiers to physical samples. Even though initially developed for samples in the geosciences, the application of IGSN can be and has already been expanded to other domains that rely on physical samples and collections. This paper describes the current architecture and technical implementation of IGSN, how IGSN relates to other sample identifiers, and how its technical systems are supported by an international governance structure. Â© 2021 The Author(s).","Persistent unique identifiers (PID) are a critical element in digital research data infrastructure to unambiguously identify, locate, and cite digital representations of a growing range of entities publications, data, instruments, organizations, funding awards, field programs, and others. The IGSN was developed as the International Geo Sample Number to provide a persistent, globally unique, web resolvable identifier for physical samples. IGSN is both a governance and technical system for assigning globally unique persistent identifiers to physical samples. Even though initially developed for samples in the geosciences, the application of IGSN can be and has already been expanded to other domains that rely on physical samples and collections. This paper describes the current architecture and technical implementation of IGSN, how IGSN relates to other sample identifiers, and how its technical systems are supported by an international governance structure."
SwissEnvEO: A Fair national environmental data repository for earth observation open science,"Environmental scientific research is highly becoming data-driven and dependent on high performance computing infrastructures to process ever increasing large volume and diverse data sets. Consequently, there is a growing recognition of the need to share data, methods, algorithms, and infrastructure to make scientific research more effective, efficient, open, transparent, reproducible, accessible, and usable by different users. However, Earth Observations (EO) Open Science is still undervalued, and different challenges remains to achieve the vision of transforming EO data into actionable knowledge by lowering the entry barrier to massive-use Big Earth Data analysis and derived information products. Currently, FAIR-compliant digital repositories cannot fully satisfy the needs of EO users, while Spatial Data Infrastructures (SDI) are not fully FAIR-compliant and have difficulties in handling Big Earth Data. In response to these issues and the need to strengthen Open and Reproducible EO science, this paper presents SwissEnvEO, a Spatial Data Infrastructure complemented with digital repository capabilities to facilitate the publication of Ready to Use information products, at national scale, derived from satellite EO data available in an EO Data Cube in full compliance with FAIR principles. Â© 2021 The Author(s).","Environmental scientific research is highly becoming data-driven and dependent on high performance computing infrastructures to process ever increasing large volume and diverse data sets. Consequently, there is a growing recognition of the need to share data, methods, algorithms, and infrastructure to make scientific research more effective, efficient, open, transparent, reproducible, accessible, and usable by different users. However, Earth Observations (EO) Open Science is still undervalued, and different challenges remains to achieve the vision of transforming EO data into actionable knowledge by lowering the entry barrier to massive-use Big Earth Data analysis and derived information products. Currently, FAIR-compliant digital repositories cannot fully satisfy the needs of EO users, while Spatial Data Infrastructures (SDI) are not fully FAIR-compliant and have difficulties in handling Big Earth Data. In response to these issues and the need to strengthen Open and Reproducible EO science, this paper presents SwissEnvEO, a Spatial Data Infrastructure complemented with digital repository capabilities to facilitate the publication of Ready to Use information products, at national scale, derived from satellite EO data available in an EO Data Cube in full compliance with FAIR principles."
Do I-PASS for FAIR? Measuring the fair-ness of research organizations,"Given the increased use of the FAIR acronym as adjective for other contexts than data or data sets, the Dutch National Coordination Point for Research Data Management initiated a Task Group to work out the concept of a FAIR research organization. The results of this Task Groups are a definition of a FAIR enabling organization and a method to measure the FAIR-ness of a research organization (The Do-I-PASS for FAIR method). The method can also aid in developing FAIR-enabling Road Maps for individual research institutions and at a national level. This practice paper describes the development of the method and provides a couple of use cases for the application of the method in daily research data management practices in research organizations. Â© 2021 The Author(s).","Given the increased use of the FAIR acronym as adjective for other contexts than data or data sets, the Dutch National Coordination Point for Research Data Management initiated a Task Group to work out the concept of a FAIR research organization. The results of this Task Groups are a definition of a FAIR enabling organization and a method to measure the FAIR-ness of a research organization (The Do-I-PASS for FAIR method). The method can also aid in developing FAIR-enabling Road Maps for individual research institutions and at a national level. This practice paper describes the development of the method and provides a couple of use cases for the application of the method in daily research data management practices in research organizations."
Public health benefits and ethical aspects in the collection and open sharing of wastewater-based epidemic data on covid-19,"Collection and open sharing of wastewater-based epidemic data potentially provide immense public health benefits during outbreak of infectious diseases such as COVID-19. By early detection and localization of unidentified infections, wastewater surveillance is expected to enable early and targeted containment of the local outbreak. Wastewater surveillance renders potentially high public health benefits when a small catchment is targeted; however, it possibly leads to stigmatization and discrimination against the targeted group. Therefore, public commitment is crucial for the collection and open sharing of wastewater-based epidemic data. With respect to the sharing of wastewater-based epidemic data, technical limitations and uncertainty of collected data also should be simultaneously shared on the basis of scientific communication. Useful application of wastewater-based epidemic data is to complement clinical epidemic data, which is possibly biased and overlooks unidentified infections. To acquire public commitment toward the collection and open sharing of wastewater-based epidemic data, stakeholders need to reach a consensus on possible options of restrictive measures taken with respect to the collected data as well as appropriate handling of the collected data to prevent stigmatization and discrimination. Â© 2021 The Author(s).","Collection and open sharing of wastewater-based epidemic data potentially provide immense public health benefits during outbreak of infectious diseases such as COVID-19. By early detection and localization of unidentified infections, wastewater surveillance is expected to enable early and targeted containment of the local outbreak. Wastewater surveillance renders potentially high public health benefits when a small catchment is targeted; however, it possibly leads to stigmatization and discrimination against the targeted group. Therefore, public commitment is crucial for the collection and open sharing of wastewater-based epidemic data. With respect to the sharing of wastewater-based epidemic data, technical limitations and uncertainty of collected data also should be simultaneously shared on the basis of scientific communication. Useful application of wastewater-based epidemic data is to complement clinical epidemic data, which is possibly biased and overlooks unidentified infections. To acquire public commitment toward the collection and open sharing of wastewater-based epidemic data, stakeholders need to reach a consensus on possible options of restrictive measures taken with respect to the collected data as well as appropriate handling of the collected data to prevent stigmatization and discrimination."
Raising curiosity about open data via the âphysiradioâ Musicalization IoT device,"Open data is a technical concept and a political movement since datasets (e.g., on environment and business) can be used to verify/falsify (ex ante and ex post) governmental policies. But data analysis is not for the masses and non-experts may not even know the existence of open data. Here the challenge is to raise interest, curiosity and the need for knowledge in the average person. Data physicalization may be of some help: by creating a familiar device (e.g., a radio) that âphysicalizesâ some publicly available data, the authors are trying to raise curiosity about the source and availability of open data and the techniques underlying data access, extraction and analysis. This paper presents the prototype of a desktop âPhysiradioâ that plays internet streams according to a mapping between weather conditions and musical genre, i.e., a musicalization process. The association (weather â musical genre) is subjective but understandable by most people: this device internal workings can be almost fully grasped by the non-experts, thus it can be used as a conversation starter. Physiradio was field-tested among coworkers, students and other people through a quanti-qualitative information gathering process. The field test data presented here can be useful to measure the efficacy in: â¢raising curiosity about internals and open data techniques â¢conveying information (i.e., verifying the mapping) Â© 2020 The Author(s). Â© 2020, Ubiquity Press. All rights reserved.","Open data is a technical concept and a political movement since datasets (, on environment and business) can be used to verify/falsify (ex ante and ex post) governmental policies. But data analysis is not for the masses and non-experts may not even know the existence of open data. Here the challenge is to raise interest, curiosity and the need for knowledge in the average person. Data physicalization may be of some help: by creating a familiar device (, a radio) that physicalizes some publicly available data, the authors are trying to raise curiosity about the source and availability of open data and the techniques underlying data access, extraction and analysis. This paper presents the prototype of a desktop Physiradio that plays internet streams according to a mapping between weather conditions and musical genre, , a musicalization process. The association (weather musical genre) is subjective but understandable by most people: this device internal workings can be almost fully grasped by the non-experts, thus it can be used as a conversation starter. Physiradio was field-tested among coworkers, students and other people through a quanti-qualitative information gathering process. The field test data presented here can be useful to measure the efficacy in: raising curiosity about internals and open data techniques conveying information (, verifying the mapping)"
Investigation and development of the workflow to clarify conditions of use for research data publishing in japan,"With the recent Open Science movement and the rise of data-intensive science, many efforts are in progress to publish research data on the web. To reuse published research data in differ-ent fields, they must be made more generalized, interoperable, and machine-readable. Among the various issues related to data publishing, the conditions of use are directly related to their reuse potential. We show herein the types of external constraints and conditions of use in research data publishing in a Japanese context through the analysis of the interview and ques-tionnaire for practitioners. Although the conditions of research data use have been discussed only in terms of their legal constraints, we organize the inclusion of the non-legal constraints and data holdersâ actual requirements. Furthermore, we develop practical guideline for exam-ining effective data publishing flow with licensing scenarios. This effort can be positioned to develop an infrastructure for data-intensive science, which will contribute to the realization of Open Science. Â© 2020 The Author(s).","With the recent Open Science movement and the rise of data-intensive science, many efforts are in progress to publish research data on the web. To reuse published research data in differ-ent fields, they must be made more generalized, interoperable, and machine-readable. Among the various issues related to data publishing, the conditions of use are directly related to their reuse potential. We show herein the types of external constraints and conditions of use in research data publishing in a Japanese context through the analysis of the interview and ques-tionnaire for practitioners. Although the conditions of research data use have been discussed only in terms of their legal constraints, we organize the inclusion of the non-legal constraints and data holders actual requirements. Furthermore, we develop practical guideline for exam-ining effective data publishing flow with licensing scenarios. This effort can be positioned to develop an infrastructure for data-intensive science, which will contribute to the realization of Open Science."
A school and a network: Codata-rda data science summer schools alumni survey,"The CODATA-RDA Schools for Research Data Science (SRDS) is a network of schools originating in the RDA in 2016. In 2019 it was recognized as an RDA output. To date, over 400 students from 40 countries have been trained in 10 schools. The majority of these students were postgraduates from low/middle-income countries (LMICs). In contrast to many other data science training approaches, the SRDS schools are designed to be 2-week, disciplinarily-agnostic, residential events where students are introduced to a broad range of tools requisite for efficient and responsible data-centric research. This paper presents the results of a survey carried out on alumni from schools held between 2016 and 2019 (45% response). The results of the survey strongly support the SRDSâs long-term goals of facilitating data science training/capacity building within LMICs, and to foster communities of early career researchers (ECRs) conducting responsible and open data science research. The survey results demonstrated that 90% of respondent alumni continued to conduct research and make use of the skills acquired at the SRDS. Modules on open and responsible research and research data management were rated as important for future research. 79% of respondents confirmed that they maintained contact with peers, and 31% had set up academic collaborations with peers and/or instructors. Many had gone on to present content from the schools in their home institutions. The survey results clearly demonstrate the impact of the SRDS, and the value of an expanding network of schools supported by the RDA and CODATA. Â© 2021 The Author(s).","The CODATA-RDA Schools for Research Data Science (SRDS) is a network of schools originating in the RDA in 2016. In 2019 it was recognized as an RDA output. To date, over 400 students from 40 countries have been trained in 10 schools. The majority of these students were postgraduates from low/middle-income countries (LMICs). In contrast to many other data science training approaches, the SRDS schools are designed to be 2-week, disciplinarily-agnostic, residential events where students are introduced to a broad range of tools requisite for efficient and responsible data-centric research. This paper presents the results of a survey carried out on alumni from schools held between 2016 and 2019 (45% response). The results of the survey strongly support the SRDSs long-term goals of facilitating data science training/capacity building within LMICs, and to foster communities of early career researchers (ECRs) conducting responsible and open data science research. The survey results demonstrated that 90% of respondent alumni continued to conduct research and make use of the skills acquired at the SRDS. Modules on open and responsible research and research data management were rated as important for future research. 79% of respondents confirmed that they maintained contact with peers, and 31% had set up academic collaborations with peers and/or instructors. Many had gone on to present content from the schools in their home institutions. The survey results clearly demonstrate the impact of the SRDS, and the value of an expanding network of schools supported by the RDA and CODATA."
"Historical scientific analog data: Life sciences facultyâs perspectives on management, reuse and preservation","Older data in paper or analog format (e.g., field/lab notebooks, photos, maps) held in labs, offices, and archives across research institutions are an often overlooked resource for potential reuse in new scientific studies. However, there are few mechanisms to help researchers find existing analog data in order to reuse it. Yet, in the literature, reuse of historical data is particularly important in studies of biodiversity and climate change. We surveyed life science researchers at the University of Minnesota to understand and explore current and potential future use of historical data, attitudes around sharing and reusing data, and preservation of the data. Large amounts of historical data existed on our campus. Most researchers had reused or shared it, and many continued to add to their data sets. Some data had been scanned, over half of researchers have re-keyed some of their data into machine-readable format, and nearly all that were converted to a digital format were stored on unstable platforms and legacy formats. Researchers also expressed concerns about long-term preservation plans, or who to contact for assistance in planning for the future of the data, since much of these data are at risk for loss. Currently produced digital data sets are subject to guidelines and requirements developed at a national level. Solutions for historical analog data could benefit from a similar high-level treatment, and it will take experts from various fields to lead this effort. Given librariesâ expertise in data management and preservation, librarians are in a position to collaborate on devising cross-disciplinary solutions. Â© 2020 The Author(s).","Older data in paper or analog format (, field/lab notebooks, photos, maps) held in labs, offices, and archives across research institutions are an often overlooked resource for potential reuse in new scientific studies. However, there are few mechanisms to help researchers find existing analog data in order to reuse it. Yet, in the literature, reuse of historical data is particularly important in studies of biodiversity and climate change. We surveyed life science researchers at the University of Minnesota to understand and explore current and potential future use of historical data, attitudes around sharing and reusing data, and preservation of the data. Large amounts of historical data existed on our campus. Most researchers had reused or shared it, and many continued to add to their data sets. Some data had been scanned, over half of researchers have re-keyed some of their data into machine-readable format, and nearly all that were converted to a digital format were stored on unstable platforms and legacy formats. Researchers also expressed concerns about long-term preservation plans, or who to contact for assistance in planning for the future of the data, since much of these data are at risk for loss. Currently produced digital data sets are subject to guidelines and requirements developed at a national level. Solutions for historical analog data could benefit from a similar high-level treatment, and it will take experts from various fields to lead this effort. Given libraries expertise in data management and preservation, librarians are in a position to collaborate on devising cross-disciplinary solutions."
Data Quality Assurance at Research Data Repositories,"This paper presents findings from a survey on the status quo of data quality assurance practices at research data repositories. The personalised online survey was conducted among repositories indexed in re3data in 2021. It covered the scope of the repository, types of data quality assessment, quality criteria, responsibilities, details of the review process, and data quality information and yielded 332 complete responses. The results demonstrate that most repositories perform data quality assurance measures, and overall, research data repositories significantly contribute to data quality. Quality assurance at research data repositories is multifaceted and nonlinear, and although there are some common patterns, individual approaches to ensuring data quality are diverse. The survey showed that data quality assurance sets high expectations for repositories and requires a lot of resources. Several challenges were discovered: for example, the adequate recognition of the contribution of data reviewers and repositories, the path dependence of data review on review processes for text publications, and the lack of data quality information. The study could not confirm that the certification status of a repository is a clear indicator of whether a repository conducts in-depth quality assurance. Â© 2022 The Author(s).","This paper presents findings from a survey on the status quo of data quality assurance practices at research data repositories. The personalised online survey was conducted among repositories indexed in re3data in 2021. It covered the scope of the repository, types of data quality assessment, quality criteria, responsibilities, details of the review process, and data quality information and yielded 332 complete responses. The results demonstrate that most repositories perform data quality assurance measures, and overall, research data repositories significantly contribute to data quality. Quality assurance at research data repositories is multifaceted and nonlinear, and although there are some common patterns, individual approaches to ensuring data quality are diverse. The survey showed that data quality assurance sets high expectations for repositories and requires a lot of resources. Several challenges were discovered: for example, the adequate recognition of the contribution of data reviewers and repositories, the path dependence of data review on review processes for text publications, and the lack of data quality information. The study could not confirm that the certification status of a repository is a clear indicator of whether a repository conducts in-depth quality assurance."
"When Your Data is My Grandparents Singing. Digitisation and Access for Cultural Records, the Pacific and Regional Archive for Digital Sources in Endangered Cultures (PARADISEC)","In this paper we discuss the Pacific and Regional Archive for Digital Sources in Endangered Cultures (PARADISEC), a research repository that explicitly aims to act as a conduit for research outputs to a range of audiences, both within and outside of academia. PARADISEC has been operating for 19 years, and has grown to hold over 390,000 files currently totaling 150 terabytes and representing 1,312 languages, many of them from Papua New Guinea and the Pacific. Our focus is on recordings and transcripts in the many small languages of the world, the songs and stories that are unique cultural expressions. While this research data is created for a particular project, it has huge value beyond academic research as it is typically oral tradition recorded in places where little else has been recorded. There is an increasing focus in academia on reproducible research and research data management, and repositories are the key to successful data management. We discuss the importance for research practice of having discipline-specific repositories. The data in our work is also cultural material that has value to the people recorded and their descendants, it is their grandparents and so we, as outsider researchers, have special responsibilities to treat the materials with respect and to ensure they are accessible to the people we have worked with. Â© 2022 The Author(s).","In this paper we discuss the Pacific and Regional Archive for Digital Sources in Endangered Cultures (PARADISEC), a research repository that explicitly aims to act as a conduit for research outputs to a range of audiences, both within and outside of academia. PARADISEC has been operating for 19 years, and has grown to hold over 390,000 files currently totaling 150 terabytes and representing 1,312 languages, many of them from Papua New Guinea and the Pacific. Our focus is on recordings and transcripts in the many small languages of the world, the songs and stories that are unique cultural expressions. While this research data is created for a particular project, it has huge value beyond academic research as it is typically oral tradition recorded in places where little else has been recorded. There is an increasing focus in academia on reproducible research and research data management, and repositories are the key to successful data management. We discuss the importance for research practice of having discipline-specific repositories. The data in our work is also cultural material that has value to the people recorded and their descendants, it is their grandparents and so we, as outsider researchers, have special responsibilities to treat the materials with respect and to ensure they are accessible to the people we have worked with."
A Deep-Learning Method for the Prediction of SocioEconomic Indicators from Street-View Imagery Using a Case Study from Brazil,"Socioeconomic indicators are essential to help design and monitor the impact of public policies on society. Such indicators are usually obtained through census data collected at 10-year intervals, which are not only temporally coarse but expensive. Over recent years other ways of collecting data and producing these indicators have been explored, in particular using the new surveillance capabilities that remote observations can provide. The objective of this paper is to evaluate the assessment of socioeconomic indicators using street-view imagery, through a case study conducted in a region of Brazil, the Vale do Ribeira, one of the poorest semi-rural regions in Brazil. In this study we used socioeconomic indicators collected by the Brazilian Institute of Geography and Statistics (IBGE) and used Google Street View (GSV) images as our source of remote observations. A pre-trained convolutional neural network (CNN) was used to predict socio-economic indicators from GSV. To evaluate the performance of the classifier, we performed five-fold cross-validation between the predicted indicator and its true value. The best performance was obtained for the highest income class, with 80% of correct prediction. We conclude that the method has the potential to predict socioeconomic indicators across a large area with social challenges such as Vale do Ribeira, and that the network model is general enough to be used even when the imagery dataset is from semi-rural areas. This demonstrates the applicability of GSV datasets for similar settings and perhaps ensuring their replicability, which is a scientific requirement that requires further experimentation/evaluation. Â© 2022 The Author(s).","Socioeconomic indicators are essential to help design and monitor the impact of public policies on society. Such indicators are usually obtained through census data collected at 10-year intervals, which are not only temporally coarse but expensive. Over recent years other ways of collecting data and producing these indicators have been explored, in particular using the new surveillance capabilities that remote observations can provide. The objective of this paper is to evaluate the assessment of socioeconomic indicators using street-view imagery, through a case study conducted in a region of Brazil, the Vale do Ribeira, one of the poorest semi-rural regions in Brazil. In this study we used socioeconomic indicators collected by the Brazilian Institute of Geography and Statistics (IBGE) and used Google Street View (GSV) images as our source of remote observations. A pre-trained convolutional neural network (CNN) was used to predict socio-economic indicators from GSV. To evaluate the performance of the classifier, we performed five-fold cross-validation between the predicted indicator and its true value. The best performance was obtained for the highest income class, with 80% of correct prediction. We conclude that the method has the potential to predict socioeconomic indicators across a large area with social challenges such as Vale do Ribeira, and that the network model is general enough to be used even when the imagery dataset is from semi-rural areas. This demonstrates the applicability of GSV datasets for similar settings and perhaps ensuring their replicability, which is a scientific requirement that requires further experimentation/evaluation."
A Survey on Publicly Available Open Datasets Derived From Electronic Health Records (EHRs) of Patients with Neuroblastoma,"Background: Neuroblastoma is a rare pediatric cancer that affects thousands of children worldwide. Information stored in electronic health records can be a useful source of data for in silico scientific studies about this disease, carried out both by humans and by computational machines. Several open datasets derived from electronic health records of anonymized patients diagnosed with neuroblastoma are available in the internet, but they were released on different websites or as supplementary information of peer-reviewed scientific publications, making them difficult to find. Methods: To solve this problem, we present here this survey of five open public datasets derived from electronic health records of patients diagnosed with neuroblastoma, all collected in a single website called Neuroblastoma Electronic Health Records Open Data Repository. Results: The five open datasets presented in this survey can be used by researchers worldwide who want to carry on scientific studies on neuroblastoma, including machine learning and computational statistics analyses. Conclusions: We believe our survey and our open data resource can have a strong impact in oncology research, allowing new scientific discoveries that can improve our understanding of neuroblastoma and therefore improve the conditions of patients. We release the five open datasets reviewed here publicly and freely on our Neuroblastoma Electronic Health Records Open Data Repository under the CC BY 4.0 license at: https://davidechicco.github.io/neuroblastoma_EHRs_data or at https://doi.org/10.5281/zenodo.6915403. Â© 2022 The Author(s).","Background: Neuroblastoma is a rare pediatric cancer that affects thousands of children worldwide. Information stored in electronic health records can be a useful source of data for in silico scientific studies about this disease, carried out both by humans and by computational machines. Several open datasets derived from electronic health records of anonymized patients diagnosed with neuroblastoma are available in the internet, but they were released on different websites or as supplementary information of peer-reviewed scientific publications, making them difficult to find. Methods: To solve this problem, we present here this survey of five open public datasets derived from electronic health records of patients diagnosed with neuroblastoma, all collected in a single website called Neuroblastoma Electronic Health Records Open Data Repository. Results: The five open datasets presented in this survey can be used by researchers worldwide who want to carry on scientific studies on neuroblastoma, including machine learning and computational statistics analyses. Conclusions: We believe our survey and our open data resource can have a strong impact in oncology research, allowing new scientific discoveries that can improve our understanding of neuroblastoma and therefore improve the conditions of patients. We release the five open datasets reviewed here publicly and freely on our Neuroblastoma Electronic Health Records Open Data Repository under the CC BY 4.0 license at: https://davidechicco.github.io/neuroblastoma_EHRs_data or at https://doi.org/10.5281/zenodo.6915403."
Sample identifiers and metadata to support data management and reuse in multidisciplinary ecosystem sciences,"Physical samples are foundational entities for research across biological, Earth, and environmental sciences. Data generated from sample-based analyses are not only the basis of individual studies, but can also be integrated with other data to answer new and broader-scale questions. Ecosystem studies increasingly rely on multidisciplinary team-science to study climate and environmental changes. While there are widely adopted conventions within certain domains to describe sample data, these have gaps when applied in a multidisciplinary context. In this study, we reviewed existing practices for identifying, characterizing, and linking related environmental samples. We then tested practicalities of assigning persistent identifiers to samples, with standardized metadata, in a pilot field test involving eight United States Department of Energy projects. Participants collected a variety of sample types, with analyses conducted across multiple facilities. We address terminology gaps for multidisciplinary research and make recommendations for assigning identifiers and metadata that supports sample tracking, integration, and reuse. Our goal is to provide a practical approach to sample management, geared towards ecosystem scientists who contribute and reuse sample data. (IGSN); physical samples; soil; water; plant; leaf; microbial communities; related identifiers; persistent identifiers. Â© 2021 The Author(s).","Physical samples are foundational entities for research across biological, Earth, and environmental sciences. Data generated from sample-based analyses are not only the basis of individual studies, but can also be integrated with other data to answer new and broader-scale questions. Ecosystem studies increasingly rely on multidisciplinary team-science to study climate and environmental changes. While there are widely adopted conventions within certain domains to describe sample data, these have gaps when applied in a multidisciplinary context. In this study, we reviewed existing practices for identifying, characterizing, and linking related environmental samples. We then tested practicalities of assigning persistent identifiers to samples, with standardized metadata, in a pilot field test involving eight United States Department of Energy projects. Participants collected a variety of sample types, with analyses conducted across multiple facilities. We address terminology gaps for multidisciplinary research and make recommendations for assigning identifiers and metadata that supports sample tracking, integration, and reuse. Our goal is to provide a practical approach to sample management, geared towards ecosystem scientists who contribute and reuse sample data. (IGSN); physical samples; soil; water; plant; leaf; microbial communities; related identifiers; persistent identifiers."
Open access and data sharing of nucleotide sequence data,"Open access, free access, and the public domain are different concepts. The International Nucleotide Sequence Database Collaboration (INSDC) permanently guarantees free and unrestricted access to nucleotide sequence data for all researchers, irrespective of nationality or affiliation. However, recent virus information is primarily distributed via the restricted-access repository known as the Global Initiative on Sharing Avian Flu Data (GISAID) supported by the World Health Organization. As compensation for the restriction, GISAID needs to meet its initial goal of benefit-sharing among countries and to curb ongoing vaccine diplomacy campaigns. Â© 2021 The Author(s).","Open access, free access, and the public domain are different concepts. The International Nucleotide Sequence Database Collaboration (INSDC) permanently guarantees free and unrestricted access to nucleotide sequence data for all researchers, irrespective of nationality or affiliation. However, recent virus information is primarily distributed via the restricted-access repository known as the Global Initiative on Sharing Avian Flu Data (GISAID) supported by the World Health Organization. As compensation for the restriction, GISAID needs to meet its initial goal of benefit-sharing among countries and to curb ongoing vaccine diplomacy campaigns."
"Activities of the Polar Environment Data Science Center of ROIS-DS, Japan","The Polar Environment Data Science Center (PEDSC) is one of the centers of the Joint Support-Center for Data Science Research (DS) of the Research Organization of Information and Systems (ROIS), which was established in 2017. The purpose of the PEDSC is to promote the opening and sharing of the scientific data obtained by research activities in the polar region led by the National Institute of Polar Research (NIPR). Activities of the PEDSC have been carried out along a five year plan with the following seven specific tasks since 2017: (1) construction of an integrated database; (2) upgrade and interoperable use of the three existing database systems (NIPR Science Database, Arctic Data archive System (ADS), and Inter-university Upper atmosphere Global Observation NETwork system (IUGONET)); (3) processing of the time-series digital data; (4) processing of the sample data; (5) data publication in the Polar Data Journal; (6) collaboration with external communities; and (7) promoting data science using the database and database system. Â© 2022 The Author(s).","The Polar Environment Data Science Center (PEDSC) is one of the centers of the Joint Support-Center for Data Science Research (DS) of the Research Organization of Information and Systems (ROIS), which was established in 2017. The purpose of the PEDSC is to promote the opening and sharing of the scientific data obtained by research activities in the polar region led by the National Institute of Polar Research (NIPR). Activities of the PEDSC have been carried out along a five year plan with the following seven specific tasks since 2017: construction of an integrated database; upgrade and interoperable use of the three existing database systems (NIPR Science Database, Arctic Data archive System (ADS), and Inter-university Upper atmosphere Global Observation NETwork system (IUGONET)); processing of the time-series digital data; processing of the sample data; data publication in the Polar Data Journal; collaboration with external communities; and promoting data science using the database and database system."
Use of available data to inform the COVID-19 outbreak in South Africa: A case study,"The coronavirus disease (COVID-19), caused by the SARS-CoV-2 virus, was declared a pandemic by the World Health Organization (WHO) in February 2020. Currently, there are no vaccines or treatments that have been approved after clinical trials. Social distancing measures, including travel bans, school closure, and quarantine applied to countries or regions are being used to limit the spread of the disease, and the demand on the healthcare infrastructure. The seclusion of groups and individuals has led to limited access to accurate information. To update the public, especially in South Africa, announcements are made by the minister of health daily. These announcements narrate the confirmed COVID-19 cases and include the age, gender, and travel history of people who have tested positive for the disease. Additionally, the South African National Institute for Communicable Diseases updates a daily infographic summarising the number of tests performed, confirmed cases, mortality rate, and the regions affected. However, the age of the patient and other nuanced data regarding the transmission is only shared in the daily announcements and not on the updated infographic. To disseminate this information, the Data Science for Social Impact research group at the University of Pretoria, South Africa, has worked on curating and applying publicly available data in a way that is computer readable so that information can be shared to the public â using both a data repository and a dashboard. Through collaborative practices, a variety of challenges related to publicly available data in South Africa came to the fore. These include shortcomings in the accessibility, integrity, and data management practices between governmental departments and the South African public. In this paper, solutions to these problems will be shared by using a publicly available data repository and dashboard as a case study. Â© 2020, Ubiquity Press. All rights reserved.","The coronavirus disease (COVID-19), caused by the SARS-CoV-2 virus, was declared a pandemic by the World Health Organization (WHO) in February 2020. Currently, there are no vaccines or treatments that have been approved after clinical trials. Social distancing measures, including travel bans, school closure, and quarantine applied to countries or regions are being used to limit the spread of the disease, and the demand on the healthcare infrastructure. The seclusion of groups and individuals has led to limited access to accurate information. To update the public, especially in South Africa, announcements are made by the minister of health daily. These announcements narrate the confirmed COVID-19 cases and include the age, gender, and travel history of people who have tested positive for the disease. Additionally, the South African National Institute for Communicable Diseases updates a daily infographic summarising the number of tests performed, confirmed cases, mortality rate, and the regions affected. However, the age of the patient and other nuanced data regarding the transmission is only shared in the daily announcements and not on the updated infographic. To disseminate this information, the Data Science for Social Impact research group at the University of Pretoria, South Africa, has worked on curating and applying publicly available data in a way that is computer readable so that information can be shared to the public using both a data repository and a dashboard. Through collaborative practices, a variety of challenges related to publicly available data in South Africa came to the fore. These include shortcomings in the accessibility, integrity, and data management practices between governmental departments and the South African public. In this paper, solutions to these problems will be shared by using a publicly available data repository and dashboard as a case study."
"Global Community Guidelines for Documenting, Sharing, and Reusing Quality Information of Individual Digital Datasets","Open-source science builds on open and free resources that include data, metadata, software, and workflows. Informed decisions on whether and how to (re)use digital datasets are dependent on an understanding about the quality of the underpinning data and relevant information. However, quality information, being difficult to curate and often context specific, is currently not readily available for sharing within and across disciplines. To help address this challenge and promote the creation and (re) use of freely and openly shared information about the quality of individual datasets, members of several groups around the world have undertaken an effort to develop international community guidelines with practical recommendations for the Earth science community, collaborating with international domain experts. The guidelines were inspired by the guiding principles of being findable, accessible, interoperable, and reusable (FAIR). Use of the FAIR dataset quality information guidelines is intended to help stakeholders, such as scientific data centers, digital data repositories, and producers, publishers, stewards and managers of data, to: i) capture, describe, and represent quality information of their datasets in a manner that is consistent with the FAIR Guiding Principles; ii) allow for the maximum discovery, trust, sharing, and reuse of their datasets; and iii) enable international access to and integration of dataset quality information. This article describes the processes that developed the guidelines that are aligned with the FAIR principles, presents a generic quality assessment workflow, describes the guidelines for preparing and disseminating dataset quality information, and outlines a path forward to improve their disciplinary diversity. Â© 2022 The Author(s).","Open-source science builds on open and free resources that include data, metadata, software, and workflows. Informed decisions on whether and how to (re)use digital datasets are dependent on an understanding about the quality of the underpinning data and relevant information. However, quality information, being difficult to curate and often context specific, is currently not readily available for sharing within and across disciplines. To help address this challenge and promote the creation and (re) use of freely and openly shared information about the quality of individual datasets, members of several groups around the world have undertaken an effort to develop international community guidelines with practical recommendations for the Earth science community, collaborating with international domain experts. The guidelines were inspired by the guiding principles of being findable, accessible, interoperable, and reusable (FAIR). Use of the FAIR dataset quality information guidelines is intended to help stakeholders, such as scientific data centers, digital data repositories, and producers, publishers, stewards and managers of data, to: i) capture, describe, and represent quality information of their datasets in a manner that is consistent with the FAIR Guiding Principles; ii) allow for the maximum discovery, trust, sharing, and reuse of their datasets; and iii) enable international access to and integration of dataset quality information. This article describes the processes that developed the guidelines that are aligned with the FAIR principles, presents a generic quality assessment workflow, describes the guidelines for preparing and disseminating dataset quality information, and outlines a path forward to improve their disciplinary diversity."
Persistent Identification for Conferences,"Persistent identification of entities plays a major role in the progress of digitization of many fields. In the scholarly publishing realm there are already persistent identifiers (PID) for papers (DOI), people (ORCID), organisation (GRID, ROR), books (ISBN) but there is no generally accepted PID system for scholarly events such as conferences or workshops yet. This article describes the relevant use cases that motivate the introduction of persistent identifiers for conferences. The use cases were mainly derived from interviews, discussions with experts and their previous work. As primary stakeholders who are involved in the typical conference event life cycle researchers, conference organizers, and data consumers were identified. The resulting list of use cases illustrates how PIDs for conference events will improve the current situation for these stakeholders and help with problems they are facing today. Â© 2022 The Author(s).","Persistent identification of entities plays a major role in the progress of digitization of many fields. In the scholarly publishing realm there are already persistent identifiers (PID) for papers (DOI), people (ORCID), organisation (GRID, ROR), books (ISBN) but there is no generally accepted PID system for scholarly events such as conferences or workshops yet. This article describes the relevant use cases that motivate the introduction of persistent identifiers for conferences. The use cases were mainly derived from interviews, discussions with experts and their previous work. As primary stakeholders who are involved in the typical conference event life cycle researchers, conference organizers, and data consumers were identified. The resulting list of use cases illustrates how PIDs for conference events will improve the current situation for these stakeholders and help with problems they are facing today."
RDM in a Decentralised University EcosystemâA Case Study of the University of Cologne,"The University of Cologne (UoC) has historically grown in highly decentralised structures. This is reflected by a two-layered library structure as well as by a number of decentralised research data management (RDM) activities established on the faculty and research consortium level. With the aim to foster networking, cooperation, and synergies between existing activities, a university-wide RDM will be established. A one-year feasibility study was commissioned by the Rectorate in 2016 and carried out by the department research management, library and computing centre. One study outcome was the adoption of a university-wide research data guideline. Based on a comprehensive RDM service portfolio, measures were developed to put a central RDM into practice. The challenges have been to find the right level of integration and adaptation of existing and established decentralised structures and to develop additional new structures and services. We will report on first steps to map out central RDM practices at the UoC and to develop a structure of cooperation between loosely coupled information infrastructure actors. Central elements of this structure are a competence center, an RDM expert network, a forum for exchange about RDM and associated topics as well as the faculties with their decentralized, domain-specific RDM services. The Cologne Competence Center for Research Data Management (C3RDM) was founded at the end of 2018 and is still in its development phase. It provides a one-stop entry point for all questions regarding RDM. The center itself provides basic and generic RDM services, such as training, consulting, and data publication support, and acts as a hub to the decentral experts, information infrastructure actors, and resources. Â© 2022 The Author(s).","The University of Cologne (UoC) has historically grown in highly decentralised structures. This is reflected by a two-layered library structure as well as by a number of decentralised research data management (RDM) activities established on the faculty and research consortium level. With the aim to foster networking, cooperation, and synergies between existing activities, a university-wide RDM will be established. A one-year feasibility study was commissioned by the Rectorate in 2016 and carried out by the department research management, library and computing centre. One study outcome was the adoption of a university-wide research data guideline. Based on a comprehensive RDM service portfolio, measures were developed to put a central RDM into practice. The challenges have been to find the right level of integration and adaptation of existing and established decentralised structures and to develop additional new structures and services. We will report on first steps to map out central RDM practices at the UoC and to develop a structure of cooperation between loosely coupled information infrastructure actors. Central elements of this structure are a competence center, an RDM expert network, a forum for exchange about RDM and associated topics as well as the faculties with their decentralized, domain-specific RDM services. The Cologne Competence Center for Research Data Management (C3RDM) was founded at the end of 2018 and is still in its development phase. It provides a one-stop entry point for all questions regarding RDM. The center itself provides basic and generic RDM services, such as training, consulting, and data publication support, and acts as a hub to the decentral experts, information infrastructure actors, and resources."
Stewardship maturity assessment tools for modernization of climate data management,"High quality and well-managed climate data are the cornerstone of all climate services. Consistently assessing how well the data are managed is one way to establish or demonstrate the trustworthiness of the data. This paper presents the World Meteorological Organizationâs (WMO) Stewardship Maturity Matrix for Climate Data (SMM-CD) and the subsidiary SMM-CD for National and Regional Purposes (SMM-CD_NRP). Both these matrices have been developed with the support of the WMO and its High-Quality Global Data Management Framework for Climate (HQ-GDMFC). These self-assessment tools enable data managers to discover WMO recommended data stewardship practices, determine a roadmap for future development and improvement, as well as compare their process against other data providers. Datasets which have been maturity assessed are included in the WMO Climate Data Catalogue, where users can include the results of these maturity assessments into their decision-making process. The SMM-CD contains four categories (data access, usability and usage, quality management, and data management) each of which has a number of aspects, with scores assigned to one of five levels. A smaller number of categories in the SMM-CD_NRP are assigned to four levels appropriate for operationally produced datasets which are national or regional in scope. We explore a number of case studies where these matrices have been applied, as well as supply links to where the Guidance Documents and Assessment Templates (which may be updated) can be found. Â© 2021 The Author(s).","High quality and well-managed climate data are the cornerstone of all climate services. Consistently assessing how well the data are managed is one way to establish or demonstrate the trustworthiness of the data. This paper presents the World Meteorological Organizations (WMO) Stewardship Maturity Matrix for Climate Data (SMM-CD) and the subsidiary SMM-CD for National and Regional Purposes (SMM-CD_NRP). Both these matrices have been developed with the support of the WMO and its High-Quality Global Data Management Framework for Climate (HQ-GDMFC). These self-assessment tools enable data managers to discover WMO recommended data stewardship practices, determine a roadmap for future development and improvement, as well as compare their process against other data providers. Datasets which have been maturity assessed are included in the WMO Climate Data Catalogue, where users can include the results of these maturity assessments into their decision-making process. The SMM-CD contains four categories (data access, usability and usage, quality management, and data management) each of which has a number of aspects, with scores assigned to one of five levels. A smaller number of categories in the SMM-CD_NRP are assigned to four levels appropriate for operationally produced datasets which are national or regional in scope. We explore a number of case studies where these matrices have been applied, as well as supply links to where the Guidance Documents and Assessment Templates (which may be updated) can be found."
Improving opportunities for new value of open data: Assessing and certifying research data repositories,"Investments in research that produce scientific and scholarly data can be leveraged by enabling the resulting research data products and services to be used by broader communities and for new purposes, extending reuse beyond the initial users and purposes for which the data were originally collected. Submitting research data to a data repository offers opportunities for the data to be used in the future, providing ways for new benefits to be realized from data reuse. Improvements to data repositories that facilitate new uses of data increase the potential for data reuse and for gains in the value of open data products and services that are associated with such reuse. Assessing and certifying the capabilities and services offered by data repositories provides opportunities for improving the repositories and for realizing the value to be attained from new uses of data. The evolution of data repository certification instruments is described and discussed in terms of the implications for the curation and continuing use of research data. Â© 2021 The Author(s).","Investments in research that produce scientific and scholarly data can be leveraged by enabling the resulting research data products and services to be used by broader communities and for new purposes, extending reuse beyond the initial users and purposes for which the data were originally collected. Submitting research data to a data repository offers opportunities for the data to be used in the future, providing ways for new benefits to be realized from data reuse. Improvements to data repositories that facilitate new uses of data increase the potential for data reuse and for gains in the value of open data products and services that are associated with such reuse. Assessing and certifying the capabilities and services offered by data repositories provides opportunities for improving the repositories and for realizing the value to be attained from new uses of data. The evolution of data repository certification instruments is described and discussed in terms of the implications for the curation and continuing use of research data."
Improving discovery and use of nasaâs earth observation data through metadata quality assessments,"High quality descriptive metadata is essential to enabling the effective discovery of Earth observation data to a growing number of diverse users. In this paper, we define a framework to assess the quality of NASAâs Earth observation metadata with the overarching goal of improving the discoverability, accessibility and usability of the data it describes. The framework, developed by the Analysis and Review of the Common Metadata Repository (ARC) team, focuses on the metadata quality dimensions of correctness, completeness, and consistency. The methodology used by the ARC team to implement the framework is described, as well as best practices, lessons learned and recommendations for implementing similar metadata quality assessment processes. Initial results from the project indicate that this methodology, in combination with community and stakeholder collaboration, is effective in improving metadata quality. Â© 2021 The Author(s).","High quality descriptive metadata is essential to enabling the effective discovery of Earth observation data to a growing number of diverse users. In this paper, we define a framework to assess the quality of NASAs Earth observation metadata with the overarching goal of improving the discoverability, accessibility and usability of the data it describes. The framework, developed by the Analysis and Review of the Common Metadata Repository (ARC) team, focuses on the metadata quality dimensions of correctness, completeness, and consistency. The methodology used by the ARC team to implement the framework is described, as well as best practices, lessons learned and recommendations for implementing similar metadata quality assessment processes. Initial results from the project indicate that this methodology, in combination with community and stakeholder collaboration, is effective in improving metadata quality."
Dataset after seven years simulating hybrid energy systems with homer legacy,"Homer Legacy software is a well-known software for simulation of small hybrid systems that can be used for both design and research. This dataset is a set of files generated by Homer Legacy bringing the simulation results of hybrid energy systems over the last seven years, as a consequence of the research work led by Dr. Alexandre Beluco, Federal University of Rio Grande do Sul, in southern Brazil. The data correspond to twelve papers published in the last seven years. Two of them describe hydro PV hybrid systems with photovoltaic panels operating on the water surface of reservoirs. One of these twelve papers suggests the modeling of hydro-power plants with reservoirs and the other the modeling of pumped hydro storage, and a third still uses these models in a place that could receive both the two types of hydroelectric power plant. The other simulated hybrid systems include wind turbines, diesel generators, batteries, among other components. This data article describes the files that integrate this dataset and the papers that have been published presenting the hybrid systems under study and discussing the results. The files that make up this dataset are available on Mendeley Data repository at https://doi.org/10.17632/ybxsttf2by.2. Â© 2020 The Author(s).","Homer Legacy software is a well-known software for simulation of small hybrid systems that can be used for both design and research. This dataset is a set of files generated by Homer Legacy bringing the simulation results of hybrid energy systems over the last seven years, as a consequence of the research work led by Dr. Alexandre Beluco, Federal University of Rio Grande do Sul, in southern Brazil. The data correspond to twelve papers published in the last seven years. Two of them describe hydro PV hybrid systems with photovoltaic panels operating on the water surface of reservoirs. One of these twelve papers suggests the modeling of hydro-power plants with reservoirs and the other the modeling of pumped hydro storage, and a third still uses these models in a place that could receive both the two types of hydroelectric power plant. The other simulated hybrid systems include wind turbines, diesel generators, batteries, among other components. This data article describes the files that integrate this dataset and the papers that have been published presenting the hybrid systems under study and discussing the results. The files that make up this dataset are available on Mendeley Data repository at https://doi.org/10.17632/ybxsttf2by.2."
Incorporating rda outputs in the design of a european research infrastructure for natural science collections,"To support future research based on natural sciences collection data, DiSSCo (Distributed System of Scientific Collections) â the European Research Infrastructure for Natural Science Collections â adopts Digital Object Architecture as the basis for its planned data infrastructure. Using the outputs of one Research Data Alliance (RDA) interest group (IG) and five working groups (WGs) we show how RDA recommendations and supporting documents have been applied to the various stages of the DiSSCo data lifecycle. Â© 2020 The Author(s).","To support future research based on natural sciences collection data, DiSSCo (Distributed System of Scientific Collections) the European Research Infrastructure for Natural Science Collections adopts Digital Object Architecture as the basis for its planned data infrastructure. Using the outputs of one Research Data Alliance (RDA) interest group (IG) and five working groups (WGs) we show how RDA recommendations and supporting documents have been applied to the various stages of the DiSSCo data lifecycle."
"A Critical Literature Review of Historic Scientific Analog Data: Uses, Successes, and Challenges","For years scientists in fields from climate change to biodiversity to hydrology have used older data to address contemporary issues. Since the 1960s researchers, recognizing the value of this data, have expressed concern about its management and potential for loss. No widespread solutions have emerged to address the myriad issues around its storage, access, and findability. This paper summarizes observations and concerns of researchers in various disciplines who have articulated problems associated with analog data and highlights examples of projects that have used historical data. The authors also examined selected papers to discover how researchers located historical data and how they used it. While many researchers are not producing huge amounts of analog data today, there are still large volumes of it that are at risk. To address this concern, the authors recommend the development of best practices for managing historic data. This will take communication across disciplines and the involvement of researchers, departments, institutions, and associations in the process. Â© 2022 The Author(s).","For years scientists in fields from climate change to biodiversity to hydrology have used older data to address contemporary issues. Since the 1960s researchers, recognizing the value of this data, have expressed concern about its management and potential for loss. No widespread solutions have emerged to address the myriad issues around its storage, access, and findability. This paper summarizes observations and concerns of researchers in various disciplines who have articulated problems associated with analog data and highlights examples of projects that have used historical data. The authors also examined selected papers to discover how researchers located historical data and how they used it. While many researchers are not producing huge amounts of analog data today, there are still large volumes of it that are at risk. To address this concern, the authors recommend the development of best practices for managing historic data. This will take communication across disciplines and the involvement of researchers, departments, institutions, and associations in the process."
Fitness for use of data objects described with quality maturity matrix at different phases of data production,"Fitness for use information should be stored to enable easy identification of data objects that are suitable for re-use - a feature which can only be assessed by the data user. With the described Quality Maturity Matrix (QMM), we want to provide a metric for a discrete measurement of the fitness for use of data objects. We use the data maturity to describe the degree of formalization and standardization of the data with respect to the quality of data and metadata. The data objects mature as they pass through the different post-production steps where they undergo different curation measures. The higher the maturity and the level in the QMM, the easier is it for the user to judge the appropriateness of the data for a possible re-use. For our development of the Quality Maturity Matrix we link the maturity levels to the five phases concept, production/processing, project collaboration/intended use, long-term archiving, and impact re-use. Each of the five levels is measured with regard to the four criteria consistency, completeness, accessibility, and accuracy. For the description we use the terms of the Open Archival Information System (OAIS). We relate our data focused QMM to some existing maturity matrices which put the focus on the maturity of the curation process rather than of the data objects themselves. In addition, we make an attempt to establish a connection between the QMM criteria of data assessment and the FAIR Data principles. Â© 2020 The Author(s).","Fitness for use information should be stored to enable easy identification of data objects that are suitable for re-use - a feature which can only be assessed by the data user. With the described Quality Maturity Matrix (QMM), we want to provide a metric for a discrete measurement of the fitness for use of data objects. We use the data maturity to describe the degree of formalization and standardization of the data with respect to the quality of data and metadata. The data objects mature as they pass through the different post-production steps where they undergo different curation measures. The higher the maturity and the level in the QMM, the easier is it for the user to judge the appropriateness of the data for a possible re-use. For our development of the Quality Maturity Matrix we link the maturity levels to the five phases concept, production/processing, project collaboration/intended use, long-term archiving, and impact re-use. Each of the five levels is measured with regard to the four criteria consistency, completeness, accessibility, and accuracy. For the description we use the terms of the Open Archival Information System (OAIS). We relate our data focused QMM to some existing maturity matrices which put the focus on the maturity of the curation process rather than of the data objects themselves. In addition, we make an attempt to establish a connection between the QMM criteria of data assessment and the FAIR Data principles."
An infrastructure for spatial linking of survey data,"Research on environmental justice comprises health and well-being aspects, as well as topics related to general social participation. In this research field, among others, there is a need for an integrated use of social science survey data and spatial science data, e.g. for combining demographic information from survey data with data on pollution from spatial data. However, for researchers it is challenging to link both data sources, because (1) the interdisciplinary nature of both data sources is different, (2) both underlie different legal restrictions, in particular regarding data privacy, and (3) methodological challenges arise regarding the use of geo-information systems (GIS) for the processing and analysis of spatial data. In this article, we present an infrastructure of distributed web services which supports researchers in the process of spatial linking. The infrastructure addresses the challenges researchers have to face during that process. We present an example case study on the investigation of environmental inequalities with regards to income and land use hazards in Germany by using georeferenced survey data of the GESIS Panel and the German Socio-economic Panel (SOEP), and by using spatial data from the Monitor of Settlement and Open Space Development (IOER Monitor). The results show that increasing income of survey respondents is associated with less exposure to land-use-related environmental hazards in Germany. Â© 2020 The Author(s).","Research on environmental justice comprises health and well-being aspects, as well as topics related to general social participation. In this research field, among others, there is a need for an integrated use of social science survey data and spatial science data, for combining demographic information from survey data with data on pollution from spatial data. However, for researchers it is challenging to link both data sources, because the interdisciplinary nature of both data sources is different, both underlie different legal restrictions, in particular regarding data privacy, and methodological challenges arise regarding the use of geo-information systems (GIS) for the processing and analysis of spatial data. In this article, we present an infrastructure of distributed web services which supports researchers in the process of spatial linking. The infrastructure addresses the challenges researchers have to face during that process. We present an example case study on the investigation of environmental inequalities with regards to income and land use hazards in Germany by using georeferenced survey data of the GESIS Panel and the German Socio-economic Panel (SOEP), and by using spatial data from the Monitor of Settlement and Open Space Development (IOER Monitor). The results show that increasing income of survey respondents is associated with less exposure to land-use-related environmental hazards in Germany."
Who does what? - Research data management at ETH Zurich,"We present the approach to Research Data Management (RDM) support for researchers taken at ETH Zurich. Overall requirements are governed by institutional guidelines for Research Integrity, fundersâ regulations, and legal obligations. The ETH approach is based on the distinction of three phases along the research data life-cycle: 1. Data Management Planning; 2. Active RDM; 3. Data Publication and Preservation. Two ETH units, namely the Scientific IT Services and the ETH Library, provide support for different aspects of these phases, building on their respective competencies. They jointly offer trainings, consulting, information, and materials for the first phase. The second phase deals with data which is in current use in active research projects. Scientific IT Services provide their own platform, openBIS, for keeping track of raw, processed and analysed data, in addition to organising samples, materials, and scientific procedures. ETH Library operates solutions for the third phase within the infrastructure of ETH Zurichâs central IT Services. The Research Collection is the institutional repository for research output including Research Data, Open Access publications, and ETH Zurichâs bibliography. Â© 2020 The Author(s).","We present the approach to Research Data Management (RDM) support for researchers taken at ETH Zurich. Overall requirements are governed by institutional guidelines for Research Integrity, funders regulations, and legal obligations. The ETH approach is based on the distinction of three phases along the research data life-cycle: 1. Data Management Planning; 2. Active RDM; 3. Data Publication and Preservation. Two ETH units, namely the Scientific IT Services and the ETH Library, provide support for different aspects of these phases, building on their respective competencies. They jointly offer trainings, consulting, information, and materials for the first phase. The second phase deals with data which is in current use in active research projects. Scientific IT Services provide their own platform, openBIS, for keeping track of raw, processed and analysed data, in addition to organising samples, materials, and scientific procedures. ETH Library operates solutions for the third phase within the infrastructure of ETH Zurichs central IT Services. The Research Collection is the institutional repository for research output including Research Data, Open Access publications, and ETH Zurichs bibliography."
Versioning data is about more than revisions: A conceptual framework and proposed principles,"A dataset, small or big, is often changed to correct errors, apply new algorithms, or add new data (e.g., as part of a time series), etc. In addition, datasets might be bundled into collections, distributed in different encodings or mirrored onto different platforms. All these differences between versions of datasets need to be understood by researchers who want to cite the exact version of the dataset that was used to underpin their research. Failing to do so reduces the reproducibility of research results. Ambiguous identification of datasets also impacts researchers and data centres who are unable to gain recognition and credit for their contributions to the collection, creation, curation and publication of individual datasets. Although the means to identify datasets using persistent identifiers have been in place for more than a decade, systematic data versioning practices are currently not available. In this work, we analysed 39 use cases and current practices of data versioning across 33 organisations. We noticed that the term âversionâ was used in a very general sense, extending beyond the more common understanding of âversionâ to refer primarily to revisions and replacements. Using concepts developed in software versioning and the Functional Requirements for Bibliographic Records (FRBR) as a conceptual framework, we developed six foundational principles for versioning of datasets: Revision, Release, Granularity, Manifestation, Provenance and Citation. These six principles provide a high-level framework for guiding the consistent practice of data versioning and can also serve as guidance for data centres or data providers when setting up their own data revision and version protocols and procedures. Â© 2021 The Author(s).","A dataset, small or big, is often changed to correct errors, apply new algorithms, or add new data (, as part of a time series), etc. In addition, datasets might be bundled into collections, distributed in different encodings or mirrored onto different platforms. All these differences between versions of datasets need to be understood by researchers who want to cite the exact version of the dataset that was used to underpin their research. Failing to do so reduces the reproducibility of research results. Ambiguous identification of datasets also impacts researchers and data centres who are unable to gain recognition and credit for their contributions to the collection, creation, curation and publication of individual datasets. Although the means to identify datasets using persistent identifiers have been in place for more than a decade, systematic data versioning practices are currently not available. In this work, we analysed 39 use cases and current practices of data versioning across 33 organisations. We noticed that the term version was used in a very general sense, extending beyond the more common understanding of version to refer primarily to revisions and replacements. Using concepts developed in software versioning and the Functional Requirements for Bibliographic Records (FRBR) as a conceptual framework, we developed six foundational principles for versioning of datasets: Revision, Release, Granularity, Manifestation, Provenance and Citation. These six principles provide a high-level framework for guiding the consistent practice of data versioning and can also serve as guidance for data centres or data providers when setting up their own data revision and version protocols and procedures."
The FAIR data maturity model: An approach to harmonise FAIR assessments,"In the past years, many methodologies and tools have been developed to assess the FAIRness of research data. These different methodologies and tools have been based on various interpretations of the FAIR principles, which makes comparison of the results of the assessments difficult. The work in the RDA FAIR Data Maturity Model Working Group reported here has delivered a set of indicators with priorities and guidelines that provide a âlingua francaâ that can be used to make the results of the assessment using those methodologies and tools comparable. The model can act as a tool that can be used by various stakeholders, including researchers, data stewards, policy makers and funding agencies, to gain insight into the current FAIRness of data as well as into the aspects that can be improved to increase the potential for reuse of research data. Through increased efficiency and effectiveness, it helps research activities to solve societal challenges and to support evidence-based decisions. The Maturity Model is publicly available and the Working Group is encouraging application of the model in practice. Experience with the model will be taken into account in the further development of the model. Â© 2020 The Author(s).","In the past years, many methodologies and tools have been developed to assess the FAIRness of research data. These different methodologies and tools have been based on various interpretations of the FAIR principles, which makes comparison of the results of the assessments difficult. The work in the RDA FAIR Data Maturity Model Working Group reported here has delivered a set of indicators with priorities and guidelines that provide a lingua franca that can be used to make the results of the assessment using those methodologies and tools comparable. The model can act as a tool that can be used by various stakeholders, including researchers, data stewards, policy makers and funding agencies, to gain insight into the current FAIRness of data as well as into the aspects that can be improved to increase the potential for reuse of research data. Through increased efficiency and effectiveness, it helps research activities to solve societal challenges and to support evidence-based decisions. The Maturity Model is publicly available and the Working Group is encouraging application of the model in practice. Experience with the model will be taken into account in the further development of the model."
Open Science-For Whom?,"Who can participate in Open Science and whose interests are served? Open Science in principle holds the potential to reduce inequality, but this is not going to happen unless it operates within a consistent framework and environment that supports this goal. Unequal power and opportunities from institutional to global level constitutes a major obstacle to human development, while we need to appreciate diversity as a key asset. How can we build an equitable global research ecosystem in accordance with the United Nations 2030 Agenda for Sustainable Development that recognises science as a global common good and an integral part of the shared cultural heritage of humankind?. Â© 2022 The Author(s).","Who can participate in Open Science and whose interests are served? Open Science in principle holds the potential to reduce inequality, but this is not going to happen unless it operates within a consistent framework and environment that supports this goal. Unequal power and opportunities from institutional to global level constitutes a major obstacle to human development, while we need to appreciate diversity as a key asset. How can we build an equitable global research ecosystem in accordance with the United Nations 2030 Agenda for Sustainable Development that recognises science as a global common good and an integral part of the shared cultural heritage of humankind?."
Research data management as an integral part of the research process of empirical disciplines using landscape ecology as an example,"Research Data Management (RDM) is regarded as an elementary component of empirical disciplines. Taking Landscape Ecology in Germany as an example the article demonstrates how to integrate RDM into the research design as a complement of the classic quality control and assurance in empirical research that has, so far, generally been limited to data production. Sharing and reuse of empirical data by scientists as well as thorough peer reviews of knowledge produced by empirical research requires that the problem of the research in question, the operationalized definitions of the objects of investigation and their representative selection are documented and archived as well as the methods of data production including indicators for data quality and all data collected and produced. On this basis, the extent to which this complemented design of research processes has already been realized is demonstrated by research projects of the Chair of Landscape Ecology at the University of Vechta, Germany. This study is part of a joined research project on Research Data Management funded by the German Federal Ministry of Education and Research. Â© 2020 The Author(s).","Research Data Management (RDM) is regarded as an elementary component of empirical disciplines. Taking Landscape Ecology in Germany as an example the article demonstrates how to integrate RDM into the research design as a complement of the classic quality control and assurance in empirical research that has, so far, generally been limited to data production. Sharing and reuse of empirical data by scientists as well as thorough peer reviews of knowledge produced by empirical research requires that the problem of the research in question, the operationalized definitions of the objects of investigation and their representative selection are documented and archived as well as the methods of data production including indicators for data quality and all data collected and produced. On this basis, the extent to which this complemented design of research processes has already been realized is demonstrated by research projects of the Chair of Landscape Ecology at the University of Vechta, Germany. This study is part of a joined research project on Research Data Management funded by the German Federal Ministry of Education and Research."
Adaptable methods for training in research data management,"The management of research data has become an essential aspect of good scientific practice. Education in research data management is, however, scarce. The low number of trainers can be attributed on the one hand to a lack of educational paths. On the other hand, qualification opportunities for academics who have already completed their studies and are in employment are missing. Within the research project FDMentor a Train-the-Trainer programme was therefore developed to teach potential multipliers of research data management, and at the same time impart basic didactic knowledge. The resulting concept was created, in addition to freely re-usable materials, to support researchers and research support staff in passing on this knowledge. In addition, the generic development and free licensing of the concept enables transferability to other thematic contexts, such as Open Access or Open Science. Â© 2021 The Author(s).","The management of research data has become an essential aspect of good scientific practice. Education in research data management is, however, scarce. The low number of trainers can be attributed on the one hand to a lack of educational paths. On the other hand, qualification opportunities for academics who have already completed their studies and are in employment are missing. Within the research project FDMentor a Train-the-Trainer programme was therefore developed to teach potential multipliers of research data management, and at the same time impart basic didactic knowledge. The resulting concept was created, in addition to freely re-usable materials, to support researchers and research support staff in passing on this knowledge. In addition, the generic development and free licensing of the concept enables transferability to other thematic contexts, such as Open Access or Open Science."
Role of a croatian national repository infrastructure in promotion and support of research data management,"The paper will give an overview of national infrastructure for digital repositories, Digital Academic Archives and Repositories (DABAR), and its role as technology steward in raising awareness about research data management (RDM) and promoting good practices in the Croatian A&R community. The University of Zagreb, University Computing Centre (SRCE) is providing national infrastructure DABAR suitable for storing and dissemination of different types of digital objects. Through DABAR, all Croatian higher education and research institutions can establish their digital repository. A strong collaboration between SRCEs DABAR team and institutions repository managers has proven to be important in the process of disseminating knowledge about research data management among researchers and the scientific community at large. The paper will provide information about this collaboration during the project RDA Europe 4.0 â The European plug-in to the global Research Data Alliance (RDA). The main goal of this collaboration is to raise awareness about the importance of managing and sharing research data. Â© 2020 The Author(s).","The paper will give an overview of national infrastructure for digital repositories, Digital Academic Archives and Repositories (DABAR), and its role as technology steward in raising awareness about research data management (RDM) and promoting good practices in the Croatian A&R community. The University of Zagreb, University Computing Centre (SRCE) is providing national infrastructure DABAR suitable for storing and dissemination of different types of digital objects. Through DABAR, all Croatian higher education and research institutions can establish their digital repository. A strong collaboration between SRCEs DABAR team and institutions repository managers has proven to be important in the process of disseminating knowledge about research data management among researchers and the scientific community at large. The paper will provide information about this collaboration during the project RDA Europe 4.0 The European plug-in to the global Research Data Alliance (RDA). The main goal of this collaboration is to raise awareness about the importance of managing and sharing research data."
Alter-value in data reuse: Non-designated communities and creative processes,"This paper builds on the investigation of data reuse in creative processes to discuss âepistemic pluralismâ and data âalter-valueâ in research data management. Focussing on a specific non-des-ignated community, we conducted semi-structured interviews with five artists in relation to five works. Data reuse is a critical component of all these works. The qualitative content analysis brings to light agonistic-antagonistic practices in data reuse and shows multiple deconstruc-tions of the notion of data value as it is portrayed in the data reuse literature. Finally, the paper brings to light the benefits of including such practices in the conceptualization of data curation. Â© 2020 The Author(s).","This paper builds on the investigation of data reuse in creative processes to discuss epistemic pluralism and data alter-value in research data management. Focussing on a specific non-des-ignated community, we conducted semi-structured interviews with five artists in relation to five works. Data reuse is a critical component of all these works. The qualitative content analysis brings to light agonistic-antagonistic practices in data reuse and shows multiple deconstruc-tions of the notion of data value as it is portrayed in the data reuse literature. Finally, the paper brings to light the benefits of including such practices in the conceptualization of data curation."
Gis project rosa: Fair principles in the petroleum industry,The ROSA Project is focused on the investigations of oil and gas industry progress in Russia and other countries. The primary objective is to examine and evaluate data on worldwide hydrocarbon occurrences. Major aim is to construct a comprehensive map of the allocation of oil and gas fields with large reserves for further analogy estimation and reconstruction of geological history. The main contribution of this work is the development of a multidimensional and multilevel database and the corresponding GIS Project for visualization. The set of multidisciplinary backgrounds in combination with a spatial algorithmic tools are used as a basis for an analytical study of worldwide hydrocarbon occurrences and estimation establishment of petroleum industry. Â© 2020 The Author(s).,The ROSA Project is focused on the investigations of oil and gas industry progress in Russia and other countries. The primary objective is to examine and evaluate data on worldwide hydrocarbon occurrences. Major aim is to construct a comprehensive map of the allocation of oil and gas fields with large reserves for further analogy estimation and reconstruction of geological history. The main contribution of this work is the development of a multidimensional and multilevel database and the corresponding GIS Project for visualization. The set of multidisciplinary backgrounds in combination with a spatial algorithmic tools are used as a basis for an analytical study of worldwide hydrocarbon occurrences and estimation establishment of petroleum industry.
Organization IDs in GermanyâResults of an Assessment of the Status Quo in 2020,"Persistent identifiers (PIDs) for scientific organizations such as research institutions and research funding agencies are a further decisive piece of the puzzle to promote standardization in the scholarly publication processâespecially in light of the already established digital object identifiers (DOIs) for research outputs and ORCID iDs for researchers. The application of these PIDs enables automated data flows and guarantees the persistent linking of information objects. Moreover, PIDs are fundamental components for the implementation of open science. For example, the application of PIDs for scientific organizations is of crucial importance when analyzing publications and the costs of the transition to open access at an institution. To find out more about the status quo of the use and adoption of organization IDs in Germany, a âSurvey on the Need for and Use of Organization IDs at Higher Education Institutions and Non-University Research Institutions in Germanyâ was conducted among 548 scientific institutions in Germany in the period from July 13 to December 4, 2020, as part of the DFG-funded project ORCID DE. One hundred and eighty-three institutions participated in what was the largest survey to date on organization IDs in Germany. The survey included questions on the knowledge, adoption, and use of organization IDs at scientific institutions. Moreover, respondent institutions were asked about their needs with regard to organization IDs and their metadata (e.g., in terms of relationships and granularity). The present paper provides a comprehensive overview of the results of the survey conducted as part of the aforementioned project and contributes to the promotion and increased awareness of organization IDs. Â© 2022 Vierkant et al.","Persistent identifiers (PIDs) for scientific organizations such as research institutions and research funding agencies are a further decisive piece of the puzzle to promote standardization in the scholarly publication processespecially in light of the already established digital object identifiers (DOIs) for research outputs and ORCID iDs for researchers. The application of these PIDs enables automated data flows and guarantees the persistent linking of information objects. Moreover, PIDs are fundamental components for the implementation of open science. For example, the application of PIDs for scientific organizations is of crucial importance when analyzing publications and the costs of the transition to open access at an institution. To find out more about the status quo of the use and adoption of organization IDs in Germany, a Survey on the Need for and Use of Organization IDs at Higher Education Institutions and Non-University Research Institutions in Germany was conducted among 548 scientific institutions in Germany in the period from July 13 to December 4, 2020, as part of the DFG-funded project ORCID DE. One hundred and eighty-three institutions participated in what was the largest survey to date on organization IDs in Germany. The survey included questions on the knowledge, adoption, and use of organization IDs at scientific institutions. Moreover, respondent institutions were asked about their needs with regard to organization IDs and their metadata (, in terms of relationships and granularity). The present paper provides a comprehensive overview of the results of the survey conducted as part of the aforementioned project and contributes to the promotion and increased awareness of organization IDs."
"Guidelines for Publicly Archiving Terrestrial Model Data to Enhance Usability, Intercomparison, and Synthesis","Scientific communities are increasingly publishing data to evaluate, accredit, and build on published research. However, guidelines for curating data for publication are sparse for model-related research, limiting the usability of archived simulation data. In particular, there are no established guidelines for archiving data related to terrestrial models that simulate land processes and their coupled interactions with climate. Terrestrial modelers have a unique set of challenges when publishing data due to the diversity of scientific domains, research questions, and the types and scales of simulations. Researchers in the U.S. Department of Energyâs (DOE) projects use a variety of multiscale models to advance robust predictions of terrestrial and subsurface ecosystem processes. Here, we synthesize archiving needs for data associated with different DOE models, and provide guidelines for publishing terrestrial model data components following FAIR (Findable, Accessible, Interoperable, Reusable) principles. The guidelines recommend archiving model inputs and testing data used in final simulation runs along with associated codes, workflow scripts, and metadata in public repositories. Researchers should consider archiving model outputs if they are within the storage limits of the repository. We also provide considerations for how to bundle files into different data publications with citable digital object identifiers. Finally, we identify repository features and tools that would enable storage and reuse of model data. Given the diversity of DOE terrestrial models, these guidelines are transferable to other model types and will enable efficient reuse of simulation data for purposes such as model intercomparisons, initialization, benchmarking, synthesis, and comparisons with field observations. Â© 2022 The Author(s).","Scientific communities are increasingly publishing data to evaluate, accredit, and build on published research. However, guidelines for curating data for publication are sparse for model-related research, limiting the usability of archived simulation data. In particular, there are no established guidelines for archiving data related to terrestrial models that simulate land processes and their coupled interactions with climate. Terrestrial modelers have a unique set of challenges when publishing data due to the diversity of scientific domains, research questions, and the types and scales of simulations. Researchers in the Department of Energys (DOE) projects use a variety of multiscale models to advance robust predictions of terrestrial and subsurface ecosystem processes. Here, we synthesize archiving needs for data associated with different DOE models, and provide guidelines for publishing terrestrial model data components following FAIR (Findable, Accessible, Interoperable, Reusable) principles. The guidelines recommend archiving model inputs and testing data used in final simulation runs along with associated codes, workflow scripts, and metadata in public repositories. Researchers should consider archiving model outputs if they are within the storage limits of the repository. We also provide considerations for how to bundle files into different data publications with citable digital object identifiers. Finally, we identify repository features and tools that would enable storage and reuse of model data. Given the diversity of DOE terrestrial models, these guidelines are transferable to other model types and will enable efficient reuse of simulation data for purposes such as model intercomparisons, initialization, benchmarking, synthesis, and comparisons with field observations."
A review of open research data policies and practices in china,"This paper initially conducts a literature review and content analysis of the open research data policies in China. Next, a series of exemplars describe data practices to promote and enable the use of open research data, including open data practices in research programs, data repositories, data journals, and citizen science. Moreover, the top four driving forces are identified and analyzed along with their responsible guiding work. In addition, the âlandscape of open research data ecology in Chinaâ is derived from the literature review and from observations of actual cases, where the interaction and mutual development of data policies, data programs, and data practices are recognized. Finally, future trends of research data practices within China and internationally are discussed. We hope the analysis provides perspective on current open data practices in China along with insight into the need for additional research on scientific data sharing and management. Â© 2021 The Author(s).","This paper initially conducts a literature review and content analysis of the open research data policies in China. Next, a series of exemplars describe data practices to promote and enable the use of open research data, including open data practices in research programs, data repositories, data journals, and citizen science. Moreover, the top four driving forces are identified and analyzed along with their responsible guiding work. In addition, the landscape of open research data ecology in China is derived from the literature review and from observations of actual cases, where the interaction and mutual development of data policies, data programs, and data practices are recognized. Finally, future trends of research data practices within China and internationally are discussed. We hope the analysis provides perspective on current open data practices in China along with insight into the need for additional research on scientific data sharing and management."
Keeping track of samples in multidisciplinary fieldwork,"We here present methods, tools and results for efficiently collecting metadata and tracking samples collected in the field. The samples and metadata were collected during scientific cruises conducted by amongst others marine biologists, oceanographers, geochemists and marine geologists in the Nansen Legacy project. It is here reported on the successful development and implementation of a system for labeling, tracking and openly publishing metadata from the cruises. The results and tools have been made openly available, as they are suitable for a range of situations, from the individual scientist working in the field to large research missions. Â© 2021 The Author(s).","We here present methods, tools and results for efficiently collecting metadata and tracking samples collected in the field. The samples and metadata were collected during scientific cruises conducted by amongst others marine biologists, oceanographers, geochemists and marine geologists in the Nansen Legacy project. It is here reported on the successful development and implementation of a system for labeling, tracking and openly publishing metadata from the cruises. The results and tools have been made openly available, as they are suitable for a range of situations, from the individual scientist working in the field to large research missions."
Interconnecting systems using machine-actionable data management plans â hackathon report,"The common standard for machine-actionable Data Management Plans (DMPs) allows for automatic exchange, integration, and validation of information provided in DMPs. In this paper, we report on the hackathon organised by the Research Data Alliance in which a group of 89 participants from 21 countries worked collaboratively on use cases exploring the utility of the standard in different settings. The work included integration of tools and services, funder templates mapping, and development of new serialisations. This paper summarises the results achieved during the hackathon and provides pointers to further resources. Â© 2021 The Author(s).","The common standard for machine-actionable Data Management Plans (DMPs) allows for automatic exchange, integration, and validation of information provided in DMPs. In this paper, we report on the hackathon organised by the Research Data Alliance in which a group of 89 participants from 21 countries worked collaboratively on use cases exploring the utility of the standard in different settings. The work included integration of tools and services, funder templates mapping, and development of new serialisations. This paper summarises the results achieved during the hackathon and provides pointers to further resources."
SparkNN: A distributed in-memory data partitioning for KNN queries on big spatial data,"The increase in GPS-enabled devices and proliferation of location-based applications have resulted in an abundance of geotagged (spatial) data. As a consequence, numerous applications have emerged that utilize the spatial data to provide different types of location-based services. However, the huge amount of available spatial data presents a challenge to the efficiency of these location-based services. Although the advent of big data frameworks like Apache Spark has enabled the processing of large amounts of data efficiently, they are designed for general (non-spatial) data. That is due to the build-in data partitioning mechanism that does not take into account the spatial proximity of the data. Therefore, these big data frameworks cannot be readily used for spatial analytics such as efficiently answering spatial queries. To fill this gap, this paper proposes SparkNN, an in-memory partitioning and indexing system for answering spatial queries, such as K-nearest neighbor, on big spatial data. SparkNN is implemented on top of Apache Spark and consists of three layers to facilitate efficient spatial queries. The first layer is a spatial-aware partitioning layer, which partitions the spatial data into several partitions ensuring that the load of the partitions is balanced and data objects with close proximity are placed in the same, or neighboring, partitions. The second layer is a local indexing layer, which provides a spatial index inside each partition to speed up the data search within the partition. The third layer is a global index, which is placed in the master node of Spark to route spatial queries to the relevant partitions. The efficiency of SparkNN was evaluated by extensive experiments with big spatial datasets. The results show SparkNN significantly outperforms the state-of-the-art Spark system when evaluated on the same set of queries. Â© 2020 The Author(s).","The increase in GPS-enabled devices and proliferation of location-based applications have resulted in an abundance of geotagged (spatial) data. As a consequence, numerous applications have emerged that utilize the spatial data to provide different types of location-based services. However, the huge amount of available spatial data presents a challenge to the efficiency of these location-based services. Although the advent of big data frameworks like Apache Spark has enabled the processing of large amounts of data efficiently, they are designed for general (non-spatial) data. That is due to the build-in data partitioning mechanism that does not take into account the spatial proximity of the data. Therefore, these big data frameworks cannot be readily used for spatial analytics such as efficiently answering spatial queries. To fill this gap, this paper proposes SparkNN, an in-memory partitioning and indexing system for answering spatial queries, such as K-nearest neighbor, on big spatial data. SparkNN is implemented on top of Apache Spark and consists of three layers to facilitate efficient spatial queries. The first layer is a spatial-aware partitioning layer, which partitions the spatial data into several partitions ensuring that the load of the partitions is balanced and data objects with close proximity are placed in the same, or neighboring, partitions. The second layer is a local indexing layer, which provides a spatial index inside each partition to speed up the data search within the partition. The third layer is a global index, which is placed in the master node of Spark to route spatial queries to the relevant partitions. The efficiency of SparkNN was evaluated by extensive experiments with big spatial datasets. The results show SparkNN significantly outperforms the state-of-the-art Spark system when evaluated on the same set of queries."
Synthetic reproduction and augmentation of covid-19 case reporting data by agent-based simulation,"We generate synthetic data documenting COVID-19 cases in Austria by the means of an agent-based simulation model. The model simulates the transmission of the SARS-CoV-2 virus in a statistical replica of the population and reproduces typical patient pathways on an individual basis while simultaneously integrating historical data on the implementation and expiration of population-wide countermeasures. The resulting data semantically and statistically aligns with an official epidemiological case reporting data set and provides an easily accessible, consistent and augmented alternative. Our synthetic data set provides additional insight into the spread of the epidemic by synthesizing information that cannot be recorded in reality. Â© 2021 The Author(s).","We generate synthetic data documenting COVID-19 cases in Austria by the means of an agent-based simulation model. The model simulates the transmission of the SARS-CoV-2 virus in a statistical replica of the population and reproduces typical patient pathways on an individual basis while simultaneously integrating historical data on the implementation and expiration of population-wide countermeasures. The resulting data semantically and statistically aligns with an official epidemiological case reporting data set and provides an easily accessible, consistent and augmented alternative. Our synthetic data set provides additional insight into the spread of the epidemic by synthesizing information that cannot be recorded in reality."
Leading fair adoption across the institution: A collaboration between an academic library and a technology provider,"Universities strive to foster knowledge sharing and greater research productivity. Some recognize that this requires research output to be findable, accessible, interoperable and reusable. But current tools do not yet allow a comprehensive adoption of these FAIR principles for making research openly and globally accessible to generate new knowledge. To address this gap, diverse stakeholders are collaborating to build effective research data management [RDM] solutions for institutional research output [publications and data] that benefit researchers, institutions, and developers. This paper illustrates a university-industry collaboration between a private U.S. university (Drexel University) and a global commercial vendor (Ex Libris, a ProQuest company). The authors examine how an emerging technology infrastructure for Research Data Management will enable librarians to help institutions adopt the FAIR principles at scale. They highlight an approach for collaborative product development that aims not to change researcher habits or add to librariansâ workloads. Their first year working together confirms factors recognized as contributing to successful collaborations, such as aligning goals, building understanding of each otherâs organizations, and sustaining honest engagement. Though FAIR offers a simple articulation to help build campus infrastructure and change culture, its implementation will vary between different groups of researchers. Libraries and technology providers have a mutual interest in collaborating to address RDM challenges, but must recognize that collaboration takes time, perseverance, and flexibility to effect change. Librarians, researchers, and administrators from such campus offices as Research, Compliance, IT, Legal, and Graduate Studies will benefit from key lessons raised by this case study. Â© 2021 The Author(s).","Universities strive to foster knowledge sharing and greater research productivity. Some recognize that this requires research output to be findable, accessible, interoperable and reusable. But current tools do not yet allow a comprehensive adoption of these FAIR principles for making research openly and globally accessible to generate new knowledge. To address this gap, diverse stakeholders are collaborating to build effective research data management [RDM] solutions for institutional research output [publications and data] that benefit researchers, institutions, and developers. This paper illustrates a university-industry collaboration between a private university (Drexel University) and a global commercial vendor (Ex Libris, a ProQuest company). The authors examine how an emerging technology infrastructure for Research Data Management will enable librarians to help institutions adopt the FAIR principles at scale. They highlight an approach for collaborative product development that aims not to change researcher habits or add to librarians workloads. Their first year working together confirms factors recognized as contributing to successful collaborations, such as aligning goals, building understanding of each others organizations, and sustaining honest engagement. Though FAIR offers a simple articulation to help build campus infrastructure and change culture, its implementation will vary between different groups of researchers. Libraries and technology providers have a mutual interest in collaborating to address RDM challenges, but must recognize that collaboration takes time, perseverance, and flexibility to effect change. Librarians, researchers, and administrators from such campus offices as Research, Compliance, IT, Legal, and Graduate Studies will benefit from key lessons raised by this case study."
KadiStudio: FAIR Modelling of Scientific Research Processes,"FAIR handling of scientific data plays a significant role in current efforts towards a more sustainable research culture and serves as a prerequisite for the fourth scientific paradigm, that is, data-driven research. To enforce the FAIR principles by ensuring the reproducibility of scientific data and tracking their provenance comprehensibly, the FAIR modelling of research processes in form of automatable workflows is necessary. By providing reusable procedures containing expert knowledge, such workflows contribute decisively to the quality and the acceleration of scientific research. In this work, the requirements for a system to be capable of modelling FAIR workflows are defined and a generic concept for modelling research processes as workflows is developed. For this, research processes are iteratively divided into impartible subprocesses at different detail levels using the input-process-output model. The concrete software implementation of the identified, universally applicable concept is finally presented in form of the workflow editor KadiStudio of the Karlsruhe Data Infrastructure for Materials Science (Kadi4Mat). Â© 2022 The Author(s).","FAIR handling of scientific data plays a significant role in current efforts towards a more sustainable research culture and serves as a prerequisite for the fourth scientific paradigm, that is, data-driven research. To enforce the FAIR principles by ensuring the reproducibility of scientific data and tracking their provenance comprehensibly, the FAIR modelling of research processes in form of automatable workflows is necessary. By providing reusable procedures containing expert knowledge, such workflows contribute decisively to the quality and the acceleration of scientific research. In this work, the requirements for a system to be capable of modelling FAIR workflows are defined and a generic concept for modelling research processes as workflows is developed. For this, research processes are iteratively divided into impartible subprocesses at different detail levels using the input-process-output model. The concrete software implementation of the identified, universally applicable concept is finally presented in form of the workflow editor KadiStudio of the Karlsruhe Data Infrastructure for Materials Science (Kadi4Mat)."
Unified geomagnetic database from different observation networks for geomagnetic hazard assessment tasks,"This paper presents the results of the creation of a geomagnetic data storage system that combines raw observation data from different geomagnetic observation networks along with derived indices and indicators of geomagnetic activity. Geomagnetic data, provided by observation networks, undergo a series of data quality validation procedures. The implemented instruments and procedures facilitate the creation of a uniform database from multiple sources. Â© 2020 The Author(s).","This paper presents the results of the creation of a geomagnetic data storage system that combines raw observation data from different geomagnetic observation networks along with derived indices and indicators of geomagnetic activity. Geomagnetic data, provided by observation networks, undergo a series of data quality validation procedures. The implemented instruments and procedures facilitate the creation of a uniform database from multiple sources."
Fairness literacy: The achillesâ heel of applying fair principles,"The SHARC Interest Group of the Research Data Alliance was established to improve research crediting and rewarding mechanisms for scientists who wish to organise their data (and material resources) for community sharing. This requires that data are findable and accessible on the Web, and comply with shared standards making them interoperable and reusable in alignment with the FAIR principles. It takes considerable time, energy, expertise and motivation. It is imperative to facilitate the processes to encourage scientists to share their data. To that aim, supporting FAIR principles compliance processes and increasing the human understanding of FAIRness criteria â i.e., promoting FAIRness literacy â and not only the machine-readability of the criteria, are critical steps in the data sharing process. Appropriate human-understandable criteria must be the first identified in the FAIRness assessment processes and roadmap. This paper reports on the lessons learned from the RDA SHARC Interest Group on identifying the processes required to prepare FAIR implementation in various communities not specifically data skilled, and on the procedures and training that must be deployed and adapted to each practice and level of understanding. These are essential milestones in developing adapted support and credit back mechanisms not yet in place. Â© 2020 The Author(s).","The SHARC Interest Group of the Research Data Alliance was established to improve research crediting and rewarding mechanisms for scientists who wish to organise their data (and material resources) for community sharing. This requires that data are findable and accessible on the Web, and comply with shared standards making them interoperable and reusable in alignment with the FAIR principles. It takes considerable time, energy, expertise and motivation. It is imperative to facilitate the processes to encourage scientists to share their data. To that aim, supporting FAIR principles compliance processes and increasing the human understanding of FAIRness criteria , promoting FAIRness literacy and not only the machine-readability of the criteria, are critical steps in the data sharing process. Appropriate human-understandable criteria must be the first identified in the FAIRness assessment processes and roadmap. This paper reports on the lessons learned from the RDA SHARC Interest Group on identifying the processes required to prepare FAIR implementation in various communities not specifically data skilled, and on the procedures and training that must be deployed and adapted to each practice and level of understanding. These are essential milestones in developing adapted support and credit back mechanisms not yet in place."
Data warehouse hybrid modeling methodology,"The classic conceptual modeling around business processes followed by the âbus matrixâ methodology of designing the data cubes of data warehouses (Kimball & Ross 2013). For a serious system, such a quantity of management questions and dimensions, the bus matrix results a difficult-to-understand conceptual data model. The subject of automation and conceptual design â to which many individual methods already have been developed â are relevant topics in todayâs literature also. In the 2010s data warehouse projects were realized in Hungarian higher education to inform the decision makers of the universities about their own institutions. As we participated in this project in 2009â2010, we faced that our bus matrix at the end contained about 80â120 indicators with nearly 200 dimensions (dimensional attributes), therefore we worked on the early stenography to formalize the management question. We provide a kind of âbusiness intelligence problem solving thinkingâ and a kind of descriptive language that can serve it and present a method which has two novelties compared to formers: 1. It is based on the management questions and its visualization. 2. As a kind of stenography, it is always based on the terminology corresponding to the current problem, so it forms an intermediate language for the data model. We introduce our method through an example in a popular research area which is activity tracking. Â© 2020 The Author(s).","The classic conceptual modeling around business processes followed by the bus matrix methodology of designing the data cubes of data warehouses (Kimball & Ross 2013). For a serious system, such a quantity of management questions and dimensions, the bus matrix results a difficult-to-understand conceptual data model. The subject of automation and conceptual design to which many individual methods already have been developed are relevant topics in todays literature also. In the 2010s data warehouse projects were realized in Hungarian higher education to inform the decision makers of the universities about their own institutions. As we participated in this project in 20092010, we faced that our bus matrix at the end contained about 80120 indicators with nearly 200 dimensions (dimensional attributes), therefore we worked on the early stenography to formalize the management question. We provide a kind of business intelligence problem solving thinking and a kind of descriptive language that can serve it and present a method which has two novelties compared to formers: 1. It is based on the management questions and its visualization. 2. As a kind of stenography, it is always based on the terminology corresponding to the current problem, so it forms an intermediate language for the data model. We introduce our method through an example in a popular research area which is activity tracking."
Fostering interdisciplinary data cultures through early career development: The RDA/US data share fellowship,"Openness and interdisciplinarity in research and data are among the challenges that are frequently discussed in the context of changing scientific and scholarly practices. Gradually, the visions of open and widely shared data are being reconciled with complex realities that stem from the disciplinary differences in data cultures. In this paper we discuss interdisciplinarity through data as a way to create research environments that are more flexible and, as a result, more amenable to change. We report our findings from facilitating and evaluating a data-oriented early-career fellowship program that was administered as part of the Research Data Alliance (RDA), a global organization that aims to enable open sharing and re-use of data. We identify ways to foster interdisciplinary data cultures among the future researchers and professionals and propose recommendations for future programs. While the short-term early career programs cannot address the systemic factors that impact openness and interdisciplinarity, such as the systems of reward and recognition or the funding structures, they can introduce mechanisms that support diversity, learning, and leadership and, ultimately, contribute to a culture change. Â© 2021 The Author(s).","Openness and interdisciplinarity in research and data are among the challenges that are frequently discussed in the context of changing scientific and scholarly practices. Gradually, the visions of open and widely shared data are being reconciled with complex realities that stem from the disciplinary differences in data cultures. In this paper we discuss interdisciplinarity through data as a way to create research environments that are more flexible and, as a result, more amenable to change. We report our findings from facilitating and evaluating a data-oriented early-career fellowship program that was administered as part of the Research Data Alliance (RDA), a global organization that aims to enable open sharing and re-use of data. We identify ways to foster interdisciplinary data cultures among the future researchers and professionals and propose recommendations for future programs. While the short-term early career programs cannot address the systemic factors that impact openness and interdisciplinarity, such as the systems of reward and recognition or the funding structures, they can introduce mechanisms that support diversity, learning, and leadership and, ultimately, contribute to a culture change."
Developing Metrics for NASA Earth Science Interdisciplinary Data Products and Services,"Metrics are measures that are able to produce quantifiable information. There are many applications of metrics in Earth science data and services; for example, metrics are frequently used to track service performance and progress. In short, developing, collecting and analyzing metrics are essential activities to better support Earth science research, applications, and education. As one of the largest repositories of Earth science data in the world, NASAâs Earth Science Data and Information System (ESDIS) Project supports twelve Distributed Active Archive Centers (DAACs). Standard metrics have been developed by the ESDIS Metrics System (EMS). These metrics are collected and analyzed routinely at each DAAC. As it is expected that the total data volume will continue to grow rapidly, and the timely developed technologies (e.g., cloud computing, AI/ML) will continue to improve data discovery and accessibility, opportunities for developing new data services for the Earth science community will also arise, especially in interdisciplinary research and applications. However, developing such metrics has become a challenge because multiple datasets are often needed. Current metrics are designed for a single predefined dataset or service, a disadvantage for collecting metrics for interdisciplinary data services. In this paper, we assess current metrics using one of the NASA DAACs, the NASA Goddard Earth Sciences Data and Information Services Center (GES DISC), as an example, to discuss challenges and opportunities, along with recommendations for developing metrics addressing interdisciplinary satellite data products and services. Highlights Overview of NASA GES DISC Earth science datasets and services â¢ Overview of existing metrics collection methods and analysis tools with examples â¢ Discuss challenges and opportunities in collecting metrics for Earth science interdisciplinary data and services Â© 2022, Ubiquity Press. All rights reserved.","Metrics are measures that are able to produce quantifiable information. There are many applications of metrics in Earth science data and services; for example, metrics are frequently used to track service performance and progress. In short, developing, collecting and analyzing metrics are essential activities to better support Earth science research, applications, and education. As one of the largest repositories of Earth science data in the world, NASAs Earth Science Data and Information System (ESDIS) Project supports twelve Distributed Active Archive Centers (DAACs). Standard metrics have been developed by the ESDIS Metrics System (EMS). These metrics are collected and analyzed routinely at each DAAC. As it is expected that the total data volume will continue to grow rapidly, and the timely developed technologies (, cloud computing, AI/ML) will continue to improve data discovery and accessibility, opportunities for developing new data services for the Earth science community will also arise, especially in interdisciplinary research and applications. However, developing such metrics has become a challenge because multiple datasets are often needed. Current metrics are designed for a single predefined dataset or service, a disadvantage for collecting metrics for interdisciplinary data services. In this paper, we assess current metrics using one of the NASA DAACs, the NASA Goddard Earth Sciences Data and Information Services Center (GES DISC), as an example, to discuss challenges and opportunities, along with recommendations for developing metrics addressing interdisciplinary satellite data products and services. Highlights Overview of NASA GES DISC Earth science datasets and services Overview of existing metrics collection methods and analysis tools with examples Discuss challenges and opportunities in collecting metrics for Earth science interdisciplinary data and services"
Research data management status of science and technology research institutes in Korea,"Recent advances in digital technology and the data-driven science paradigm has led to a prolif-eration of research data, which are becoming more important in scholarly communications. The sharing and reuse of research data can play a key role in enhancing the reusability and repro-ducibility of research, and data from publicly funded projects are assumed to be public goods. This is seen as a movement of open science and, more specifically, open research data. Many countries, such as the USA, UK, and Australia, are pushing ahead with implementing policies and infrastructure for open research data. In this paper, we present survey results pertaining to the creation, management, and utilization of data for researchers from government-funded research institutes of science and technology in Korea. We then introduce recent regulations stipulating a mandated data management plan for national R&D projects and on-going efforts to realize open research data in Korea. Â© 2020 The Author(s).","Recent advances in digital technology and the data-driven science paradigm has led to a prolif-eration of research data, which are becoming more important in scholarly communications. The sharing and reuse of research data can play a key role in enhancing the reusability and repro-ducibility of research, and data from publicly funded projects are assumed to be public goods. This is seen as a movement of open science and, more specifically, open research data. Many countries, such as the USA, UK, and Australia, are pushing ahead with implementing policies and infrastructure for open research data. In this paper, we present survey results pertaining to the creation, management, and utilization of data for researchers from government-funded research institutes of science and technology in Korea. We then introduce recent regulations stipulating a mandated data management plan for national R&D projects and on-going efforts to realize open research data in Korea."
SASSCAL webSAPI: A web scraping application programming interface to support access to SASSCALâs weather data,"The Southern African Science Service Centre for Climate and Land Management (SASSCAL) was initiated to support regional weather monitoring and climate research in Southern Africa. As a result, several Automatic Weather Stations (AWSs) were implemented to provide numerical weather data within the collaborating countries. Meanwhile, access to the SASSCAL weather data is limited to a number of records that are achieved via a series of clicks. Currently, end users can not efficaciously extract the desired weather values. Thus, the data is not fully utilised by end users. This work contributes with an open source Web Scraping Application Programming Interface (WebSAPI) through an interactive dashboard. The objective is to extend functionalities of the SASSCAL Weathernet for: data extraction, statistical data analysis and visualisation. The SASSCAL WebSAPI was developed using the R statistical environment. It deploys web scraping and data wrangling techniques to support access to SASSCAL weather data. This WebSAPI reduces the risk of human error, and the researcherâs effort of generating desired data sets. The proposed framework for the SASSCAL WebSAPI can be modified for other weather data banks while taking into consideration the legality and ethics of the toolkit. Â© 2021 The Author(s).","The Southern African Science Service Centre for Climate and Land Management (SASSCAL) was initiated to support regional weather monitoring and climate research in Southern Africa. As a result, several Automatic Weather Stations (AWSs) were implemented to provide numerical weather data within the collaborating countries. Meanwhile, access to the SASSCAL weather data is limited to a number of records that are achieved via a series of clicks. Currently, end users can not efficaciously extract the desired weather values. Thus, the data is not fully utilised by end users. This work contributes with an open source Web Scraping Application Programming Interface (WebSAPI) through an interactive dashboard. The objective is to extend functionalities of the SASSCAL Weathernet for: data extraction, statistical data analysis and visualisation. The SASSCAL WebSAPI was developed using the R statistical environment. It deploys web scraping and data wrangling techniques to support access to SASSCAL weather data. This WebSAPI reduces the risk of human error, and the researchers effort of generating desired data sets. The proposed framework for the SASSCAL WebSAPI can be modified for other weather data banks while taking into consideration the legality and ethics of the toolkit."
Open data challenges in climate science,"The purpose of this paper is to explore challenges in open climate data experienced by data scientists at the Centre for Environmental Data Analysis (CEDA). This paper explores two of the five Vâs of Big Data, Volume and Variety. These challenges are explored using the Sentinel satellite data and Climate Modelling Intercomparison Project phase six (CMIP6) data held in the CEDA Archive. To address the Big Data Volume challenge, this paper describes the approach developed by CEDA to manage large volumes of data through the allocation of storage as filesets. These filesets allow CEDA to plan and track dataset storage volumes, a flexible approach which could be adopted by any data centre. CEDA utilise the implementation of the Climate and Forecast (CF) conventions and standard names within archived data wherever possible to overcome the challenge of Variety. Collaboration from the international science community through contributions to the moderation of CF standard names ensures these data then adhere to the FAIR (Findable, Accessible, Interoperable and Reusable) data principles. Utilising data standards such as the CF standard names is recommended because it promotes data exchange and allows data from different sources to be compared. Addressing these Open Data challenges is crucial to ensure valuable climate data are made available to the scientific community to facilitate research that addresses one of societyâs most pressing issues â climate change. Â© 2020, Ubiquity Press. All rights reserved.","The purpose of this paper is to explore challenges in open climate data experienced by data scientists at the Centre for Environmental Data Analysis (CEDA). This paper explores two of the five Vs of Big Data, Volume and Variety. These challenges are explored using the Sentinel satellite data and Climate Modelling Intercomparison Project phase six (CMIP6) data held in the CEDA Archive. To address the Big Data Volume challenge, this paper describes the approach developed by CEDA to manage large volumes of data through the allocation of storage as filesets. These filesets allow CEDA to plan and track dataset storage volumes, a flexible approach which could be adopted by any data centre. CEDA utilise the implementation of the Climate and Forecast (CF) conventions and standard names within archived data wherever possible to overcome the challenge of Variety. Collaboration from the international science community through contributions to the moderation of CF standard names ensures these data then adhere to the FAIR (Findable, Accessible, Interoperable and Reusable) data principles. Utilising data standards such as the CF standard names is recommended because it promotes data exchange and allows data from different sources to be compared. Addressing these Open Data challenges is crucial to ensure valuable climate data are made available to the scientific community to facilitate research that addresses one of societys most pressing issues climate change."
On the application of principal component analysis to classification problems,"Principal Component Analysis (PCA) is a commonly used technique that uses the correlation structure of the original variables to reduce the dimensionality of the data. This reduction is achieved by considering only the first few principal components for a subsequent analysis. The usual inclusion criterion is defined by the proportion of the total variance of the principal components exceeding a predetermined threshold. We show that in certain classification problems, even extremely high inclusion threshold can negatively impact the classification accuracy. The omission of small variance principal components can severely diminish the performance of the models. We noticed this phenomenon in classification analyses using high dimension ECG data where the most common classification methods lost between 1 and 6% of accuracy even when using 99% inclusion threshold. However, this issue can even occur in low dimension data with simple correlation structure as our numerical example shows. We conclude that the exclusion of any principal components should be carefully investigated. Â© 2021 The Author(s).","Principal Component Analysis (PCA) is a commonly used technique that uses the correlation structure of the original variables to reduce the dimensionality of the data. This reduction is achieved by considering only the first few principal components for a subsequent analysis. The usual inclusion criterion is defined by the proportion of the total variance of the principal components exceeding a predetermined threshold. We show that in certain classification problems, even extremely high inclusion threshold can negatively impact the classification accuracy. The omission of small variance principal components can severely diminish the performance of the models. We noticed this phenomenon in classification analyses using high dimension ECG data where the most common classification methods lost between 1 and 6% of accuracy even when using 99% inclusion threshold. However, this issue can even occur in low dimension data with simple correlation structure as our numerical example shows. We conclude that the exclusion of any principal components should be carefully investigated."
Machine Learning Applied for Spectra Classification in X-ray Free Electorn Laser Sciences,"Spectroscopy experiment techniques are widely used and produce a huge amount of data especially in facilities with very high repetition rates. At the European XFEL, X-ray pulses can be generated with only 220ns separation in time and a maximum of 27000 pulses per second. In experiments at the different scientific instruments, spectral changes can indicate the change of the system under investigation and so the progress of the experiment. Immediate feedback on the actual state (e.g. time-resolved status of the sample) would be essential to quickly judge how to proceed with the experiment. Hence, we aim to capture two major spectral changes. These are the change of intensity distribution (e.g. drop or appearance) of peaks at certain locations, and the shift of the peaks in the spectrum. Machine Learning (ML) opens up new avenues for data-driven analysis in spectroscopy by offering the possibility for quickly recognizing such specific changes and implementing an online feedback system which can be used near real-time during data collection. On the other hand, ML requires lots of data that are clearly annotated. Hence, it is important that experimental data should be managed along the FAIR principles. In the case of XFEL experiments, we suggest introducing NeXus glossary and the corresponding data format standards for future experiments. An example is presented to demonstrate how Neural Network-based ML can be used for accurately classifying the state of an experiment if properly annotated data is provided. Â© 2022 The Author(s).","Spectroscopy experiment techniques are widely used and produce a huge amount of data especially in facilities with very high repetition rates. At the European XFEL, X-ray pulses can be generated with only 220ns separation in time and a maximum of 27000 pulses per second. In experiments at the different scientific instruments, spectral changes can indicate the change of the system under investigation and so the progress of the experiment. Immediate feedback on the actual state ( time-resolved status of the sample) would be essential to quickly judge how to proceed with the experiment. Hence, we aim to capture two major spectral changes. These are the change of intensity distribution ( drop or appearance) of peaks at certain locations, and the shift of the peaks in the spectrum. Machine Learning opens up new avenues for data-driven analysis in spectroscopy by offering the possibility for quickly recognizing such specific changes and implementing an online feedback system which can be used near real-time during data collection. On the other hand, ML requires lots of data that are clearly annotated. Hence, it is important that experimental data should be managed along the FAIR principles. In the case of XFEL experiments, we suggest introducing NeXus glossary and the corresponding data format standards for future experiments. An example is presented to demonstrate how Neural Network-based ML can be used for accurately classifying the state of an experiment if properly annotated data is provided."
We Can Make a Better Use of ORCID: Five Observed Misapplications,"Since 2012, the âOpen Researcher and Contributor IDâ organisation (ORCID) has been successfully running a worldwide registry, with the aim of âproviding a unique, persistent identifier for individuals to use as they engage in research, scholarship, and innovation activitiesâ. Any service in the scholarly communication ecosystem (e.g., publishers, repositories, CRIS systems, etc.) can contribute to a non-ambiguous scholarly record by including, during metadata deposition, referrals to iDs in the ORCID registry. The OpenAIRE Research Graph is a scholarly knowledge graph that aggregates both records from the ORCID registry and publication records with ORCID referrals from publishers and repositories worldwide to yield research impact monitoring and Open Science statistics. Graph data analytics revealed âanomaliesâ due to ORCID registry âmisapplicationsâ, caused by wrong ORCID referrals and misexploitation of the ORCID registry. Albeit these affect just a minority of ORCID records, they inevitably affect the quality of the ORCID infrastructure and may fuel the rise of detractors and scepticism about the service. In this paper, we classify and qualitatively document such misapplications, identifying five ORCID registrant-related and ORCID referral-related anomalies to raise awareness among ORCID users. We describe the current countermeasures taken by ORCID and, where applicable, provide recommendations. Finally, we elaborate on the importance of a community-steered Open Science infrastructure and the benefits this approach has brought and may bring to ORCID. Â© 2022 The Author(s).","Since 2012, the Open Researcher and Contributor ID organisation (ORCID) has been successfully running a worldwide registry, with the aim of providing a unique, persistent identifier for individuals to use as they engage in research, scholarship, and innovation activities. Any service in the scholarly communication ecosystem (, publishers, repositories, CRIS systems, etc.) can contribute to a non-ambiguous scholarly record by including, during metadata deposition, referrals to iDs in the ORCID registry. The OpenAIRE Research Graph is a scholarly knowledge graph that aggregates both records from the ORCID registry and publication records with ORCID referrals from publishers and repositories worldwide to yield research impact monitoring and Open Science statistics. Graph data analytics revealed anomalies due to ORCID registry misapplications, caused by wrong ORCID referrals and misexploitation of the ORCID registry. Albeit these affect just a minority of ORCID records, they inevitably affect the quality of the ORCID infrastructure and may fuel the rise of detractors and scepticism about the service. In this paper, we classify and qualitatively document such misapplications, identifying five ORCID registrant-related and ORCID referral-related anomalies to raise awareness among ORCID users. We describe the current countermeasures taken by ORCID and, where applicable, provide recommendations. Finally, we elaborate on the importance of a community-steered Open Science infrastructure and the benefits this approach has brought and may bring to ORCID."
Methods to Capture User Information Needs: Design Principles for Open Data Intermediaries and Data Providers,"Data providers share open government data (OGD) to be transformed by intermediaries into products and services (solutions). OGD is believed to lead to many benefits. However, OGD is not reaching its expected level of reuse, which can come from a lack of awareness, interest to fulfil the userâs needs, or novel perspectives to understand them. This paper presents a set of design principles to develop tailored mixed methods that capture activity-based information needs of users which could be satisfied by building information products based on OGD. The produced insights can help data providers and intermediaries to realign ideas of solutions to the userâs information needs. The set of nine design principles are developed using design science research and are based on previous research and empirical testing. They have been implemented with two groups of users and three groups of data providers as participants of, respectively, face-to-face and digital workshops. The design principles and the produced insights were evaluated with practitioners. Implications for practice are that starting with the usersâ information needs can open a broader range of solutions and potential paths of OGD reuses, while following the design principles can help the practitioners cope with the fuzziness of the information needs and ideation process. For research, we propose a novel method that goes beyond the exclusive data provider-intermediary interaction to study new paths to improve the realization of OGD benefits. Â© 2021 The Author(s).","Data providers share open government data (OGD) to be transformed by intermediaries into products and services (solutions). OGD is believed to lead to many benefits. However, OGD is not reaching its expected level of reuse, which can come from a lack of awareness, interest to fulfil the users needs, or novel perspectives to understand them. This paper presents a set of design principles to develop tailored mixed methods that capture activity-based information needs of users which could be satisfied by building information products based on OGD. The produced insights can help data providers and intermediaries to realign ideas of solutions to the users information needs. The set of nine design principles are developed using design science research and are based on previous research and empirical testing. They have been implemented with two groups of users and three groups of data providers as participants of, respectively, face-to-face and digital workshops. The design principles and the produced insights were evaluated with practitioners. Implications for practice are that starting with the users information needs can open a broader range of solutions and potential paths of OGD reuses, while following the design principles can help the practitioners cope with the fuzziness of the information needs and ideation process. For research, we propose a novel method that goes beyond the exclusive data provider-intermediary interaction to study new paths to improve the realization of OGD benefits."
On the importance of 3d surface information for remote sensing classification tasks,"There has been a surge in remote sensing machine learning applications that operate on data from active or passive sensors as well as multi-sensor combinations (Ma et al. (2019)). Despite this surge, however, there has been relatively little study on the comparative value of 3D surface information for machine learning classification tasks. Adding 3D surface information to RGB imagery can provide crucial geometric information for semantic classes such as buildings, and can thus improve out-of-sample predictive performance. In this paper, we examine in-sample and out-of-sample classification performance of Fully Convolutional Neural Networks (FCNNs) and Support Vector Machines (SVMs) trained with and without 3D normalized digital surface model (nDSM) information. We assess classification performance using multispectral imagery from the International Society for Photogrammetry and Remote Sensing (ISPRS) 2D Semantic Labeling contest and the United States Special Operations Command (USSOCOM) Urban 3D Challenge. We find that providing RGB classifiers with additional 3D nDSM information results in little increase in in-sample classification performance, suggesting that spectral information alone may be sufficient for the given classification tasks. However, we observe that providing these RGB classifiers with additional nDSM information leads to significant gains in out-of-sample predictive performance. Specifically, we observe an average improvement in out-of-sample all-class accuracy of 14.4% on the ISPRS dataset and an average improvement in out-of-sample F1 score of 8.6% on the USSOCOM dataset. In addition, the experiments establish that nDSM information is critical in machine learning and classification settings that face training sample scarcity. Â© 2021, Ubiquity Press. All rights reserved.","There has been a surge in remote sensing machine learning applications that operate on data from active or passive sensors as well as multi-sensor combinations (Ma et al. ). Despite this surge, however, there has been relatively little study on the comparative value of 3D surface information for machine learning classification tasks. Adding 3D surface information to RGB imagery can provide crucial geometric information for semantic classes such as buildings, and can thus improve out-of-sample predictive performance. In this paper, we examine in-sample and out-of-sample classification performance of Fully Convolutional Neural Networks (FCNNs) and Support Vector Machines (SVMs) trained with and without 3D normalized digital surface model (nDSM) information. We assess classification performance using multispectral imagery from the International Society for Photogrammetry and Remote Sensing (ISPRS) 2D Semantic Labeling contest and the United States Special Operations Command (USSOCOM) Urban 3D Challenge. We find that providing RGB classifiers with additional 3D nDSM information results in little increase in in-sample classification performance, suggesting that spectral information alone may be sufficient for the given classification tasks. However, we observe that providing these RGB classifiers with additional nDSM information leads to significant gains in out-of-sample predictive performance. Specifically, we observe an average improvement in out-of-sample all-class accuracy of 14.4% on the ISPRS dataset and an average improvement in out-of-sample F1 score of 8.6% on the USSOCOM dataset. In addition, the experiments establish that nDSM information is critical in machine learning and classification settings that face training sample scarcity."
Affiliation information in datacite dataset metadata: A Flemish case study,"This article aims to evaluate how and to what extent metadata of datasets indexed in DataCite offer clear human- or machine-readable information that enables the research data to be linked to a particular research institution. Two main pathways are explored. First, researchers can encode their affiliation information at the moment of data submission. This can be done by means of free-text metadata fields or via the inclusion of identifiers such as GRID/ROR and ORCID. Second, affiliation information can be traced indirectly through linking between a dataset and associated publications, given that the metadata of publications is often more explicit about affiliation information than the metadata of datasets. Both pathways of affiliation information encoding are evaluated on the basis of metadata pertaining to datasets created at the five Flemish universities. It is shown that good practices such as encoding of affiliation information in a dedicated metadata field or inclusion of ORCID in the metadata are on the rise, but could be expanded further. Finally, the establishment of links between datasets and related publications is often lacking in dataset metadata, although there are important differences between data repositories, as is also demonstrated in a more data-intensive follow-up analysis based on random samples of metadata records. It is important that data repositories address this issue by providing a metadata field clearly dedicated to associated publications, prominently displayed on the landing page of the dataset. Â© 2021 The Author(s).","This article aims to evaluate how and to what extent metadata of datasets indexed in DataCite offer clear human- or machine-readable information that enables the research data to be linked to a particular research institution. Two main pathways are explored. First, researchers can encode their affiliation information at the moment of data submission. This can be done by means of free-text metadata fields or via the inclusion of identifiers such as GRID/ROR and ORCID. Second, affiliation information can be traced indirectly through linking between a dataset and associated publications, given that the metadata of publications is often more explicit about affiliation information than the metadata of datasets. Both pathways of affiliation information encoding are evaluated on the basis of metadata pertaining to datasets created at the five Flemish universities. It is shown that good practices such as encoding of affiliation information in a dedicated metadata field or inclusion of ORCID in the metadata are on the rise, but could be expanded further. Finally, the establishment of links between datasets and related publications is often lacking in dataset metadata, although there are important differences between data repositories, as is also demonstrated in a more data-intensive follow-up analysis based on random samples of metadata records. It is important that data repositories address this issue by providing a metadata field clearly dedicated to associated publications, prominently displayed on the landing page of the dataset."
Quality Management Framework for Climate Datasets,"Data from a variety of research programmes are increasingly used by policy makers, researchers, and private sectors to make data-driven decisions related to climate change and variability. Climate services are emerging as the link to narrow the gap between climate science and downstream users. The Global Framework for Climate Services (GFCS) of the World Meteorological Organization (WMO) offers an umbrella for the development of climate services and has identified the quality assessment, along with its use in user guidance, as a key aspect of the service provision. This offers an extra stimulus for discussing what type of quality information to focus on and how to present it to downstream users. Quality has become an important keyword for those working on data in both the private and public sectors and significant resources are now devoted to quality management of processes and products. Quality management guarantees reliability and usability of the product served, it is a key element to build trust between consumers and suppliers. Untrustworthy data could lead to a negative economic impact at best and a safety hazard at worst. In a progressive commitment to establish this relation of trust, as well as providing sufficient guidance for users, the Copernicus Climate Change Service (C3S) has made significant investments in the development of an Evaluation and Quality Control (EQC) function. This function offers a homogeneous user-driven service for the quality of the C3S Climate Data Store (CDS). Here we focus on the EQC component targeting the assessment of the CDS datasets, which include satellite and in-situ observations, reanalysis, climate projections, and seasonal forecasts. The EQC function is characterised by a two-tier review system designed to guarantee the quality of the dataset information. While the need of assessing the quality of climate data is well recognised, the methodologies, the metrics, the evaluation framework, and how to present all this information to the users have never been developed before in an operational service, encompassing all the main climate dataset categories. Building the underlying technical solutions poses unprecedented challenges and makes the C3S EQC approach unique. This paper describes the development and the implementation of the operational EQC function providing an overarching quality management service for the whole CDS data. Â© 2022 The Author(s).","Data from a variety of research programmes are increasingly used by policy makers, researchers, and private sectors to make data-driven decisions related to climate change and variability. Climate services are emerging as the link to narrow the gap between climate science and downstream users. The Global Framework for Climate Services (GFCS) of the World Meteorological Organization (WMO) offers an umbrella for the development of climate services and has identified the quality assessment, along with its use in user guidance, as a key aspect of the service provision. This offers an extra stimulus for discussing what type of quality information to focus on and how to present it to downstream users. Quality has become an important keyword for those working on data in both the private and public sectors and significant resources are now devoted to quality management of processes and products. Quality management guarantees reliability and usability of the product served, it is a key element to build trust between consumers and suppliers. Untrustworthy data could lead to a negative economic impact at best and a safety hazard at worst. In a progressive commitment to establish this relation of trust, as well as providing sufficient guidance for users, the Copernicus Climate Change Service (C3S) has made significant investments in the development of an Evaluation and Quality Control (EQC) function. This function offers a homogeneous user-driven service for the quality of the C3S Climate Data Store (CDS). Here we focus on the EQC component targeting the assessment of the CDS datasets, which include satellite and in-situ observations, reanalysis, climate projections, and seasonal forecasts. The EQC function is characterised by a two-tier review system designed to guarantee the quality of the dataset information. While the need of assessing the quality of climate data is well recognised, the methodologies, the metrics, the evaluation framework, and how to present all this information to the users have never been developed before in an operational service, encompassing all the main climate dataset categories. Building the underlying technical solutions poses unprecedented challenges and makes the C3S EQC approach unique. This paper describes the development and the implementation of the operational EQC function providing an overarching quality management service for the whole CDS data."
Making Drone Data FAIR Through a Community-Developed Information Framework,"Small Uncrewed Aircraft Systems (sUAS) are an increasingly common tool for data collection in many scientific fields. However, there are few standards or best practices guiding the collection, sharing, or publication of data collected with these tools. This makes collaboration, data quality control, and reproducibility challenging. To that end, we have used iterative rounds of data modeling and user engagement to develop a Minimum Information Framework (MIF) to guide sUAS users in collecting the metadata necessary to ensure that their data is trust-worthy, shareable and reusable. This paper briefly outlines our methods and the MIF itself, which includes 74 metadata terms in four classes that sUAS users should consider collecting for any given study. The MIF provides a foundation which can be used for developing standards and best practices. Â© 2023 The Author(s).","Small Uncrewed Aircraft Systems (sUAS) are an increasingly common tool for data collection in many scientific fields. However, there are few standards or best practices guiding the collection, sharing, or publication of data collected with these tools. This makes collaboration, data quality control, and reproducibility challenging. To that end, we have used iterative rounds of data modeling and user engagement to develop a Minimum Information Framework (MIF) to guide sUAS users in collecting the metadata necessary to ensure that their data is trust-worthy, shareable and reusable. This paper briefly outlines our methods and the MIF itself, which includes 74 metadata terms in four classes that sUAS users should consider collecting for any given study. The MIF provides a foundation which can be used for developing standards and best practices."
A framework for data-driven solutions with covid-19 illustrations,"Dataâdriven solutions have long been keenly sought after as tools for driving the worldâs fast changing business environment, with business leaders seeking to enhance decision making processes within their organisations. In the current era of Big Data, applications of data tools in addressing global, regional and national challenges have steadily grown in almost all fields across the globe. However, working in silos has continued to impede research progress, creating knowledge gaps and challenges across geographical borders, legislations, sectors and fields. There are many examples of the challenges the world faces in tackling global issues, including the complex interactions of the 17 Sustainable Development Goals (SDG) and the spatioâtemporal variations of the impact of the on-going COVIDâ19 pandemic. Both challenges can be seen as nonâorthogonal, strongly correlated and requiring an interdisciplinary approach to address. We present a generic framework for filling such gaps, based on two data-driven algorithms that combine data, machine learning and interdisciplinarity to bridge societal knowledge gaps. The novelty of the algorithms derives from their robust builtâin mechanics for handling data randomness. Animation applications on structured COVIDâ19 related data obtained from the European Centre for Disease Prevention and Control (ECDC) and the UK Office of National Statistics exhibit great potentials for decision-support systems. Predictive findings are based on unstructured dataâa large COVIDâ19 XâRay data, 3181 image files, obtained from GitHub and Kaggle. Our results exhibit consistent performance across samples, resonating with cross-disciplinary discussions on novel paths for data-driven interdisciplinary research. Â© 2021, Ubiquity Press. All rights reserved.","Datadriven solutions have long been keenly sought after as tools for driving the worlds fast changing business environment, with business leaders seeking to enhance decision making processes within their organisations. In the current era of Big Data, applications of data tools in addressing global, regional and national challenges have steadily grown in almost all fields across the globe. However, working in silos has continued to impede research progress, creating knowledge gaps and challenges across geographical borders, legislations, sectors and fields. There are many examples of the challenges the world faces in tackling global issues, including the complex interactions of the 17 Sustainable Development Goals (SDG) and the spatiotemporal variations of the impact of the on-going COVID19 pandemic. Both challenges can be seen as nonorthogonal, strongly correlated and requiring an interdisciplinary approach to address. We present a generic framework for filling such gaps, based on two data-driven algorithms that combine data, machine learning and interdisciplinarity to bridge societal knowledge gaps. The novelty of the algorithms derives from their robust builtin mechanics for handling data randomness. Animation applications on structured COVID19 related data obtained from the European Centre for Disease Prevention and Control (ECDC) and the UK Office of National Statistics exhibit great potentials for decision-support systems. Predictive findings are based on unstructured dataa large COVID19 XRay data, 3181 image files, obtained from GitHub and Kaggle. Our results exhibit consistent performance across samples, resonating with cross-disciplinary discussions on novel paths for data-driven interdisciplinary research."
Call to action for global access to and harmonization of quality information of individual earth science datasets,"Knowledge about the quality of data and metadata is important to support informed decisions on the (re)use of individual datasets and is an essential part of the ecosystem that supports open science. Quality assessments reflect the reliability and usability of data. They need to be consistently curated, fully traceable, and adequately documented, as these are crucial for sound decision-and policy-making efforts that rely on data. Quality assessments also need to be consistently represented and readily integrated across systems and tools to allow for improved sharing of information on quality at the dataset level for individual quality attribute or dimension. Although the need for assessing the quality of data and associated information is well recognized, methodologies for an evaluation framework and presentation of resultant quality information to end users may not have been comprehensively addressed within and across disciplines. Global interdisciplinary domain experts have come together to systematically explore needs, challenges and impacts of consistently curating and representing quality information through the entire lifecycle of a dataset. This paper describes the findings of that effort, argues the importance of sharing dataset quality information, calls for community action to develop practical guidelines, and outlines community recommendations for developing such guidelines. Practical guidelines will allow for global access to and harmonization of quality information at the level of individual Earth science datasets, which in turn will support open science. Â© 2021 The Author(s).","Knowledge about the quality of data and metadata is important to support informed decisions on the (re)use of individual datasets and is an essential part of the ecosystem that supports open science. Quality assessments reflect the reliability and usability of data. They need to be consistently curated, fully traceable, and adequately documented, as these are crucial for sound decision-and policy-making efforts that rely on data. Quality assessments also need to be consistently represented and readily integrated across systems and tools to allow for improved sharing of information on quality at the dataset level for individual quality attribute or dimension. Although the need for assessing the quality of data and associated information is well recognized, methodologies for an evaluation framework and presentation of resultant quality information to end users may not have been comprehensively addressed within and across disciplines. Global interdisciplinary domain experts have come together to systematically explore needs, challenges and impacts of consistently curating and representing quality information through the entire lifecycle of a dataset. This paper describes the findings of that effort, argues the importance of sharing dataset quality information, calls for community action to develop practical guidelines, and outlines community recommendations for developing such guidelines. Practical guidelines will allow for global access to and harmonization of quality information at the level of individual Earth science datasets, which in turn will support open science."
Implementing a registry federation for materials science data discovery,"As a result of a number of national initiatives, we are seeing rapid growth in the data important to materials science that are available over the web. Consequently, it is becoming increasingly difficult for researchers to learn what data are available and how to access them. To address this problem, the Research Data Alliance (RDA) Working Group for International Materials Science Registries (IMRR) was established to bring together materials science and information technology experts to develop an international federation of registries that can be used for global discovery of data resources for materials science. A resource registry collects high-level metadata descriptions of resources such as data repositories, archives, websites, and services that are useful for data-driven research. By making the collection searchable, it aids scientists in industry, universities, and government laboratories to discover data relevant to their research and work interests. We present the results of our successful piloting of a registry federation for materials science data discovery. In particular, we out a blueprint for creating such a federation that is capable of amassing a global view of all available materials science data, and we enumerate the requirements for the standards that make the registries interoperable within the federation. These standards include a protocol for exchanging resource descriptions and a standard metadata schema for encoding those descriptions. We summarize how we leveraged an existing standard (OAI-PMH) for metadata exchange. Finally, we review the registry software developed to realize the federation and describe the user experience. Â© 2021 The Author(s).","As a result of a number of national initiatives, we are seeing rapid growth in the data important to materials science that are available over the web. Consequently, it is becoming increasingly difficult for researchers to learn what data are available and how to access them. To address this problem, the Research Data Alliance (RDA) Working Group for International Materials Science Registries (IMRR) was established to bring together materials science and information technology experts to develop an international federation of registries that can be used for global discovery of data resources for materials science. A resource registry collects high-level metadata descriptions of resources such as data repositories, archives, websites, and services that are useful for data-driven research. By making the collection searchable, it aids scientists in industry, universities, and government laboratories to discover data relevant to their research and work interests. We present the results of our successful piloting of a registry federation for materials science data discovery. In particular, we out a blueprint for creating such a federation that is capable of amassing a global view of all available materials science data, and we enumerate the requirements for the standards that make the registries interoperable within the federation. These standards include a protocol for exchanging resource descriptions and a standard metadata schema for encoding those descriptions. We summarize how we leveraged an existing standard (OAI-PMH) for metadata exchange. Finally, we review the registry software developed to realize the federation and describe the user experience."
ESA EO data preservation system,"The European Space Agency (ESA) has the mandate to assure the long-term preservation, sharing and exploitation of space data and its associated knowledge. ESAâs aim is to turn space exploration and space-related activities into an overall societal project involving a wide variety of stakeholders. To this end, it brings together and coordinates as many countries as possible under the banner of space missions. It is a basic principle that ESA deals with its stakeholders openly and with real transparency, an approach that has contributed to its long-term success. The Earth Observation (EO) Data Preservation System has the main objective of providing the required infrastructure and services to assure ESA and Third Party Missions (TPM) EO Data Records and Associated Knowledge preservation and accessibility, and to support the cooperation activities with national and international organizations in the data preservation domain. The generic âEO Missions/Sensors Preserved Datasetâ content includes Data Records and Associated Knowledge. Â© 2020 The Author(s).","The European Space Agency (ESA) has the mandate to assure the long-term preservation, sharing and exploitation of space data and its associated knowledge. ESAs aim is to turn space exploration and space-related activities into an overall societal project involving a wide variety of stakeholders. To this end, it brings together and coordinates as many countries as possible under the banner of space missions. It is a basic principle that ESA deals with its stakeholders openly and with real transparency, an approach that has contributed to its long-term success. The Earth Observation (EO) Data Preservation System has the main objective of providing the required infrastructure and services to assure ESA and Third Party Missions (TPM) EO Data Records and Associated Knowledge preservation and accessibility, and to support the cooperation activities with national and international organizations in the data preservation domain. The generic EO Missions/Sensors Preserved Dataset content includes Data Records and Associated Knowledge."
39 hints to facilitate the use of semantics for data on agriculture and nutrition,"In this paper, we report on the outputs and adoption of the Agrisemantics Working Group of the Research Data Alliance (RDA), consisting of a set of recommendations to facilitate the adoption of semantic technologies and methods for the purpose of data interoperability in the field of agriculture and nutrition. From 2016 to 2019, the group gathered researchers and practitioners at the crossing point between information technology and agricultural science, to study all aspects in the life cycle of semantic resources: Conceptualization, edition, sharing, standardization, services, alignment, long term support. First, the working group realized a landscape study, a study of the uses of semantics in agrifood, then collected use cases for the exploitation of semantics resources â a generic term to encompass vocabularies, terminologies, thesauri, ontologies. The resulting requirements were synthesized into 39 âhintsâ for users and developers of semantic resources, and providers of semantic resource services. We believe adopting these recommendations will engage agrifood sciences in a necessary transition to leverage data production, sharing and reuse and the adoption of the FAIR data principles. The paper includes examples of adoption of those requirements, and a discussion of their contribution to the field of data science. Â© 2020 The Author(s).","In this paper, we report on the outputs and adoption of the Agrisemantics Working Group of the Research Data Alliance (RDA), consisting of a set of recommendations to facilitate the adoption of semantic technologies and methods for the purpose of data interoperability in the field of agriculture and nutrition. From 2016 to 2019, the group gathered researchers and practitioners at the crossing point between information technology and agricultural science, to study all aspects in the life cycle of semantic resources: Conceptualization, edition, sharing, standardization, services, alignment, long term support. First, the working group realized a landscape study, a study of the uses of semantics in agrifood, then collected use cases for the exploitation of semantics resources a generic term to encompass vocabularies, terminologies, thesauri, ontologies. The resulting requirements were synthesized into 39 hints for users and developers of semantic resources, and providers of semantic resource services. We believe adopting these recommendations will engage agrifood sciences in a necessary transition to leverage data production, sharing and reuse and the adoption of the FAIR data principles. The paper includes examples of adoption of those requirements, and a discussion of their contribution to the field of data science."
Yard: A tool for curating research outputs,"Repositories increasingly accept research outputs and associated artifacts that underlie reported findings, leading to potential changes in the demand for data curation and repository services. This paper describes a curation tool that responds to this challenge by economizing and optimizing curation efforts. The curation tool is implemented at Yale Universityâs Institution for Social and Policy Studies (ISPS) as YARD. By standardizing the curation workflow, YARD helps create high quality data packages that are findable, accessible, interoperable, and reusable (FAIR) and promotes research transparency by connecting the activities of researchers, curators, and publishers through a single pipeline. Â© 2020, Ubiquity Press. All rights reserved.","Repositories increasingly accept research outputs and associated artifacts that underlie reported findings, leading to potential changes in the demand for data curation and repository services. This paper describes a curation tool that responds to this challenge by economizing and optimizing curation efforts. The curation tool is implemented at Yale Universitys Institution for Social and Policy Studies (ISPS) as YARD. By standardizing the curation workflow, YARD helps create high quality data packages that are findable, accessible, interoperable, and reusable (FAIR) and promotes research transparency by connecting the activities of researchers, curators, and publishers through a single pipeline."
"Going digital: Persistent identifiers for research samples, resources and instruments","The uptake of Persistent Identifiers (PIDs) has increased in recent years and has improved the Findability, Accessibility, Interoperability and Reusability (FAIR) of various research related objects (e.g., data, software, researchers and research organisations). The uptake of PIDs for physical aspects of research (such as samples, artefacts, reagents and analyses instruments) has thus far been embraced primarily for use in the fields of Earth and life Sciences. Wider adoption of PIDs for physical aspects of research can improve the findability and accessibility of these resources, which will allow for data to be put into more detailed context. By using PIDs all the information about a sample or artefact could be more easily available in a single location, allowing for persistent links to other sources of relevant information. Through the use of interoperable (metadata) standards and shared forms of documentation it will be easier to collaborate across multiple disciplines and the reusability of resulting data and the physical samples and artefacts themselves will improve. Wider adoption of PIDs for physical aspects of research is challenging, as research communities will have to work together to establish relevant standards that are meaningful across multiple domains. The infrastructure for wider adoption already exists, it is now up to research communities to adopt standards and PIDs for the physical aspects of their research and up to funding and research institutes to support this broader adoption. Â© 2020 The Author(s).","The uptake of Persistent Identifiers (PIDs) has increased in recent years and has improved the Findability, Accessibility, Interoperability and Reusability (FAIR) of various research related objects (, data, software, researchers and research organisations). The uptake of PIDs for physical aspects of research (such as samples, artefacts, reagents and analyses instruments) has thus far been embraced primarily for use in the fields of Earth and life Sciences. Wider adoption of PIDs for physical aspects of research can improve the findability and accessibility of these resources, which will allow for data to be put into more detailed context. By using PIDs all the information about a sample or artefact could be more easily available in a single location, allowing for persistent links to other sources of relevant information. Through the use of interoperable (metadata) standards and shared forms of documentation it will be easier to collaborate across multiple disciplines and the reusability of resulting data and the physical samples and artefacts themselves will improve. Wider adoption of PIDs for physical aspects of research is challenging, as research communities will have to work together to establish relevant standards that are meaningful across multiple domains. The infrastructure for wider adoption already exists, it is now up to research communities to adopt standards and PIDs for the physical aspects of their research and up to funding and research institutes to support this broader adoption."
Virtual european solar & planetary access (VESPA): A planetary science virtual observatory cornerstone,"The Europlanet-2020 programme, which ended on Aug 31st, 2019, included an activity called VESPA (Virtual European Solar and Planetary Access), which focused on adapting Virtual Observatory (VO) techniques to handle Planetary Science data. This paper describes some aspects of VESPA at the end of this 4-years development phase and at the onset of the newly selected Europlanet-2024 programme starting in 2020. The main objectives of VESPA are to facilitate searches both in big archives and in small databases, to enable data analysis by providing simple data access and online visualization functions, and to allow research teams to publish derived data in an interoperable environment as easily as possible. VESPA encompasses a wide scope, including surfaces, atmospheres, magnetospheres and planetary plasmas, small bodies, heliophysics, exoplanets, and spectroscopy in solid phase. This system relies in particular on standards and tools developed for the Astronomy VO (IVOA) and extends them where required to handle specificities of Solar System studies. It also aims at making the VO compatible with tools and protocols developed in different contexts, for instance GIS for planetary surfaces, or time series tools for plasma-related measurements. An essential part of the activity is to publish a significant amount of high-quality data in this system, with a focus on derived products resulting from data analysis or simulations. Â© 2020 The Author(s).","The Europlanet-2020 programme, which ended on Aug 31st, 2019, included an activity called VESPA (Virtual European Solar and Planetary Access), which focused on adapting Virtual Observatory (VO) techniques to handle Planetary Science data. This paper describes some aspects of VESPA at the end of this 4-years development phase and at the onset of the newly selected Europlanet-2024 programme starting in 2020. The main objectives of VESPA are to facilitate searches both in big archives and in small databases, to enable data analysis by providing simple data access and online visualization functions, and to allow research teams to publish derived data in an interoperable environment as easily as possible. VESPA encompasses a wide scope, including surfaces, atmospheres, magnetospheres and planetary plasmas, small bodies, heliophysics, exoplanets, and spectroscopy in solid phase. This system relies in particular on standards and tools developed for the Astronomy VO (IVOA) and extends them where required to handle specificities of Solar System studies. It also aims at making the VO compatible with tools and protocols developed in different contexts, for instance GIS for planetary surfaces, or time series tools for plasma-related measurements. An essential part of the activity is to publish a significant amount of high-quality data in this system, with a focus on derived products resulting from data analysis or simulations."
Recommendations for Discipline-Specific FAIRness Evaluation Derived from Applying an Ensemble of Evaluation Tools,"From a research data repositoriesâ perspective, offering research data management services in line with the FAIR principles is becoming increasingly important. However, there exists no globally established and trusted approach to evaluate FAIRness to date. Here, we apply five different available FAIRness evaluation approaches to selected data archived in the World Data Center for Climate (WDCC). Two approaches are purely automatic, two approaches are purely manual and one approach applies a hybrid method (manual and automatic combined). The results of our evaluation show an overall mean FAIR score of WDCC-archived (meta) data of 0.67 of 1, with a range of 0.5 to 0.88. Manual approaches show higher scores than automated ones and the hybrid approach shows the highest score. Computed statistics indicate that the test approaches show an overall good agreement at the data collection level. We find that while neither one of the five valuation approaches is fully fit-for-purpose to evaluate (discipline-specific) FAIRness, all have their individual strengths. Specifically, manual approaches capture contextual aspects of FAIRness relevant for reuse, whereas automated approaches focus on the strictly standardised aspects of machine actionability. Correspondingly, the hybrid method combines the advantages and eliminates the deficiencies of manual and automatic evaluation approaches. Based on our results, we recommend future FAIRness evaluation tools to be based on a mature hybrid approach. Especially the design and adoption of the discipline-specific aspects of FAIRness will have to be conducted in concerted community efforts. Â© 2022 The Author(s).","From a research data repositories perspective, offering research data management services in line with the FAIR principles is becoming increasingly important. However, there exists no globally established and trusted approach to evaluate FAIRness to date. Here, we apply five different available FAIRness evaluation approaches to selected data archived in the World Data Center for Climate (WDCC). Two approaches are purely automatic, two approaches are purely manual and one approach applies a hybrid method (manual and automatic combined). The results of our evaluation show an overall mean FAIR score of WDCC-archived (meta) data of 0.67 of 1, with a range of 0.5 to 0.88. Manual approaches show higher scores than automated ones and the hybrid approach shows the highest score. Computed statistics indicate that the test approaches show an overall good agreement at the data collection level. We find that while neither one of the five valuation approaches is fully fit-for-purpose to evaluate (discipline-specific) FAIRness, all have their individual strengths. Specifically, manual approaches capture contextual aspects of FAIRness relevant for reuse, whereas automated approaches focus on the strictly standardised aspects of machine actionability. Correspondingly, the hybrid method combines the advantages and eliminates the deficiencies of manual and automatic evaluation approaches. Based on our results, we recommend future FAIRness evaluation tools to be based on a mature hybrid approach. Especially the design and adoption of the discipline-specific aspects of FAIRness will have to be conducted in concerted community efforts."
Ever-est: The platform allowing scientists to cross-fertilize and cross-validate data,"Over recent decades large amounts of data about our Planet have become available. If this information could be easily discoverable, accessible and properly exploited, preserved and shared, it would potentially represent a wealth of information for a whole spectrum of stakeholders: from scientists and researchers to the highest level of decision and policy makers. By creating a Virtual Research Environment (VRE) using a service oriented architecture (SOA) tailored to the needs of Earth Science (ES) communities, the EVER-EST (http://ever-est.eu) project provides a range of both generic and domain specific data analysis and management services to support a dynamic approach to collaborative research. EVER-EST provides the means to overcome existing barriers to sharing of Earth Science data and information allowing research teams to discover, access, share and process heterogeneous data, algorithms, results and experiences within and across their communities, including those domains beyond Earth Science. The main objective of this paper is to present the EVER-EST platform in all its components describing the most relevant use cases implemented by the Virtual Research Communities (VRCs) involved in the project. Â© 2020, Ubiquity Press. All rights reserved.","Over recent decades large amounts of data about our Planet have become available. If this information could be easily discoverable, accessible and properly exploited, preserved and shared, it would potentially represent a wealth of information for a whole spectrum of stakeholders: from scientists and researchers to the highest level of decision and policy makers. By creating a Virtual Research Environment (VRE) using a service oriented architecture (SOA) tailored to the needs of Earth Science (ES) communities, the EVER-EST (http://ever-est.eu) project provides a range of both generic and domain specific data analysis and management services to support a dynamic approach to collaborative research. EVER-EST provides the means to overcome existing barriers to sharing of Earth Science data and information allowing research teams to discover, access, share and process heterogeneous data, algorithms, results and experiences within and across their communities, including those domains beyond Earth Science. The main objective of this paper is to present the EVER-EST platform in all its components describing the most relevant use cases implemented by the Virtual Research Communities (VRCs) involved in the project."
Open data for sustainable development on a knowledge-based economy: The case of Botswana,"A review of sustainable economic development perspectives reveals a lack of data-driven approaches that meet the needs of knowledge-based economies. This paper presents a conceptual design artefact, a theoretical framework that maps the open data pathway toward the achievement of a knowledge-based economy and sustainable economic development with a specific reference to Botswana. The proposed framework models the transition from open data to open knowledge. It further establishes the potential impact of that transition on the realisation of a knowledge-based economy, sustainable economic development, and the attainment of a knowledge society. The method adopted in the development of the framework involves three processes: 1) review of literature on key research concepts; 2) identification of relationships between research concepts; and 3) design and development of the proposed open data framework. The proposed framework will serve as a point of reference in open data-driven economic transitions and transformations in Botswana. This design artefact can be customised to meet the economic needs of other developing countries. Â© 2020 The Author(s).","A review of sustainable economic development perspectives reveals a lack of data-driven approaches that meet the needs of knowledge-based economies. This paper presents a conceptual design artefact, a theoretical framework that maps the open data pathway toward the achievement of a knowledge-based economy and sustainable economic development with a specific reference to Botswana. The proposed framework models the transition from open data to open knowledge. It further establishes the potential impact of that transition on the realisation of a knowledge-based economy, sustainable economic development, and the attainment of a knowledge society. The method adopted in the development of the framework involves three processes: 1) review of literature on key research concepts; 2) identification of relationships between research concepts; and 3) design and development of the proposed open data framework. The proposed framework will serve as a point of reference in open data-driven economic transitions and transformations in Botswana. This design artefact can be customised to meet the economic needs of other developing countries."
Capacity development and collaboration for sustainable african agriculture: Amplification of impact through hackathons,"The paper describes the concept of INSPIRE Kampala virtual hackathons, with the main focus to build and strengthen relationships between several European Union (EU) projects and African communities that started in 2019 with the Nairobi INSPIRE Hackathon. The main focus is exploring a new model for capacity building based on virtual hackathons as an excellent opportunity for bringing together people from different work environments, culture and disciplinary backgrounds. This paper is describing experience and lessons learned from the Kampala INSPIRE Hackathon. INSPIRE Hackathons have evolved over a five year period since it started and during this period we developed a model of fully virtual Hackathons, which we recognise as optimal for Africa. The paper describes all stages of Hackathon building: definition of themes, selection of mentors, development, webinars as tools for sharing experience, final presentation, selection of winners and awarding ceremony. As important we consider also planning other actions, because we donât see INSPIRE Hackathon as an event, but as a continuous process. Demonstration part of paper describes the lessons learnt from the winning challenge: Desert Locus Monitoring. The description of all phases demonstrate Kampala INSPIRE Hackathon approach. On the basis of experience we defined strategy for the future, how to continue and successfully extend such a model in Africa. Â© 2021 The Author(s).","The paper describes the concept of INSPIRE Kampala virtual hackathons, with the main focus to build and strengthen relationships between several European Union (EU) projects and African communities that started in 2019 with the Nairobi INSPIRE Hackathon. The main focus is exploring a new model for capacity building based on virtual hackathons as an excellent opportunity for bringing together people from different work environments, culture and disciplinary backgrounds. This paper is describing experience and lessons learned from the Kampala INSPIRE Hackathon. INSPIRE Hackathons have evolved over a five year period since it started and during this period we developed a model of fully virtual Hackathons, which we recognise as optimal for Africa. The paper describes all stages of Hackathon building: definition of themes, selection of mentors, development, webinars as tools for sharing experience, final presentation, selection of winners and awarding ceremony. As important we consider also planning other actions, because we dont see INSPIRE Hackathon as an event, but as a continuous process. Demonstration part of paper describes the lessons learnt from the winning challenge: Desert Locus Monitoring. The description of all phases demonstrate Kampala INSPIRE Hackathon approach. On the basis of experience we defined strategy for the future, how to continue and successfully extend such a model in Africa."
"Correction to: Out of cite, out of mind: The current state of practice, policy, and technology for the citation of data (Data Science Journal, 2013)","This article details a correction to the article: Task Group on Data Citation Standards and Practices, C.-I., 2013. Out of Cite, Out of Mind: The Current State of Practice, Policy, and Technology for the Citation of Data. Data Science Journal, 12, pp. CIDCR1âCIDCR7. Â© 2021 The Author(s).","This article details a correction to the article: Task Group on Data Citation Standards and Practices, -, 2013. Out of Cite, Out of Mind: The Current State of Practice, Policy, and Technology for the Citation of Data. Data Science Journal, 12, pp. CIDCR1CIDCR7."
Implementing the rda research data policy framework in slovenian scientific journals,"The paper aims to present the implementation of the RDA research data policy framework in Slovenian scientific journals within the project RDA Node Slovenia. The activity aimed to implement the practice of data sharing and data citation in Slovenian scientific journals and was based on internationally renowned practices and policies, particularly the Research Data Policy Framework of the RDA Data Policy Standardization and Implementation Interest Group. Following this, the RDA Node Slovenia coordination prepared a guidance document that allowed the four pilot participating journals (from fields of archaeology, history, linguistics and social sciences) to adjust their journal policies regarding data sharing, data citation, adapted the definitions of research data and suggested appropriate data repositories that suit their dis-ciplinary specifics. The comparison of results underlines how discipline-specific the aspects of data-sharing are. The pilot proved that a grass-root approach in advancing open science can be successful and well-received in the research community, however, it also pointed out several issues in scientific publishing that would benefit from a planned action on a national level. The context of an underdeveloped data sharing culture, slow implementation of open data strategy by the national research funder and sparse national data service infrastructure creates a unique environment for this study, the result of which can be used in similar contexts worldwide. Â© 2020, Ubiquity Press. All rights reserved.","The paper aims to present the implementation of the RDA research data policy framework in Slovenian scientific journals within the project RDA Node Slovenia. The activity aimed to implement the practice of data sharing and data citation in Slovenian scientific journals and was based on internationally renowned practices and policies, particularly the Research Data Policy Framework of the RDA Data Policy Standardization and Implementation Interest Group. Following this, the RDA Node Slovenia coordination prepared a guidance document that allowed the four pilot participating journals (from fields of archaeology, history, linguistics and social sciences) to adjust their journal policies regarding data sharing, data citation, adapted the definitions of research data and suggested appropriate data repositories that suit their dis-ciplinary specifics. The comparison of results underlines how discipline-specific the aspects of data-sharing are. The pilot proved that a grass-root approach in advancing open science can be successful and well-received in the research community, however, it also pointed out several issues in scientific publishing that would benefit from a planned action on a national level. The context of an underdeveloped data sharing culture, slow implementation of open data strategy by the national research funder and sparse national data service infrastructure creates a unique environment for this study, the result of which can be used in similar contexts worldwide."
From conceptualization to implementation: Fair assessment of research data objects,"Funders and policy makers have strongly recommended the uptake of the FAIR principles in scientific data management. Several initiatives are working on the implementation of the principles and standardized applications to systematically evaluate data FAIRness. This paper presents practical solutions, namely metrics and tools, developed by the FAIRsFAIR project to pilot the FAIR assessment of research data objects in trustworthy data repositories. The metrics are mainly built on the indicators developed by the RDA FAIR Data Maturity Model Working Group. The toolsâ design and evaluation followed an iterative process. We present two applications of the metrics: an awareness-raising self-assessment tool and an automated FAIR data assessment tool. Initial results of testing the tools with researchers and data repositories are discussed, and future improvements suggested including the next steps to enable FAIR data assessment in the broader research data ecosystem. Â© 2021 The Author(s).","Funders and policy makers have strongly recommended the uptake of the FAIR principles in scientific data management. Several initiatives are working on the implementation of the principles and standardized applications to systematically evaluate data FAIRness. This paper presents practical solutions, namely metrics and tools, developed by the FAIRsFAIR project to pilot the FAIR assessment of research data objects in trustworthy data repositories. The metrics are mainly built on the indicators developed by the RDA FAIR Data Maturity Model Working Group. The tools design and evaluation followed an iterative process. We present two applications of the metrics: an awareness-raising self-assessment tool and an automated FAIR data assessment tool. Initial results of testing the tools with researchers and data repositories are discussed, and future improvements suggested including the next steps to enable FAIR data assessment in the broader research data ecosystem."
Time-series trend of pandemic sars-cov-2 variants visualized using batch-learning self-organizing map for oligonucleotide compositions,"To confront the global threat of coronavirus disease 2019, a massive number of the severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) genome sequences have been decoded, with the results promptly released through the GISAID database. Based on variant types, eight clades have already been defined in GISAID, but the diversity can be far greater. Owing to the explosive increase in available sequences, it is important to develop new technologies that can easily grasp the whole picture of the big-sequence data and support efficient knowledge discovery. An ability to efficiently clarify the detailed time-series changes in genome-wide mutation patterns will enable us to promptly identify and characterize dangerous variants that rapidly increase their population frequency. Here, we collectively analyzed over 150,000 SARS-CoV-2 genomes to understand their overall features and time-dependent changes using a batch-learning self-organizing map (BLSOM) for oligonucleotide composition, which is an unsupervised machine learning method. BLSOM can separate clades defined by GISAID with high precision, and each clade is subdivided into clusters, which shows a differential increase/decrease pattern based on geographic region and time. This allowed us to identify prevalent strains in each region and to show the commonality and diversity of the prevalent strains. Comprehensive characterization of the oligonucleotide composition of SARS-CoV-2 and elucidation of time-series trends of the population frequency of variants can clarify the viral adaptation processes after invasion into the human population and the time-dependent trend of prevalent epidemic strains across various regions, such as continents. Â© 2021 The Author(s).","To confront the global threat of coronavirus disease 2019, a massive number of the severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) genome sequences have been decoded, with the results promptly released through the GISAID database. Based on variant types, eight clades have already been defined in GISAID, but the diversity can be far greater. Owing to the explosive increase in available sequences, it is important to develop new technologies that can easily grasp the whole picture of the big-sequence data and support efficient knowledge discovery. An ability to efficiently clarify the detailed time-series changes in genome-wide mutation patterns will enable us to promptly identify and characterize dangerous variants that rapidly increase their population frequency. Here, we collectively analyzed over 150,000 SARS-CoV-2 genomes to understand their overall features and time-dependent changes using a batch-learning self-organizing map (BLSOM) for oligonucleotide composition, which is an unsupervised machine learning method. BLSOM can separate clades defined by GISAID with high precision, and each clade is subdivided into clusters, which shows a differential increase/decrease pattern based on geographic region and time. This allowed us to identify prevalent strains in each region and to show the commonality and diversity of the prevalent strains. Comprehensive characterization of the oligonucleotide composition of SARS-CoV-2 and elucidation of time-series trends of the population frequency of variants can clarify the viral adaptation processes after invasion into the human population and the time-dependent trend of prevalent epidemic strains across various regions, such as continents."
"Developing open science in Africa: Barriers, solutions and opportunities","The paper argues for the development of open science in Africa as a means of energising national science systems and their roles in supporting public and private sectors and the general public. It focuses on the complexity of the social and economic challenges created by climate change and the demographic explosion and the difficulty of confronting them in the absence of an adequate digital infrastructure. Although a well-coordinated, federated multi-state open science system would be a means of overcoming this barrier, African science systems largely operate independently of each other, creating siloes of incompatible policies, practices and data sets that are not mutually consistent or inter-operable. Africaâs linguistic chasms of English, French, Portuguese, Spanish and indigenous languages create further barriers. As international science moves towards greater openness and data sharing to address the complexity inherent in major global challenges, Africaâs stance needs radical overhaul. The paper draws on the ques-tionnaire data from 15 African Science Granting Councils and the state-of-the-art Report to them on âOpen Science in Research and Innovation for Development in Africaâ. It concludes that a well-developed Open Science system for Africa, would develop and enhance collaborations and partnerships among Africans to tackle the challenges that they face and accelerate innovation and development. Â© 2020 The Author(s).","The paper argues for the development of open science in Africa as a means of energising national science systems and their roles in supporting public and private sectors and the general public. It focuses on the complexity of the social and economic challenges created by climate change and the demographic explosion and the difficulty of confronting them in the absence of an adequate digital infrastructure. Although a well-coordinated, federated multi-state open science system would be a means of overcoming this barrier, African science systems largely operate independently of each other, creating siloes of incompatible policies, practices and data sets that are not mutually consistent or inter-operable. Africas linguistic chasms of English, French, Portuguese, Spanish and indigenous languages create further barriers. As international science moves towards greater openness and data sharing to address the complexity inherent in major global challenges, Africas stance needs radical overhaul. The paper draws on the ques-tionnaire data from 15 African Science Granting Councils and the state-of-the-art Report to them on Open Science in Research and Innovation for Development in Africa. It concludes that a well-developed Open Science system for Africa, would develop and enhance collaborations and partnerships among Africans to tackle the challenges that they face and accelerate innovation and development."
The CARE principles for indigenous data governance,"Concerns about secondary use of data and limited opportunities for benefit-sharing have focused attention on the tension that Indigenous communities feel between (1) protecting Indigenous rights and interests in Indigenous data (including traditional knowledges) and (2) supporting open data, machine learning, broad data sharing, and big data initiatives. The International Indigenous Data Sovereignty Interest Group (within the Research Data Alliance) is a network of nation-state based Indigenous data sovereignty networks and individuals that developed the âCARE Principles for Indigenous Data Governanceâ (Collective Benefit, Authority to Control, Responsibility, and Ethics) in consultation with Indigenous Peoples, scholars, non-profit organizations, and governments. The CARE Principles are people- and purpose-oriented, reflecting the crucial role of data in advancing innovation, governance, and self-determination among Indigenous Peoples. The Principles complement the existing data-centric approach represented in the âFAIR Guiding Principles for scientific data management and stewardshipâ (Findable, Accessible, Interoperable, Reusable). The CARE Principles build upon earlier work by the Te Mana Raraunga Maori Data Sovereignty Network, US Indigenous Data Sovereignty Network, Maiam nayri Wingara Aboriginal and Torres Strait Islander Data Sovereignty Collective, and numerous Indigenous Peoples, nations, and communities. The goal is that stewards and other users of Indigenous data will âBe FAIR and CARE.' In this first formal publication of the CARE Principles, we articulate their rationale, describe their relation to the FAIR Principles, and present examples of their application. Â© 2020 The Author(s).","Concerns about secondary use of data and limited opportunities for benefit-sharing have focused attention on the tension that Indigenous communities feel between protecting Indigenous rights and interests in Indigenous data (including traditional knowledges) and supporting open data, machine learning, broad data sharing, and big data initiatives. The International Indigenous Data Sovereignty Interest Group (within the Research Data Alliance) is a network of nation-state based Indigenous data sovereignty networks and individuals that developed the CARE Principles for Indigenous Data Governance (Collective Benefit, Authority to Control, Responsibility, and Ethics) in consultation with Indigenous Peoples, scholars, non-profit organizations, and governments. The CARE Principles are people- and purpose-oriented, reflecting the crucial role of data in advancing innovation, governance, and self-determination among Indigenous Peoples. The Principles complement the existing data-centric approach represented in the FAIR Guiding Principles for scientific data management and stewardship (Findable, Accessible, Interoperable, Reusable). The CARE Principles build upon earlier work by the Te Mana Raraunga Maori Data Sovereignty Network, US Indigenous Data Sovereignty Network, Maiam nayri Wingara Aboriginal and Torres Strait Islander Data Sovereignty Collective, and numerous Indigenous Peoples, nations, and communities. The goal is that stewards and other users of Indigenous data will Be FAIR and CARE.' In this first formal publication of the CARE Principles, we articulate their rationale, describe their relation to the FAIR Principles, and present examples of their application."
Oddpub â a text-mining algorithm to detect data sharing in biomedical publications,"Open research data are increasingly recognized as a quality indicator and an important resource to increase transparency, robustness and collaboration in science. However, no standardized way of reporting Open Data in publications exists, making it difficult to find shared datasets and assess the prevalence of Open Data in an automated fashion. We developed ODDPub (Open Data Detection in Publications), a text-mining algorithm that screens biomedical publications and detects cases of Open Data. Using English-language original research publications from a single biomedical research institution (n = 8689) and randomly selected from PubMed (n = 1500) we iteratively developed a set of derived keyword categories. ODDPub can detect data sharing through field-specific repositories, general-purpose repositories or the supplement. Additionally, it can detect shared analysis code (Open Code). To validate ODDPub, we manually screened 792 publications randomly selected from PubMed. On this validation dataset, our algorithm detected Open Data publications with a sensitivity of 0.73 and specificity of 0.97. Open Data was detected for 11.5% (n = 91) of publications. Open Code was detected for 1.4% (n = 11) of publications with a sensitivity of 0.73 and specificity of 1.00. We compared our results to the linked datasets found in the databases PubMed and Web of Science. Our algorithm can automatically screen large numbers of publications for Open Data. It can thus be used to assess Open Data sharing rates on the level of subject areas, journals, or institutions. It can also identify individual Open Data publications in a larger publication corpus. ODDPub is published as an R package on GitHub. Â© 2019 The Author(s).","Open research data are increasingly recognized as a quality indicator and an important resource to increase transparency, robustness and collaboration in science. However, no standardized way of reporting Open Data in publications exists, making it difficult to find shared datasets and assess the prevalence of Open Data in an automated fashion. We developed ODDPub (Open Data Detection in Publications), a text-mining algorithm that screens biomedical publications and detects cases of Open Data. Using English-language original research publications from a single biomedical research institution (n = 8689) and randomly selected from PubMed (n = 1500) we iteratively developed a set of derived keyword categories. ODDPub can detect data sharing through field-specific repositories, general-purpose repositories or the supplement. Additionally, it can detect shared analysis code (Open Code). To validate ODDPub, we manually screened 792 publications randomly selected from PubMed. On this validation dataset, our algorithm detected Open Data publications with a sensitivity of 0.73 and specificity of 0.97. Open Data was detected for 11.5% (n = 91) of publications. Open Code was detected for 1.4% (n = 11) of publications with a sensitivity of 0.73 and specificity of 1.00. We compared our results to the linked datasets found in the databases PubMed and Web of Science. Our algorithm can automatically screen large numbers of publications for Open Data. It can thus be used to assess Open Data sharing rates on the level of subject areas, journals, or institutions. It can also identify individual Open Data publications in a larger publication corpus. ODDPub is published as an R package on GitHub."
Barriers to Full Participation in the Open Science Life Cycle among Early Career Researchers,"Open science (OS) is currently dominated by a small subset of practices that occur late in the scientific process. Early career researchers (ECRs) will play a key role in transitioning the scientific community to more widespread use of OS from pre-registration to publication, but they also face unique challenges in adopting these practices. Here, we discuss these challenges across the OS life cycle. Our essay relies on the published literature, an informal survey of 32 ECRs from 14 countries, and discussions among members of the Global Working Group on Open Science (Global Young Academy and National Young Academies). We break the OS life cycle into four stagesâstudy design and tracking (pre-registration, open processes), data collection (citizen science, open hardware, open software, open data), publication (open access publishing, open peer review, open data), and outreach (open educational resources, citizen science)âand map potential barriers at each stage. The most frequently discussed barriers across the OS life cycle were a lack of awareness and training, prohibitively high time commitments, and restrictions and/or a lack of incentives by supervisors. We found that OS practices are highly fragmented and that awareness is particularly low for OS practices that occur during the study design and tracking stage, possibly creating âpath-dependenciesâ that reduce the likelihood of OS practices at later stages. We note that, because ECRs face unique barriers to adopting OS, there is a need for specifically targeted policies such as mandatory training at the graduate level and promotion incentives. Â© 2022 The Author(s).","Open science (OS) is currently dominated by a small subset of practices that occur late in the scientific process. Early career researchers (ECRs) will play a key role in transitioning the scientific community to more widespread use of OS from pre-registration to publication, but they also face unique challenges in adopting these practices. Here, we discuss these challenges across the OS life cycle. Our essay relies on the published literature, an informal survey of 32 ECRs from 14 countries, and discussions among members of the Global Working Group on Open Science (Global Young Academy and National Young Academies). We break the OS life cycle into four stagesstudy design and tracking (pre-registration, open processes), data collection (citizen science, open hardware, open software, open data), publication (open access publishing, open peer review, open data), and outreach (open educational resources, citizen science)and map potential barriers at each stage. The most frequently discussed barriers across the OS life cycle were a lack of awareness and training, prohibitively high time commitments, and restrictions and/or a lack of incentives by supervisors. We found that OS practices are highly fragmented and that awareness is particularly low for OS practices that occur during the study design and tracking stage, possibly creating path-dependencies that reduce the likelihood of OS practices at later stages. We note that, because ECRs face unique barriers to adopting OS, there is a need for specifically targeted policies such as mandatory training at the graduate level and promotion incentives."
Digital objects â FAIR digital objects: Which services are required?,"Some of the early Research Data Alliance working groups reused the notion of digital objects as digital entities described by metadata and referenced by a persistent identifier. In recent times the FAIR principles became a prominent role as framework for the sustainability of scientific data. Both approaches had always machine actionability, the capability of computational systems to use services on data without human intervention, in their focus. The more technical approach of digital objects turned out to provide a complementary view on several aspects of the policy framework of FAIR from a technical perspective. After a deeper analysis and integration of these concepts by a group of European data experts the discussion intensified on so called FAIR digital objects. But they need to be accompanied by services as building blocks for automated processes. We will describe the components of this framework and its potentials here, and also which services inside this framework are required. Â© 2020 The Author(s).","Some of the early Research Data Alliance working groups reused the notion of digital objects as digital entities described by metadata and referenced by a persistent identifier. In recent times the FAIR principles became a prominent role as framework for the sustainability of scientific data. Both approaches had always machine actionability, the capability of computational systems to use services on data without human intervention, in their focus. The more technical approach of digital objects turned out to provide a complementary view on several aspects of the policy framework of FAIR from a technical perspective. After a deeper analysis and integration of these concepts by a group of European data experts the discussion intensified on so called FAIR digital objects. But they need to be accompanied by services as building blocks for automated processes. We will describe the components of this framework and its potentials here, and also which services inside this framework are required."
A controlled vocabulary and metadata schema for materials science data discovery,"The International Materials Resource Registries (IMRR) working group of the Research Data Alliance (RDA) was created to spur initial development of a federated registry system to allow for easier discovery and access to materials data. As part of this effort, a controlled vocabulary and metadata schema were developed with contributions from members of the working group and other experts. Here we describe the process, the resulting vocabulary and XML schema, and lessons learned in the development and use of the schema. Â© 2021 The Author(s).","The International Materials Resource Registries (IMRR) working group of the Research Data Alliance (RDA) was created to spur initial development of a federated registry system to allow for easier discovery and access to materials data. As part of this effort, a controlled vocabulary and metadata schema were developed with contributions from members of the working group and other experts. Here we describe the process, the resulting vocabulary and XML schema, and lessons learned in the development and use of the schema."
Research data management for masterâs students: From awareness to action,"This article provides an analysis of how sixteen recently graduated masterâs students from the Netherlands perceive research data management. It is important to study the masterâs studentsâ attitudes towards this, as students in this phase prepare themselves for their career. Some of them might become future academics or policymakers, thus, potentially, the future advocates of good data management and reproducible science. In general, students were rather unsure what âdata managementâ meant and would often confuse it with data analysis, study design or methodology, or ethics and privacy. When students defined the concept, they focussed on privacy aspects. Concepts such as open data and the âFAIRâ principles were rarely mentioned, even though these are the cornerstones of contemporary data management efforts. In practice, the students managed their own data in an ad hoc way, and only a few of them worked with a clear data management plan. Illustrative of this is that half of the interviewees did not know where to find their data anymore. Furthermore, their study programmes had diverse approaches to data management education. Most of the classes offered were limited in scope. Nevertheless, the students seemed to be aware of the importance of data management and were willing to learn more about good data management practices. This report helps to catch an important first glimpse of how masterâs students (from different scientific backgrounds) think about research data management. Only by knowing this, accurate measures can be taken to improve data management awareness and skills. The article also provides some useful recommendations on what such measures might be, and introduces some of the steps already taken by the Delft University of Technology (TU Delft). Â© 2020 The Author(s).","This article provides an analysis of how sixteen recently graduated masters students from the Netherlands perceive research data management. It is important to study the masters students attitudes towards this, as students in this phase prepare themselves for their career. Some of them might become future academics or policymakers, thus, potentially, the future advocates of good data management and reproducible science. In general, students were rather unsure what data management meant and would often confuse it with data analysis, study design or methodology, or ethics and privacy. When students defined the concept, they focussed on privacy aspects. Concepts such as open data and the FAIR principles were rarely mentioned, even though these are the cornerstones of contemporary data management efforts. In practice, the students managed their own data in an ad hoc way, and only a few of them worked with a clear data management plan. Illustrative of this is that half of the interviewees did not know where to find their data anymore. Furthermore, their study programmes had diverse approaches to data management education. Most of the classes offered were limited in scope. Nevertheless, the students seemed to be aware of the importance of data management and were willing to learn more about good data management practices. This report helps to catch an important first glimpse of how masters students (from different scientific backgrounds) think about research data management. Only by knowing this, accurate measures can be taken to improve data management awareness and skills. The article also provides some useful recommendations on what such measures might be, and introduces some of the steps already taken by the Delft University of Technology (TU Delft)."
From FAIR leading practices to FAIR implementation and back: An inclusive approach to FAIR at leiden university libraries,"Leiden University (LU) adopted a data management regulation in 2016. The regulation embraces the Findable, Accessible, Interoperable and Reusable (FAIR) principles. To implement the regulation a programme was established. The focus of the programme was initially to raise awareness and to set up services to make data Findable and Accessible and to train researchers on data management planning. In 2019, the programme entered its second phase, with an increased focus on Interoperable and Reusable data, and on implementing the machine-actionable aspects of FAIR data. This step is non-trivial, however, because of the fast-developing FAIR research data international research field that requires fast adoption of leading practices by support professionals with the adequate skills. In this paper we describe how LU aims to close the feedback loop between international bottom-up organisations such as GOFAIR, the Research Data Alliance and CODATA on the one hand and university staff engaged in developing and implementing emerging FAIR leading practices on the other. During processes such as these, it is of crucial importance to focus primarily on the needs of researchers. We describe how LU builds up its support for FAIR data before, during and after research through its involvement in leading practices, training and consultancy and end with recommendations for other universities wanting to implement the FAIR principles. Â© 2020 The Author(s).","Leiden University (LU) adopted a data management regulation in 2016. The regulation embraces the Findable, Accessible, Interoperable and Reusable (FAIR) principles. To implement the regulation a programme was established. The focus of the programme was initially to raise awareness and to set up services to make data Findable and Accessible and to train researchers on data management planning. In 2019, the programme entered its second phase, with an increased focus on Interoperable and Reusable data, and on implementing the machine-actionable aspects of FAIR data. This step is non-trivial, however, because of the fast-developing FAIR research data international research field that requires fast adoption of leading practices by support professionals with the adequate skills. In this paper we describe how LU aims to close the feedback loop between international bottom-up organisations such as GOFAIR, the Research Data Alliance and CODATA on the one hand and university staff engaged in developing and implementing emerging FAIR leading practices on the other. During processes such as these, it is of crucial importance to focus primarily on the needs of researchers. We describe how LU builds up its support for FAIR data before, during and after research through its involvement in leading practices, training and consultancy and end with recommendations for other universities wanting to implement the FAIR principles."
Research data management challenges in citizen science projects and recommendations for library support services. A scoping review and case study,"Citizen science (CS) projects are part of a new era of data aggregation and harmonisation that facilitates interconnections between different datasets. Increasing the value and reuse of CS data has received growing attention with the appearance of the FAIR principles and systematic research data management (RDM) practises, which are often promoted by university libraries. However, RDM initiatives in CS appear diversified and if CS have special needs in terms of RDM is unclear. Therefore, the aim of this article is firstly to identify RDM challenges for CS projects and secondly, to discuss how university libraries may support any such challenges. A scoping review and a case study of Danish CS projects were performed to identify RDM challenges. 48 articles were selected for data extraction. Four academic project leaders were interviewed about RDM practices in their CS projects. Challenges and recommendations identified in the review and case study are often not specific for CS. However, finding CS data, engaging specific populations, attributing volunteers and handling sensitive data including health data are some of the challenges requiring special attention by CS project managers. Scientific requirements or national practices do not always encompass the nature of CS projects. Based on the identified challenges, it is recommended that university libraries focus their services on 1) identifying legal and ethical issues that the project managers should be aware of in their projects, 2) elaborating these issues in a Terms of Participation that also specifies data handling and sharing to the citizen scientist, and 3) motivating the project manager to good data handling practises. Adhering to the FAIR principles and good RDM practices in CS projects will continuously secure contextualisation and data quality. High data quality increases the value and reuse of the data and, therefore, the empowerment of the citizen scientists. Â© 2021 The Author(s).","Citizen science (CS) projects are part of a new era of data aggregation and harmonisation that facilitates interconnections between different datasets. Increasing the value and reuse of CS data has received growing attention with the appearance of the FAIR principles and systematic research data management (RDM) practises, which are often promoted by university libraries. However, RDM initiatives in CS appear diversified and if CS have special needs in terms of RDM is unclear. Therefore, the aim of this article is firstly to identify RDM challenges for CS projects and secondly, to discuss how university libraries may support any such challenges. A scoping review and a case study of Danish CS projects were performed to identify RDM challenges. 48 articles were selected for data extraction. Four academic project leaders were interviewed about RDM practices in their CS projects. Challenges and recommendations identified in the review and case study are often not specific for CS. However, finding CS data, engaging specific populations, attributing volunteers and handling sensitive data including health data are some of the challenges requiring special attention by CS project managers. Scientific requirements or national practices do not always encompass the nature of CS projects. Based on the identified challenges, it is recommended that university libraries focus their services on 1) identifying legal and ethical issues that the project managers should be aware of in their projects, 2) elaborating these issues in a Terms of Participation that also specifies data handling and sharing to the citizen scientist, and 3) motivating the project manager to good data handling practises. Adhering to the FAIR principles and good RDM practices in CS projects will continuously secure contextualisation and data quality. High data quality increases the value and reuse of the data and, therefore, the empowerment of the citizen scientists."
Implementing data management workflows in research groups through integrated library consultancy,"Comprehensive research data management is fundamental to ensuring reproducible, open scientific research. However, sufficient research data assistance is often not offered at universities, and when offered, typically only provides basic services that are viewed as optional. Integrating information specialists into research groups provides a potentially promising means of improving data management by providing personalized data management workflows. Workflows are comprehensive, executable guides that require planning, implementation, feedback, and adaptation. Comprehensive data management workflows should include a file organization scheme, the creation of data management roles for members, a data storage/sharing guide, and training and evaluation. Librarians, who regularly interact with faculty and students and are familiar with data management tools, are uniquely situated to assist with the creation and assessment of these workflows. Â© 2021 The Author(s).","Comprehensive research data management is fundamental to ensuring reproducible, open scientific research. However, sufficient research data assistance is often not offered at universities, and when offered, typically only provides basic services that are viewed as optional. Integrating information specialists into research groups provides a potentially promising means of improving data management by providing personalized data management workflows. Workflows are comprehensive, executable guides that require planning, implementation, feedback, and adaptation. Comprehensive data management workflows should include a file organization scheme, the creation of data management roles for members, a data storage/sharing guide, and training and evaluation. Librarians, who regularly interact with faculty and students and are familiar with data management tools, are uniquely situated to assist with the creation and assessment of these workflows."
OSSDIP: Open Source Secure Data Infrastructure and Processes Supporting Data Visiting,"Meeting the conflicting goals of protecting and maintaining control over sensitive data while also allowing access by third parties constitutes a significant challenge. Secure data infrastructures support data visiting in a highly controlled and monitored environment which, if properly set-up and operated, provide high security guarantees through a combination of technical, legal and procedural mechanisms. To ease the process of deploying such a secure data infrastructure, we present a detailed documentation of the architecture and processes of such an infrastructure and provide a pre-configured reference implementation based entirely on open source software that can be flexibly configured to meet differing security requirements and deployment scenarios. We combine mechanisms for data visiting on secured infrastructure components with optional components of data anonymization and fingerprinting, covered by extensive logging and monitoring functions and embedded in defined processes and contractual frameworks. The set-up is based upon the experience of operating such a secure infrastructure in the medical domain for almost ten years, addressing the emerging need to make such a solution available to a larger set of stakeholders. We show that our system significantly enhances data visiting, offers a higher level of data isolation and present our open source reference implementation thereof. Â© 2022 The Author(s).","Meeting the conflicting goals of protecting and maintaining control over sensitive data while also allowing access by third parties constitutes a significant challenge. Secure data infrastructures support data visiting in a highly controlled and monitored environment which, if properly set-up and operated, provide high security guarantees through a combination of technical, legal and procedural mechanisms. To ease the process of deploying such a secure data infrastructure, we present a detailed documentation of the architecture and processes of such an infrastructure and provide a pre-configured reference implementation based entirely on open source software that can be flexibly configured to meet differing security requirements and deployment scenarios. We combine mechanisms for data visiting on secured infrastructure components with optional components of data anonymization and fingerprinting, covered by extensive logging and monitoring functions and embedded in defined processes and contractual frameworks. The set-up is based upon the experience of operating such a secure infrastructure in the medical domain for almost ten years, addressing the emerging need to make such a solution available to a larger set of stakeholders. We show that our system significantly enhances data visiting, offers a higher level of data isolation and present our open source reference implementation thereof."
Application profile for machine-actionable data management plans,"This paper presents the application profile for machine-actionable data management plans that allows information from traditional data management plans to be expressed in a machine-actionable way. We describe the methodology and research conducted to define the application profile. We also discuss design decisions made during its development and present systems which have adopted it. The application profile was developed in an open and consensus-driven manner within the DMP Common Standards Working Group of the Research Data Alliance and is its official recommendation. Â© 2021, Ubiquity Press. All rights reserved.",This paper presents the application profile for machine-actionable data management plans that allows information from traditional data management plans to be expressed in a machine-actionable way. We describe the methodology and research conducted to define the application profile. We also discuss design decisions made during its development and present systems which have adopted it. The application profile was developed in an open and consensus-driven manner within the DMP Common Standards Working Group of the Research Data Alliance and is its official recommendation.
Data Management Plans: Implications for Automated Analyses,"Data management plans (DMPs) are an essential part of planning data-driven research projects and ensuring long-term access and use of research data and digital objects; however, as text-based documents, DMPs must be analyzed manually for conformance to funder requirements. This study presents a comparison of DMPs evaluations for 21 funded projects using 1) an automated means of analysis to identify elements that align with best practices in support of open research initiatives and 2) a manually-applied scorecard measuring these same elements. The automated analysis revealed that terms related to availability (90% of DMPs), metadata (86% of DMPs), and sharing (81% of DMPs) were reliably supplied. Manual analysis revealed 86% (n = 18) of funded DMPs were adequate, with strong discussions of data management personnel (average score: 2 out of 2), data sharing (average score 1.83 out of 2), and limitations to data sharing (average score: 1.65 out of 2). This study reveals that the automated approach to DMP assessment yields less granular yet similar results to manual assessments of the DMPs that are more efficiently produced. Additional observations and recommendations are also presented to make data management planning exercises and automated analysis even more useful going forward. Â© 2023 The Author(s).","Data management plans (DMPs) are an essential part of planning data-driven research projects and ensuring long-term access and use of research data and digital objects; however, as text-based documents, DMPs must be analyzed manually for conformance to funder requirements. This study presents a comparison of DMPs evaluations for 21 funded projects using 1) an automated means of analysis to identify elements that align with best practices in support of open research initiatives and 2) a manually-applied scorecard measuring these same elements. The automated analysis revealed that terms related to availability (90% of DMPs), metadata (86% of DMPs), and sharing (81% of DMPs) were reliably supplied. Manual analysis revealed 86% (n = 18) of funded DMPs were adequate, with strong discussions of data management personnel (average score: 2 out of 2), data sharing (average score 1.83 out of 2), and limitations to data sharing (average score: 1.65 out of 2). This study reveals that the automated approach to DMP assessment yields less granular yet similar results to manual assessments of the DMPs that are more efficiently produced. Additional observations and recommendations are also presented to make data management planning exercises and automated analysis even more useful going forward."
Relationship between the metadata and relevance criteria of scientific data,"The purpose of this study is to explore the information retrieval process in scientific data and to better understand the concepts and internal relationships of metadata and relevance criteria. Qualitative and quantitative analyses were performed using interview and eye movement data from 36 subjects. The results show that users paid attention to 45 types of metadata and used nine relevance criteria to judge the relevance of scientific data. There was a complex relationship between the metadata and criteria, mainly manifesting as one stimulusâmultiple responses and multiple stimuliâone response. Metadata associated with the relevance criterion of topicality is the most complex, which includes common metadata and subject-related metadata. Metadata associated with the other relevance criteria (such as quality and authority) has no obvious professional characteristics. Whatâs more, because of the essential difference between scientific data and documents, users use different criteria. When retrieving data, users pay more attention to the availability of data and whether they can be further analyzed and processed. This study clarifies the concepts of metadata and relevance criteria as well as their roles in relevance judgments. In addition, this study deepens the understanding of the scientific data relevance judgments and their cognitive process and provides a theoretical basis for improving scientific data-sharing platforms. Â© 2021 The Author(s).","The purpose of this study is to explore the information retrieval process in scientific data and to better understand the concepts and internal relationships of metadata and relevance criteria. Qualitative and quantitative analyses were performed using interview and eye movement data from 36 subjects. The results show that users paid attention to 45 types of metadata and used nine relevance criteria to judge the relevance of scientific data. There was a complex relationship between the metadata and criteria, mainly manifesting as one stimulusmultiple responses and multiple stimulione response. Metadata associated with the relevance criterion of topicality is the most complex, which includes common metadata and subject-related metadata. Metadata associated with the other relevance criteria (such as quality and authority) has no obvious professional characteristics. Whats more, because of the essential difference between scientific data and documents, users use different criteria. When retrieving data, users pay more attention to the availability of data and whether they can be further analyzed and processed. This study clarifies the concepts of metadata and relevance criteria as well as their roles in relevance judgments. In addition, this study deepens the understanding of the scientific data relevance judgments and their cognitive process and provides a theoretical basis for improving scientific data-sharing platforms."
Developing an open data portal for the ESA climate change initiative,"We introduce the rationale for, and architecture of, the European Space Agency Climate Change Initiative (CCI) Open Data Portal (http://cci.esa.int/data/). The Open Data Portal hosts a set of richly diverse datasets - 13 âEssential Climate Variablesâ - from the CCI programme in a consistent and harmonised form and to provides a single point of access for the (>100 TB) data for broad dissemination to an international user community. These data have been produced by a range of different institutions and vary across both scientific and spatio-temporal characteristics. This heterogeneity of the data together with the range of services to be supported presented significant technical challenges. An iterative development methodology was key to tackling these challenges: the system developed exploits a workflow which takes data that conforms to the CCI data specification, ingests it into a managed archive and uses both manual and automatically generated metadata to support data discovery, browse, and delivery services. It utilises both Earth System Grid Federation (ESGF) data nodes and the Open Geospatial Consortium Catalogue Service for the Web (OGC-CSW) interface, serving data into both the ESGF and the Global Earth Observation System of Systems (GEOSS). A key part of the system is a new vocabulary server, populated with CCI specific terms and relationships which integrates OGC-CSW and ESGF search services together, developed as part of a dialogue between domain scientists and linked data specialists. These services have enabled the development of a unified user interface for graphical search and visualisation - the CCI Open Data Portal Web Presence. Â© 2020 The Author(s).","We introduce the rationale for, and architecture of, the European Space Agency Climate Change Initiative Open Data Portal (http://esa.int/data/). The Open Data Portal hosts a set of richly diverse datasets - 13 Essential Climate Variables - from the CCI programme in a consistent and harmonised form and to provides a single point of access for the (>100 TB) data for broad dissemination to an international user community. These data have been produced by a range of different institutions and vary across both scientific and spatio-temporal characteristics. This heterogeneity of the data together with the range of services to be supported presented significant technical challenges. An iterative development methodology was key to tackling these challenges: the system developed exploits a workflow which takes data that conforms to the CCI data specification, ingests it into a managed archive and uses both manual and automatically generated metadata to support data discovery, browse, and delivery services. It utilises both Earth System Grid Federation (ESGF) data nodes and the Open Geospatial Consortium Catalogue Service for the Web (OGC-CSW) interface, serving data into both the ESGF and the Global Earth Observation System of Systems (GEOSS). A key part of the system is a new vocabulary server, populated with CCI specific terms and relationships which integrates OGC-CSW and ESGF search services together, developed as part of a dialogue between domain scientists and linked data specialists. These services have enabled the development of a unified user interface for graphical search and visualisation - the CCI Open Data Portal Web Presence."
Using classified and unclassified land cover data to estimate the footprint of human settlement,"Accurate, up-to-date maps of and georeferenced data about human population distribution are essential for meeting the United Nations Sustainable Development Goals progress measures, for supporting real-time crisis mapping and response efforts, and for performing many demographic and economic analyses. In December 2014, Esri published the initial version of the World Population Estimate (WPE) image service to ArcGIS Online. The service represents a dasymetric footprint of human settlement at 250-meter resolution. It is global and contains an estimate of the 2013 population for each populated cell. In 2016 Esri published an additional image service representing the earthâs population in 2015 at 162-meter resolution. Esriâs WPE is produced by combining classified land cover data indicating predominantly built-up or agricultural locations with Landsat8 Panchromatic imagery, road intersections, and known populated places. The model detects where settlement is likely to exist beyond the areas classified as predominantly built up. The result is a global dasymetric raster surface of the footprint of settlement with a score of the likelihood of human settlement for each cell of the footprint. Population data are apportioned to this settlement likelihood surface by overlaying population counts in polygons representing census enumeration units or political units representing population surveys. This paper presents the method developed at Esri for producing the estimate of settlement likelihood. Â© 2018 The Author(s).","Accurate, up-to-date maps of and georeferenced data about human population distribution are essential for meeting the United Nations Sustainable Development Goals progress measures, for supporting real-time crisis mapping and response efforts, and for performing many demographic and economic analyses. In December 2014, Esri published the initial version of the World Population Estimate (WPE) image service to ArcGIS Online. The service represents a dasymetric footprint of human settlement at 250-meter resolution. It is global and contains an estimate of the 2013 population for each populated cell. In 2016 Esri published an additional image service representing the earths population in 2015 at 162-meter resolution. Esris WPE is produced by combining classified land cover data indicating predominantly built-up or agricultural locations with Landsat8 Panchromatic imagery, road intersections, and known populated places. The model detects where settlement is likely to exist beyond the areas classified as predominantly built up. The result is a global dasymetric raster surface of the footprint of settlement with a score of the likelihood of human settlement for each cell of the footprint. Population data are apportioned to this settlement likelihood surface by overlaying population counts in polygons representing census enumeration units or political units representing population surveys. This paper presents the method developed at Esri for producing the estimate of settlement likelihood."
Efficient stratified sampling graphing method for mass data,"Sequentially linking data during polyline graphing of mass data (millions of points or more) generally results in poor graphing efficiency. Numerous curves are buried behind each pixel and cannot be displayed due to resolution limits of the width of the X-axis. Herein, a new efficient stratified sampling graphing method is proposed. The test results demonstrated that: (1) The full dataset is divided into 2X subsets, where X is the width of the X-axis in pixels, and the maximum and minimum values of the data in each subset are respectively calculated and linked in order of appearance. This method yields 4X sampled data graphs that are highly consistent with the full dataset graphs. (2) When the dataset is divided into 2X, 4X, 6X, 8X, or more subsets (progressively increasing by even multiples), the similarity gradually increases. The average similarities can reach approximately 99.24% and 99.93% in the 2X and 50X subsets, respectively. We think that 2X is the optimal subset allocation, which can achieve a high similarity, but also achieve the fastest sampling speed. (3) Compared with the speed of full dataset graphing, the overall speed of the âsingle-thread sampling + graphingâ is increased by approximately 70 times, and that of the âthreadPool sampling + graphingâ was enhanced by approximately 200 times. The method employs the minimum amount of sampled data to obtain the full dataset graph that users expect to see, thereby significantly improving graphing speeds of mass data. Â© 2019, Ubiquity Press. All rights reserved.","Sequentially linking data during polyline graphing of mass data (millions of points or more) generally results in poor graphing efficiency. Numerous curves are buried behind each pixel and cannot be displayed due to resolution limits of the width of the X-axis. Herein, a new efficient stratified sampling graphing method is proposed. The test results demonstrated that: The full dataset is divided into 2X subsets, where X is the width of the X-axis in pixels, and the maximum and minimum values of the data in each subset are respectively calculated and linked in order of appearance. This method yields 4X sampled data graphs that are highly consistent with the full dataset graphs. When the dataset is divided into 2X, 4X, 6X, 8X, or more subsets (progressively increasing by even multiples), the similarity gradually increases. The average similarities can reach approximately 99.24% and 99.93% in the 2X and 50X subsets, respectively. We think that 2X is the optimal subset allocation, which can achieve a high similarity, but also achieve the fastest sampling speed. Compared with the speed of full dataset graphing, the overall speed of the single-thread sampling + graphing is increased by approximately 70 times, and that of the threadPool sampling + graphing was enhanced by approximately 200 times. The method employs the minimum amount of sampled data to obtain the full dataset graph that users expect to see, thereby significantly improving graphing speeds of mass data."
The state of assessing data stewardship maturity â An overview,"Data stewardship encompasses all activities that preserve and improve the information content, accessibility, and usability of data and metadata. Recent regulations, mandates, policies, and guidelines set forth by the U.S. government, federal other, and funding agencies, scientific societies and scholarly publishers, have levied stewardship requirements on digital scientific data. This elevated level of requirements has increased the need for a formal approach to stewardship activities that supports compliance verification and reporting. Meeting or verifying compliance with stewardship requirements requires assessing the current state, identifying gaps, and, if necessary, defining a roadmap for improvement. This, however, touches on standards and best practices in multiple knowledge domains. Therefore, data stewardship practitioners, especially these at data repositories or data service centers or associated with data stewardship programs, can benefit from knowledge of existing maturity assessment models. This article provides an overview of the current state of assessing stewardship maturity for federally funded digital scientific data. A brief description of existing maturity assessment models and related application(s) is provided. This helps stewardship practitioners to readily obtain basic information about these models. It allows them to evaluate each modelâs suitability for their unique verification and improvement needs. Â© 2018, Ubiquity Press Ltd. All rights reserved.","Data stewardship encompasses all activities that preserve and improve the information content, accessibility, and usability of data and metadata. Recent regulations, mandates, policies, and guidelines set forth by the government, federal other, and funding agencies, scientific societies and scholarly publishers, have levied stewardship requirements on digital scientific data. This elevated level of requirements has increased the need for a formal approach to stewardship activities that supports compliance verification and reporting. Meeting or verifying compliance with stewardship requirements requires assessing the current state, identifying gaps, and, if necessary, defining a roadmap for improvement. This, however, touches on standards and best practices in multiple knowledge domains. Therefore, data stewardship practitioners, especially these at data repositories or data service centers or associated with data stewardship programs, can benefit from knowledge of existing maturity assessment models. This article provides an overview of the current state of assessing stewardship maturity for federally funded digital scientific data. A brief description of existing maturity assessment models and related application(s) is provided. This helps stewardship practitioners to readily obtain basic information about these models. It allows them to evaluate each models suitability for their unique verification and improvement needs."
Managing digital research objects in an expanding science ecosystem: 2017 conference summary,"Digital research objects are packets of information that scientists can use to organize and store their data. There are currently many different methods in use for optimizing digital objects for research purposes. These methods have been applied to many scientific disciplines but differ in architecture and approach. The goals of this joint digital research object (DRO) conference were to discuss the challenge of characterizing DROs at scale in volume and over time and possible organizing principles that might connect current DRO architectures. One of the primary challenges concerns convincing scientists that these tools and practices will actually make the research process easier and more fruitful. This conference included work from CENDI, the National Federal STI Managers Group, the National Federation of Advanced Information Services (NFAIS), the Research Data Alliance (RDA), and the National Academy of Science (NAS). Â© 2018, Ubiquity Press Ltd. All rights reserved.","Digital research objects are packets of information that scientists can use to organize and store their data. There are currently many different methods in use for optimizing digital objects for research purposes. These methods have been applied to many scientific disciplines but differ in architecture and approach. The goals of this joint digital research object (DRO) conference were to discuss the challenge of characterizing DROs at scale in volume and over time and possible organizing principles that might connect current DRO architectures. One of the primary challenges concerns convincing scientists that these tools and practices will actually make the research process easier and more fruitful. This conference included work from CENDI, the National Federal STI Managers Group, the National Federation of Advanced Information Services (NFAIS), the Research Data Alliance (RDA), and the National Academy of Science (NAS)."
"Establishing, developing, and sustaining a community of data champions","Supporting good practice in Research Data Management (RDM) is challenging for higher education institutions, in part because of the diversity of research practices and data types across disciplines. While centralised research data support units now exist in many universities, these typically possess neither the discipline-specific expertise nor the resources to offer appropriate targeted training and support within every academic unit. One solution to this problem is to identify suitable individuals with discipline-specific expertise that are already embedded within each unit, and empower these individuals to advocate for good RDM and to deliver support locally. This article focuses on an ongoing example of this approach: The Data Champion Programme at the University of Cambridge, UK. We describe how the Data Champion programme was established; the programmeâs reach, impact, strengths and weaknesses after two years of operation; and our anticipated challenges and planned strategies for maintaining the programme over the medium- and long-term. Â© 2019 The Author(s).","Supporting good practice in Research Data Management (RDM) is challenging for higher education institutions, in part because of the diversity of research practices and data types across disciplines. While centralised research data support units now exist in many universities, these typically possess neither the discipline-specific expertise nor the resources to offer appropriate targeted training and support within every academic unit. One solution to this problem is to identify suitable individuals with discipline-specific expertise that are already embedded within each unit, and empower these individuals to advocate for good RDM and to deliver support locally. This article focuses on an ongoing example of this approach: The Data Champion Programme at the University of Cambridge, UK. We describe how the Data Champion programme was established; the programmes reach, impact, strengths and weaknesses after two years of operation; and our anticipated challenges and planned strategies for maintaining the programme over the medium- and long-term."
Implementing in the VAMDC the new paradigms for data citation from the research data alliance,"VAMDC bridged the gap between atomic and molecular (A&M) producers and users by providing an interoperable e-infrastructure connecting A&M databases, as well as tools to extract and manipulate those data. The current paper highlights how the new paradigms for data citation produced by the Research Data Alliance in order to address the citation issues in the data-driven science landscape, have successfully been implemented on the VAMDC e-infrastructure. Â© 2019 The Author(s).","VAMDC bridged the gap between atomic and molecular (A&M) producers and users by providing an interoperable e-infrastructure connecting A&M databases, as well as tools to extract and manipulate those data. The current paper highlights how the new paradigms for data citation produced by the Research Data Alliance in order to address the citation issues in the data-driven science landscape, have successfully been implemented on the VAMDC e-infrastructure."
What do we know about the stewardship gap,"In the 21st century, digital data drive innovation and decision-making in nearly every field. However, little is known about the total size, characteristics, and sustainability of these data. In the scholarly sphere, it is widely suspected that there is a gap between the amount of valuable digital data that is produced and the amount that is effectively stewarded and made accessible. The Stewardship Gap Project (http://bit.ly/stewardshipgap) investigates characteristics of, and measures, the stewardship gap for sponsored scholarly activity in the United States. This paper presents a preliminary definition of the stewardship gap based on a review of relevant literature and investigates areas of the stewardship gap for which metrics have been developed and measurements made, and where work to measure the stewardship gap is yet to be done. The main findings presented are 1) there is not one stewardship gap but rather multiple âgapsâ that contribute to whether data is responsibly stewarded; 2) there are relationships between the gaps that can be used to guide strategies for addressing the various stewardship gaps; and 3) there are imbalances in the types and depths of studies that have been conducted to measure the stewardship gap. Â© 2018 The Author(s).","In the 21st century, digital data drive innovation and decision-making in nearly every field. However, little is known about the total size, characteristics, and sustainability of these data. In the scholarly sphere, it is widely suspected that there is a gap between the amount of valuable digital data that is produced and the amount that is effectively stewarded and made accessible. The Stewardship Gap Project (http://bit.ly/stewardshipgap) investigates characteristics of, and measures, the stewardship gap for sponsored scholarly activity in the United States. This paper presents a preliminary definition of the stewardship gap based on a review of relevant literature and investigates areas of the stewardship gap for which metrics have been developed and measurements made, and where work to measure the stewardship gap is yet to be done. The main findings presented are 1) there is not one stewardship gap but rather multiple gaps that contribute to whether data is responsibly stewarded; 2) there are relationships between the gaps that can be used to guide strategies for addressing the various stewardship gaps; and 3) there are imbalances in the types and depths of studies that have been conducted to measure the stewardship gap."
Development of a climate forcing observation system for Africa: Data-related considerations,"In the case of the African continent, the estimates of most climate forcing components are associated with large uncertainties, above all the greenhouse gas budget. The EU-funded SEACRIFOG project is designing an observation network which aims at reducing these uncertainties. In this practice paper, we present the various steps towards the design of this network and discuss the data-related implications. This includes the formulation of appropriate observational requirements for each variable considered essential to quantify Africa-wide climate forcing as well as an assessment of corresponding available observational infrastructures and data in order to determine data gaps, needs and priorities. The results are intended to inform the design of an interoperable African data infrastructure for environmental observations. Â© 2019 The Author(s).","In the case of the African continent, the estimates of most climate forcing components are associated with large uncertainties, above all the greenhouse gas budget. The EU-funded SEACRIFOG project is designing an observation network which aims at reducing these uncertainties. In this practice paper, we present the various steps towards the design of this network and discuss the data-related implications. This includes the formulation of appropriate observational requirements for each variable considered essential to quantify Africa-wide climate forcing as well as an assessment of corresponding available observational infrastructures and data in order to determine data gaps, needs and priorities. The results are intended to inform the design of an interoperable African data infrastructure for environmental observations."
Building infrastructure for african human genomic data management,"Human genomic data are large and complex, and require adequate infrastructure for secure storage and transfer. The NIH and The Wellcome Trust have funded multiple projects on genomic research, including the Human Heredity and Health in Africa (H3Africa) initiative, and data are required to be deposited into the public domain. The European Genome-phenome Archive (EGA) is a repository for sequence and genotype data where the data access is controlled by access committees. Access is determined by a formal application procedure for the purpose of secure storage and distribution, and must be in line with the informed consent of the study participants. H3Africa researchers based in Africa and generating their own data can benefit tremendously from the data sharing capabilities of the internet by using the appropriate technologies. The H3Africa Data Archive is an effort between the H3Africa data generating projects, H3ABioNet and the EGA to store and submit genomic data to public repositories. H3ABioNet maintains the security of the H3Africa Data Archive, ensures ethical security compliance, supports users with data submission and facilitates the data transfer. The goal is to ensure efficient data flow between researchers, the archive and the EGA or other public repositories. To comply with the H3Africa data sharing and release policy, nine months after the data is in secure storage, H3ABioNet converts the data into an XML format ready for submission to EGA. This article describes the infrastructure that has been developed for African human genomic data management. Â© Parker, Z, et al.","Human genomic data are large and complex, and require adequate infrastructure for secure storage and transfer. The NIH and The Wellcome Trust have funded multiple projects on genomic research, including the Human Heredity and Health in Africa (H3Africa) initiative, and data are required to be deposited into the public domain. The European Genome-phenome Archive (EGA) is a repository for sequence and genotype data where the data access is controlled by access committees. Access is determined by a formal application procedure for the purpose of secure storage and distribution, and must be in line with the informed consent of the study participants. H3Africa researchers based in Africa and generating their own data can benefit tremendously from the data sharing capabilities of the internet by using the appropriate technologies. The H3Africa Data Archive is an effort between the H3Africa data generating projects, H3ABioNet and the EGA to store and submit genomic data to public repositories. H3ABioNet maintains the security of the H3Africa Data Archive, ensures ethical security compliance, supports users with data submission and facilitates the data transfer. The goal is to ensure efficient data flow between researchers, the archive and the EGA or other public repositories. To comply with the H3Africa data sharing and release policy, nine months after the data is in secure storage, H3ABioNet converts the data into an XML format ready for submission to EGA. This article describes the infrastructure that has been developed for African human genomic data management."
Data sharing at scale: A heuristic for affirming data cultures,"Addressing the most pressing contemporary social, environmental, and technological challenges will require integrating insights and sharing data across disciplines, geographies, and cultures. Strengthening international data sharing networks will not only demand advancing technical, legal, and logistical infrastructure for publishing data in open, accessible formats; it will also require recognizing, respecting, and learning to work across diverse data cultures. This essay introduces a heuristic for pursuing richer characterizations of the âdata culturesâ at play in international, interdisciplinary data sharing. The heuristic prompts cultural analysts to query the contexts of data sharing for a particular discipline, institution, geography, or project at seven scales â the meta, macro, meso, micro, techno, data, and nano. The essay articulates examples of the diverse cultural forces acting upon and interacting with researchers in different communities at each scale. The heuristic we introduce in this essay aims to elicit from researchers the beliefs, values, practices, incentives, and restrictions that impact how they think about and approach data sharing â not in an effort to iron out differences between disciplines, but instead to showcase and affirm the diversity of traditions and modes of analysis that have shaped how data gets collected, organized, and interpreted in diverse settings. Â© 2019 The Author(s).","Addressing the most pressing contemporary social, environmental, and technological challenges will require integrating insights and sharing data across disciplines, geographies, and cultures. Strengthening international data sharing networks will not only demand advancing technical, legal, and logistical infrastructure for publishing data in open, accessible formats; it will also require recognizing, respecting, and learning to work across diverse data cultures. This essay introduces a heuristic for pursuing richer characterizations of the data cultures at play in international, interdisciplinary data sharing. The heuristic prompts cultural analysts to query the contexts of data sharing for a particular discipline, institution, geography, or project at seven scales the meta, macro, meso, micro, techno, data, and nano. The essay articulates examples of the diverse cultural forces acting upon and interacting with researchers in different communities at each scale. The heuristic we introduce in this essay aims to elicit from researchers the beliefs, values, practices, incentives, and restrictions that impact how they think about and approach data sharing not in an effort to iron out differences between disciplines, but instead to showcase and affirm the diversity of traditions and modes of analysis that have shaped how data gets collected, organized, and interpreted in diverse settings."
Developing a research data policy framework for all journals and publishers: An output of the data policy standardisation and implementation interest group (IG) of the research data alliance (RDA),"More journals and publishers â and funding agencies and institutions â are introducing research data policies. But as the prevalence of policies increases, there is potential to confuse researchers and support staff with numerous or conflicting policy requirements. We define and describe 14 features of journal research data policies and arrange these into a set of six standard policy types or tiers, which can be adopted by journals and publishers to promote data sharing in a way that encourages good practice and is appropriate for their audienceâs perceived needs. Policy features include coverage of topics such as data citation, data repositories, data availability statements, data standards and formats, and peer review of research data. These policy features and types have been created by reviewing the policies of multiple scholarly publishers, which collectively publish more than 10,000 journals, and through discussions and consensus building with multiple stakeholders in research data policy via the Data Policy Standardisation and Implementation Interest Group of the Research Data Alliance. Implementation guidelines for the standard research data policies for journals and publishers are also provided, along with template policy texts which can be implemented by journals in their Information for Authors and publishing workflows. We conclude with a call for collaboration across the scholarly publishing and wider research community to drive further implementation and adoption of consistent research data policies. Â© 2020 The Author(s).","More journals and publishers and funding agencies and institutions are introducing research data policies. But as the prevalence of policies increases, there is potential to confuse researchers and support staff with numerous or conflicting policy requirements. We define and describe 14 features of journal research data policies and arrange these into a set of six standard policy types or tiers, which can be adopted by journals and publishers to promote data sharing in a way that encourages good practice and is appropriate for their audiences perceived needs. Policy features include coverage of topics such as data citation, data repositories, data availability statements, data standards and formats, and peer review of research data. These policy features and types have been created by reviewing the policies of multiple scholarly publishers, which collectively publish more than 10,000 journals, and through discussions and consensus building with multiple stakeholders in research data policy via the Data Policy Standardisation and Implementation Interest Group of the Research Data Alliance. Implementation guidelines for the standard research data policies for journals and publishers are also provided, along with template policy texts which can be implemented by journals in their Information for Authors and publishing workflows. We conclude with a call for collaboration across the scholarly publishing and wider research community to drive further implementation and adoption of consistent research data policies."
"Diversity of woody species in djamde wildlife reserve, Northern Togo, West Africa","Djamde Wildlife Reserve, with an area of about 8,000 ha, is located in Kozah prefecture (Kara region in northern Togo), between 9Â°31â and 9Â°35â N latitude and 1Â°01â and 1Â°05â E longitude. This reserve was created by the merger in 2003 of the classified forests of Djamde and Kindja. These two forests were classified respectively by Decision No. 766-54/EF and No. 765-54/EF signed on July, 29th 1954 by the colonial authority. To date, there is still no legislative enactment that establishes Djamdeâs reserve status. Since 2003, its management has been granted to TOGO-FAUNE Company. Different plant communities identified are: clear forests, gallery forests, shrubby savannas and a mosaic of wooded/tree savannas. The inventory of woody species carried out on circular plots, allowed to identify 126 ligneous species distributed in 40 botanical families and 94 genera. Eleven of these species are classified in the IUCN Red List with the status of âextinction riskâ. The pressures on the biodiversity of Djamdeâs reserve include poaching, wood fuel production and vegetation fires. Aim of this project is to create an Ecological Corridor between Wildlife Reserve and Sarakawaâs Park and to promote public-private partnership in protected area management. Â© 2019 The Author(s).","Djamde Wildlife Reserve, with an area of about 8,000 ha, is located in Kozah prefecture (Kara region in northern Togo), between 931 and 935 N latitude and 101 and 105 E longitude. This reserve was created by the merger in 2003 of the classified forests of Djamde and Kindja. These two forests were classified respectively by Decision No. 766-54/EF and No. 765-54/EF signed on July, 29th 1954 by the colonial authority. To date, there is still no legislative enactment that establishes Djamdes reserve status. Since 2003, its management has been granted to TOGO-FAUNE Company. Different plant communities identified are: clear forests, gallery forests, shrubby savannas and a mosaic of wooded/tree savannas. The inventory of woody species carried out on circular plots, allowed to identify 126 ligneous species distributed in 40 botanical families and 94 genera. Eleven of these species are classified in the IUCN Red List with the status of extinction risk. The pressures on the biodiversity of Djamdes reserve include poaching, wood fuel production and vegetation fires. Aim of this project is to create an Ecological Corridor between Wildlife Reserve and Sarakawas Park and to promote public-private partnership in protected area management."
How do people make relevance judgment of scientific data?,"Many efforts have been made to explore user relevance judgment for documents, images, web pages and music in the field of information retrial. However, there is a lack of attention to scientific data even when scientists and researchers are facing an increasing data deluge. In this study, we carried out a two-phase (first exploratory and then empirical) research to explore relevance judgment patterns of scientific data users. In the exploratory study, we interviewed 23 subjects who participated in a national competition related to scientific data. Five relevance criteria (RC) and seven paths of their usage were identified by content analysis of the transcribed records of the interview. Based on the results of the first phase, seven hypotheses were proposed and verified by partial least squares structural equation modelling (PLS-SEM). The study identified five RC, i.e. topicality, accessibility, authority, quality and usefulness used by scientific data users. Three patterns were identified including 1) data topicality judgment as the first step or starting point, 2) data reliability judgment as the necessary process and 3) data utility judgment as final purpose. These findings provide new understanding of relevance judgement and behaviours of scientific data users, and could benefit the design for cognitive retrieval systems and algorithms specific to scientific data. Â© 2020 The Author(s).","Many efforts have been made to explore user relevance judgment for documents, images, web pages and music in the field of information retrial. However, there is a lack of attention to scientific data even when scientists and researchers are facing an increasing data deluge. In this study, we carried out a two-phase (first exploratory and then empirical) research to explore relevance judgment patterns of scientific data users. In the exploratory study, we interviewed 23 subjects who participated in a national competition related to scientific data. Five relevance criteria (RC) and seven paths of their usage were identified by content analysis of the transcribed records of the interview. Based on the results of the first phase, seven hypotheses were proposed and verified by partial least squares structural equation modelling (PLS-SEM). The study identified five RC, topicality, accessibility, authority, quality and usefulness used by scientific data users. Three patterns were identified including 1) data topicality judgment as the first step or starting point, 2) data reliability judgment as the necessary process and 3) data utility judgment as final purpose. These findings provide new understanding of relevance judgement and behaviours of scientific data users, and could benefit the design for cognitive retrieval systems and algorithms specific to scientific data."
A method for extending ontologies with application to the materials science domain,"In the materials science domain the data-driven science paradigm has become the focus since the beginning of the 2000s. A large number of research groups and communities are building and developing data-driven workflows. However, much of the data and knowledge is stored in different heterogeneous data sources maintained by different groups. This leads to a reduced availability of the data and poor interoperability between systems in this domain. Ontology-based techniques are an important way to reduce these problems and a number of efforts have started. In this paper we investigate efforts in the materials science, and in particular in the nanotechnology domain, and show how such ontologies developed by domain experts, can be improved. We use a phrase-based topic model approach and formal topical concept analysis on unstructured text in this domain to suggest additional concepts and axioms for the ontology that should be validated by a domain expert. We describe the techniques and show the usefulness of the approach through an experiment where we extend two nanotechnology ontologies using approximately 600 titles and abstracts. Â© 2019 The Author(s).","In the materials science domain the data-driven science paradigm has become the focus since the beginning of the 2000s. A large number of research groups and communities are building and developing data-driven workflows. However, much of the data and knowledge is stored in different heterogeneous data sources maintained by different groups. This leads to a reduced availability of the data and poor interoperability between systems in this domain. Ontology-based techniques are an important way to reduce these problems and a number of efforts have started. In this paper we investigate efforts in the materials science, and in particular in the nanotechnology domain, and show how such ontologies developed by domain experts, can be improved. We use a phrase-based topic model approach and formal topical concept analysis on unstructured text in this domain to suggest additional concepts and axioms for the ontology that should be validated by a domain expert. We describe the techniques and show the usefulness of the approach through an experiment where we extend two nanotechnology ontologies using approximately 600 titles and abstracts."
A generic research data infrastructure for long tail research data management,"The advent of data intensive science has fueled the generation of digital scientific data. Undoubtedly, digital research data plays a pivotal role in transparency and re-producibility of scientific results as well as in steering the innovation in a research process. However, the main challenges for science policy and infrastructure projects are to develop practices and solutions for research data management which in compliance with good scientific standards make the research data discoverable, citeble and accessible for society potential reuse. GeRDI â the Generic Research Data (RD) Infrastructure â is such a research data management initiative which targets long tail content that stems from research communities belonging to different domain and research practices. It provides a generic and open software which connects research data infrastructures of communities to enable the investigation of multidisciplinary research questions. Â© 2019 The Author(s).","The advent of data intensive science has fueled the generation of digital scientific data. Undoubtedly, digital research data plays a pivotal role in transparency and re-producibility of scientific results as well as in steering the innovation in a research process. However, the main challenges for science policy and infrastructure projects are to develop practices and solutions for research data management which in compliance with good scientific standards make the research data discoverable, citeble and accessible for society potential reuse. GeRDI the Generic Research Data (RD) Infrastructure is such a research data management initiative which targets long tail content that stems from research communities belonging to different domain and research practices. It provides a generic and open software which connects research data infrastructures of communities to enable the investigation of multidisciplinary research questions."
Data curation profiling to assess data management training needs and practices to inform a toolkit,"The purpose of this paper is to explore current data management training needs and practices for Belmont Forum member agencies and researchers to inform a Toolkit. Fourteen Belmont Forum affiliated individuals were interviewed following a predetermined set of questions to create data curation profiles of their funded work. The data curation profile questionnaire includes questions related to data management, storage, stakeholders, costs, training, and credentials. The interview findings highlight gaps in existing knowledge of data management theory and practice that could impact data re-use. Results of these interviews were used to populate a Toolkit of data management training and effective practice resources specifically developed to train Belmont Forum grant awardees. The results also highlight some attitudes and behaviours of current scientists, researchers, and agency representatives, and the impact of the implementation of data management plans on the open science movement. Â© 2020 The Author(s).","The purpose of this paper is to explore current data management training needs and practices for Belmont Forum member agencies and researchers to inform a Toolkit. Fourteen Belmont Forum affiliated individuals were interviewed following a predetermined set of questions to create data curation profiles of their funded work. The data curation profile questionnaire includes questions related to data management, storage, stakeholders, costs, training, and credentials. The interview findings highlight gaps in existing knowledge of data management theory and practice that could impact data re-use. Results of these interviews were used to populate a Toolkit of data management training and effective practice resources specifically developed to train Belmont Forum grant awardees. The results also highlight some attitudes and behaviours of current scientists, researchers, and agency representatives, and the impact of the implementation of data management plans on the open science movement."
Developing a model guidelines addressing legal impediments to open access to publicly funded research data in Malaysia,"The objective of this study is to develop a model guidelines addressing legal impediments to open access to publicly funded research data in Malaysia. Previous studies have identified legal impediments to open access arising from intellectual property, confidentiality, privacy, national security, patent and tort laws. The legal impediments have not been fully addressed by public research funding agencies in Malaysia, thus the need for a model guidelines to be developed. This study conducted a comparative analysis of the principles/policies/guidelines on open access to research data of the civil society, government bodies, research funding agencies and research institutions in Australia, Canada, the EU, the UK and the USA. This comparative analysis attempts to identify the appropriate measures to address the legal impediments to open access to research data. This model guidelines is of international standard and suitable for adoption by public research funding agencies and research institutions in Malaysia. Hence, the model guidelines can become a benchmark in pursuing the objective of enabling open access to publicly funded research data in Malaysia. Â© 2019 The Author(s).","The objective of this study is to develop a model guidelines addressing legal impediments to open access to publicly funded research data in Malaysia. Previous studies have identified legal impediments to open access arising from intellectual property, confidentiality, privacy, national security, patent and tort laws. The legal impediments have not been fully addressed by public research funding agencies in Malaysia, thus the need for a model guidelines to be developed. This study conducted a comparative analysis of the principles/policies/guidelines on open access to research data of the civil society, government bodies, research funding agencies and research institutions in Australia, Canada, the EU, the UK and the USA. This comparative analysis attempts to identify the appropriate measures to address the legal impediments to open access to research data. This model guidelines is of international standard and suitable for adoption by public research funding agencies and research institutions in Malaysia. Hence, the model guidelines can become a benchmark in pursuing the objective of enabling open access to publicly funded research data in Malaysia."
Bringing citations and usage metrics together to make data count,"Over the last years, many organizations have been working on infrastructure to facilitate sharing and reuse of research data. This means that researchers now have ways of making their data available, but not necessarily incentives to do so. Several Research Data Alliance (RDA) working groups have been working on ways to start measuring activities around research data to provide input for new Data Level Metrics (DLMs). These DLMs are a critical step towards providing researchers with credit for their work. In this paper, we describe the outcomes of the work of the Scholarly Link Exchange (Scholix) working group and the Data Usage Metrics working group. The Scholix working group developed a framework that allows organizations to expose and discover links between articles and datasets, thereby providing an indication of data citations. The Data Usage Metrics group works on a standard for the measurement and display of Data Usage Metrics. Here we explain how publishers and data repositories can contribute to and benefit from these initiatives. Together, these contributions feed into several hubs that enable data repositories to start displaying DLMs. Once these DLMs are available, researchers are in a better position to make their data count and be rewarded for their work. Â© 2019 The Author(s).","Over the last years, many organizations have been working on infrastructure to facilitate sharing and reuse of research data. This means that researchers now have ways of making their data available, but not necessarily incentives to do so. Several Research Data Alliance (RDA) working groups have been working on ways to start measuring activities around research data to provide input for new Data Level Metrics (DLMs). These DLMs are a critical step towards providing researchers with credit for their work. In this paper, we describe the outcomes of the work of the Scholarly Link Exchange (Scholix) working group and the Data Usage Metrics working group. The Scholix working group developed a framework that allows organizations to expose and discover links between articles and datasets, thereby providing an indication of data citations. The Data Usage Metrics group works on a standard for the measurement and display of Data Usage Metrics. Here we explain how publishers and data repositories can contribute to and benefit from these initiatives. Together, these contributions feed into several hubs that enable data repositories to start displaying DLMs. Once these DLMs are available, researchers are in a better position to make their data count and be rewarded for their work."
GeoSimMR: A mapreduce algorithm for detecting communities based on distance and interest in social networks,"Analyzing social networks has received a lot of reviews in the recent literature. Many papers have been proposed to provide new techniques for mining social networks to help further study this huge amount of data. However, to the best of our knowledge, none of them considered the semantic meaning of the nodes interests while clustering the network. In this work, we propose a new algorithm, namely GeoSim, for clustering users in any social network site into communities based on the semantic meaning of the nodes interests as well as their relationships with each other. Moreover, this paper proposes a parallel version of the GeoSim algorithm that utilizes the MapReduce model to run on multiple machines simultaneously and get faster results. The two versions of the algorithm (centralized and parallel) are examined thoroughly to test their performance. The experiments show that both versions of the GeoSim algorithm achieve high community detection accuracy and scale linearly with the size of the cluster. Â© 2019 The Author(s).","Analyzing social networks has received a lot of reviews in the recent literature. Many papers have been proposed to provide new techniques for mining social networks to help further study this huge amount of data. However, to the best of our knowledge, none of them considered the semantic meaning of the nodes interests while clustering the network. In this work, we propose a new algorithm, namely GeoSim, for clustering users in any social network site into communities based on the semantic meaning of the nodes interests as well as their relationships with each other. Moreover, this paper proposes a parallel version of the GeoSim algorithm that utilizes the MapReduce model to run on multiple machines simultaneously and get faster results. The two versions of the algorithm (centralized and parallel) are examined thoroughly to test their performance. The experiments show that both versions of the GeoSim algorithm achieve high community detection accuracy and scale linearly with the size of the cluster."
A conceptual enterprise framework for managing scientific data stewardship,"Scientific data stewardship is an important part of long-term preservation and the use/reuse of digital research data. It is critical for ensuring trustworthiness of data, products, and services, which is important for decision-making. Recent U.S. federal government directives and scientific organization guidelines have levied specific requirements, increasing the need for a more formal approach to ensuring that stewardship activities support compliance verification and reporting. However, many science data centers lack an integrated, systematic, and holistic framework to support such efforts. The current business-and process-oriented stewardship frameworks are too costly and lengthy for most data centers to implement. They often do not explicitly address the federal stewardship requirements and/or the uniqueness of geospatial data. This work proposes a data-centric conceptual enterprise framework for managing stewardship activities, based on the philosophy behind the Plan-Do-Check-Act (PDCA) cycle, a proven industrial concept. This framework, which includes the application of maturity assessment models, allows for quantitative evaluation of how organizations manage their stewardship activities and supports informed decision-making for continual improvement towards full compliance with federal, agency, and user requirements. Â© 2018, Ubiquity Press Ltd. All rights reserved.","Scientific data stewardship is an important part of long-term preservation and the use/reuse of digital research data. It is critical for ensuring trustworthiness of data, products, and services, which is important for decision-making. Recent federal government directives and scientific organization guidelines have levied specific requirements, increasing the need for a more formal approach to ensuring that stewardship activities support compliance verification and reporting. However, many science data centers lack an integrated, systematic, and holistic framework to support such efforts. The current business-and process-oriented stewardship frameworks are too costly and lengthy for most data centers to implement. They often do not explicitly address the federal stewardship requirements and/or the uniqueness of geospatial data. This work proposes a data-centric conceptual enterprise framework for managing stewardship activities, based on the philosophy behind the Plan-Do-Check-Act (PDCA) cycle, a proven industrial concept. This framework, which includes the application of maturity assessment models, allows for quantitative evaluation of how organizations manage their stewardship activities and supports informed decision-making for continual improvement towards full compliance with federal, agency, and user requirements."
Data distribution centre support for the ipcc sixth assessment,"The information provided in the Intergovernmental Panel on Climate Change (IPCC; http://ipcc.ch) Assessment Reports (ARs) inform climate change policy development. Within the IPCC the scientific coordination of the ARs is conducted by three Working Groups (WGs) comprising of the Bureaus supported by their Technical Support Units (TSUs). Data management support is provided by the IPCC Data Distribution Centre (DDC; http://ipcc-data.org), which is overseen by the Task Group on Data Support for Climate Change Assessments (TG-Data; formerly TGICA). The DDC is a federated structure that is currently managed by the Centre for Environmental Data Analysis (CEDA; http://www.ceda.ac.uk/), United Kingdom; the World Data Center for Climate (WDCC; http://www.wdc-climate.de), Germany; and the Center for International Earth Science Information Network (CIESIN; http://www.ciesin.columbia.edu/) at Columbia University, U.S. For the IPCC Sixth Assessment cycle (AR6), analyses of climate simulations and observations published in scientific literature will be assessed. The reports will include figures and tables prepared from the underlying digital information. The DDC plays an increasingly important role in facilitating the exchange of data, as well as curating the assessed datasets, scripts and provenance records to facilitate the assessment process and to support the traceability of AR6 results through long-term continuity of data management and curation. These issues, among others, are addressed by the DDC support group (https://cedadev.github.io/ipcc_ddc) currently consisting of members from the three TSUs and the three DDC managers. Â© 2019 The Author(s).","The information provided in the Intergovernmental Panel on Climate Change (IPCC; http://ipcc.ch) Assessment Reports (ARs) inform climate change policy development. Within the IPCC the scientific coordination of the ARs is conducted by three Working Groups (WGs) comprising of the Bureaus supported by their Technical Support Units (TSUs). Data management support is provided by the IPCC Data Distribution Centre (DDC; http://ipcc-data.org), which is overseen by the Task Group on Data Support for Climate Change Assessments (TG-Data; formerly TGICA). The DDC is a federated structure that is currently managed by the Centre for Environmental Data Analysis (CEDA; http://www.ceda.ac.uk/), United Kingdom; the World Data Center for Climate (WDCC; http://www.wdc-climate.de), Germany; and the Center for International Earth Science Information Network (CIESIN; http://www.ciesin.columbia.edu/) at Columbia University, For the IPCC Sixth Assessment cycle (AR6), analyses of climate simulations and observations published in scientific literature will be assessed. The reports will include figures and tables prepared from the underlying digital information. The DDC plays an increasingly important role in facilitating the exchange of data, as well as curating the assessed datasets, scripts and provenance records to facilitate the assessment process and to support the traceability of AR6 results through long-term continuity of data management and curation. These issues, among others, are addressed by the DDC support group (https://cedadev.github.io/ipcc_ddc) currently consisting of members from the three TSUs and the three DDC managers."
Experimental data of Muon hodoscope URAGAN for investigations of geoffective processes in the heliosphere,"Muon hodoscope URAGAN continuously detects the angular distribution of muons in a wide range of zenith angles and allows one to obtain information about variations, both in the intensity and in angular characteristics of the muon flux, caused by active processes in the heliosphere, the magnetosphere and atmosphere of the Earth. Various parameters of the muon flux anisotropy and methods of calculation of these parameters are discussed. Real-time processing of a continuous flow of multidimensional data from the muon hodoscope URAGAN is quite a challenge. In the article, methods of formation and primary analysis of the data, their processing in real time and obtaining time series of various parameters of integral counting rate and angular anisotropy of the muon flux, which are important for the physical analysis of modulation processes of cosmic rays in the heliosphere, are presented. Â© 2020, Ubiquity Press. All rights reserved.","Muon hodoscope URAGAN continuously detects the angular distribution of muons in a wide range of zenith angles and allows one to obtain information about variations, both in the intensity and in angular characteristics of the muon flux, caused by active processes in the heliosphere, the magnetosphere and atmosphere of the Earth. Various parameters of the muon flux anisotropy and methods of calculation of these parameters are discussed. Real-time processing of a continuous flow of multidimensional data from the muon hodoscope URAGAN is quite a challenge. In the article, methods of formation and primary analysis of the data, their processing in real time and obtaining time series of various parameters of integral counting rate and angular anisotropy of the muon flux, which are important for the physical analysis of modulation processes of cosmic rays in the heliosphere, are presented."
Analysis of rainfall and temperature data using ensemble empirical mode decomposition,"Climatic variables such as rainfall and temperature have nonlinear and non-stationary characteristics such that analysing them using linear methods inconclusive results are found. Ensemble empirical mode decomposition (EEMD) is a data-adaptive method that is best suitable for data with nonlinear and non-stationary characteristics. The average monthly rainfall and temperature data for a selected region in South Africa are decomposed into intrinsic mode functions (IMFs) at different time scales using EEMD. The IMFs exhibit an inter-annual to inter-decadal variability. The influence of climatic oscillations such as El-NiÃ±o Southern Oscillation (ENSO) and quasi-biennial oscillation (QBO) is identified. The influence of temperature variability on rainfall is also shown at different time scales. Based on the results obtained, the EEMD method is found to be suitable to identify different oscillations in the rainfall and temperature data. Â© Zvarevashe, W, et al.","Climatic variables such as rainfall and temperature have nonlinear and non-stationary characteristics such that analysing them using linear methods inconclusive results are found. Ensemble empirical mode decomposition (EEMD) is a data-adaptive method that is best suitable for data with nonlinear and non-stationary characteristics. The average monthly rainfall and temperature data for a selected region in South Africa are decomposed into intrinsic mode functions (IMFs) at different time scales using EEMD. The IMFs exhibit an inter-annual to inter-decadal variability. The influence of climatic oscillations such as El-Nio Southern Oscillation (ENSO) and quasi-biennial oscillation (QBO) is identified. The influence of temperature variability on rainfall is also shown at different time scales. Based on the results obtained, the EEMD method is found to be suitable to identify different oscillations in the rainfall and temperature data."
The EnviDat concept for an institutional environmental data portal,"EnviDat is the environmental data portal developed by the Swiss Federal Institute for Forest, Snow and Landscape Research WSL. The strategic initiative EnviDat highlights the importance WSL lays on Research Data Management (RDM) at the institutional level and demonstrates the commitment to accessible research data in order to advance environmental science. EnviDat focuses on registering and publishing environmental data sets and provides unified and efficient access to the WSLâs comprehensive reservoir of environmental monitoring and research data. Research data management is organized in a decentralized manner where the responsibility to curate research data remains with the experts and the original data providers. EnviDat supports data producers and data users in registration, documentation, storage, publication, search and retrieval of a wide range of heterogeneous data sets from the environmental domain. Innovative features include (i) a flexible, three-layer metadata schema, (ii) an additive data discovery model that considers spatial data and (iii) a DataCRediT mechanism designed for specifying data authorship. In addition, the overall user-friendly appearance in EnviDat provides an important opportunity for showcasing WSL research activities and results. The EnviDat portal builds on a conceptual system consisting of a core system, a set of guiding principles and a number of key services. Its development closely follows the conceptual framework, being guided by principles towards the ultimate goal of providing useful services for researchers. Â© 2018 The Author(s).","EnviDat is the environmental data portal developed by the Swiss Federal Institute for Forest, Snow and Landscape Research WSL. The strategic initiative EnviDat highlights the importance WSL lays on Research Data Management (RDM) at the institutional level and demonstrates the commitment to accessible research data in order to advance environmental science. EnviDat focuses on registering and publishing environmental data sets and provides unified and efficient access to the WSLs comprehensive reservoir of environmental monitoring and research data. Research data management is organized in a decentralized manner where the responsibility to curate research data remains with the experts and the original data providers. EnviDat supports data producers and data users in registration, documentation, storage, publication, search and retrieval of a wide range of heterogeneous data sets from the environmental domain. Innovative features include a flexible, three-layer metadata schema, an additive data discovery model that considers spatial data and a DataCRediT mechanism designed for specifying data authorship. In addition, the overall user-friendly appearance in EnviDat provides an important opportunity for showcasing WSL research activities and results. The EnviDat portal builds on a conceptual system consisting of a core system, a set of guiding principles and a number of key services. Its development closely follows the conceptual framework, being guided by principles towards the ultimate goal of providing useful services for researchers."
Landesinitiative NFDI â A central point of contact for RDM for higher education institutions in the German state of north Rhine-Westphalia,"This paper gives an overview of activities regarding RDM in Germany including the national political context as well as initiatives on federal state level. The knowledge about Germanyâs federal system, which also entails the autonomy of the federal states regarding the higher education system, is fundamental to understand the different approaches towards RDM in Germany. The state initiatives of Thuringia, Baden-Wuerttemberg and Hesse are described to compare them to the state initiative (Landesinitiative NFDI) of Germanyâs most populous state of North-Rhine Westphalia (NRW). The aim of the initiative in NRW is to initiate the collaboration between institutions, to link current RDM activities in NRW and to prepare the local institutions for the participation in a National Research Data Infrastructure (Nationale Forschungsdateninfrastruktur, NFDI). Â© 2018 The Author(s).","This paper gives an overview of activities regarding RDM in Germany including the national political context as well as initiatives on federal state level. The knowledge about Germanys federal system, which also entails the autonomy of the federal states regarding the higher education system, is fundamental to understand the different approaches towards RDM in Germany. The state initiatives of Thuringia, Baden-Wuerttemberg and Hesse are described to compare them to the state initiative (Landesinitiative NFDI) of Germanys most populous state of North-Rhine Westphalia (NRW). The aim of the initiative in NRW is to initiate the collaboration between institutions, to link current RDM activities in NRW and to prepare the local institutions for the participation in a National Research Data Infrastructure (Nationale Forschungsdateninfrastruktur, NFDI)."
Abnormal pattern prediction: Detecting fraudulent insurance property claims with semi-supervised machine-learning,"Abnormal pattern prediction has received a great deal of attention from both academia and industry, with various applications (e.g., fraud, terrorism, intrusion detection, etc.). In practice, many abnormal pattern prediction problems are characterized by the simultaneous presence of skewed data, a large number of unlabeled data and a dynamic and changing pattern. In this paper, we propose a methodology based on semi-supervised techniques and we introduce a new metric-the Cluster-Score-for fraud detection which can deal with these practical challenges. Specifically, the methodology involves transmuting unsupervised models into supervised models using the Cluster-Score metric, which defines an objective boundary between clusters and evaluates the homogeneity of the abnormalities in the cluster construction. The objectives are to increase the number of fraudulent claims detected and to reduce the proportion of claims investigated that are, in fact, non-fraudulent. The results from applying our methodology considerably improved these objectives. The experiments were performed on a real world data-set and are the results of building a fraud detection system. Â© 2019 The Author(s).","Abnormal pattern prediction has received a great deal of attention from both academia and industry, with various applications (, fraud, terrorism, intrusion detection, etc.). In practice, many abnormal pattern prediction problems are characterized by the simultaneous presence of skewed data, a large number of unlabeled data and a dynamic and changing pattern. In this paper, we propose a methodology based on semi-supervised techniques and we introduce a new metric-the Cluster-Score-for fraud detection which can deal with these practical challenges. Specifically, the methodology involves transmuting unsupervised models into supervised models using the Cluster-Score metric, which defines an objective boundary between clusters and evaluates the homogeneity of the abnormalities in the cluster construction. The objectives are to increase the number of fraudulent claims detected and to reduce the proportion of claims investigated that are, in fact, non-fraudulent. The results from applying our methodology considerably improved these objectives. The experiments were performed on a real world data-set and are the results of building a fraud detection system."
Marine data services at national oceanographic data centre-India,"In this paper we introduce about the marine data archived at Indian National Centre for Ocean Information Services (INCOIS), Ministry of Earth Sciences, India. Heterogeneous data from in situ, remote sensing and ocean models are archived. In-situ ocean observations includes data from Lagrangian as well Eulerian platforms like Argo floats, moored buoys etc, while remote sensing include data from NOAA satellite series, OceanScat etc. The data generated is translated into ocean information services through analysis and modelling. Data is disseminated to users using variety of means like web with GIS features, ERDDAP, Live Access server with facilities to search, visualize and download. Â© 2018, Ubiquity Press Ltd. All rights reserved.","In this paper we introduce about the marine data archived at Indian National Centre for Ocean Information Services (INCOIS), Ministry of Earth Sciences, India. Heterogeneous data from in situ, remote sensing and ocean models are archived. In-situ ocean observations includes data from Lagrangian as well Eulerian platforms like Argo floats, moored buoys etc, while remote sensing include data from NOAA satellite series, OceanScat etc. The data generated is translated into ocean information services through analysis and modelling. Data is disseminated to users using variety of means like web with GIS features, ERDDAP, Live Access server with facilities to search, visualize and download."
Intelligent electronic management of library by radio frequency identification technology,"In the information era, the knowledge learned in the campus can hardly meet the needs of the society, so the library has become one of the choices for people to supplement their knowledge. With the increasing demand for libraries, the traditional library management method has been difficult to support the demand. In order to promote the intelligent transformation of the library, the basic structure and principle of Radio Frequency Identification Devices (RFID) technology were briefly introduced in this paper, and the intelligent library management system based on RFID was also introduced. Then the library of Huaqiao University was divided into two regions, and the performance of collection counting, book query and passage efficiency of traditional library management based on magnetic stripe together with bar code and intelligent library management based on RFID were compared. The results showed that the collection counting efficiency of intelligent libraries was much higher than that of traditional libraries, the high accuracy and stability of book query avoided the dislocation and chaos like traditional libraries, and there was no congestion even during rush hours because of the higher passing efficiency. Â© 2019 The Author(s).","In the information era, the knowledge learned in the campus can hardly meet the needs of the society, so the library has become one of the choices for people to supplement their knowledge. With the increasing demand for libraries, the traditional library management method has been difficult to support the demand. In order to promote the intelligent transformation of the library, the basic structure and principle of Radio Frequency Identification Devices (RFID) technology were briefly introduced in this paper, and the intelligent library management system based on RFID was also introduced. Then the library of Huaqiao University was divided into two regions, and the performance of collection counting, book query and passage efficiency of traditional library management based on magnetic stripe together with bar code and intelligent library management based on RFID were compared. The results showed that the collection counting efficiency of intelligent libraries was much higher than that of traditional libraries, the high accuracy and stability of book query avoided the dislocation and chaos like traditional libraries, and there was no congestion even during rush hours because of the higher passing efficiency."
Interdisciplinary comparison of scientific impact of publications using the citation-ratio,"The commonly used indexes for evaluating the scientific impact of publications and individual researchers do not allow accurate comparison between disciplines with varying citation frequencies. The Citation-Ratio (CR) was developed to measure impact of an individual publication and allow field-normalised comparison. The CR equals the total number of citations of a publication divided by the median of citations of its references and was tested for the top 5% of the most-cited publications of 13 selected disciplines in sciences, social sciences and humanities. Each publication had a CR = 0 until it was firstly cited. At CR = 1 the number of citations equalled the median of citations of the references. CRs of the most-cited publications mostly ranged between 1 and 10 and were not significantly different across the selected disciplines. In contrast, the total number of citations of the same publications were significantly different across disciplines. One of the advantages of the CR is that it can be calculated for any publication as long as it has references (e.g. books, book chapters, reports, and symposium contributions). Â© 2019 The Author(s).","The commonly used indexes for evaluating the scientific impact of publications and individual researchers do not allow accurate comparison between disciplines with varying citation frequencies. The Citation-Ratio (CR) was developed to measure impact of an individual publication and allow field-normalised comparison. The CR equals the total number of citations of a publication divided by the median of citations of its references and was tested for the top 5% of the most-cited publications of 13 selected disciplines in sciences, social sciences and humanities. Each publication had a CR = 0 until it was firstly cited. At CR = 1 the number of citations equalled the median of citations of the references. CRs of the most-cited publications mostly ranged between 1 and 10 and were not significantly different across the selected disciplines. In contrast, the total number of citations of the same publications were significantly different across disciplines. One of the advantages of the CR is that it can be calculated for any publication as long as it has references ( books, book chapters, reports, and symposium contributions)."
Automatic data standardization for the global cryosphere watch data portal,"The Global Cryosphere Watch (GCW) was initiated by the World Meteorological Organization (WMO) as a mechanism to support the delivery of Earth Systemmonitoring, modelling and prediction services focused on the cryosphere. GCW fosters international coordination and partnerships with the goal of providing authoritative, clear and usable data, information and analyses on the past, current and future state of the cryosphere. It fosters sustained and mutually beneficial partnerships between research and operational institutions, by linking research and operations as well as scientists and practitioners. This is important as most available cryospheric data come from the scientific community. It is generally managed by research institutes which often do not have the infrastructure, the resources, nor the mandate to enable FAIR data management, which is necessary for interoperability and discovery at data level. This implies that data do not fit into standardized systems or dataflows for broader data access and exchange (as exists at the WMO) and thus have been unavailable for operational meteorological and climate applications. This lack of standardization also impairs the reuse of data within the scientific community. GCW is bridging this gap through a data portal and software stack enabling the transformation of sparsely documented and highly variable data into standardized and well documented data suitable for downstream applications with data level interoperability. A processing engine converts raw data provided by the data producers into NetCDF-CF standard files with NetCDF Attribute Convention for Dataset Discovery (ACDD) metadata. The data portal web front end harvests the metadata necessary for its search engine through an OPeNDAP server so no manual editing of the medatadata is necessary. When a user downloads some data from the web portal, it gets the requested data through the OPeNDAP server. Â© 2020 The Author(s).","The Global Cryosphere Watch (GCW) was initiated by the World Meteorological Organization (WMO) as a mechanism to support the delivery of Earth Systemmonitoring, modelling and prediction services focused on the cryosphere. GCW fosters international coordination and partnerships with the goal of providing authoritative, clear and usable data, information and analyses on the past, current and future state of the cryosphere. It fosters sustained and mutually beneficial partnerships between research and operational institutions, by linking research and operations as well as scientists and practitioners. This is important as most available cryospheric data come from the scientific community. It is generally managed by research institutes which often do not have the infrastructure, the resources, nor the mandate to enable FAIR data management, which is necessary for interoperability and discovery at data level. This implies that data do not fit into standardized systems or dataflows for broader data access and exchange (as exists at the WMO) and thus have been unavailable for operational meteorological and climate applications. This lack of standardization also impairs the reuse of data within the scientific community. GCW is bridging this gap through a data portal and software stack enabling the transformation of sparsely documented and highly variable data into standardized and well documented data suitable for downstream applications with data level interoperability. A processing engine converts raw data provided by the data producers into NetCDF-CF standard files with NetCDF Attribute Convention for Dataset Discovery (ACDD) metadata. The data portal web front end harvests the metadata necessary for its search engine through an OPeNDAP server so no manual editing of the medatadata is necessary. When a user downloads some data from the web portal, it gets the requested data through the OPeNDAP server."
Expanding the research data management service portfolio at bielefeld university according to the three-pillar principle towards data FAIRness,"Research Data Management at Bielefeld University is considered as a cross-cutting task among central facilities and research groups at the faculties. While initially started as project âBielefeld Data Informiumâ lasting over seven years (2010â2015), it is now being expanded by setting up a Competence Center for Research Data. The evolution of the institutional RDM is based on the three-pillar principle: 1. Policies, 2. Technical infrastructure and 3. Support structures. The problem of data quality and the issues with reproducibility of research data is addressed in the project Conquaire. It is creating an infrastructure for the processing and versioning of research data which will finally allow publishing of research data in the institutional repository. Conquaire extends the existing RDM infrastructure in three ways: with a Collaborative Platform, Data Quality Checking, and Reproducible Research. Â© 2019 The Author(s).","Research Data Management at Bielefeld University is considered as a cross-cutting task among central facilities and research groups at the faculties. While initially started as project Bielefeld Data Informium lasting over seven years (20102015), it is now being expanded by setting up a Competence Center for Research Data. The evolution of the institutional RDM is based on the three-pillar principle: 1. Policies, 2. Technical infrastructure and 3. Support structures. The problem of data quality and the issues with reproducibility of research data is addressed in the project Conquaire. It is creating an infrastructure for the processing and versioning of research data which will finally allow publishing of research data in the institutional repository. Conquaire extends the existing RDM infrastructure in three ways: with a Collaborative Platform, Data Quality Checking, and Reproducible Research."
Research of LOB data compression and read-write efficiency in oracle database,"Aiming at the problems of huge storage space, low exchange speed and low read-write speed of the current specific oracle database, the read-write speed and exchange speed tests are performed on the compressed and uncompressed Clob and Blob data by three compression algorithms, including Bzip2, Gzip and GzipIO respectively. The read speed test is performed by the direct read, substr read, and substr+threadPool read techniques. The results show that: (1) Blob is superior to Clob in terms of storage, exchange, or read-write speed; (2) For the specific database, Blob+Gzip is the optimal storage structure of the minute and second data. The read-write speed is greatly improved, and the overall capacity of the database is reduced to 7% (or less). The exchange rate of the second data is at least 7.89 times of the present rate, and the station data can be exchanged to the disciplinary center within 2â3 hours (currently 1.5 days); (3) The simplest and most widely used direct read method by software developers has poor database read efficiency, while the substr+threadPool technique shows higher database read efficiency no matter for Clob or Blob, for compressed or uncompressed, which brings a leap-forward improvement in the read speed of LOB data. The results of this paper are of high reference significance to the LOB data storage design and software development. Â© 2019 The Author(s).","Aiming at the problems of huge storage space, low exchange speed and low read-write speed of the current specific oracle database, the read-write speed and exchange speed tests are performed on the compressed and uncompressed Clob and Blob data by three compression algorithms, including Bzip2, Gzip and GzipIO respectively. The read speed test is performed by the direct read, substr read, and substr+threadPool read techniques. The results show that: Blob is superior to Clob in terms of storage, exchange, or read-write speed; For the specific database, Blob+Gzip is the optimal storage structure of the minute and second data. The read-write speed is greatly improved, and the overall capacity of the database is reduced to 7% (or less). The exchange rate of the second data is at least 7.89 times of the present rate, and the station data can be exchanged to the disciplinary center within 23 hours (currently 1.5 days); The simplest and most widely used direct read method by software developers has poor database read efficiency, while the substr+threadPool technique shows higher database read efficiency no matter for Clob or Blob, for compressed or uncompressed, which brings a leap-forward improvement in the read speed of LOB data. The results of this paper are of high reference significance to the LOB data storage design and software development."
Geoscientistsâ perspectives on cyberinfrastructure needs: A collection of user scenarios,"Cyberinfrastructure (CI) is a standard tool in the geosciences, but the creation of successful CI remains difficult, and expensive projects can have significant consequences for scientific communities if they do not result in success. In this paper, we present an effort to solicit feedback on cyberinfrastructure needs from a broad community of geoscientists by means of user scenarios to inform the National Science Foundationâs (NSF) EarthCube program. The method for the user scenarios was semi-structured interviews, a total of 50 of which were collected from a broad range of scientists and analyzed. A wide variety of challenges were identified, with the most commonly articulated challenges being an inability to find data of interest in an online repository, the heterogeneity of data and metadata, the lack of needed software (which in turn drove redundant development of needed software in multiple groups), and insufficient or unstable funding for long-term cyberinfrastructure. While the user scenarios do not provide formal requirements in the software engineering sense, they do provide expressions of user challenges that, in many cases, are sufficiently detailed to inform high-level requirement development. Â© 2019 The Author(s).","Cyberinfrastructure is a standard tool in the geosciences, but the creation of successful CI remains difficult, and expensive projects can have significant consequences for scientific communities if they do not result in success. In this paper, we present an effort to solicit feedback on cyberinfrastructure needs from a broad community of geoscientists by means of user scenarios to inform the National Science Foundations (NSF) EarthCube program. The method for the user scenarios was semi-structured interviews, a total of 50 of which were collected from a broad range of scientists and analyzed. A wide variety of challenges were identified, with the most commonly articulated challenges being an inability to find data of interest in an online repository, the heterogeneity of data and metadata, the lack of needed software (which in turn drove redundant development of needed software in multiple groups), and insufficient or unstable funding for long-term cyberinfrastructure. While the user scenarios do not provide formal requirements in the software engineering sense, they do provide expressions of user challenges that, in many cases, are sufficiently detailed to inform high-level requirement development."
An automated machine learning based decision support system to predict hotel booking cancellations,"Booking cancellations negatively contribute to the production of accurate forecasts, which comprise a critical tool in the hospitality industry. Research has shown that with todayâs computational power and advanced machine learning algorithms it is possible to build models to predict bookings cancellation likelihood. However, the effectiveness of these models has never been evaluated in a real environment. To fill this gap and investigate how these models can be implemented in a decision support system and its impact on demand-management decisions, a prototype was built and deployed in two hotels. The prototype, based on an automated machine learning system designed to learn continuously, lead to two important research contributions. First, the development of a training method and weighting mechanism designed to capture changes in cancellations patterns over time and learn from previous daysâ predictions hits and errors. Second, the creation of a new measure - Minimum Frequency - to measure the precision of predictions over time. From a business standpoint, the prototype demonstrated its effectiveness, with results exceeding 84% in accuracy, 82% in precision, and 88% in Area Under the Curve (AUC). The system allowed hotels to predict their net demand and thus making better decisions about which bookings to accept and reject, what prices to make, and how many rooms to oversell. The systematic prediction of bookings with high probability of being canceled allowed hotels to reduce cancellations by 37 percentage points by acting to avoid their cancellation. Â© 2019 The Author(s).","Booking cancellations negatively contribute to the production of accurate forecasts, which comprise a critical tool in the hospitality industry. Research has shown that with todays computational power and advanced machine learning algorithms it is possible to build models to predict bookings cancellation likelihood. However, the effectiveness of these models has never been evaluated in a real environment. To fill this gap and investigate how these models can be implemented in a decision support system and its impact on demand-management decisions, a prototype was built and deployed in two hotels. The prototype, based on an automated machine learning system designed to learn continuously, lead to two important research contributions. First, the development of a training method and weighting mechanism designed to capture changes in cancellations patterns over time and learn from previous days predictions hits and errors. Second, the creation of a new measure - Minimum Frequency - to measure the precision of predictions over time. From a business standpoint, the prototype demonstrated its effectiveness, with results exceeding 84% in accuracy, 82% in precision, and 88% in Area Under the Curve (AUC). The system allowed hotels to predict their net demand and thus making better decisions about which bookings to accept and reject, what prices to make, and how many rooms to oversell. The systematic prediction of bookings with high probability of being canceled allowed hotels to reduce cancellations by 37 percentage points by acting to avoid their cancellation."
Recommendations to improve downloads of large earth observation data,"With the volume of Earth observation data expanding rapidly, cloud computing is quickly changing the way these data are processed, analyzed, and visualized. Collocating freely available Earth observation data on a cloud computing infrastructure may create opportunities unforeseen by the original data provider for innovation and value-added data re-use, but existing systems at data centers are not designed for supporting requests for large data transfers. A lack of common methodology necessitates that each data center handle such requests from different cloud vendors differently. Guidelines are needed to support enabling all cloud vendors to utilize a common methodology for bulk-downloading data from data centers, thus preventing the providers from building custom capabilities to meet the needs of individual vendors. This paper presents recommendations distilled from use cases provided by three cloud vendors (Amazon, Google, and Microsoft) and are based on the vendorsâ interactions with data systems at different Federal agencies and organizations. These specific recommendations range from obvious steps for improving data usability (such as ensuring the use of standard data formats and commonly supported projections) to non-obvious undertakings important for enabling bulk data downloads at scale. These recommendations can be used to evaluate and improve existing data systems for high-volume data transfers, and their adoption can lead to cloud vendors utilizing a common methodology. Â© 2018 The Author(s).","With the volume of Earth observation data expanding rapidly, cloud computing is quickly changing the way these data are processed, analyzed, and visualized. Collocating freely available Earth observation data on a cloud computing infrastructure may create opportunities unforeseen by the original data provider for innovation and value-added data re-use, but existing systems at data centers are not designed for supporting requests for large data transfers. A lack of common methodology necessitates that each data center handle such requests from different cloud vendors differently. Guidelines are needed to support enabling all cloud vendors to utilize a common methodology for bulk-downloading data from data centers, thus preventing the providers from building custom capabilities to meet the needs of individual vendors. This paper presents recommendations distilled from use cases provided by three cloud vendors (Amazon, Google, and Microsoft) and are based on the vendors interactions with data systems at different Federal agencies and organizations. These specific recommendations range from obvious steps for improving data usability (such as ensuring the use of standard data formats and commonly supported projections) to non-obvious undertakings important for enabling bulk data downloads at scale. These recommendations can be used to evaluate and improve existing data systems for high-volume data transfers, and their adoption can lead to cloud vendors utilizing a common methodology."
Disparity of imputed data from small area estimate approaches â A case study on diabetes prevalence at the county level in the U.S.,"This paper assesses concordance and inconsistency among three small area estimation methods that are currently providing county-level health indicators in the United States. The three methods are multi-level logistic regression, spatial logistic regression, and spatial Poison regression, all proposed since 2010. Diabetes prevalence is estimated for each county in the continental United States from the 2012 sample of Behavioral Risk Factor Surveillance System. The mapping results show that all three methods displayed elevated diabetes prevalence in the South. While the Pearson correlation coefficients among three model-based estimates were all above 0.60, the highest one was 0.80 between the multilevel and spatial logistic methods. While point estimates are apparently different among the three small area estimate methods, their top and bottom of quintile distributions are fairly consistent based on Bangdiwalaâs B-statistic, suggesting that outputs from each method would support consistent policy making in terms of identifying top and bottom percent counties. Â© 2018, Ubiquity Press Ltd. All rights reserved.","This paper assesses concordance and inconsistency among three small area estimation methods that are currently providing county-level health indicators in the United States. The three methods are multi-level logistic regression, spatial logistic regression, and spatial Poison regression, all proposed since 2010. Diabetes prevalence is estimated for each county in the continental United States from the 2012 sample of Behavioral Risk Factor Surveillance System. The mapping results show that all three methods displayed elevated diabetes prevalence in the South. While the Pearson correlation coefficients among three model-based estimates were all above 0.60, the highest one was 0.80 between the multilevel and spatial logistic methods. While point estimates are apparently different among the three small area estimate methods, their top and bottom of quintile distributions are fairly consistent based on Bangdiwalas B-statistic, suggesting that outputs from each method would support consistent policy making in terms of identifying top and bottom percent counties."
Data discovery paradigms: User requirements and recommendations for data repositories,"As data repositories make more data openly available it becomes challenging for researchers to find what they need either from a repository or through web search engines. This study attempts to investigate data usersâ requirements and the role that data repositories can play in supporting data discoverability by meeting those requirements. We collected 79 data discovery use cases (or data search scenarios), from which we derived nine functional requirements for data repositories through qualitative analysis. We then applied usability heuristic evaluation and expert review methods to identify best practices that data repositories can implement to meet each functional requirement. We propose the following ten recommendations for data repository operators to consider for improving data discoverability and userâs data search experience: 1. Provide a range of query interfaces to accommodate various data search behaviours. 2. Provide multiple access points to find data. 3. Make it easier for researchers to judge relevance, accessibility and reusability of a data collection from a search summary. 4. Make individual metadata records readable and analysable. 5. Enable sharing and downloading of bibliographic references. 6. Expose data usage statistics. 7. Strive for consistency with other repositories. 8. Identify and aggregate metadata records that describe the same data object. 9. Make metadata records easily indexed and searchable by major web search engines. 10. Follow API search standards and community adopted vocabularies for interoperability. Â© 2019 The Author(s).","As data repositories make more data openly available it becomes challenging for researchers to find what they need either from a repository or through web search engines. This study attempts to investigate data users requirements and the role that data repositories can play in supporting data discoverability by meeting those requirements. We collected 79 data discovery use cases (or data search scenarios), from which we derived nine functional requirements for data repositories through qualitative analysis. We then applied usability heuristic evaluation and expert review methods to identify best practices that data repositories can implement to meet each functional requirement. We propose the following ten recommendations for data repository operators to consider for improving data discoverability and users data search experience: 1. Provide a range of query interfaces to accommodate various data search behaviours. 2. Provide multiple access points to find data. 3. Make it easier for researchers to judge relevance, accessibility and reusability of a data collection from a search summary. 4. Make individual metadata records readable and analysable. 5. Enable sharing and downloading of bibliographic references. 6. Expose data usage statistics. 7. Strive for consistency with other repositories. 8. Identify and aggregate metadata records that describe the same data object. 9. Make metadata records easily indexed and searchable by major web search engines. 10. Follow API search standards and community adopted vocabularies for interoperability."
ROCP: A rapid ontology construction platform from unstructured data,"The domain ontology, which plays a significant role in knowledge-based systems, still needs the manual work of domain experts to be constructed currently. The main motivation of this paper is to provide a semi-automatic platform which can construct fairly comprehensive domain ontology from unstructured data. Firstly, a brief QA process is proposed to simplify the interaction with the domain experts. A novel algorithm MPVW, which extends from the classical algorithm TF-IDF, is proposed to extract the terminologies from domain documents. MPVW balanced more parameters and factors to evaluate the feature of terminologies. The 3-layers taxonomy and terminology hyponymy height provide sufficient guide and prompt for domain experts to construct ontology from terminologies. According to our approach we have developed ROCP, a rapid ontology construction platform which has been applied in the space debris mitigation domain. The experimental data indicates that ROCP has sufficient accuracy to extract terminologies. Meanwhile, it is effective to relieve the labor of domain experts to construct domain ontology. Â© 2018 The Author(s).","The domain ontology, which plays a significant role in knowledge-based systems, still needs the manual work of domain experts to be constructed currently. The main motivation of this paper is to provide a semi-automatic platform which can construct fairly comprehensive domain ontology from unstructured data. Firstly, a brief QA process is proposed to simplify the interaction with the domain experts. A novel algorithm MPVW, which extends from the classical algorithm TF-IDF, is proposed to extract the terminologies from domain documents. MPVW balanced more parameters and factors to evaluate the feature of terminologies. The 3-layers taxonomy and terminology hyponymy height provide sufficient guide and prompt for domain experts to construct ontology from terminologies. According to our approach we have developed ROCP, a rapid ontology construction platform which has been applied in the space debris mitigation domain. The experimental data indicates that ROCP has sufficient accuracy to extract terminologies. Meanwhile, it is effective to relieve the labor of domain experts to construct domain ontology."
A column styled composable schema matcher for semantic data-types,"Schema matching exists as a long-standing challenge in many database related applications, such as data integration, where two databases with different schema have to be integrated. With the evolvement from database to big data, the schema matching has been enriched with various purposes and application contexts, ranging from data integration, to service integration, to semantic data clouding, until more recent exploratory data analysis over big data. These enriched contexts increase the demand for schema matching between semantic data-types, such as XML, RDF etc. The existing integration approaches have not dealt with the challenges of defining a relation between XML and other semantic data-types. To address these challenges, this paper studies the problem of schema mapping from XML to RDF in two folds. Firstly, testify the validity of single matcher in a column based manner for the semantic data types. Secondly, testify the validity of a highly configurable framework that utilizes hierarchical classification in order to construct a composable pipeline. We propose and implement a Reconfigurable pipeline for Semi-Automatic Schema Matching (REPSASM), which aims to solve the customizability of the matching problem by providing an environment in which a user can create, configure and experiment with their own schema-matching procedure. The experiments performed within this work show that the configurability and hierarchical classification improves the matching result, and it proposes an algorithm to automatically optimize such a hierarchy pipeline. Â© 2019 The Author(s).","Schema matching exists as a long-standing challenge in many database related applications, such as data integration, where two databases with different schema have to be integrated. With the evolvement from database to big data, the schema matching has been enriched with various purposes and application contexts, ranging from data integration, to service integration, to semantic data clouding, until more recent exploratory data analysis over big data. These enriched contexts increase the demand for schema matching between semantic data-types, such as XML, RDF etc. The existing integration approaches have not dealt with the challenges of defining a relation between XML and other semantic data-types. To address these challenges, this paper studies the problem of schema mapping from XML to RDF in two folds. Firstly, testify the validity of single matcher in a column based manner for the semantic data types. Secondly, testify the validity of a highly configurable framework that utilizes hierarchical classification in order to construct a composable pipeline. We propose and implement a Reconfigurable pipeline for Semi-Automatic Schema Matching (REPSASM), which aims to solve the customizability of the matching problem by providing an environment in which a user can create, configure and experiment with their own schema-matching procedure. The experiments performed within this work show that the configurability and hierarchical classification improves the matching result, and it proposes an algorithm to automatically optimize such a hierarchy pipeline."
Teaching research data management for students,"Sound skills in managing research data are a fundamental requirement in any discipline of research. Therefore, research data management should be included in academic education of students as early as possible. We have been teaching an interdisciplinary full semesterâs course on research data management for six years. We report how we established the course. We describe our competency-based approach to teaching research data management and the curriculum of topics that we consider essential. We evaluate our approach by a survey done among the participants of the course and summarize the lessons we learned in teaching the course. Â© 2019 The Author(s).","Sound skills in managing research data are a fundamental requirement in any discipline of research. Therefore, research data management should be included in academic education of students as early as possible. We have been teaching an interdisciplinary full semesters course on research data management for six years. We report how we established the course. We describe our competency-based approach to teaching research data management and the curriculum of topics that we consider essential. We evaluate our approach by a survey done among the participants of the course and summarize the lessons we learned in teaching the course."
Data sharing practices among researchers at south african universities,"Research data management practices have gained momentum the world over. This is due to increased demands by governments and other funding agencies to have research data archived and shared as widely as possible. This paper sought to establish the data sharing practices of researchers in South Africa. The study further sought to establish the level of collaboration among researchers in sharing research data at the university level. The outcomes of the survey will help the researchers to develop appropriate data literacy awareness programmes meant to stimulate growth in data sharing practices for the benefit of research, not only in South Africa, but the world at large. A survey research method was used to gather data from willing public universities in South Africa. A similar study was conducted in other countries such as the United Kingdom, France and Turkey but the Researchers believe that circumstances in the developed world may differ with the South African research environment, hence the current study. The major finding of this study was that most researchers preferred to use data produced by others but less keen on sharing their own data. This study is the first of its kind in South Africa which investigates data sharing practices of researchers from multi-disciplinary fields at the university level and will contribute immensely to the growing body of literature in the area of research data management. Â© 2019 The Author(s).","Research data management practices have gained momentum the world over. This is due to increased demands by governments and other funding agencies to have research data archived and shared as widely as possible. This paper sought to establish the data sharing practices of researchers in South Africa. The study further sought to establish the level of collaboration among researchers in sharing research data at the university level. The outcomes of the survey will help the researchers to develop appropriate data literacy awareness programmes meant to stimulate growth in data sharing practices for the benefit of research, not only in South Africa, but the world at large. A survey research method was used to gather data from willing public universities in South Africa. A similar study was conducted in other countries such as the United Kingdom, France and Turkey but the Researchers believe that circumstances in the developed world may differ with the South African research environment, hence the current study. The major finding of this study was that most researchers preferred to use data produced by others but less keen on sharing their own data. This study is the first of its kind in South Africa which investigates data sharing practices of researchers from multi-disciplinary fields at the university level and will contribute immensely to the growing body of literature in the area of research data management."
The time efficiency gain in sharing and reuse of research data,"Among the frequently stated benefits of sharing research data are time efficiency or increased productivity. The assumption is that reuse or secondary use of research data saves researchers time in not having to produce data for a publication themselves. This can make science more efficient and productive. However, if there is no reuse, time costs in making data available for reuse will have been made with no return on this investment. In this paper a mathematical model is used to calculate the break-even point for time spent sharing in a scientific community, versus time gain by reuse. This is done for several scenarios; from simple to complex datasets to share and reuse, and at different sharing rates. The results indicate that sharing research data can indeed cause an efficiency revenue for the scientific community. However, this is not a given in all modeled scenarios. The scientific community with the lowest reuse needed to reach a break-even point is one that has few sharing researchers and low time investments for sharing and reuse. This suggests it would be beneficial to have a critical selection of datasets that are worth the effort to prepare for reuse in other scientific studies. In addition, stimulating reuse of datasets in itself would be beneficial to increase efficiency in scientific communities. Â© 2019 The Author(s).","Among the frequently stated benefits of sharing research data are time efficiency or increased productivity. The assumption is that reuse or secondary use of research data saves researchers time in not having to produce data for a publication themselves. This can make science more efficient and productive. However, if there is no reuse, time costs in making data available for reuse will have been made with no return on this investment. In this paper a mathematical model is used to calculate the break-even point for time spent sharing in a scientific community, versus time gain by reuse. This is done for several scenarios; from simple to complex datasets to share and reuse, and at different sharing rates. The results indicate that sharing research data can indeed cause an efficiency revenue for the scientific community. However, this is not a given in all modeled scenarios. The scientific community with the lowest reuse needed to reach a break-even point is one that has few sharing researchers and low time investments for sharing and reuse. This suggests it would be beneficial to have a critical selection of datasets that are worth the effort to prepare for reuse in other scientific studies. In addition, stimulating reuse of datasets in itself would be beneficial to increase efficiency in scientific communities."
Designing transnational hydroclimatological observation networks and data sharing policies in West Africa,"Surface observations provide ground evidence of climate change to support the scientific guidance paving the way to better adaptation and mitigation actions. The West African Science Service Centre on Climate Change and Adapted Land Use (WASCAL) has designed a multi-stakeholder initiative to rescue the deteriorated near-surface weather, climate and hydrological equipment of West African countries. The main goal for this multi-stakeholder framework was to monitor the climate and collect long term and high-quality records of essential climate variables in support of research, education, capacity building, and climate services provision. Proactive and inclusive partnership initiatives were developed to jointly (re)design and (re)implement near surface observatiories with the national meteorological and hydrological services or agencies (NMHS/As) in West Africa. The co-production scheme used by this framework succeeded in evaluating the existing observations networks, to modernizing sensors and field equipment, and densifying the sites in order to improve the quality of data collection, transmission, archiving, processing and sharing policies. After more than four years of community-of-practice, the existing regional basic hydroclimatic was increased/upgraded by 45% with automatic weather observing systems while fifty automatic water level, ten water quality sensors, three mesoscale research catchments, and several pilot sites to benefit countriesâ services provision, research infrastructure, education, and capacity building. Country-specific data sharing policies were harmonized and signed to support data services delivery. This practice paper exposes the concepts, outcomes, challenges, lessons learned and the ways forward in setting-up the framework and keeping it on working to leverage the co-production of data & information services for better-informed decision-making in the field of sustainable development in West Africa. Â© 2019 The Author(s).","Surface observations provide ground evidence of climate change to support the scientific guidance paving the way to better adaptation and mitigation actions. The West African Science Service Centre on Climate Change and Adapted Land Use (WASCAL) has designed a multi-stakeholder initiative to rescue the deteriorated near-surface weather, climate and hydrological equipment of West African countries. The main goal for this multi-stakeholder framework was to monitor the climate and collect long term and high-quality records of essential climate variables in support of research, education, capacity building, and climate services provision. Proactive and inclusive partnership initiatives were developed to jointly (re)design and (re)implement near surface observatiories with the national meteorological and hydrological services or agencies (NMHS/As) in West Africa. The co-production scheme used by this framework succeeded in evaluating the existing observations networks, to modernizing sensors and field equipment, and densifying the sites in order to improve the quality of data collection, transmission, archiving, processing and sharing policies. After more than four years of community-of-practice, the existing regional basic hydroclimatic was increased/upgraded by 45% with automatic weather observing systems while fifty automatic water level, ten water quality sensors, three mesoscale research catchments, and several pilot sites to benefit countries services provision, research infrastructure, education, and capacity building. Country-specific data sharing policies were harmonized and signed to support data services delivery. This practice paper exposes the concepts, outcomes, challenges, lessons learned and the ways forward in setting-up the framework and keeping it on working to leverage the co-production of data & information services for better-informed decision-making in the field of sustainable development in West Africa."
Automatic acquisition and sustainable use of political-ecological data,"The sustainable management of anthropogenically-impacted ecosystems will require ongoing monitoring and advocacy by people across the globe. To this end, automatic methods are developed herein for acquiring several types of such political-ecological data. On the political side, a method is developed for gathering news articles about human actions that affect the ecosystem along with a method for identifying themes in social media that concern the consumption of an ecosystemâs products. On the ecosystem side, a method is derived for estimating wildlife abundance from purchasable high-resolution satellite images. A simple website architecture is described for holding this data and enabling its use in developing sustainable conservation policies. A rhino conservation website illustrates this architecture. A fundamental contradiction between the desire for open data on the locations of endangered flora and fauna versus the need to hide these locations from poachers is addressed through a new security protocol that enables the secure distribution of sensitive ecosystem data to trusted data consumers. Â© 2018, Ubiquity Press Ltd. All rights reserved.","The sustainable management of anthropogenically-impacted ecosystems will require ongoing monitoring and advocacy by people across the globe. To this end, automatic methods are developed herein for acquiring several types of such political-ecological data. On the political side, a method is developed for gathering news articles about human actions that affect the ecosystem along with a method for identifying themes in social media that concern the consumption of an ecosystems products. On the ecosystem side, a method is derived for estimating wildlife abundance from purchasable high-resolution satellite images. A simple website architecture is described for holding this data and enabling its use in developing sustainable conservation policies. A rhino conservation website illustrates this architecture. A fundamental contradiction between the desire for open data on the locations of endangered flora and fauna versus the need to hide these locations from poachers is addressed through a new security protocol that enables the secure distribution of sensitive ecosystem data to trusted data consumers."
"Ontology usability scale: Context-aware metrics for the effectiveness, efficiency and satisfaction of ontology uses","Both ontology builders and users need a way to evaluate ontologies in terms of usability, but existing ontology evaluation approaches do not fit this purpose. We propose the Ontology Usability Scale (OUS), a ten-item Likert scale derived from statements prepared according to a semiotic framework and an online poll in the Semantic Web community to provide a practical way of ontology usability evaluation. Case studies were conducted to bookkeep current usability evaluation results for ontologies expecting revisions in the future, and discussions of the poll results are presented to help proper use and customization of the OUS. Â© 2018, Ubiquity Press Ltd. All rights reserved.","Both ontology builders and users need a way to evaluate ontologies in terms of usability, but existing ontology evaluation approaches do not fit this purpose. We propose the Ontology Usability Scale (OUS), a ten-item Likert scale derived from statements prepared according to a semiotic framework and an online poll in the Semantic Web community to provide a practical way of ontology usability evaluation. Case studies were conducted to bookkeep current usability evaluation results for ontologies expecting revisions in the future, and discussions of the poll results are presented to help proper use and customization of the OUS."
The life cycle of structural biology data,"Research data is acquired, interpreted, published, reused, and sometimes eventually discarded. Understanding this life cycle better will help the development of appropriate infrastructural services, ones which make it easier for researchers to preserve, share, and find data. Structural biology is a discipline within the life sciences, one that investigates the molecular basis of life by discovering and interpreting the shapes and motions of macromolecules. Structural biology has a strong tradition of data sharing, expressed by the founding of the Protein Data Bank (PDB) in 1971. The culture of structural biology is therefore already in line with the perspective that data from publicly funded research projects are public data. This review is based on the data life cycle as defined by the UK Data Archive. It identifies six stages: creating data, processing data, analysing data, preserving data, giving access to data, and re-using data. For clarity, Ê»preserving dataÊ¼ and Ê»giving access to dataÊ¼ are discussed together. A final stage to the life cycle, Ê»discarding dataÊ¼, is also discussed. The review concludes with recommendations for future improvements to the IT infrastructure for structural biology. Â© 2018 The Author(s).","Research data is acquired, interpreted, published, reused, and sometimes eventually discarded. Understanding this life cycle better will help the development of appropriate infrastructural services, ones which make it easier for researchers to preserve, share, and find data. Structural biology is a discipline within the life sciences, one that investigates the molecular basis of life by discovering and interpreting the shapes and motions of macromolecules. Structural biology has a strong tradition of data sharing, expressed by the founding of the Protein Data Bank (PDB) in 1971. The culture of structural biology is therefore already in line with the perspective that data from publicly funded research projects are public data. This review is based on the data life cycle as defined by the UK Data Archive. It identifies six stages: creating data, processing data, analysing data, preserving data, giving access to data, and re-using data. For clarity, preserving data and giving access to data are discussed together. A final stage to the life cycle, discarding data, is also discussed. The review concludes with recommendations for future improvements to the IT infrastructure for structural biology."
The definition of reuse,"The ability to reuse research data is now considered a key benefit for the wider research community. Researchers of all disciplines are confronted with the pressure to share their research data so that it can be reused. The demand for data use and reuse has implications on how we document, publish and share research in the first place, and, perhaps most importantly, it affects how we measure the impact of research, which is commonly a measurement of its use and reuse. It is surprising that research communities, policy makers, etc. have not clearly defined what use and reuse is yet. We postulate that a clear definition of use and reuse is needed to establish better metrics for a comprehensive scholarly record of individuals, institutions, organizations, etc. Hence, this article presents a first definition of reuse of research data. Characteristics of reuse are identified by examining the etymology of the term and the analysis of the current discourse, leading to a range of reuse scenarios that show the complexity of todayâs research landscape, which has been moving towards a data-driven approach. The analysis underlines that there is no reason to distinguish use and reuse. We discuss what that means for possible new metrics that attempt to cover Open Science practices more comprehensively. We hope that the resulting definition will enable a better and more refined strategy for Open Science. Â© 2019 The Author(s).","The ability to reuse research data is now considered a key benefit for the wider research community. Researchers of all disciplines are confronted with the pressure to share their research data so that it can be reused. The demand for data use and reuse has implications on how we document, publish and share research in the first place, and, perhaps most importantly, it affects how we measure the impact of research, which is commonly a measurement of its use and reuse. It is surprising that research communities, policy makers, etc. have not clearly defined what use and reuse is yet. We postulate that a clear definition of use and reuse is needed to establish better metrics for a comprehensive scholarly record of individuals, institutions, organizations, etc. Hence, this article presents a first definition of reuse of research data. Characteristics of reuse are identified by examining the etymology of the term and the analysis of the current discourse, leading to a range of reuse scenarios that show the complexity of todays research landscape, which has been moving towards a data-driven approach. The analysis underlines that there is no reason to distinguish use and reuse. We discuss what that means for possible new metrics that attempt to cover Open Science practices more comprehensively. We hope that the resulting definition will enable a better and more refined strategy for Open Science."
Resembling population density distribution with massive mobile phone data,"As the mobile phone data (CDR data) has gained an increasing interest in research, such as social science, transportation, urban informatics, and big data, this study aims at examining the representativeness of the CDR data in terms of resemblance of the actual population density distribution from three perspectives; operatorâs market share, urban-rural user population ratio, and user gender ratio. The results reveal that the representativeness of the data does not scale at the same rate with the operatorâs market share, the urban-rural user population ratio of 80:20 can best represent the population density distribution, and an equal mixture of male and female user population can best resemble the population density distribution. This study is the first investigation into the representativeness of the CDR data. The findings provide useful information, which can serve an insightful guideline when dealing with the CDR data. Â© 2018 The Author(s).","As the mobile phone data (CDR data) has gained an increasing interest in research, such as social science, transportation, urban informatics, and big data, this study aims at examining the representativeness of the CDR data in terms of resemblance of the actual population density distribution from three perspectives; operators market share, urban-rural user population ratio, and user gender ratio. The results reveal that the representativeness of the data does not scale at the same rate with the operators market share, the urban-rural user population ratio of 80:20 can best represent the population density distribution, and an equal mixture of male and female user population can best resemble the population density distribution. This study is the first investigation into the representativeness of the CDR data. The findings provide useful information, which can serve an insightful guideline when dealing with the CDR data."
Importance and incorporation of user feedback in earth science data stewardship,"Since August 1994, The National Aeronautics and Space Administrationâs (NASAâs) Earth Observing System Data and Information System (EOSDIS) has been serving a global community of users, currently over 4 million each year, with Earth science data in a variety of disciplines. NASAâs Earth Science Data and Information System Project (ESDIS) is responsible for EOSDIS with its 12 Distributed Archive Centers (DAACs). During the life of EOSDIS, various mechanisms for user feedback have been extremely important and valuable to its evolution. Some inputs from user groups have resulted in fundamental changes in EOSDIS, while others have provided ideas for incremental changes. The purpose of this paper is to share this experience and the benefits that have resulted from the user feedback. Notable among user community groups that have had significant influence on EOSDIS are: the EOSDIS Advisory Panel, the National Research Councilâs Committee on Global Change Research and the DAAC User Working Groups (UWGs). In addition, an annual survey of EOSDIS users resulting in the American Customer Satisfaction Index (ACSI) provides a score as well as very helpful user suggestions for system improvements. Also, each DAAC has a user services group that receives on-going requests for help and other comments from users. The ESDIS Project has established a mechanism through the âearthdataâ web site (http://earthdata.nasa.gov) for users to provide feedback which is routed to appropriate individuals. In addition, focused efforts have been made for user needs assessment, and usability studies are used in making changes to the systems for improving user experience. Â© 2019, Ubiquity Press. All rights reserved.","Since August 1994, The National Aeronautics and Space Administrations (NASAs) Earth Observing System Data and Information System (EOSDIS) has been serving a global community of users, currently over 4 million each year, with Earth science data in a variety of disciplines. NASAs Earth Science Data and Information System Project (ESDIS) is responsible for EOSDIS with its 12 Distributed Archive Centers (DAACs). During the life of EOSDIS, various mechanisms for user feedback have been extremely important and valuable to its evolution. Some inputs from user groups have resulted in fundamental changes in EOSDIS, while others have provided ideas for incremental changes. The purpose of this paper is to share this experience and the benefits that have resulted from the user feedback. Notable among user community groups that have had significant influence on EOSDIS are: the EOSDIS Advisory Panel, the National Research Councils Committee on Global Change Research and the DAAC User Working Groups (UWGs). In addition, an annual survey of EOSDIS users resulting in the American Customer Satisfaction Index (ACSI) provides a score as well as very helpful user suggestions for system improvements. Also, each DAAC has a user services group that receives on-going requests for help and other comments from users. The ESDIS Project has established a mechanism through the earthdata web site (http://earthdata.nasa.gov) for users to provide feedback which is routed to appropriate individuals. In addition, focused efforts have been made for user needs assessment, and usability studies are used in making changes to the systems for improving user experience."
Research data publication: Moving beyond the metaphor,"Metaphors are a quick and easy way of grasping (often complicated) concepts and ideas, but like any useful tools, they should be used carefully. There are as many arguments about how datasets are like cakes1 as there are about how datasets arenât like cakes.2 It can be easy to categorise a dataset as being a special class of academic paper. Positively, this means that the tools and services for scholarly publication can be utilised to transmit and verify datasets, improving visibility, reproducibility, and attribution for the dataset creators. Negatively, if a dataset doesnât fit within the criteria to meet the âacademic publicationâ mould (e.g. because it is being continually versioned and updated, or it is still being collected and will be for decades) it might be considered to be of less value to the community. It is often said that âall models are wrong, but some are usefulâ (Box, 1979). Hence we need to determine the usefulness and limits of models and metaphors, especially when trying to develop new processes and systems. This paper further develops the metaphors for data outlined in Parsons and Fox (2013), and gives real world examples of the metaphors from scientific data stored in the Centre for Environmental Data Analysis (CEDA) â a discipline-specific environmental data repository, and the processes that created the datasets. Â© 2019 The Author(s).","Metaphors are a quick and easy way of grasping (often complicated) concepts and ideas, but like any useful tools, they should be used carefully. There are as many arguments about how datasets are like cakes1 as there are about how datasets arent like cakes.2 It can be easy to categorise a dataset as being a special class of academic paper. Positively, this means that the tools and services for scholarly publication can be utilised to transmit and verify datasets, improving visibility, reproducibility, and attribution for the dataset creators. Negatively, if a dataset doesnt fit within the criteria to meet the academic publication mould ( because it is being continually versioned and updated, or it is still being collected and will be for decades) it might be considered to be of less value to the community. It is often said that all models are wrong, but some are useful (Box, 1979). Hence we need to determine the usefulness and limits of models and metaphors, especially when trying to develop new processes and systems. This paper further develops the metaphors for data outlined in Parsons and Fox , and gives real world examples of the metaphors from scientific data stored in the Centre for Environmental Data Analysis (CEDA) a discipline-specific environmental data repository, and the processes that created the datasets."
Building an international consensus on multi-disciplinary metadata standards: A CODATA case history in nanotechnology,"Science today is rapidly becoming both multi-disciplinary and data-driven. These two trends pose new challenges to the capture, management, sharing, and dissemination of research data. Multi-disciplinary science means diverse data generation communities and equally diverse user groups. Data-driven means that sharing data among different communities is more important than ever because of the growth of modeling and knowledge discovery. Nanotechnology is a prime example, involving chemistry, physics, materials science, toxicology, environmental science, and many other disciplines. During the past few years, CODATA has created an international, multi-disciplinary Working Group that has developed a number of critically important metadata standards to facilitate sharing nanomaterials data. In this paper, we discuss the challenges faced in starting and executing this work, as well as the approaches taken to make progress on producing internationally accepted metadata standards. Many of these approaches are directly applicable to other multi-disciplinary subjects. Â© 2019 The Author(s).","Science today is rapidly becoming both multi-disciplinary and data-driven. These two trends pose new challenges to the capture, management, sharing, and dissemination of research data. Multi-disciplinary science means diverse data generation communities and equally diverse user groups. Data-driven means that sharing data among different communities is more important than ever because of the growth of modeling and knowledge discovery. Nanotechnology is a prime example, involving chemistry, physics, materials science, toxicology, environmental science, and many other disciplines. During the past few years, CODATA has created an international, multi-disciplinary Working Group that has developed a number of critically important metadata standards to facilitate sharing nanomaterials data. In this paper, we discuss the challenges faced in starting and executing this work, as well as the approaches taken to make progress on producing internationally accepted metadata standards. Many of these approaches are directly applicable to other multi-disciplinary subjects."
"Intelligent infrastructure, ubiquitous mobility, and smart libraries â Innovate for the future","This paper presents an empirical research on the strategic development of a large-scale transdisciplinary area, named Intelligent Infrastructure for Human-Centered Communities or IIHCC, in the institutional context of Virginia Tech. In such an innovative space, this study investigated the change dynamics, system thinking, and adaptive design strategies associated with the IIHCC evolution and librariesâ innovation. It employed a mixed-methods approach combining semi-structured interviews, ethnographic participant observation, and document analysis. The results present an emerging horizon of intelligent infrastructure and ubiquitous mobility and the evolving knowledge space and shifting data sphere in this cyber-physical-human integrated environment. Within such developments, this study discusses the developing scenarios of âsmartâ libraries as innovative testbeds for data exploration, community knowledge base, and intelligent information interface. It further projects an intelligent, learning, and adaptive library system, featuring exemplary data science platform and dynamic data management mechanism, smart design and innovation space, as well as collective intelligence and creative partnership. During this extraordinary time of horizon change, this timely work informs academic library transformation and its architectural innovation in the age of âsmartness.â. Â© 2019 The Author(s).","This paper presents an empirical research on the strategic development of a large-scale transdisciplinary area, named Intelligent Infrastructure for Human-Centered Communities or IIHCC, in the institutional context of Virginia Tech. In such an innovative space, this study investigated the change dynamics, system thinking, and adaptive design strategies associated with the IIHCC evolution and libraries innovation. It employed a mixed-methods approach combining semi-structured interviews, ethnographic participant observation, and document analysis. The results present an emerging horizon of intelligent infrastructure and ubiquitous mobility and the evolving knowledge space and shifting data sphere in this cyber-physical-human integrated environment. Within such developments, this study discusses the developing scenarios of smart libraries as innovative testbeds for data exploration, community knowledge base, and intelligent information interface. It further projects an intelligent, learning, and adaptive library system, featuring exemplary data science platform and dynamic data management mechanism, smart design and innovation space, as well as collective intelligence and creative partnership. During this extraordinary time of horizon change, this timely work informs academic library transformation and its architectural innovation in the age of smartness.."
"âData Stewardship Wizardâ: A Tool Bringing Together Researchers, Data Stewards, and Data Experts around Data Management Planning","The Data Stewardship Wizard is a tool for data management planning that is focused on getting the most value out of data management planning for the project itself rather than on fulfilling obligations. It is based on FAIR Data Stewardship, in which each data-related decision in a project acts to optimize the Findability, Accessibility, Interoperability and/or Reusability of the data. The background to this philosophy is that the first reuser of the data is the researcher themselves. The tool encourages the consulting of expertise and experts, can help researchers avoid risks they did not know they would encounter by confronting them with practical experience from others, and can help them discover helpful technologies they did not know existed. In this paper, we discuss the context and motivation for the tool, we explain its architecture and we present key functions, such as the knowledge model evolvability and migrations, assembling data management plans, metrics and evaluation of data management plans. Â© 2019 The Author(s).","The Data Stewardship Wizard is a tool for data management planning that is focused on getting the most value out of data management planning for the project itself rather than on fulfilling obligations. It is based on FAIR Data Stewardship, in which each data-related decision in a project acts to optimize the Findability, Accessibility, Interoperability and/or Reusability of the data. The background to this philosophy is that the first reuser of the data is the researcher themselves. The tool encourages the consulting of expertise and experts, can help researchers avoid risks they did not know they would encounter by confronting them with practical experience from others, and can help them discover helpful technologies they did not know existed. In this paper, we discuss the context and motivation for the tool, we explain its architecture and we present key functions, such as the knowledge model evolvability and migrations, assembling data management plans, metrics and evaluation of data management plans."
A survey of machine learning approaches and techniques for student dropout prediction,"School dropout is absenteeism from school for no good reason for a continuous number of days. Addressing this challenge requires a thorough understanding of the underlying issues and effective planning for interventions. Over the years machine learning has gained much attention on addressing the problem of students dropout. This is because machine learning techniques can effectively facilitate determination of at-risk students and timely planning for interventions. In order to collect, organize, and synthesize existing knowledge in the field of machine learning on addressing student dropout; literature in academic journals, books and case studies have been surveyed. The survey reveal that, several machine learning algorithms have been proposed in literature. However, most of those algorithms have been developed and tested in developed countries. Hence, developing countries are facing lack of research on the use of machine learning on addressing this problem. Furthermore, many studies focus on addressing student dropout using student level datasets. However, developing countries need to include school level datasets due to the issue of limited resources. Therefore, this paper presents an overview of machine learning in education with the focus on techniques for student dropout prediction. Furthermore, the paper highlights open challenges for future research directions. Â© 2019 The Author(s).","School dropout is absenteeism from school for no good reason for a continuous number of days. Addressing this challenge requires a thorough understanding of the underlying issues and effective planning for interventions. Over the years machine learning has gained much attention on addressing the problem of students dropout. This is because machine learning techniques can effectively facilitate determination of at-risk students and timely planning for interventions. In order to collect, organize, and synthesize existing knowledge in the field of machine learning on addressing student dropout; literature in academic journals, books and case studies have been surveyed. The survey reveal that, several machine learning algorithms have been proposed in literature. However, most of those algorithms have been developed and tested in developed countries. Hence, developing countries are facing lack of research on the use of machine learning on addressing this problem. Furthermore, many studies focus on addressing student dropout using student level datasets. However, developing countries need to include school level datasets due to the issue of limited resources. Therefore, this paper presents an overview of machine learning in education with the focus on techniques for student dropout prediction. Furthermore, the paper highlights open challenges for future research directions."
Proposed guideline for minimum information stroke research and clinical data reporting,"The management and analyses of large datasets is one of the grand challenges of modern biomedical research. Establishing methods to harmonise and standardise data collection, reporting, sharing and the employed data dictionaries, can support the resolution of these challenges whilst improving research quality, data quality and integrity, allowing sustainable knowledge transfer through re-usability, interoperability, reproducibility. The current project aimed to develop and propose a standardised reporting guideline for stroke research and clinical data reporting. Through systematic consolidation and harmonization of published data collection and reporting standards, several recommendations were drafted for the proposed guideline. These recommendations were reviewed by domain-researchers and clinicians using an online survey, developed in REDCap. The survey was completed by 20 international stroke-specialists, majority of respondents were based in Africa (10), followed by America, Europe and Australia (10). Of these respondents; the majority were working as dual clinician-researchers (57%) with more than 10 yearsâ experience in the field (78%). Data elements within the reporting standard were classified as participant-, study-and experiment-level information, further subdivided into essential or optional information, and defined using existing ontologies. The proposed reporting guideline can be employed for research utility and adapted for clinical utility as well. It is accompanied with an associated XML schema for REDCap implementation, to increase the user friendliness of data capturing, sharing, reporting and governance. Ultimately, the adoption of common reporting in stroke research has the potential to ensure that researchers gain the maximum benefit from their generated data and data collections. Â© 2019 The Author(s).","The management and analyses of large datasets is one of the grand challenges of modern biomedical research. Establishing methods to harmonise and standardise data collection, reporting, sharing and the employed data dictionaries, can support the resolution of these challenges whilst improving research quality, data quality and integrity, allowing sustainable knowledge transfer through re-usability, interoperability, reproducibility. The current project aimed to develop and propose a standardised reporting guideline for stroke research and clinical data reporting. Through systematic consolidation and harmonization of published data collection and reporting standards, several recommendations were drafted for the proposed guideline. These recommendations were reviewed by domain-researchers and clinicians using an online survey, developed in REDCap. The survey was completed by 20 international stroke-specialists, majority of respondents were based in Africa , followed by America, Europe and Australia . Of these respondents; the majority were working as dual clinician-researchers (57%) with more than 10 years experience in the field (78%). Data elements within the reporting standard were classified as participant-, study-and experiment-level information, further subdivided into essential or optional information, and defined using existing ontologies. The proposed reporting guideline can be employed for research utility and adapted for clinical utility as well. It is accompanied with an associated XML schema for REDCap implementation, to increase the user friendliness of data capturing, sharing, reporting and governance. Ultimately, the adoption of common reporting in stroke research has the potential to ensure that researchers gain the maximum benefit from their generated data and data collections."
MASER: A science ready toolbox for low frequency radio astronomy,"MASER (Measurements, Analysis, and Simulation of Emission in the Radio range) is a comprehensive infrastructure dedicated to time-dependent low frequency radio astronomy (up to about 50 MHz). The main radio sources observed in this spectral range are the Sun, the magnetized planets (Earth, Jupiter, Saturn), and our Galaxy, which are observed either from ground or space. Ground observatories can capture high resolution data streams with a high sensitivity. Conversely, space-borne instruments can observe below the ionospheric cut-off (at about 10 MHz) and can be placed closer to the studied object. Several tools have been developed in the last decade for sharing space physics data. Data visualization tools developed by various institutes are available to share, display and analyse space physics time series and spectrograms. The MASER team has selected a sub-set of those tools and applied them to low frequency radio astronomy. MASER also includes a Python software library for reading raw data from agency archives. Â© 2020 The Author(s).","MASER (Measurements, Analysis, and Simulation of Emission in the Radio range) is a comprehensive infrastructure dedicated to time-dependent low frequency radio astronomy (up to about 50 MHz). The main radio sources observed in this spectral range are the Sun, the magnetized planets (Earth, Jupiter, Saturn), and our Galaxy, which are observed either from ground or space. Ground observatories can capture high resolution data streams with a high sensitivity. Conversely, space-borne instruments can observe below the ionospheric cut-off (at about 10 MHz) and can be placed closer to the studied object. Several tools have been developed in the last decade for sharing space physics data. Data visualization tools developed by various institutes are available to share, display and analyse space physics time series and spectrograms. The MASER team has selected a sub-set of those tools and applied them to low frequency radio astronomy. MASER also includes a Python software library for reading raw data from agency archives."
Knowledge grid: An intelligent system for collaboration and knowledge management in nigerian universities,"Nigeria has 169 universities approved by National University Commission (NUC) with 43 Federal Universities, 47 State Universities and 79 Private Universities. The universities are located in different geographical areas and collaboration among these universities is difficult and this makes accessibility to data and information resource of the universities a problem. As a result quite a number of knowledge built from these resources are locally managed within each university private network that is not shared with other universities due to lack of virtual collaboration on a grid computing platform. Knowledge building and management in a country is a function of the information that one could have access to with ease but non-collaboration of the universities in a virtual environment is a hindrance to quality national knowledge building and management. Thus the need arises to embrace grid system as platform for all universities to pool their resources together and hence make their information resources available and accessible for knowledge building and management. This report presents an overview of Nigeria universitiesâ operations as regard their research and innovation policies and vis-Ã -vis governance and resource management across public and private universities. Also we did a systematic review and analysis of knowledge grid (KG) technology vis-Ã -vis its use for knowledge management in Nigerian universities. We focused on identifying the strengths and challenges of knowledge grid development and implementation vis-Ã -vis the technological, economic and social implications involved in the process. Our future work is to develop a robust knowledge grid model that will allow collaboration among universities in Nigerian community and hence facilitates information and knowledge sharing and management among the universities. Â© 2020 The Author(s).","Nigeria has 169 universities approved by National University Commission (NUC) with 43 Federal Universities, 47 State Universities and 79 Private Universities. The universities are located in different geographical areas and collaboration among these universities is difficult and this makes accessibility to data and information resource of the universities a problem. As a result quite a number of knowledge built from these resources are locally managed within each university private network that is not shared with other universities due to lack of virtual collaboration on a grid computing platform. Knowledge building and management in a country is a function of the information that one could have access to with ease but non-collaboration of the universities in a virtual environment is a hindrance to quality national knowledge building and management. Thus the need arises to embrace grid system as platform for all universities to pool their resources together and hence make their information resources available and accessible for knowledge building and management. This report presents an overview of Nigeria universities operations as regard their research and innovation policies and vis--vis governance and resource management across public and private universities. Also we did a systematic review and analysis of knowledge grid (KG) technology vis--vis its use for knowledge management in Nigerian universities. We focused on identifying the strengths and challenges of knowledge grid development and implementation vis--vis the technological, economic and social implications involved in the process. Our future work is to develop a robust knowledge grid model that will allow collaboration among universities in Nigerian community and hence facilitates information and knowledge sharing and management among the universities."
Multiset analysis of consequences of natural disasters impacts on large-scale industrial systems,"Paper is dedicated to the new approach to distributed industrial systems (IS) sustainability/ vulnerability assessment. This approach is based on the unitary multiset grammars (UMG) as a flexible and convenient tool designed specially for large systems analysis and optimization. UMG description of IS technological base as well as multiset representation of order completed by the IS, its resource base and impact on the IS are presented. Criterion for recognition of IS sustainability to the impact is formulated. UMG extension for natural disasters impacts (NDI) representation is introduced, and criterion for recognition of IS sustainability to the NDI is also presented. The solution of the reverse problem, concerning part of the order, which may be completed by the affected IS, is described. Implementation issues are considered. Â© 2018 The Author(s).","Paper is dedicated to the new approach to distributed industrial systems (IS) sustainability/ vulnerability assessment. This approach is based on the unitary multiset grammars (UMG) as a flexible and convenient tool designed specially for large systems analysis and optimization. UMG description of IS technological base as well as multiset representation of order completed by the IS, its resource base and impact on the IS are presented. Criterion for recognition of IS sustainability to the impact is formulated. UMG extension for natural disasters impacts (NDI) representation is introduced, and criterion for recognition of IS sustainability to the NDI is also presented. The solution of the reverse problem, concerning part of the order, which may be completed by the affected IS, is described. Implementation issues are considered."
Data tracking analysis of the geomagnetic fixed-station network in China,"Data tracking analysis is an important mechanism for increasing data analysis capacity and eliminating interference from observational data. In this study, the technique was applied to the geomagnetic fixed-station network to improve the efficiency and accuracy of analysis to extract useful information. This paper introduces the scope, workflow, analysis platform, abnormal variation status, and results of the geomagnetic data tracking analysis. We present some typical examples of abnormal variations in addition to our proposals for future work. Â© 2018, Ubiquity Press Ltd. All rights reserved.","Data tracking analysis is an important mechanism for increasing data analysis capacity and eliminating interference from observational data. In this study, the technique was applied to the geomagnetic fixed-station network to improve the efficiency and accuracy of analysis to extract useful information. This paper introduces the scope, workflow, analysis platform, abnormal variation status, and results of the geomagnetic data tracking analysis. We present some typical examples of abnormal variations in addition to our proposals for future work."
Curating scientific information in knowledge infrastructures,"Interpreting observational data is a fundamental task in the sciences, specifically in earth and environmental science where observational data are increasingly acquired, curated, and published systematically by environmental research infrastructures. Typically subject to substantial processing, observational data are used by research communities, their research groups and individual scientists, who interpret such primary data for their meaning in the context of research investigations. The result of interpretation is informationâmeaningful secondary or derived dataâabout the observed environment. Research infrastructures and research communities are thus essential to evolving uninterpreted observational data to information. In digital form, the classical bearer of information are the commonly known â(elaborated) data products,â for instance maps. In such form, meaning is generally implicit e.g., in map colour coding, and thus largely inaccessible to machines. The systematic acquisition, curation, possible publishing and further processing of information gained in observational data interpretationâas machine readable data and their machine readable meaningâis not common practice among environmental research infrastructures. For a use case in aerosol science, we elucidate these problems and present a Jupyter based prototype infrastructure that exploits a machine learning approach to interpretation and could support a research community in interpreting observational data and, more importantly, in curating and further using resulting information about a studied natural phenomenon. Â© 2018 The Author(s).","Interpreting observational data is a fundamental task in the sciences, specifically in earth and environmental science where observational data are increasingly acquired, curated, and published systematically by environmental research infrastructures. Typically subject to substantial processing, observational data are used by research communities, their research groups and individual scientists, who interpret such primary data for their meaning in the context of research investigations. The result of interpretation is informationmeaningful secondary or derived dataabout the observed environment. Research infrastructures and research communities are thus essential to evolving uninterpreted observational data to information. In digital form, the classical bearer of information are the commonly known (elaborated) data products, for instance maps. In such form, meaning is generally implicit , in map colour coding, and thus largely inaccessible to machines. The systematic acquisition, curation, possible publishing and further processing of information gained in observational data interpretationas machine readable data and their machine readable meaningis not common practice among environmental research infrastructures. For a use case in aerosol science, we elucidate these problems and present a Jupyter based prototype infrastructure that exploits a machine learning approach to interpretation and could support a research community in interpreting observational data and, more importantly, in curating and further using resulting information about a studied natural phenomenon."
Reviving an old and valuable collection of microscope slides through the use of citizen science,"Since the federation of Australia in 1901 Geoscience Australia, and its predecessor organisations, have amassed a significant collection of microscope slides of a variety of physical samples from across Australia, Antarctica, and adjacent regions. The extensive nature of the collection and the diverse and often remote nature of the source locations means that the cost of recreating the collection, if possible, would be $AU100Ms. The original samples were collected as part of either extensive government geological mapping programs or more specific scientific expeditions conducted for major Government initiatives. They are technically open to anyone (industry, educational institutions, the public), but are essentially unknown and almost impossible to access. Management of this collection was based on an aged card catalogue and ledger system developed in the pre-digital era. The aged management system, with increasing deterioration of the physical media, combined with loss of access to even some of the original contributors meant that rescue work was needed. Rescuing the collection made use of non-traditional means, including the extensive use of web-based citizen science and reference to a small number of onsite volunteers. Through essentially a volunteer effort, from a group more used to biology related items, the project has seen the transcription of some 40,000 sample metadata records (more than 2.5 times our current electronic holdings). This paper examines the process undertaken and advocates the approach that has made it successful. It promotes the value and benefits to Geoscience Australia, participating volunteers and potential users of the collection. Â© 2019, Ubiquity Press. All rights reserved.","Since the federation of Australia in 1901 Geoscience Australia, and its predecessor organisations, have amassed a significant collection of microscope slides of a variety of physical samples from across Australia, Antarctica, and adjacent regions. The extensive nature of the collection and the diverse and often remote nature of the source locations means that the cost of recreating the collection, if possible, would be $AU100Ms. The original samples were collected as part of either extensive government geological mapping programs or more specific scientific expeditions conducted for major Government initiatives. They are technically open to anyone (industry, educational institutions, the public), but are essentially unknown and almost impossible to access. Management of this collection was based on an aged card catalogue and ledger system developed in the pre-digital era. The aged management system, with increasing deterioration of the physical media, combined with loss of access to even some of the original contributors meant that rescue work was needed. Rescuing the collection made use of non-traditional means, including the extensive use of web-based citizen science and reference to a small number of onsite volunteers. Through essentially a volunteer effort, from a group more used to biology related items, the project has seen the transcription of some 40,000 sample metadata records (more than 2.5 times our current electronic holdings). This paper examines the process undertaken and advocates the approach that has made it successful. It promotes the value and benefits to Geoscience Australia, participating volunteers and potential users of the collection."
Fostering data sharing in multidisciplinary research communities: A case study in the geospatial domain,"The sharing of research data allows for information reuse and knowledge advancement but its realization is often a challenge and seldom successful in practice. We propose a workflow for the design of a User Support System (USS) aimed at tutoring research groups in data sharing by considering their social and domain backgrounds. Our engagement approach focuses on multidisciplinary geospatial research, particularly when interoperable data sharing is required. Specifically, we first characterize the research community on the basis of the behavior and competences in data management by its groups and then target the needs of the latter with specific facilities. We address for the first time in literature the issue of modeling research groups as targets of the USS and provide a roadmap to standardize USS activities across different communities. We describe the implementation of the workflow in the context of an Italian research project and we assess the impact of the USS in terms of increase in the number of nodes and resources in the projectâs data infrastructure, and of fulfilment of the expectations by the research groups. Â© 2019 The Author(s).","The sharing of research data allows for information reuse and knowledge advancement but its realization is often a challenge and seldom successful in practice. We propose a workflow for the design of a User Support System (USS) aimed at tutoring research groups in data sharing by considering their social and domain backgrounds. Our engagement approach focuses on multidisciplinary geospatial research, particularly when interoperable data sharing is required. Specifically, we first characterize the research community on the basis of the behavior and competences in data management by its groups and then target the needs of the latter with specific facilities. We address for the first time in literature the issue of modeling research groups as targets of the USS and provide a roadmap to standardize USS activities across different communities. We describe the implementation of the workflow in the context of an Italian research project and we assess the impact of the USS in terms of increase in the number of nodes and resources in the projects data infrastructure, and of fulfilment of the expectations by the research groups."
Text and image compression based on data mining perspective,"Data Compression has been one of the enabling technologies for the on-going digital multimedia revolution for decades which resulted in renowned algorithms like Huffman Encoding, LZ77, Gzip, RLE and JPEG etc. Researchers have looked into the character/word based approaches to Text and Image Compression missing out the larger aspect of pattern mining from large databases. The central theme of our compression research focuses on the Compression perspective of Data Mining as suggested by Naren Ramakrishnan et al. wherein efficient versions of seminal algorithms of Text/Image compression are developed using various Frequent Pattern Mining(FPM)/Clustering techniques. This paper proposes a cluster of novel and hybrid efficient text and image compression algorithms employing efficient data structures like Hash and Graphs. We have retrieved optimal set of patterns through pruning which is efficient in terms of database scan/storage space by reducing the code table size. Moreover, a detailed analysis of time and space complexity is performed for some of our approaches and various text structures are proposed. Simulation results over various spare/dense benchmark text corpora indicate 18% to 751% improvement in compression ratio over other state of the art techniques. In Image compression, our results showed up to 45% improvement in compression ratio and up to 40% in image quality efficiency. Â© 2018, Ubiquity Press Ltd. All rights reserved.","Data Compression has been one of the enabling technologies for the on-going digital multimedia revolution for decades which resulted in renowned algorithms like Huffman Encoding, LZ77, Gzip, RLE and JPEG etc. Researchers have looked into the character/word based approaches to Text and Image Compression missing out the larger aspect of pattern mining from large databases. The central theme of our compression research focuses on the Compression perspective of Data Mining as suggested by Naren Ramakrishnan et al. wherein efficient versions of seminal algorithms of Text/Image compression are developed using various Frequent Pattern Mining(FPM)/Clustering techniques. This paper proposes a cluster of novel and hybrid efficient text and image compression algorithms employing efficient data structures like Hash and Graphs. We have retrieved optimal set of patterns through pruning which is efficient in terms of database scan/storage space by reducing the code table size. Moreover, a detailed analysis of time and space complexity is performed for some of our approaches and various text structures are proposed. Simulation results over various spare/dense benchmark text corpora indicate 18% to 751% improvement in compression ratio over other state of the art techniques. In Image compression, our results showed up to 45% improvement in compression ratio and up to 40% in image quality efficiency."
Indigenous data governance: Strategies from united states native nations,"Data have become the new global currency, and a powerful force in making decisions and wielding power. As the world engages with open data, big data reuse, and data linkage, what do data-driven futures look like for communities plagued by data inequities? Indigenous data stakeholders and non-Indigenous allies have explored this question over the last three years in a series of meetings through the Research Data Alliance (RDA). Drawing on RDA and other gatherings, and a systematic scan of literature and practice, we consider possible answers to this question in the context of Indigenous peoples vis-Ã¡-vis two emerging concepts: Indigenous data sovereignty and Indigenous data governance. Specifically, we focus on the data challenges facing Native nations and the intersection of data, tribal sovereignty, and power. Indigenous data sovereignty is the right of each Native nation to govern the collection, ownership, and application of the tribeâs data. Native nations exercise Indigenous data sovereignty through the interrelated processes of Indigenous data governance and decolonizing data. This paper explores the implications of Indigenous data sovereignty and Indigenous data governance for Native nations and others. We argue for the repositioning of authority over Indigenous data back to Indigenous peoples. At the same time, we recognize that there are significant obstacles to rebuilding effective Indigenous data systems and the process will require resources, time, and partnerships among Native nations, other governments, and data agents. Â© 2019 The Author(s).","Data have become the new global currency, and a powerful force in making decisions and wielding power. As the world engages with open data, big data reuse, and data linkage, what do data-driven futures look like for communities plagued by data inequities? Indigenous data stakeholders and non-Indigenous allies have explored this question over the last three years in a series of meetings through the Research Data Alliance (RDA). Drawing on RDA and other gatherings, and a systematic scan of literature and practice, we consider possible answers to this question in the context of Indigenous peoples vis--vis two emerging concepts: Indigenous data sovereignty and Indigenous data governance. Specifically, we focus on the data challenges facing Native nations and the intersection of data, tribal sovereignty, and power. Indigenous data sovereignty is the right of each Native nation to govern the collection, ownership, and application of the tribes data. Native nations exercise Indigenous data sovereignty through the interrelated processes of Indigenous data governance and decolonizing data. This paper explores the implications of Indigenous data sovereignty and Indigenous data governance for Native nations and others. We argue for the repositioning of authority over Indigenous data back to Indigenous peoples. At the same time, we recognize that there are significant obstacles to rebuilding effective Indigenous data systems and the process will require resources, time, and partnerships among Native nations, other governments, and data agents."
Unpacking the âblack boxâ of public expenditure data in africa: Quantification of agricultural spending using mozambiqueâs budget reports,"This paper undertakes a detailed examination of the availability and quality of data on public expenditures in agriculture in Africa. We consider the case of Mozambique, a country charac-terised by low income and low administrative capacity, but also by a policy environment that has turned a focused lens on public funding to agriculture. We explore the extent to which domestic analysts may be able to access and use such data to reliably quantify public resource allocation to the sector, and to unpack the âblack boxâ of what goes into country-level public expenditure statistics. We find that data are, surprisingly, freely available in great abundance. This has encouraging aspects but also pitfalls: On the one hand, data that are often out of public sight are openly accessible for Mozambican researchers to draw upon. But the drawback of high abundance emanates from its manifestation in the form of a proliferation of multiple classification systems used to create a fine disaggregation of public funds data; given Mozambiqueâs limited public sector capacity, this has meant that each classification system leaves a lot to be desired, making it hard to use any single one to accurately and fully reliably reconstruct the amount of public resources going to agriculture. Making the hard choice to eliminate some of the classification systems, and dedicate this freed-up capacity to be more thorough on the retained ones, would better serve domestic users of such data, as well as the government, which is both a consumer and producer of these data. Â© 2018, Ubiquity Press Ltd. All rights reserved.","This paper undertakes a detailed examination of the availability and quality of data on public expenditures in agriculture in Africa. We consider the case of Mozambique, a country charac-terised by low income and low administrative capacity, but also by a policy environment that has turned a focused lens on public funding to agriculture. We explore the extent to which domestic analysts may be able to access and use such data to reliably quantify public resource allocation to the sector, and to unpack the black box of what goes into country-level public expenditure statistics. We find that data are, surprisingly, freely available in great abundance. This has encouraging aspects but also pitfalls: On the one hand, data that are often out of public sight are openly accessible for Mozambican researchers to draw upon. But the drawback of high abundance emanates from its manifestation in the form of a proliferation of multiple classification systems used to create a fine disaggregation of public funds data; given Mozambiques limited public sector capacity, this has meant that each classification system leaves a lot to be desired, making it hard to use any single one to accurately and fully reliably reconstruct the amount of public resources going to agriculture. Making the hard choice to eliminate some of the classification systems, and dedicate this freed-up capacity to be more thorough on the retained ones, would better serve domestic users of such data, as well as the government, which is both a consumer and producer of these data."
NASA's earth observing data and information system - Near-term challenges,"NASAâs Earth Observing System Data and Information System (EOSDIS) has been a central component of the NASA Earth observation program since the 1990âs. EOSDIS manages data covering a wide range of Earth science disciplines including cryosphere, land cover change, polar processes, field campaigns, ocean surface, digital elevation, atmospheric dynamics and composition, and inter-disciplinary research, and many others. One of the key components of EOSDIS is a set of twelve discipline-based Distributed Active Archive Centers (DAACs) distributed across the United States. Managed by NASAâs Earth Science Data and Information System (ESDIS) Project at the Goddard Space Flight Center, these DAACs serve over 4 million users globally. The ESDIS Project provides the infrastructure support for EOSDIS, which includes other components such as common metadata and metrics management systems, specialized network systems, standards management, and centralized support for use of commercial cloud capabilities. Given the long-term requirements, and the rapid pace of information technology and changing expectations of the user community, EOSDIS has evolved continually over the past three decades. However, many challenges remain. Challenges in three key areas are addressed in this paper: managing volume and variety, enabling data discovery and access, and incorporating user feedback and concerns. Â© 2019 The Author(s).","NASAs Earth Observing System Data and Information System (EOSDIS) has been a central component of the NASA Earth observation program since the 1990 EOSDIS manages data covering a wide range of Earth science disciplines including cryosphere, land cover change, polar processes, field campaigns, ocean surface, digital elevation, atmospheric dynamics and composition, and inter-disciplinary research, and many others. One of the key components of EOSDIS is a set of twelve discipline-based Distributed Active Archive Centers (DAACs) distributed across the United States. Managed by NASAs Earth Science Data and Information System (ESDIS) Project at the Goddard Space Flight Center, these DAACs serve over 4 million users globally. The ESDIS Project provides the infrastructure support for EOSDIS, which includes other components such as common metadata and metrics management systems, specialized network systems, standards management, and centralized support for use of commercial cloud capabilities. Given the long-term requirements, and the rapid pace of information technology and changing expectations of the user community, EOSDIS has evolved continually over the past three decades. However, many challenges remain. Challenges in three key areas are addressed in this paper: managing volume and variety, enabling data discovery and access, and incorporating user feedback and concerns."
Text mining and data information analysis for network public opinion,"Network public opinion information is massive and complex, and it is difficult to make effective use of manual means. In this paper, a method based on pattern matching and machine learning (PMML) was proposed to analyze the emotional tendencies of network public opinion. Firstly, the key words in public opinion were extracted, then the patterns were extracted and matched, and the emotional tendencies of words were calculated to obtain the pattern sequence vectors. Support vector machine (SVM) classifier was used to classify emotional tendencies. The Internet reviews of Meituan hotel were taken as the experimental subject. PMML method was found to have a high classification performance, with a maximum accuracy of 86.75%. It suggested the effectiveness of the proposed method. Then PMML method was used to classify the emotional tendencies of the collected reviews, and the results showed that the negative emotional tendency was greater than the positive tendency, which showed the inadequacy of Meituan hotel. The experiments in this paper provide some basis for the application of PMML in sentiment analysis of Internet public opinion. Â© 2019 The Author(s).","Network public opinion information is massive and complex, and it is difficult to make effective use of manual means. In this paper, a method based on pattern matching and machine learning (PMML) was proposed to analyze the emotional tendencies of network public opinion. Firstly, the key words in public opinion were extracted, then the patterns were extracted and matched, and the emotional tendencies of words were calculated to obtain the pattern sequence vectors. Support vector machine (SVM) classifier was used to classify emotional tendencies. The Internet reviews of Meituan hotel were taken as the experimental subject. PMML method was found to have a high classification performance, with a maximum accuracy of 86.75%. It suggested the effectiveness of the proposed method. Then PMML method was used to classify the emotional tendencies of the collected reviews, and the results showed that the negative emotional tendency was greater than the positive tendency, which showed the inadequacy of Meituan hotel. The experiments in this paper provide some basis for the application of PMML in sentiment analysis of Internet public opinion."
Data without software are just numbers,"Great strides have been made to encourage researchers to archive data created by research and provide the necessary systems to support their storage. Additionally it is recognised that data are meaningless unless their provenance is preserved, through appropriate meta-data. Alongside this is a pressing need to ensure the quality and archiving of the software that generates data, through simulation, control of experiment or data-collection and that which analyses, modifies and draws value from raw data. In order to meet the aims of reproducibility we argue that data management alone is insufficient: it must be accompanied by good software practices, the training to facilitate it and the support of stakeholders, including appropriate recognition for software as a research output. Â© 2020 The Author(s).","Great strides have been made to encourage researchers to archive data created by research and provide the necessary systems to support their storage. Additionally it is recognised that data are meaningless unless their provenance is preserved, through appropriate meta-data. Alongside this is a pressing need to ensure the quality and archiving of the software that generates data, through simulation, control of experiment or data-collection and that which analyses, modifies and draws value from raw data. In order to meet the aims of reproducibility we argue that data management alone is insufficient: it must be accompanied by good software practices, the training to facilitate it and the support of stakeholders, including appropriate recognition for software as a research output."
Additions to the last millennium reanalysis multi-proxy database,"Progress in paleoclimatology increasingly occurs via data syntheses. We describe additions to a collection prepared for use in paleoclimate state estimation, specifically the Last Millennium Reanalysis (LMR). The 2290 additional series include 2152 tree ring chronologies and 138 other series. They supplement the collection used previously and together form a database titled LMRdb 1.0.0. The additional data draws from lake core, ice core, coral, speleothem, and tree ring archives, using published data primarily from the NOAA Paleoclimatology archive and a set of tree ring width chronologies standardized from raw International Tree Ring Data Bank ring width series. In contrast to many previous paleo compilations, the data were not selected (screened) on the basis of their environmental correlation, multi-century length, or other attributes. The inclusion of proxies sensitive to moisture and other environmental variables expands their use in data assimilation. A preliminary calibration using linear regression with mean annual temperature reveals characteristics of the proxy series and their relationship to temperature, as well as the noise and error characteristics of the records. The additional records are structured as individual files in the NOAA Paleoclimatology format and archived at NOAA Paleoclimatology (Anderson et al. 2018) and will continue to be improved and expanded as part of the LMR Project. The additions represent a four-fold increase in the number of records available for assimilation, provide expanded geographic coverage, and add additional proxy variables. Applications include data assimilation, proxy system model development, and paleoclimate reconstruction using climate field reconstruction and other methods. Â© 2019 The Author(s).","Progress in paleoclimatology increasingly occurs via data syntheses. We describe additions to a collection prepared for use in paleoclimate state estimation, specifically the Last Millennium Reanalysis (LMR). The 2290 additional series include 2152 tree ring chronologies and 138 other series. They supplement the collection used previously and together form a database titled LMRdb 1.0.0. The additional data draws from lake core, ice core, coral, speleothem, and tree ring archives, using published data primarily from the NOAA Paleoclimatology archive and a set of tree ring width chronologies standardized from raw International Tree Ring Data Bank ring width series. In contrast to many previous paleo compilations, the data were not selected (screened) on the basis of their environmental correlation, multi-century length, or other attributes. The inclusion of proxies sensitive to moisture and other environmental variables expands their use in data assimilation. A preliminary calibration using linear regression with mean annual temperature reveals characteristics of the proxy series and their relationship to temperature, as well as the noise and error characteristics of the records. The additional records are structured as individual files in the NOAA Paleoclimatology format and archived at NOAA Paleoclimatology (Anderson et al. 2018) and will continue to be improved and expanded as part of the LMR Project. The additions represent a four-fold increase in the number of records available for assimilation, provide expanded geographic coverage, and add additional proxy variables. Applications include data assimilation, proxy system model development, and paleoclimate reconstruction using climate field reconstruction and other methods."
Enhancing the research data management of computer-based educational assessments in Switzerland,"Since 2006 the education authorities in Switzerland have been obliged by the Constitution to harmonize important benchmarks in the educational system throughout Switzerland. With the development of national educational objectives in four disciplines an important basis for the implementation of this constitutional mandate was created. In 2013 the Swiss National Core Skills Assessment Program (in German: ÃGK â ÃberprÃ¼fung der Grundkompetenzen) was initiated to investigate the skills of students, starting with three of four domains: mathematics, language of teaching and first foreign language in grades 2, 6 and 9. ÃGK uses a computer-based test and a sample size of 25.000 students per year. A huge challenge for computer-based educational assessment is the research data management process. Data from several different systems and tools existing in different formats has to be merged to obtain data products researchers can utilize. The long term preservation has to be adapted as well. In this paper, we describe our current processes and data sources as well as our ideas for enhancing the data management. Â© 2018, Ubiquity Press Ltd. All rights reserved.","Since 2006 the education authorities in Switzerland have been obliged by the Constitution to harmonize important benchmarks in the educational system throughout Switzerland. With the development of national educational objectives in four disciplines an important basis for the implementation of this constitutional mandate was created. In 2013 the Swiss National Core Skills Assessment Program (in German: GK berprfung der Grundkompetenzen) was initiated to investigate the skills of students, starting with three of four domains: mathematics, language of teaching and first foreign language in grades 2, 6 and 9. GK uses a computer-based test and a sample size of 25.000 students per year. A huge challenge for computer-based educational assessment is the research data management process. Data from several different systems and tools existing in different formats has to be merged to obtain data products researchers can utilize. The long term preservation has to be adapted as well. In this paper, we describe our current processes and data sources as well as our ideas for enhancing the data management."
Understanding human mobility patterns in a developing country using mobile phone data,"This study demonstrates the use of mobile phone data to derive country-wide mobility patterns. We identified significant locations of users such as home, work, and other based on a combined measure of frequency, duration, time, and day of mobile phone interactions. Consecutive mobile phone records of users are used to identify stay and pass-by locations. A stay location is where users spend a significant amount of their time measured through their mobile phone usage. Trips are constructed for each user between two consecutive stay locations in a day and then categorized by purpose and time of the day. Three measures of entropy are used to further understand the regularity of userâs spatiotemporal mobility patterns. The results show that userâs in a high entropy cluster has high percentage of non-home based trips (77%), and userâs in a low entropy cluster has high percentage of commuting trips (49%), indicating high regularity. A set of doubly constrained trip distribution models is estimated. To measure travel cost, the concept of a centroid point that assumes the origins and destinations of all trips are concentrated at an arbitrary location such as the centroid of a zone is replaced by multiple origins and destinations represented by cell tower locations. Note that a cell tower location can only be used as trips origin/destination location when a stay is detected. The travel cost measured between cell tower locations has resulted in shorter trip distances and the model estimation shows less sensitivity to the distance-decay effect. Â© 2019 The Author(s).","This study demonstrates the use of mobile phone data to derive country-wide mobility patterns. We identified significant locations of users such as home, work, and other based on a combined measure of frequency, duration, time, and day of mobile phone interactions. Consecutive mobile phone records of users are used to identify stay and pass-by locations. A stay location is where users spend a significant amount of their time measured through their mobile phone usage. Trips are constructed for each user between two consecutive stay locations in a day and then categorized by purpose and time of the day. Three measures of entropy are used to further understand the regularity of users spatiotemporal mobility patterns. The results show that users in a high entropy cluster has high percentage of non-home based trips (77%), and users in a low entropy cluster has high percentage of commuting trips (49%), indicating high regularity. A set of doubly constrained trip distribution models is estimated. To measure travel cost, the concept of a centroid point that assumes the origins and destinations of all trips are concentrated at an arbitrary location such as the centroid of a zone is replaced by multiple origins and destinations represented by cell tower locations. Note that a cell tower location can only be used as trips origin/destination location when a stay is detected. The travel cost measured between cell tower locations has resulted in shorter trip distances and the model estimation shows less sensitivity to the distance-decay effect."
"The Norwegian National Ground Segment; Preservation, distribution and exploitation of sentinel data","In order to take advantage of the Sentinel program, the Norwegian Space Agency decided to establish a national collaborative ground segment for satellite data with the purpose of simplifying data access, ensure support for operational national services and long term preservation of data. This is the NBS where MET Norway has the technical responsibility in terms of providing the infrastructure and storage capacity for data management. Serving the data through two separate platforms, the end users have access to the data in its original format in addition to Sentinel-1 and Sentinel-2 products in NetCDF-4/CF. Using the latter format, services like regridding, subsetting, visualization and aggregation are integrated utilizing OPeNDAP in combination with OGC WMS and OGC WPS. In addition, data uploading and retrieving operations are simplified for an end user since streaming of data by means of OPeNDAP is supported in multiple programming languages. Due to the strong coupling between space based earth observations, in-situ observation, model data etc, disseminating data in a generic data management system utilizing NetCDF-4/CF and OPeNDAP is convenient for seamless integration across branches. However, the current CF version is not mature for handling all parts of the Sentinel data but future development looks very promising. Â© 2019 The Author(s).","In order to take advantage of the Sentinel program, the Norwegian Space Agency decided to establish a national collaborative ground segment for satellite data with the purpose of simplifying data access, ensure support for operational national services and long term preservation of data. This is the NBS where MET Norway has the technical responsibility in terms of providing the infrastructure and storage capacity for data management. Serving the data through two separate platforms, the end users have access to the data in its original format in addition to Sentinel-1 and Sentinel-2 products in NetCDF-4/CF. Using the latter format, services like regridding, subsetting, visualization and aggregation are integrated utilizing OPeNDAP in combination with OGC WMS and OGC WPS. In addition, data uploading and retrieving operations are simplified for an end user since streaming of data by means of OPeNDAP is supported in multiple programming languages. Due to the strong coupling between space based earth observations, in-situ observation, model data etc, disseminating data in a generic data management system utilizing NetCDF-4/CF and OPeNDAP is convenient for seamless integration across branches. However, the current CF version is not mature for handling all parts of the Sentinel data but future development looks very promising."
"Impacts and challenges of ICT based scale-up campaigns: Lessons learnt from the use of SMS to support maize farmers in the UPTAKE project, Tanzania","Providing smallholder farmers with support through conventional government extension approaches is challenging as the number of extension agents is decreasing. At the same time, new information and communication technologies (ICTs), such as short message services (SMS) sent via mobile phones, show considerable promise to complement existing extension services. In the UP-scaling Technology in Agriculture through Knowledge and Extension (UPTAKE) Project, ICTs were used to create awareness and increase uptake and adoption of agricultural innovations by maize farmers in Tanzania. Two SMS-based maize campaigns were implemented during the 2016/2017 and 2017/2018 cropping seasons in the Southern Highlands of Tanzania. Prior to the start of the campaigns, formative research to determine maize production knowledge, practices and challenges was conducted in Mbeya and Songwe Region. After the campaign a telephone survey, key informant interviews and focus group discussions were conducted. During the campaign, about 3.8 million SMS were disseminated to over 55,000 farmers. 73% were male, 19% owned smart phones and 86% farmed maize on up to 1.2 hectares of land. Farmers reported maize production challenges as: unreliable markets, inadequate extension services, pest outbreaks and lack of knowledge to identify counterfeit inputs particularly seeds and fertilizers. The UPTAKE mobile SMS campaign was a new approach to agricultural extension in this area. A telephone survey amongst a sample of farmers who received the SMS revealed that 53% of respondents considered that this was now their preferred as a source of information compared to traditional sources including neighbours and family members, demonstration plots, agricultural extension workers and radios. Key lessons learnt relate to management of databases of farmer contacts, importance of participatory processes in developing content and designing SMS campaigns, and the need for flexibility and promptness in responding to emerging threats such as delayed rains and outbreaks of pests. Good practices like buy in and authorizations from the government administrative structures and compliance with countryâs regulations on communication are integral to the success of ICT projects. Â© 2020 The Author(s).","Providing smallholder farmers with support through conventional government extension approaches is challenging as the number of extension agents is decreasing. At the same time, new information and communication technologies (ICTs), such as short message services (SMS) sent via mobile phones, show considerable promise to complement existing extension services. In the UP-scaling Technology in Agriculture through Knowledge and Extension (UPTAKE) Project, ICTs were used to create awareness and increase uptake and adoption of agricultural innovations by maize farmers in Tanzania. Two SMS-based maize campaigns were implemented during the 2016/2017 and 2017/2018 cropping seasons in the Southern Highlands of Tanzania. Prior to the start of the campaigns, formative research to determine maize production knowledge, practices and challenges was conducted in Mbeya and Songwe Region. After the campaign a telephone survey, key informant interviews and focus group discussions were conducted. During the campaign, about 3.8 million SMS were disseminated to over 55,000 farmers. 73% were male, 19% owned smart phones and 86% farmed maize on up to 1.2 hectares of land. Farmers reported maize production challenges as: unreliable markets, inadequate extension services, pest outbreaks and lack of knowledge to identify counterfeit inputs particularly seeds and fertilizers. The UPTAKE mobile SMS campaign was a new approach to agricultural extension in this area. A telephone survey amongst a sample of farmers who received the SMS revealed that 53% of respondents considered that this was now their preferred as a source of information compared to traditional sources including neighbours and family members, demonstration plots, agricultural extension workers and radios. Key lessons learnt relate to management of databases of farmer contacts, importance of participatory processes in developing content and designing SMS campaigns, and the need for flexibility and promptness in responding to emerging threats such as delayed rains and outbreaks of pests. Good practices like buy in and authorizations from the government administrative structures and compliance with countrys regulations on communication are integral to the success of ICT projects."
Policy needs to go hand in hand with practice: The learning and listening approach to data management,"In this paper, we explain our strategy for developing research data management policies at TU Delft. Policies can be important drivers for research institutions in the implementation of good data management practices. As Rans and Jones note (Rans and Jones 2013), "" Policies provide clarity of purpose and may help in the framing of roles, responsibilities and requisite actions. They also legitimise making the case for investmentâ. However, policy development often tends to place the researchers in a passive position, while they are the ones managing research data on a daily basis. Therefore, at TU Delft, we have taken an alternative approach: a policy needs to go hand in hand with practice. The policy development was initiated by the Research Data Services at TU Delft Library, but as the process continued, other stakeholders, such as legal and IT departments, got involved. Finally, the faculty-based Data Stewards have played a key role in leading the consultations with the research community that led to the development of the faculty-specific policies. This allows for disciplinary differences to be reflected in the policies and to create a closer connection between policies and day-to-day research practice. Our primary intention was to keep researchers and research practices at the centre of our strategy for data management. We did not want to introduce and mandate requirements before adequate infrastructure and professional support were available to our research community and before our researchers were themselves willing to discuss formalisation of data management practices. This paper describes the key steps taken and the most important decisions made during the development of RDM policies at TU Delft. Â© 2019 The Author(s).","In this paper, we explain our strategy for developing research data management policies at TU Delft. Policies can be important drivers for research institutions in the implementation of good data management practices. As Rans and Jones note (Rans and Jones 2013), "" Policies provide clarity of purpose and may help in the framing of roles, responsibilities and requisite actions. They also legitimise making the case for investment. However, policy development often tends to place the researchers in a passive position, while they are the ones managing research data on a daily basis. Therefore, at TU Delft, we have taken an alternative approach: a policy needs to go hand in hand with practice. The policy development was initiated by the Research Data Services at TU Delft Library, but as the process continued, other stakeholders, such as legal and IT departments, got involved. Finally, the faculty-based Data Stewards have played a key role in leading the consultations with the research community that led to the development of the faculty-specific policies. This allows for disciplinary differences to be reflected in the policies and to create a closer connection between policies and day-to-day research practice. Our primary intention was to keep researchers and research practices at the centre of our strategy for data management. We did not want to introduce and mandate requirements before adequate infrastructure and professional support were available to our research community and before our researchers were themselves willing to discuss formalisation of data management practices. This paper describes the key steps taken and the most important decisions made during the development of RDM policies at TU Delft."
The landscape of rights and licensing initiatives for data sharing,"Over the last twenty years, a wide variety of resources have been developed to address the rights and licensing problems inherent with contemporary data sharing practices. The landscape of developments is this area is increasingly confusing and difficult to navigate, due to the complexity of intellectual property and ethics issues associated with sharing sensitive data. This paper seeks to address this challenge, examining the landscape and presenting a Version 1.0 directory of resources. A multi-method study was pursued, with an environmental scan examining 20 resources, resulting in three high-level categories: standards, tools, and community initiatives; and a content analysis revealing the subcategories of rights, licensing, metadata & ontologies. A timeline confirms a shift in licensing standardization priorities from open data to more nuanced and technologically robust solutions, over time, to accommodate for more sensitive data types. This paper reports on the research undertaking, and comments on the potential for using license-specific metadata supplements and developing data-centric rights and licensing ontologies. Â© 2019 The Author(s).","Over the last twenty years, a wide variety of resources have been developed to address the rights and licensing problems inherent with contemporary data sharing practices. The landscape of developments is this area is increasingly confusing and difficult to navigate, due to the complexity of intellectual property and ethics issues associated with sharing sensitive data. This paper seeks to address this challenge, examining the landscape and presenting a Version 1.0 directory of resources. A multi-method study was pursued, with an environmental scan examining 20 resources, resulting in three high-level categories: standards, tools, and community initiatives; and a content analysis revealing the subcategories of rights, licensing, metadata & ontologies. A timeline confirms a shift in licensing standardization priorities from open data to more nuanced and technologically robust solutions, over time, to accommodate for more sensitive data types. This paper reports on the research undertaking, and comments on the potential for using license-specific metadata supplements and developing data-centric rights and licensing ontologies."
Building open access to research (OAR) data infrastructure at NIST,"As a National Metrology Institute (NMI), the USA National Institute of Standards and Technology (NIST) scientists, engineers and technology experts conduct research across a full spectrum of physical science domains. NIST is a non-regulatory agency within the U.S. Department of Commerce with a mission to promote U.S. innovation and industrial competitiveness by advancing measurement science, standards, and technology in ways that enhance economic security and improve our quality of life. NIST research results in the production and distribution of standard reference materials, calibration services, and datasets. These are generated from a wide range of complex laboratory instrumentation, expert analyses, and calibration processes. In response to a government open data policy, and in collaboration with the broader research community, NIST has developed a federated Open Access to Research (OAR) scientific data infrastructure aligned with FAIR (Findable, Accessible, Interoperable, Reusable) data principles. Through the OAR initiatives, NISTâs Material Measurement Laboratory Office of Data and Informatics (ODI) recently released a new scientific data discovery portal and public data repository. These science- oriented applications provide dissemination and public access for data from across the broad spectrum of NIST research disciplines, including chemistry, biology, materials science (such as crystallography, nanomaterials, etc.), physics, disaster resilience, cyberinfrastructure, communications, forensics, and others. NISTâs public data consist of carefully curated Standard Reference Data, legacy high valued data, and new research data publications. The repository is thus evolving both in content and features as the nature of research progresses. Implementation of the OAR infrastructure is key to NISTâs role in sharing high integrity reproducible research for measurement science in a rapidly changing world. Â© 2019 The Author(s).","As a National Metrology Institute (NMI), the USA National Institute of Standards and Technology (NIST) scientists, engineers and technology experts conduct research across a full spectrum of physical science domains. NIST is a non-regulatory agency within the Department of Commerce with a mission to promote innovation and industrial competitiveness by advancing measurement science, standards, and technology in ways that enhance economic security and improve our quality of life. NIST research results in the production and distribution of standard reference materials, calibration services, and datasets. These are generated from a wide range of complex laboratory instrumentation, expert analyses, and calibration processes. In response to a government open data policy, and in collaboration with the broader research community, NIST has developed a federated Open Access to Research (OAR) scientific data infrastructure aligned with FAIR (Findable, Accessible, Interoperable, Reusable) data principles. Through the OAR initiatives, NISTs Material Measurement Laboratory Office of Data and Informatics (ODI) recently released a new scientific data discovery portal and public data repository. These science- oriented applications provide dissemination and public access for data from across the broad spectrum of NIST research disciplines, including chemistry, biology, materials science (such as crystallography, nanomaterials, etc.), physics, disaster resilience, cyberinfrastructure, communications, forensics, and others. NISTs public data consist of carefully curated Standard Reference Data, legacy high valued data, and new research data publications. The repository is thus evolving both in content and features as the nature of research progresses. Implementation of the OAR infrastructure is key to NISTs role in sharing high integrity reproducible research for measurement science in a rapidly changing world."
"Supporting the interdisciplinary, long-term research project âpatterns in soil-vegetation-atmosphere-systemsâ by data management services","Science conducted in cross-institutional, interdisciplinary, long-term research projects requires active sharing of data, documents and further information. Thus, within the Collaborative Research Centre/Transregio 32 âPatterns in Soil-Vegetation-Atmosphere Systemsâ, funded by the German Research Foundation, research data management (RDM) services have been available since early 2007. These services were established to support all researchers during their entire individual research studies. They cover provision of general guidance, support and training for RDM. To fulfil the scientistsâ needs and requests with regard to storage, backup, documentation, search and sharing of data with other project members, a project-specific RDM system was designed and implemented. This system was developed and continuously modified in collaboration with the scientists to facilitate their system acceptance. Besides the mentioned services, the system supports further common services such as controlled access to data, rights management, data publication with DOI and data statistics (on repository and single data level). All RDM services provided for the scientists are thus bundled and available to the users in one system: a âone-stop-shopâ. After more than ten years of RDM service provision for the CRC/TR32, the repository statistics clearly visualize the use of the diverse RDM system services. Furthermore, it has been shown that an RDM adapted to the needs of interdisciplinary researchers can be fruitful and indispensable when scientists conduct their research study e.g. with a time lag. RDM services established at an early stage can contribute to a successful long-term research project. Â© 2019 The Author(s).","Science conducted in cross-institutional, interdisciplinary, long-term research projects requires active sharing of data, documents and further information. Thus, within the Collaborative Research Centre/Transregio 32 Patterns in Soil-Vegetation-Atmosphere Systems, funded by the German Research Foundation, research data management (RDM) services have been available since early 2007. These services were established to support all researchers during their entire individual research studies. They cover provision of general guidance, support and training for RDM. To fulfil the scientists needs and requests with regard to storage, backup, documentation, search and sharing of data with other project members, a project-specific RDM system was designed and implemented. This system was developed and continuously modified in collaboration with the scientists to facilitate their system acceptance. Besides the mentioned services, the system supports further common services such as controlled access to data, rights management, data publication with DOI and data statistics (on repository and single data level). All RDM services provided for the scientists are thus bundled and available to the users in one system: a one-stop-shop. After more than ten years of RDM service provision for the CRC/TR32, the repository statistics clearly visualize the use of the diverse RDM system services. Furthermore, it has been shown that an RDM adapted to the needs of interdisciplinary researchers can be fruitful and indispensable when scientists conduct their research study with a time lag. RDM services established at an early stage can contribute to a successful long-term research project."
Analysis of several years of DI magnetometer comparison results by the geomagnetic network of China and IAGA,"The comparison of absolute geomagnetic instruments is an important component of geomagnetic observation. To promote high quality standards in geomagnetic data acquisition, the International Association of Geomagnetism and Aeronomy (IAGA) organizes an international comparison every two years. In China, this comparison is part of quality control process for geomagnetic observation data, and is organized by the Geomagnetic Network of China (GNC). In this paper, the comparison results from several years are analysed in detail, and some useful information is presented that will help to guide the observatoryâs future observation work and improve data quality. In addition, the quality of the absolute observation data of GNC and IAGA is evaluated using a statistical method. This should aid scientists who use these data to understand their research results. Â© 2019 The Author(s).","The comparison of absolute geomagnetic instruments is an important component of geomagnetic observation. To promote high quality standards in geomagnetic data acquisition, the International Association of Geomagnetism and Aeronomy (IAGA) organizes an international comparison every two years. In China, this comparison is part of quality control process for geomagnetic observation data, and is organized by the Geomagnetic Network of China (GNC). In this paper, the comparison results from several years are analysed in detail, and some useful information is presented that will help to guide the observatorys future observation work and improve data quality. In addition, the quality of the absolute observation data of GNC and IAGA is evaluated using a statistical method. This should aid scientists who use these data to understand their research results."
"Science metadata management, interoperability and data citations of the National Institute of Polar Research, Japan","The Polar Data Centre (PDC) of the National Institute of Polar Research (NIPR) has a responsibility to manage polar science data as part of the National Antarctic Data Centre and the Science Committee on Antarctic Research. During the International Polar Year (IPY 2007â2008), a remarkable number of data/metadata involving multi-disciplinary science activities were compiled. Although the long-term stewardship of the accumulation of metadata falls to the data center of NIPR, the work has been in collaboration with the Global Change Master Directory, the Polar Information Commons, the World Data System and other data science bodies/communities under the International Council for Science. In addition, links with other data centers, such as the Data Integration and Analysis System Program of the Global Earth Observation System of Systems and the Polar Data Catalogue of Canada were initiated in 2014 using the Open Archives Initiative Protocol for Metadata Harvesting. The metadata compiled by the PDC were recently modified using an automatic attributing system and DataCite through the Japan Link Center. Â© 2018 The Author(s).","The Polar Data Centre (PDC) of the National Institute of Polar Research (NIPR) has a responsibility to manage polar science data as part of the National Antarctic Data Centre and the Science Committee on Antarctic Research. During the International Polar Year (IPY 20072008), a remarkable number of data/metadata involving multi-disciplinary science activities were compiled. Although the long-term stewardship of the accumulation of metadata falls to the data center of NIPR, the work has been in collaboration with the Global Change Master Directory, the Polar Information Commons, the World Data System and other data science bodies/communities under the International Council for Science. In addition, links with other data centers, such as the Data Integration and Analysis System Program of the Global Earth Observation System of Systems and the Polar Data Catalogue of Canada were initiated in 2014 using the Open Archives Initiative Protocol for Metadata Harvesting. The metadata compiled by the PDC were recently modified using an automatic attributing system and DataCite through the Japan Link Center."
A comprehensive video dataset for multi-modal recognition systems,"This paper presents a comprehensive, highly defined and fully labelled video dataset. This dataset consists of videos related to 67 different subjects. The videos contain similar text and the text contains digits from 1 to 20 recited by 67 different subjects using the same experimental setup. This dataset can be used as a unique resource for researchers and analysts for training deep neural networks to build highly efficient and accurate recognition models in various domains of computer vision such as face recognition model, expression recognition model, speech recognition model, text recognition, etc. In this paper, we also train models related to face recognition and speech recognition on our dataset and also compare the results with the publically available datasets to show the effectiveness of our dataset. The experimental results show that our comprehensive dataset is more accurate than other dataset on which the models are tested. Â© 2019 The Author(s).","This paper presents a comprehensive, highly defined and fully labelled video dataset. This dataset consists of videos related to 67 different subjects. The videos contain similar text and the text contains digits from 1 to 20 recited by 67 different subjects using the same experimental setup. This dataset can be used as a unique resource for researchers and analysts for training deep neural networks to build highly efficient and accurate recognition models in various domains of computer vision such as face recognition model, expression recognition model, speech recognition model, text recognition, etc. In this paper, we also train models related to face recognition and speech recognition on our dataset and also compare the results with the publically available datasets to show the effectiveness of our dataset. The experimental results show that our comprehensive dataset is more accurate than other dataset on which the models are tested."
On identifying terrorists using their victory signs,"In certain cases, the only evidence to identify terrorists, who are seen in digital images or videos is their handsâ shapes, particularly, the victory sign as performed by many of them when they intentionally hide their faces, and/or distort their voices. This paper proposes new methods to identify those persons for the first time from their victory sign. These methods are based on features extracted from the fingers areas using shape moments in addition to other features related to fingers contours. To evaluate the proposed methods and to show the feasibility of this study we have created a victory sign database for 400 volunteers using a mobile phone camera. The experimental results using different classifiers show encouraging identification results; as the best precision/recall were achieved by merging normalized features from both methods using linear discriminate analysis classifier with 96.6% precision and 96.3 recall. Such a high performance achieved by the proposed methods shows their great potential to be applied for terroristsâ identification from their victory sign. Â© 2018 The Author(s).","In certain cases, the only evidence to identify terrorists, who are seen in digital images or videos is their hands shapes, particularly, the victory sign as performed by many of them when they intentionally hide their faces, and/or distort their voices. This paper proposes new methods to identify those persons for the first time from their victory sign. These methods are based on features extracted from the fingers areas using shape moments in addition to other features related to fingers contours. To evaluate the proposed methods and to show the feasibility of this study we have created a victory sign database for 400 volunteers using a mobile phone camera. The experimental results using different classifiers show encouraging identification results; as the best precision/recall were achieved by merging normalized features from both methods using linear discriminate analysis classifier with 96.6% precision and 96.3 recall. Such a high performance achieved by the proposed methods shows their great potential to be applied for terrorists identification from their victory sign."
Virtual research environment for regional climatic processes analysis: Ontological approach to spatial data systematization,"This paper describes a Virtual Research Environment (VRE) based on a web GIS platform âClimate+â, which provides an access to analytic instruments processing 19 collections of meteorological and climate data of several international organizations. This environment provides systematization of spatial data and related climate information and allows a user getting analysis results using geoinformation technologies. The ontology approach to this systematization is described, making it possible to match semantics of meteorological and climate parameters presented in different collections and used in solving various applied problems. Â© 2018, Ubiquity Press Ltd. All rights reserved.","This paper describes a Virtual Research Environment (VRE) based on a web GIS platform Climate+, which provides an access to analytic instruments processing 19 collections of meteorological and climate data of several international organizations. This environment provides systematization of spatial data and related climate information and allows a user getting analysis results using geoinformation technologies. The ontology approach to this systematization is described, making it possible to match semantics of meteorological and climate parameters presented in different collections and used in solving various applied problems."
A discussion of value metrics for data repositories in earth and environmental sciences,"Despite growing recognition of the importance of public data to the modern economy and to scientific progress, long-term investment in the repositories that manage and disseminate scientific data in easily accessible-ways remains elusive. Repositories are asked to demonstrate that there is a net value of their data and services to justify continued funding or attract new funding sources. Here, representatives from a number of environmental and Earth science repositories evaluate approaches for assessing the costs and benefits of publishing scientific data in their repositories, identifying various metrics that repositories typically use to report on the impact and value of their data products and services, plus additional metrics that would be useful but are not typically measured. We rated each metric by (a) the difficulty of implementation by our specific repositories and (b) its importance for value determination. As managers of environmental data repositories, we find that some of the most easily obtainable data-use metrics (such as data downloads and page views) may be less indicative of value than metrics that relate to discoverability and broader use. Other intangible but equally important metrics (e.g., laws or regulations impacted, lives saved, new proposals generated), will require considerable additional research to describe and develop, plus resources to implement at scale. As value can only be determined from the point of view of a stakeholder, it is likely that multiple sets of metrics will be needed, tailored to specific stakeholder needs. Moreover, economically based analyses or the use of specialists in the field are expensive and can happen only as resources permit. Â© 2019 The Author(s).","Despite growing recognition of the importance of public data to the modern economy and to scientific progress, long-term investment in the repositories that manage and disseminate scientific data in easily accessible-ways remains elusive. Repositories are asked to demonstrate that there is a net value of their data and services to justify continued funding or attract new funding sources. Here, representatives from a number of environmental and Earth science repositories evaluate approaches for assessing the costs and benefits of publishing scientific data in their repositories, identifying various metrics that repositories typically use to report on the impact and value of their data products and services, plus additional metrics that would be useful but are not typically measured. We rated each metric by (a) the difficulty of implementation by our specific repositories and (b) its importance for value determination. As managers of environmental data repositories, we find that some of the most easily obtainable data-use metrics (such as data downloads and page views) may be less indicative of value than metrics that relate to discoverability and broader use. Other intangible but equally important metrics (, laws or regulations impacted, lives saved, new proposals generated), will require considerable additional research to describe and develop, plus resources to implement at scale. As value can only be determined from the point of view of a stakeholder, it is likely that multiple sets of metrics will be needed, tailored to specific stakeholder needs. Moreover, economically based analyses or the use of specialists in the field are expensive and can happen only as resources permit."
Real estate evaluation model based on genetic algorithm optimized neural network,"With the rapid development of society, the real estate economy, as an important part of Chinese economy, is showing a growing trend. But it is also the most likely to generate bubble economy, causing financial risks; it will trigger a series of social contradictions and cause social unrest in severe cases. Therefore, it is urgent to improve and optimize the real estate evaluation model. In this study, the real estate was evaluated based on the neural network model optimized by genetic algorithm. Through sorting out and summarizing the real estate data in a period of time, the corresponding model was established and the test data were obtained. The average relative error value of the genetic algorithm optimized neural network model was 3.552, which was smaller than that of the Back-Propagation (BP) neural network prediction model. The experimental conclusion that the new network model was better than the traditional model was obtained. This work opens up a new route of real estate evaluation. Â© 2019 The Author(s).","With the rapid development of society, the real estate economy, as an important part of Chinese economy, is showing a growing trend. But it is also the most likely to generate bubble economy, causing financial risks; it will trigger a series of social contradictions and cause social unrest in severe cases. Therefore, it is urgent to improve and optimize the real estate evaluation model. In this study, the real estate was evaluated based on the neural network model optimized by genetic algorithm. Through sorting out and summarizing the real estate data in a period of time, the corresponding model was established and the test data were obtained. The average relative error value of the genetic algorithm optimized neural network model was 3.552, which was smaller than that of the Back-Propagation (BP) neural network prediction model. The experimental conclusion that the new network model was better than the traditional model was obtained. This work opens up a new route of real estate evaluation."
The Australian research data commons,"A research data commons can provide researchers with the data and resources necessary to conduct world class research. More than this, a research data commons can be transformational in facilitating change in the way research is conducted, in terms of both research culture and the availability of research data and analytical tools. This paper describes frameworks needed to build a transformational data commons, through examination of the development of the Australian Research Data Commons (ARDC) ARDC was formed in 2018 as part of a 20-year vision to transform Australiaâs research culture by enabling access to the digital data and eResearch platforms that can significantly enhance research capacity. ARDC is located within both national and international eResearch ecosystems, and its unique positioning must be understood, alongside the achievements of its three predecessor organisations, to understand the niche from which ARDC aims to provide maximum value and impact. Consideration is given to the challenges inherent in both the current Australian ecosystem and beyond, to articulate ARDCâs focus going forward. The paper concludes with consideration of the international dimension, drawing on discussions around the development of a global data commons. Â© 2019 The Author(s).","A research data commons can provide researchers with the data and resources necessary to conduct world class research. More than this, a research data commons can be transformational in facilitating change in the way research is conducted, in terms of both research culture and the availability of research data and analytical tools. This paper describes frameworks needed to build a transformational data commons, through examination of the development of the Australian Research Data Commons (ARDC) ARDC was formed in 2018 as part of a 20-year vision to transform Australias research culture by enabling access to the digital data and eResearch platforms that can significantly enhance research capacity. ARDC is located within both national and international eResearch ecosystems, and its unique positioning must be understood, alongside the achievements of its three predecessor organisations, to understand the niche from which ARDC aims to provide maximum value and impact. Consideration is given to the challenges inherent in both the current Australian ecosystem and beyond, to articulate ARDCs focus going forward. The paper concludes with consideration of the international dimension, drawing on discussions around the development of a global data commons."
Practical application of a data stewardship maturity matrix for the NOAA onestop project,"Assessing the stewardship maturity of individual datasets is an essential part of ensuring and improving the way datasets are documented, preserved, and disseminated to users. It is a critical step towards meeting U.S. federal regulations, organizational requirements, and user needs. However, it is challenging to do so consistently and quantifiably. The Data Stewardship Maturity Matrix (DSMM), developed jointly by NOAAâs National Centers for Environmental Information (NCEI) and the Cooperative Institute for Climate and Satellites-North Carolina (CICS-NC), provides a uniform framework for consistently rating stewardship maturity of individual datasets in nine key components: preservability, accessibility, usability, production sustainability, data quality assurance, data quality control/monitoring, data quality assessment, transparency/traceability, and data integrity. So far, the DSMM has been applied to over 800 individual datasets that are archived and/or managed by NCEI, in support of the NOAAâs OneStop Data Discovery and Access Framework Project. As a part of the OneStop-ready process, tools, implementation guidance, workflows, and best practices are developed to assist the application of the DSMM and described in this paper. The DSMM ratings are also consistently captured in the ISO standard-based dataset-level quality metadata and citable quality descriptive information documents, which serve as interoperable quality information to both machine and human end-users. These DSMM implementation and integration workflows and best practices could be adopted by other data management and stewardship projects or adapted for applications of other maturity assessment models. Â© 2019 The Author(s).","Assessing the stewardship maturity of individual datasets is an essential part of ensuring and improving the way datasets are documented, preserved, and disseminated to users. It is a critical step towards meeting federal regulations, organizational requirements, and user needs. However, it is challenging to do so consistently and quantifiably. The Data Stewardship Maturity Matrix (DSMM), developed jointly by NOAAs National Centers for Environmental Information (NCEI) and the Cooperative Institute for Climate and Satellites-North Carolina (CICS-NC), provides a uniform framework for consistently rating stewardship maturity of individual datasets in nine key components: preservability, accessibility, usability, production sustainability, data quality assurance, data quality control/monitoring, data quality assessment, transparency/traceability, and data integrity. So far, the DSMM has been applied to over 800 individual datasets that are archived and/or managed by NCEI, in support of the NOAAs OneStop Data Discovery and Access Framework Project. As a part of the OneStop-ready process, tools, implementation guidance, workflows, and best practices are developed to assist the application of the DSMM and described in this paper. The DSMM ratings are also consistently captured in the ISO standard-based dataset-level quality metadata and citable quality descriptive information documents, which serve as interoperable quality information to both machine and human end-users. These DSMM implementation and integration workflows and best practices could be adopted by other data management and stewardship projects or adapted for applications of other maturity assessment models."
A regional project in support of the SADC cyber-infrastructure framework implementation: Weather and climate,"Early warning systems in the areas of weather and climate for supporting decision making and strategic intervention in key sectors (e.g. water, health, energy, disaster risk management, and agriculture) rely on the use of earth observations and numerical models that require supercomputing resources. Such resources are now primarily provided through High Performance Computing (HPC) facilities. As a result of a global increase in availability and accessibility of supercomputing HPC facilities, numerical models that can now be employed have become more complex. Furthermore, resolutions now used and achievable have increased significantly. The Southern African Development Community (SADC) Cyber-Infrastructure (CI) Framework aims to build increased capacity in regional research and education networks, data sharing infrastructure and trained human capital - to make efficient and effective use of the CI resources. Through the implementation of the regional CI framework and national initiatives, several member states in Southern Africa now have HPC facilities. The availability of this infrastructure in the region provides opportunities for domains, domain scientists and collaboration through research and development projects. For meteorology, this will support more local and regional weather and climate scientists. For meteorological services, this will mean increased in-house and in-country capacity to run models, with less reliance on external resources from developed countries. This paper discusses a regional weather and climate implementation project of the SADC CI. Â© 2019 The Author(s).","Early warning systems in the areas of weather and climate for supporting decision making and strategic intervention in key sectors ( water, health, energy, disaster risk management, and agriculture) rely on the use of earth observations and numerical models that require supercomputing resources. Such resources are now primarily provided through High Performance Computing (HPC) facilities. As a result of a global increase in availability and accessibility of supercomputing HPC facilities, numerical models that can now be employed have become more complex. Furthermore, resolutions now used and achievable have increased significantly. The Southern African Development Community (SADC) Cyber-Infrastructure Framework aims to build increased capacity in regional research and education networks, data sharing infrastructure and trained human capital - to make efficient and effective use of the CI resources. Through the implementation of the regional CI framework and national initiatives, several member states in Southern Africa now have HPC facilities. The availability of this infrastructure in the region provides opportunities for domains, domain scientists and collaboration through research and development projects. For meteorology, this will support more local and regional weather and climate scientists. For meteorological services, this will mean increased in-house and in-country capacity to run models, with less reliance on external resources from developed countries. This paper discusses a regional weather and climate implementation project of the SADC"
Proper attribution for curation and maintenance of research collections: Metadata recommendations of the RDA/TDWG working group,"Research collections are an important tool for understanding the Earth, its systems, and human interaction. Despite the importance of collections, many are not maintained or curated as thoroughly as we would like. Part of the reason for this is the lack of professional reward for collection, curation, or maintenance. To address this gap in attribution metadata, the Research Data Alliance (RDA) and the Biodiversity Information Standards (TDWG) organization co-endorsed a Working Group to create recommendations for the representation of attribution metadata. After 18 months, this Working Group recommended the use of PROV entities and properties to link people (Agent), the curatorial actions they perform (Activity), and the digital or physical objects they are curating (Entity). Assigning a Role to an Agent is optional. These recommendations are discussed in the context of the RDA, TDWG, and existing standards. Future work includes adapting these recommendations to the specific needs of TDWG and developing a pilot application in collaboration with ORCID and the Data Futures project. Â© 2019 The Author(s).","Research collections are an important tool for understanding the Earth, its systems, and human interaction. Despite the importance of collections, many are not maintained or curated as thoroughly as we would like. Part of the reason for this is the lack of professional reward for collection, curation, or maintenance. To address this gap in attribution metadata, the Research Data Alliance (RDA) and the Biodiversity Information Standards (TDWG) organization co-endorsed a Working Group to create recommendations for the representation of attribution metadata. After 18 months, this Working Group recommended the use of PROV entities and properties to link people (Agent), the curatorial actions they perform (Activity), and the digital or physical objects they are curating (Entity). Assigning a Role to an Agent is optional. These recommendations are discussed in the context of the RDA, TDWG, and existing standards. Future work includes adapting these recommendations to the specific needs of TDWG and developing a pilot application in collaboration with ORCID and the Data Futures project."
"Identifying and implementing relevant research data management services for the library at the university of dodoma, Tanzania","Research Data Management (RDM) services are increasingly becoming a subject of interest for academic and research libraries globally â this is also the case in developing countries. The interest is motivated by a need to support research activities through data sharing and collaboration both locally and internationally. Many institutions, especially in the developed countries, have implemented RDM services to accelerate research and innovation through e-Research but extensive RDM is not so common in developing countries. In reality many African universities and research institutions are yet to implement the most basic of data management services. We believe that the absence of political will and national government mandates on data management often hold back the development and implementation of RDM services. Similarly, research funding agencies are not yet applying sufficient pressure to ensure that Africa complies with the requirement to deposit research data in trusted repositories. While the context was acknowledged the University of Dodoma library staff realized that it is urgent to prepare for the inevitable â the time when RDM will be a requirement for research funding support. This paper presents the results of research conducted at the University of Dodoma, Tanzania. The purpose of the research was to identify and report on relevant RDM services that need to be implemented so that researchers and university management could collaborate and make our research data accessible to the international community. This paper presents findings on important issues for consideration when planning to develop and implement RDM services at a developing country academic institution. The paper also mentions the requirements for the sustainability of these initiatives. Â© 2020 The Author(s).","Research Data Management (RDM) services are increasingly becoming a subject of interest for academic and research libraries globally this is also the case in developing countries. The interest is motivated by a need to support research activities through data sharing and collaboration both locally and internationally. Many institutions, especially in the developed countries, have implemented RDM services to accelerate research and innovation through e-Research but extensive RDM is not so common in developing countries. In reality many African universities and research institutions are yet to implement the most basic of data management services. We believe that the absence of political will and national government mandates on data management often hold back the development and implementation of RDM services. Similarly, research funding agencies are not yet applying sufficient pressure to ensure that Africa complies with the requirement to deposit research data in trusted repositories. While the context was acknowledged the University of Dodoma library staff realized that it is urgent to prepare for the inevitable the time when RDM will be a requirement for research funding support. This paper presents the results of research conducted at the University of Dodoma, Tanzania. The purpose of the research was to identify and report on relevant RDM services that need to be implemented so that researchers and university management could collaborate and make our research data accessible to the international community. This paper presents findings on important issues for consideration when planning to develop and implement RDM services at a developing country academic institution. The paper also mentions the requirements for the sustainability of these initiatives."
The impact of targeted data management training for field research projects - A case study,We present a joint effort at Virginia Tech between a research group in the Department of Fish and Wildlife Conservation and Data Services in the University Libraries to improve data management for long-term ecological field research projects in the Florida Panhandle. Consultative research data management support from Data Services in the University Libraries played an integral role in the development of the training curriculum. Emphasizing the importance of data quality to the field workers at the beginning of this training curriculum was a vital part of its success. Also critical for success was the research groupâs investment of time and effort to work with field workers and improve data management systems. We compare this case study to three others in the literature to compare and contrast data management processes and procedures. This case study serves as one example of how targeted training and efforts in data and project management for a research project can lead to substantial improvements in research data quality. Â© 2019 The Author(s).,We present a joint effort at Virginia Tech between a research group in the Department of Fish and Wildlife Conservation and Data Services in the University Libraries to improve data management for long-term ecological field research projects in the Florida Panhandle. Consultative research data management support from Data Services in the University Libraries played an integral role in the development of the training curriculum. Emphasizing the importance of data quality to the field workers at the beginning of this training curriculum was a vital part of its success. Also critical for success was the research groups investment of time and effort to work with field workers and improve data management systems. We compare this case study to three others in the literature to compare and contrast data management processes and procedures. This case study serves as one example of how targeted training and efforts in data and project management for a research project can lead to substantial improvements in research data quality.
Time series prediction model of grey wolf optimized echo state network,"As a novel recursion neural network, Echo State Networks (ESN) are characterized by strong nonlinear prediction capability and effective and straightforward training algorithms. However, conventional ESN predictions require a large volume of training samples. Meanwhile, the time sequence data are complicated and unstable, resulting in insufficient learning of this network and difficult training. As a result, the accuracies of conventional ESN predictions are limited. Aimed at this issue, a time series prediction model of Grey Wolf optimized ESN has been proposed. Wout of ESN was optimized using the Grey Wolf algorithm and predictions of time series data were achieved using simplified training. The results indicated that the optimized time series prediction method exhibits superior prediction accuracy at a small sample size, compared with conventional prediction methods. Â© 2019 The Author(s).","As a novel recursion neural network, Echo State Networks (ESN) are characterized by strong nonlinear prediction capability and effective and straightforward training algorithms. However, conventional ESN predictions require a large volume of training samples. Meanwhile, the time sequence data are complicated and unstable, resulting in insufficient learning of this network and difficult training. As a result, the accuracies of conventional ESN predictions are limited. Aimed at this issue, a time series prediction model of Grey Wolf optimized ESN has been proposed. Wout of ESN was optimized using the Grey Wolf algorithm and predictions of time series data were achieved using simplified training. The results indicated that the optimized time series prediction method exhibits superior prediction accuracy at a small sample size, compared with conventional prediction methods."
Shapelet classification algorithm based on efficient subsequence matching,"Shapelet classification algorithms are an accurate classification method for time series data. Existing shapelet classifying processes are relatively inefficient and slow due to the large amount of necessary complex distance computations. This paper therefore introduces piecewise aggregate approximation(PAA) representation and an efficient subsequence matching algorithm for shapelet classification algorithms; the paper also proposes shapelet transformation classification algorithm based on efficient series matching. First, the proposed algorithm took the PAA representation for appropriate dimension reduction, and then used a subsequence matching algorithm to simplify the data classification process. The research experimented on 14 public time series datasets taken from UCI and UCR, used the original and new algorithm for classification, and compared the efficiency and accuracy of the two methods. Experimental results showed that the efficient subsequence matching algorithm could be combined with the shapelet classification algorithm; the new algorithm could ensure relatively high classification accuracy, effectively simplified the algorithm calculation process, and improved classification efficiency. Â© 2018, Ubiquity Press Ltd. All rights reserved.","Shapelet classification algorithms are an accurate classification method for time series data. Existing shapelet classifying processes are relatively inefficient and slow due to the large amount of necessary complex distance computations. This paper therefore introduces piecewise aggregate approximation(PAA) representation and an efficient subsequence matching algorithm for shapelet classification algorithms; the paper also proposes shapelet transformation classification algorithm based on efficient series matching. First, the proposed algorithm took the PAA representation for appropriate dimension reduction, and then used a subsequence matching algorithm to simplify the data classification process. The research experimented on 14 public time series datasets taken from UCI and UCR, used the original and new algorithm for classification, and compared the efficiency and accuracy of the two methods. Experimental results showed that the efficient subsequence matching algorithm could be combined with the shapelet classification algorithm; the new algorithm could ensure relatively high classification accuracy, effectively simplified the algorithm calculation process, and improved classification efficiency."
Risk assessment for scientific data,"Ongoing stewardship is required to keep data collections and archives in existence. Scientific data collections may face a range of risk factors that could hinder, constrain, or limit current or future data use. Identifying such risk factors to data use is a key step in preventing or minimizing data loss. This paper presents an analysis of data risk factors that scientific data collections may face, and a data risk assessment matrix to support data risk assessments to help ameliorate those risks. The goals of this work are to inform and enable effective data risk assessment by: a) individuals and organizations who manage data collections, and b) individuals and organizations who want to help to reduce the risks associated with data preservation and stewardship. The data risk assessment framework presented in this paper provides a platform from which risk assessments can begin, and a reference point for discussions of data stewardship resource allocations and priorities. Â© 2020 The Author(s).","Ongoing stewardship is required to keep data collections and archives in existence. Scientific data collections may face a range of risk factors that could hinder, constrain, or limit current or future data use. Identifying such risk factors to data use is a key step in preventing or minimizing data loss. This paper presents an analysis of data risk factors that scientific data collections may face, and a data risk assessment matrix to support data risk assessments to help ameliorate those risks. The goals of this work are to inform and enable effective data risk assessment by: a) individuals and organizations who manage data collections, and b) individuals and organizations who want to help to reduce the risks associated with data preservation and stewardship. The data risk assessment framework presented in this paper provides a platform from which risk assessments can begin, and a reference point for discussions of data stewardship resource allocations and priorities."
Facilitating and improving environmental research data repository interoperability,"Environmental research data repositories provide much needed services for data preservation and data dissemination to diverse communities with domain specific or programmatic data needs and standards. Due to independent development these repositories serve their communities well, but were developed with different technologies, data models and using different ontologies. Hence, the effectiveness and efficiency of these services can be vastly improved if repositories work together adhering to a shared community platform that focuses on the implementation of agreed upon standards and best practices for curation and dissemination of data. Such a community platform drives forward the convergence of technologies and practices that will advance cross-domain interoperability. It will also facilitate contributions from investigators through standardized and streamlined workflows and provide increased visibility for the role of data managers and the curation services provided by data repositories, beyond preservation infrastructure. Ten specific suggestions for such standardizations are outlined without any suggestions for priority or technical implementation. Although the recommendations are for repositories to implement, they have been chosen specifically with the data provider/data curator and synthesis scientist in mind. Â© 2018 The Author(s).","Environmental research data repositories provide much needed services for data preservation and data dissemination to diverse communities with domain specific or programmatic data needs and standards. Due to independent development these repositories serve their communities well, but were developed with different technologies, data models and using different ontologies. Hence, the effectiveness and efficiency of these services can be vastly improved if repositories work together adhering to a shared community platform that focuses on the implementation of agreed upon standards and best practices for curation and dissemination of data. Such a community platform drives forward the convergence of technologies and practices that will advance cross-domain interoperability. It will also facilitate contributions from investigators through standardized and streamlined workflows and provide increased visibility for the role of data managers and the curation services provided by data repositories, beyond preservation infrastructure. Ten specific suggestions for such standardizations are outlined without any suggestions for priority or technical implementation. Although the recommendations are for repositories to implement, they have been chosen specifically with the data provider/data curator and synthesis scientist in mind."
Introduction: Open data and Africa,"This introduction outlines the contents of the special collection âOpen Data and Africaâ, which documents the goals and aspirations associated with Open Data means in Africa today: what opportunities they offer, what challenges they pose and what the implications follow from the increasing political and institutional support for this concept. Â© 2018 The Author(s).","This introduction outlines the contents of the special collection Open Data and Africa, which documents the goals and aspirations associated with Open Data means in Africa today: what opportunities they offer, what challenges they pose and what the implications follow from the increasing political and institutional support for this concept."
Different preservation levels: The case of scholarly digital editions,"Ensuring the long-term availability of research data forms an integral part of data management services. Where OAIS compliant digital preservation has been established in recent years, in almost all cases the services aim at the preservation of file-based objects. In the Digital Humanities, research data is often represented in highly structured aggregations, such as Scholarly Digital Editions. Naturally, scholars would like their editions to remain functionally complete as long as possible. Besides standard components like webservers, the presentation typically relies on project specific code interacting with client software like webbrowsers. Especially the latter being subject to rapid change over time invariably makes such environments awkward to maintain once funding has ended. Pragmatic approaches have to be found in order to balance the curation effort and the maintainability of access to research data over time. A sketch of four potential service levels aiming at the long-term availability of research data in the humanities is outlined: (1) Continuous Maintenance, (2) Application Conservation, (3) Application Data Preservation, and (4) Bitstream Preservation. The first being too costly and the last hardly satisfactory in general, we suggest that the implementation of services by an infrastructure provider should concentrate on service levels 2 and 3. We explain their strengths and limitations considering the example of two Scholarly Digital Editions. Â© 2019 The Author(s). ttribution 4.0 Internatio.","Ensuring the long-term availability of research data forms an integral part of data management services. Where OAIS compliant digital preservation has been established in recent years, in almost all cases the services aim at the preservation of file-based objects. In the Digital Humanities, research data is often represented in highly structured aggregations, such as Scholarly Digital Editions. Naturally, scholars would like their editions to remain functionally complete as long as possible. Besides standard components like webservers, the presentation typically relies on project specific code interacting with client software like webbrowsers. Especially the latter being subject to rapid change over time invariably makes such environments awkward to maintain once funding has ended. Pragmatic approaches have to be found in order to balance the curation effort and the maintainability of access to research data over time. A sketch of four potential service levels aiming at the long-term availability of research data in the humanities is outlined: Continuous Maintenance, Application Conservation, Application Data Preservation, and Bitstream Preservation. The first being too costly and the last hardly satisfactory in general, we suggest that the implementation of services by an infrastructure provider should concentrate on service levels 2 and 3. We explain their strengths and limitations considering the example of two Scholarly Digital Editions."
The history and future of data citation in practice,"In this review, we adopt the definition that âData citation is a reference to data for the purpose of credit attribution and facilitation of access to the dataâ (TGDCSP 2013: CIDCR6). Furthermore, access should be enabled for both humans and machines (DCSG 2014). We use this to discuss how data citation has evolved over the last couple of decades and to highlight issues that need more research and attention. Data citation is not a new concept, but it has changed and evolved considerably since the beginning of the digital age. Basic practice is now established and slowly but increasingly being implemented. Nonetheless, critical issues remain. These issues are primarily because we try to address multiple human and computational concerns with a system originally designed in a non-digital world for more limited use cases. The community is beginning to challenge past assumptions, separate the multiple concerns (credit, access, reference, provenance, impact, etc.), and apply different approaches for different use cases. Â© 2019 The Author(s).","In this review, we adopt the definition that Data citation is a reference to data for the purpose of credit attribution and facilitation of access to the data (TGDCSP 2013: CIDCR6). Furthermore, access should be enabled for both humans and machines (DCSG 2014). We use this to discuss how data citation has evolved over the last couple of decades and to highlight issues that need more research and attention. Data citation is not a new concept, but it has changed and evolved considerably since the beginning of the digital age. Basic practice is now established and slowly but increasingly being implemented. Nonetheless, critical issues remain. These issues are primarily because we try to address multiple human and computational concerns with a system originally designed in a non-digital world for more limited use cases. The community is beginning to challenge past assumptions, separate the multiple concerns (credit, access, reference, provenance, impact, etc.), and apply different approaches for different use cases."
Application of natural language processing algorithms to the task of automatic classification of Russian scientific texts,"This work is devoted to the study of applicability of modern methods of machine learning to the task of automatic classification of scientific articles and abstracts. For this purpose, the study of such models of machine learning as artificial neural networks, random forest, logistic regression, and support vector machine was carried out with taking into account such a feature of scientific texts as a large number of terms specific for various categories. Separately, the stages of data collection and extraction of text characteristics are considered. The results of research are used in development of a decision support system for assignment of scientific texts to the code of the department or abstract journal of All-Russian Institute of Scientific and Technical Information of Russian Academy of Sciences. Â© 2019 The Author(s).","This work is devoted to the study of applicability of modern methods of machine learning to the task of automatic classification of scientific articles and abstracts. For this purpose, the study of such models of machine learning as artificial neural networks, random forest, logistic regression, and support vector machine was carried out with taking into account such a feature of scientific texts as a large number of terms specific for various categories. Separately, the stages of data collection and extraction of text characteristics are considered. The results of research are used in development of a decision support system for assignment of scientific texts to the code of the department or abstract journal of All-Russian Institute of Scientific and Technical Information of Russian Academy of Sciences."
Who bears the burden of long-lived molecular biology databases?,"In the early 1990s the life sciences quickly adopted online databases to facilitate wide-spread dissemination and use of scientific data. Starting in 1991, the journal Nucleic Acids Research published an annual Database Issue dedicated to articles describing molecular biology databases. Analysis of these articles reveals a set of long-lived databases which have remained available for more than 15 years. Given the pervasive challenge of sustaining community resources, these databases provide an opportunity to examine what factors contribute to persistence by addressing two questions 1) which organizations fund these long-lived databases? and 2) which organizations maintain these long-lived databases? Funding and operating organizations for 67 databases were determined through review of Database Issue articles. The results reveal a diverse set of contributing organizations with financial and operational support spread across six categories: academic, consortium/collective, government, industry, philanthropic, and society/association. The majority of databases reported support from more than one funding organization, of which government organizations were most common source of funds. Operational responsibilities were more distributed, with academic organizations serving as the most common hosts. Although there is evidence of diversification overall, the most acknowledged funding and operating organizations contribute to disproportionately large percentages of the long-lived databases investigated here. Â© 2020 The Author(s).","In the early 1990s the life sciences quickly adopted online databases to facilitate wide-spread dissemination and use of scientific data. Starting in 1991, the journal Nucleic Acids Research published an annual Database Issue dedicated to articles describing molecular biology databases. Analysis of these articles reveals a set of long-lived databases which have remained available for more than 15 years. Given the pervasive challenge of sustaining community resources, these databases provide an opportunity to examine what factors contribute to persistence by addressing two questions 1) which organizations fund these long-lived databases? and 2) which organizations maintain these long-lived databases? Funding and operating organizations for 67 databases were determined through review of Database Issue articles. The results reveal a diverse set of contributing organizations with financial and operational support spread across six categories: academic, consortium/collective, government, industry, philanthropic, and society/association. The majority of databases reported support from more than one funding organization, of which government organizations were most common source of funds. Operational responsibilities were more distributed, with academic organizations serving as the most common hosts. Although there is evidence of diversification overall, the most acknowledged funding and operating organizations contribute to disproportionately large percentages of the long-lived databases investigated here."
"Genomic research data generation, analysis and sharing â challenges in the African setting","Genomics is the study of the genetic material that constitutes the genomes of organisms. This genetic material can be sequenced and it provides a powerful tool for the study of human, plant and animal evolutionary history and diseases. Genomics research is becoming increasingly commonplace due to significant advances in and reducing costs of technologies such as sequencing. This has led to new challenges including increasing cost and complexity of data. There is, therefore, an increasing need for computing infrastructure and skills to manage, store, analyze and interpret the data. In addition, there is a significant cost associated with recruitment of participants and collection and processing of biological samples, particularly for large human genetics studies on specific diseases. As a result, researchers are often reluctant to share the data due to the effort and associated cost. In Africa, where researchers are most commonly at the study recruitment, determination of phenotypes and collection of biological samples end of the genomic research spectrum, rather than the generation of genomic data, data sharing without adequate safeguards for the interests of the primary data generators is a concern. There are substantial ethical considerations in the sharing of human genomics data. The broad consent for data sharing preferred by genomics researchers and funders does not necessarily align with the expectations of researchers, research participants, legal authorities and bioethicists. In Africa, this is complicated by concerns about comprehension of genomics research studies, quality of research ethics reviews and understanding of the implications of broad consent, secondary analyses of shared data, return of results and incidental findings. Additional challenges with genomics research in Africa include the inability to transfer, store, process and analyze large-scale genomics data on the continent, because this requires highly specialized skills and expensive computing infrastructure which are often unavailable. Recently initiatives such as H3Africa and H3ABioNet which aim to build capacity for large-scale genomics projects in Africa have emerged. Here we describe such initiatives, including the challenges faced in the generation, analysis and sharing of genomic data and how these challenges are being overcome. Â© 2017 The Author(s).","Genomics is the study of the genetic material that constitutes the genomes of organisms. This genetic material can be sequenced and it provides a powerful tool for the study of human, plant and animal evolutionary history and diseases. Genomics research is becoming increasingly commonplace due to significant advances in and reducing costs of technologies such as sequencing. This has led to new challenges including increasing cost and complexity of data. There is, therefore, an increasing need for computing infrastructure and skills to manage, store, analyze and interpret the data. In addition, there is a significant cost associated with recruitment of participants and collection and processing of biological samples, particularly for large human genetics studies on specific diseases. As a result, researchers are often reluctant to share the data due to the effort and associated cost. In Africa, where researchers are most commonly at the study recruitment, determination of phenotypes and collection of biological samples end of the genomic research spectrum, rather than the generation of genomic data, data sharing without adequate safeguards for the interests of the primary data generators is a concern. There are substantial ethical considerations in the sharing of human genomics data. The broad consent for data sharing preferred by genomics researchers and funders does not necessarily align with the expectations of researchers, research participants, legal authorities and bioethicists. In Africa, this is complicated by concerns about comprehension of genomics research studies, quality of research ethics reviews and understanding of the implications of broad consent, secondary analyses of shared data, return of results and incidental findings. Additional challenges with genomics research in Africa include the inability to transfer, store, process and analyze large-scale genomics data on the continent, because this requires highly specialized skills and expensive computing infrastructure which are often unavailable. Recently initiatives such as H3Africa and H3ABioNet which aim to build capacity for large-scale genomics projects in Africa have emerged. Here we describe such initiatives, including the challenges faced in the generation, analysis and sharing of genomic data and how these challenges are being overcome."
Recommended versus certified repositories: Mind the gap,"Researchers are increasingly required to make research data publicly available in data repositories. Although several organisations propose criteria to recommend and evaluate the quality of data repositories, there is no consensus of what constitutes a good data repository. In this paper, we investigate, first, which data repositories are recommended by various stakeholders (publishers, funders, and community organizations) and second, which repositories are certified by a number of organisations. We then compare these two lists of repositories, and the criteria for recommendation and certification. We find that criteria used by organisations recommending and certifying repositories are similar, although the certification criteria are generally more detailed. We distil the lists of criteria into seven main categories: âMissionâ, âCommunity/Recognitionâ, âLegal and Contractual Complianceâ, âAccess/Accessibilityâ, âTechnical Structure/Interfaceâ, âRetrievabilityâ and âPreservationâ. Although the criteria are similar, the lists of repositories that are recommended by the various agencies are very different. Out of all of the recommended repositories, less than 6% obtained certification. As certification is becoming more important, steps should be taken to decrease this gap between recommended and certified repositories, and ensure that certification standards become applicable, and applied, to the repositories which researchers are currently using. Â© 2017 The Author(s).","Researchers are increasingly required to make research data publicly available in data repositories. Although several organisations propose criteria to recommend and evaluate the quality of data repositories, there is no consensus of what constitutes a good data repository. In this paper, we investigate, first, which data repositories are recommended by various stakeholders (publishers, funders, and community organizations) and second, which repositories are certified by a number of organisations. We then compare these two lists of repositories, and the criteria for recommendation and certification. We find that criteria used by organisations recommending and certifying repositories are similar, although the certification criteria are generally more detailed. We distil the lists of criteria into seven main categories: Mission, Community/Recognition, Legal and Contractual Compliance, Access/Accessibility, Technical Structure/Interface, Retrievability and Preservation. Although the criteria are similar, the lists of repositories that are recommended by the various agencies are very different. Out of all of the recommended repositories, less than 6% obtained certification. As certification is becoming more important, steps should be taken to decrease this gap between recommended and certified repositories, and ensure that certification standards become applicable, and applied, to the repositories which researchers are currently using."
Connecting the persistent identifier ecosystem: Building the technical and human infrastructure for open research,"The persistent identifier (PID) landscape extends to cover objects, individuals and organisations engaged in the process of research. Established services such as DataCite, Crossref, ORCID and ISNI are providing a foundation for a trusted ecosystem and a new generation of services. Scalable identifier systems will support researchers and capture research activity in a holistic way, across the entire lifecycle. Challenges remain â siloed services are not interoperable; important types of objects are not adequately covered, many processes remain manual, and adoption, while strong, is not consistent across disciplines. This article draws on the work of the EU-funded THOR project to take stock of the current state of interoperability across the PID landscape and to discuss the next steps towards an integrated research record. Examples illustrate how this interconnectivity is facilitated technically, as well as social and human challenges in fostering adoption. User stories highlight how this network of persistent identifier services is facilitating good practice in open research and where its limitations lie. Â© 2017 The Author(s).","The persistent identifier (PID) landscape extends to cover objects, individuals and organisations engaged in the process of research. Established services such as DataCite, Crossref, ORCID and ISNI are providing a foundation for a trusted ecosystem and a new generation of services. Scalable identifier systems will support researchers and capture research activity in a holistic way, across the entire lifecycle. Challenges remain siloed services are not interoperable; important types of objects are not adequately covered, many processes remain manual, and adoption, while strong, is not consistent across disciplines. This article draws on the work of the EU-funded THOR project to take stock of the current state of interoperability across the PID landscape and to discuss the next steps towards an integrated research record. Examples illustrate how this interconnectivity is facilitated technically, as well as social and human challenges in fostering adoption. User stories highlight how this network of persistent identifier services is facilitating good practice in open research and where its limitations lie."
"14 years of PID services at the German national library of science and technology (TIB): Connected frameworks, research data and lessons learned from a national research library perspective","In an ideal research world, any scientific content should be citable and the coherent content, as well as the citation itself, should be persistent. However, todayâs scientists do not only produce traditional research papers â they produce comprehensive digital resources and collections. TIBâs mission is to develop a supportive framework for a sustainable access to such digital content â focusing on areas of engineering as well as architecture, chemistry, information technology, mathematics and physics. The term digital content comprises all digitally available resources such as audiovisual media, databases, texts, images, spreadsheets, digital lab journals, multimedia, 3D objects, statistics and software code. In executing this mission, TIB provides services for the management of digital content during ongoing and for finished research. This includes: â a technical and administrative infrastructure for indexing, cataloguing, DOI registration and licensing for text and digital objects, namely the TIB DOI registration which is active since 2005, â the administration of the ORCID DE consortium, an institutional network fostering the adoption of ORCID across academic institutions in Germany, â training and consultancy for data management, complemented with a digital repository for the deposition and provision of accessible, traceable and citable research data (RADAR), â a Research and Development Department where innovative projects focus on the visualization and the sustainable access to digital information, and â the development of a supportive framework within the German research data community which accompanies the life cycle of scientific knowledge generation and transfer. Its goal is to harmonize (meta)data display and exchange primarily on a national level (LEIBNIZ DATA project). Â© 2017 The Author(s).","In an ideal research world, any scientific content should be citable and the coherent content, as well as the citation itself, should be persistent. However, todays scientists do not only produce traditional research papers they produce comprehensive digital resources and collections. TIBs mission is to develop a supportive framework for a sustainable access to such digital content focusing on areas of engineering as well as architecture, chemistry, information technology, mathematics and physics. The term digital content comprises all digitally available resources such as audiovisual media, databases, texts, images, spreadsheets, digital lab journals, multimedia, 3D objects, statistics and software code. In executing this mission, TIB provides services for the management of digital content during ongoing and for finished research. This includes: a technical and administrative infrastructure for indexing, cataloguing, DOI registration and licensing for text and digital objects, namely the TIB DOI registration which is active since 2005, the administration of the ORCID DE consortium, an institutional network fostering the adoption of ORCID across academic institutions in Germany, training and consultancy for data management, complemented with a digital repository for the deposition and provision of accessible, traceable and citable research data (RADAR), a Research and Development Department where innovative projects focus on the visualization and the sustainable access to digital information, and the development of a supportive framework within the German research data community which accompanies the life cycle of scientific knowledge generation and transfer. Its goal is to harmonize (meta)data display and exchange primarily on a national level (LEIBNIZ DATA project)."
Identifiers for earth science data sets: Where we have been and where we need to go,"Considerable attention has been devoted to the use of persistent identifiers for assets of interest to scientific and other communities alike over the last two decades. Among persistent identifiers, Digital Object Identifiers (DOIs) stand out quite prominently, with approximately 133 million DOIs assigned to various objects as of February 2017. While the assignment of DOIs to objects such as scientific publications has been in place for many years, their assignment to Earth science data sets is more recent. Applying persistent identifiers to data setsenables improved tracking of their use and reuse, facilitates the crediting of data producers, and aids reproducibility through associating research with the exact data set(s) used. Maintaining provenance - i.e., tracing back lineage of significant scientific conclusions to the entities (data sets, algorithms, instruments, satellites, etc.) that lead to the conclusions, would be prohibitive without persistent identifiers. This paper provides a brief background on the use of persistent identifiers in general within the US, and DOIs more specifically. We examine their recent use for Earth science data sets, and outline successes and some remaining challenges. Among the challenges, for example, is the ability to conveniently and consistently obtain data citation statistics using the DOIs assigned by organizations that manage data sets. Â© 2017 The Author(s).","Considerable attention has been devoted to the use of persistent identifiers for assets of interest to scientific and other communities alike over the last two decades. Among persistent identifiers, Digital Object Identifiers (DOIs) stand out quite prominently, with approximately 133 million DOIs assigned to various objects as of February 2017. While the assignment of DOIs to objects such as scientific publications has been in place for many years, their assignment to Earth science data sets is more recent. Applying persistent identifiers to data setsenables improved tracking of their use and reuse, facilitates the crediting of data producers, and aids reproducibility through associating research with the exact data set(s) used. Maintaining provenance - , tracing back lineage of significant scientific conclusions to the entities (data sets, algorithms, instruments, satellites, etc.) that lead to the conclusions, would be prohibitive without persistent identifiers. This paper provides a brief background on the use of persistent identifiers in general within the US, and DOIs more specifically. We examine their recent use for Earth science data sets, and outline successes and some remaining challenges. Among the challenges, for example, is the ability to conveniently and consistently obtain data citation statistics using the DOIs assigned by organizations that manage data sets."
Building geoscience semantic web applications using established ontologies,"The EarthCollab project is using the VIVO Semantic Web software suite to support the discovery of information, data, and potential collaborators within the geodesy and polar science communities. This paper discusses the ontology selection, consolidation, and reuse efforts of EarthCollab. EarthCollab's ontology design approach heavily emphasizes ontology reuse, bringing together existing ontologies to support diverse use cases related to the discovery of geoscience information and resources. We developed a small local ontology to tie these existing ontologies together and to build appropriate geoscience-relevant connections. Five key ontology decision drivers are presented to outline EarthCollab's ontology design process and decision points: use cases, existing systems and metadata, semantic application dependencies, external ontology characteristics, and community recommendations for good ontological modeling practices. Â© 2016 The Author(s).","The EarthCollab project is using the VIVO Semantic Web software suite to support the discovery of information, data, and potential collaborators within the geodesy and polar science communities. This paper discusses the ontology selection, consolidation, and reuse efforts of EarthCollab. EarthCollab's ontology design approach heavily emphasizes ontology reuse, bringing together existing ontologies to support diverse use cases related to the discovery of geoscience information and resources. We developed a small local ontology to tie these existing ontologies together and to build appropriate geoscience-relevant connections. Five key ontology decision drivers are presented to outline EarthCollab's ontology design process and decision points: use cases, existing systems and metadata, semantic application dependencies, external ontology characteristics, and community recommendations for good ontological modeling practices."
Collaborations and partnerships in NASAâs Earth science data systems,"NASA has been collecting Earth observation data from spaceborne instruments since 1960. Today, there are tens of satellites orbiting the Earth and collecting frequent global observations for the benefit of mankind. Collaboration between NASA and organizations in the US and other countries has been extremely important in maintaining the Earth observation capabilities as well as collecting, organizing and managing the data. These collaborations have occurred in the form of: 1. NASAâs developing and launching spacecraft and instruments for operation by other agencies; 2. Instruments from collaborating organizations being flown on NASA satellites; and 3. Instruments from NASA being flown on satellites from collaborating organizations. In addition, there are collaborations such as joint science teams, data exchanges, and participation in international organizations to promote interoperability of various data systems. The purpose of this paper is to describe some of the Earth science data-related collaborative efforts in which NASA participates, and highlight a few results relevant to Earth system science research obtained through such collaborations. Â© 2017 The Author(s).","NASA has been collecting Earth observation data from spaceborne instruments since 1960. Today, there are tens of satellites orbiting the Earth and collecting frequent global observations for the benefit of mankind. Collaboration between NASA and organizations in the US and other countries has been extremely important in maintaining the Earth observation capabilities as well as collecting, organizing and managing the data. These collaborations have occurred in the form of: 1. NASAs developing and launching spacecraft and instruments for operation by other agencies; 2. Instruments from collaborating organizations being flown on NASA satellites; and 3. Instruments from NASA being flown on satellites from collaborating organizations. In addition, there are collaborations such as joint science teams, data exchanges, and participation in international organizations to promote interoperability of various data systems. The purpose of this paper is to describe some of the Earth science data-related collaborative efforts in which NASA participates, and highlight a few results relevant to Earth system science research obtained through such collaborations."
Editorial: 20 years of persistent identifiers â Applications and future directions,"Persistent identifiers (PID) have existed for more than 20 years and have become well established as a means for identifying literature and data on the web. They were invented to address the problem of disappearing internet links, also known as âlink rotâ, which was seen as undermining the emerging digital record of science. A number of PID systems have since been developed, and their utility for the management of the scientific record has been reviewed. Since the initial launch of the Handle System, we have seen many more uses for persistent identification, besides literature and data. A session at the European Geosciences Union General Assembly 2016 was dedicated to â20 years of persistent identifiers â where do we go next?â A number of contributions from this session have since been developed into full papers that form this Data Science Journal Special Collection, with additional solicited contributions. Together, these papers give us an overview of the use of persistent identifiers in research information infrastructures and possible future directions. Â© 2017 The Author(s).","Persistent identifiers (PID) have existed for more than 20 years and have become well established as a means for identifying literature and data on the web. They were invented to address the problem of disappearing internet links, also known as link rot, which was seen as undermining the emerging digital record of science. A number of PID systems have since been developed, and their utility for the management of the scientific record has been reviewed. Since the initial launch of the Handle System, we have seen many more uses for persistent identification, besides literature and data. A session at the European Geosciences Union General Assembly 2016 was dedicated to 20 years of persistent identifiers where do we go next? A number of contributions from this session have since been developed into full papers that form this Data Science Journal Special Collection, with additional solicited contributions. Together, these papers give us an overview of the use of persistent identifiers in research information infrastructures and possible future directions."
Organizational resilience in data archives: Three case studies in social science data archives,"As public investment in archiving research data grows, there has been increasing attention to the longevity or sustainability of the data repositories that curate such data. While there have been many conceptual frameworks developed and case reports of individual archives and digital repositories, there have been few empirical studies of how such archives persist over time. In this paper, we draw upon organizational studies theories to approach the issue of sustainability from an organizational perspective, focusing specifically on the organizational histories of three social science data archives (SSDA): ICPSR, UKDA, and LIS. Using a framework of organizational resilience to understand how archives perceive crisis, respond to it, and learn from experience, this article reports on an empirical study of sustainability in these long-lived SSDAs. The study draws from archival documents and interviews to examine how sustainability can and should be conceptualized as on-going processes over time and not as a quality at a single moment. Implications for research and practice in data archive sustainability are discussed.","As public investment in archiving research data grows, there has been increasing attention to the longevity or sustainability of the data repositories that curate such data. While there have been many conceptual frameworks developed and case reports of individual archives and digital repositories, there have been few empirical studies of how such archives persist over time. In this paper, we draw upon organizational studies theories to approach the issue of sustainability from an organizational perspective, focusing specifically on the organizational histories of three social science data archives (SSDA): ICPSR, UKDA, and LIS. Using a framework of organizational resilience to understand how archives perceive crisis, respond to it, and learn from experience, this article reports on an empirical study of sustainability in these long-lived SSDAs. The study draws from archival documents and interviews to examine how sustainability can and should be conceptualized as on-going processes over time and not as a quality at a single moment. Implications for research and practice in data archive sustainability are discussed."
Are scientific data repositories coping with research data publishing?,"Research data publishing is intended as the release of research data to make it possible for practitioners to (re)use them according to ""open science"" dynamics. There are three main actors called to deal with research data publishing practices: researchers, publishers, and data repositories. This study analyses the solutions offered by generalist scientific data repositories, i.e., repositories supporting the deposition of any type of research data. These repositories cannot make any assumption on the application domain. They are actually called to face with the almost open ended typologies of data used in science. The current practices promoted by such repositories are analysed with respect to eight key aspects of data publishing, i.e., dataset formatting, documentation, licensing, publication costs, validation, availability, discovery and access, and citation. From this analysis it emerges that these repositories implement well consolidated practices and pragmatic solutions for literature repositories. These practices and solutions can not totally meet the needs of management and use of datasets resources, especially in a context where rapid technological changes continuously open new exploitation prospects. Â© 2016 by the authors.","Research data publishing is intended as the release of research data to make it possible for practitioners to (re)use them according to ""open science"" dynamics. There are three main actors called to deal with research data publishing practices: researchers, publishers, and data repositories. This study analyses the solutions offered by generalist scientific data repositories, , repositories supporting the deposition of any type of research data. These repositories cannot make any assumption on the application domain. They are actually called to face with the almost open ended typologies of data used in science. The current practices promoted by such repositories are analysed with respect to eight key aspects of data publishing, , dataset formatting, documentation, licensing, publication costs, validation, availability, discovery and access, and citation. From this analysis it emerges that these repositories implement well consolidated practices and pragmatic solutions for literature repositories. These practices and solutions can not totally meet the needs of management and use of datasets resources, especially in a context where rapid technological changes continuously open new exploitation prospects."
On the reuse of scientific data,"While science policy promotes data sharing and open data, these are not ends in themselves. Arguments for data sharing are to reproduce research, to make public assets available to the public, to leverage investments in research, and to advance research and innovation. To achieve these expected benefits of data sharing, data must actually be reused by others. Data sharing practices, especially motivations and incentives, have received far more study than has data reuse, perhaps because of the array of contested concepts on which reuse rests and the disparate contexts in which it occurs. Here we explicate concepts of data, sharing, and open data as a means to examine data reuse. We explore distinctions between use and reuse of data. Lastly we propose six research questions on data reuse worthy of pursuit by the community: How can uses of data be distinguished from reuses? When is reproducibility an essential goal? When is data integration an essential goal? What are the tradeoffs between collecting new data and reusing existing data? How do motivations for data collection influence the ability to reuse data? How do standards and formats for data release influence reuse opportunities? We conclude by summarizing the implications of these questions for science policy and for investments in data reuse.","While science policy promotes data sharing and open data, these are not ends in themselves. Arguments for data sharing are to reproduce research, to make public assets available to the public, to leverage investments in research, and to advance research and innovation. To achieve these expected benefits of data sharing, data must actually be reused by others. Data sharing practices, especially motivations and incentives, have received far more study than has data reuse, perhaps because of the array of contested concepts on which reuse rests and the disparate contexts in which it occurs. Here we explicate concepts of data, sharing, and open data as a means to examine data reuse. We explore distinctions between use and reuse of data. Lastly we propose six research questions on data reuse worthy of pursuit by the community: How can uses of data be distinguished from reuses? When is reproducibility an essential goal? When is data integration an essential goal? What are the tradeoffs between collecting new data and reusing existing data? How do motivations for data collection influence the ability to reuse data? How do standards and formats for data release influence reuse opportunities? We conclude by summarizing the implications of these questions for science policy and for investments in data reuse."
Design and implementation of a training course on big data use in water management,"Big Data has great potential to be applied to research in the field of geosciences. Motivated by the opportunity provided by the Data Integration and Analysis System (DIAS) of Japan, we organized an intensive two-week course that aims to educate participants on Big Data and its exploitation to solve water management problems. When developing and implementing the Program, we identified two main challenges: (1) assuring that the training has a lasting effect and (2) developing an interdisciplinary curriculum suitable for participants of diverse professional backgrounds. To address these challenges, we introduced several distinctive features. The Program was based on experiential learning â the participants were required to solve real problems and worked in international and multidisciplinary teams. The lectures were strictly relevant to the case-study problems. Significant time was devoted to hands-on exercises, and participants received immediate feedback on individual assignments to ensure skills development. Our evaluation of the two occasions of the Program in 2015 and 2016 indicates significant positive outcomes. The successful completion of the individual assignments confirmed that the participants gained key skills related to the usage of DIAS and other tools. The final solutions to the case-study problems showed that the participants were able to integrate and apply the obtained knowledge, indicating that the Programâs format and curriculum were effective. We found that participants used DIAS in subsequent studies and work, thus suggesting that the Program had long-lasting effects. Our experience indicates that despite time constraints, short courses can effectively encourage researchers and practitioners to explore opportunities provided by Big Data. Â© 2017 The Author(s).","Big Data has great potential to be applied to research in the field of geosciences. Motivated by the opportunity provided by the Data Integration and Analysis System (DIAS) of Japan, we organized an intensive two-week course that aims to educate participants on Big Data and its exploitation to solve water management problems. When developing and implementing the Program, we identified two main challenges: assuring that the training has a lasting effect and developing an interdisciplinary curriculum suitable for participants of diverse professional backgrounds. To address these challenges, we introduced several distinctive features. The Program was based on experiential learning the participants were required to solve real problems and worked in international and multidisciplinary teams. The lectures were strictly relevant to the case-study problems. Significant time was devoted to hands-on exercises, and participants received immediate feedback on individual assignments to ensure skills development. Our evaluation of the two occasions of the Program in 2015 and 2016 indicates significant positive outcomes. The successful completion of the individual assignments confirmed that the participants gained key skills related to the usage of DIAS and other tools. The final solutions to the case-study problems showed that the participants were able to integrate and apply the obtained knowledge, indicating that the Programs format and curriculum were effective. We found that participants used DIAS in subsequent studies and work, thus suggesting that the Program had long-lasting effects. Our experience indicates that despite time constraints, short courses can effectively encourage researchers and practitioners to explore opportunities provided by Big Data."
DataSHIELD - New directions and dimensions,"In disciplines such as biomedicine and social sciences, sharing and combining sensitive individual-level data is often prohibited by ethical-legal or governance constraints and other barriers such as the control of intellectual property or the huge sample sizes. DataSHIELD (Data Aggregation Through Anonymous Summary-statistics from Harmonised Individual-levEL Databases) is a distributed approach that allows the analysis of sensitive individual-level data from one study, and the co-analysis of such data from several studies simultaneously without physically pooling them or disclosing any data. Following initial proof of principle, a stable DataSHIELD platform has now been implemented in a number of epidemiological consortia. This paper reports three new applications of ÃataSHIELD including application to post-publication sensitive data analysis, text data analysis and privacy protected data visualisation. Expansion of DataSHIELD analytic functionality and application to additional data types demonstrate the broad applications of the software beyond biomedical sciences. Â© 2017 The Author(s).","In disciplines such as biomedicine and social sciences, sharing and combining sensitive individual-level data is often prohibited by ethical-legal or governance constraints and other barriers such as the control of intellectual property or the huge sample sizes. DataSHIELD (Data Aggregation Through Anonymous Summary-statistics from Harmonised Individual-levEL Databases) is a distributed approach that allows the analysis of sensitive individual-level data from one study, and the co-analysis of such data from several studies simultaneously without physically pooling them or disclosing any data. Following initial proof of principle, a stable DataSHIELD platform has now been implemented in a number of epidemiological consortia. This paper reports three new applications of ataSHIELD including application to post-publication sensitive data analysis, text data analysis and privacy protected data visualisation. Expansion of DataSHIELD analytic functionality and application to additional data types demonstrate the broad applications of the software beyond biomedical sciences."
Computing statistics from private data,"In several domains, privacy presents a significant obstacle to scientific and analytic research, and limits the economic, social, health and scholastic benefits that could be derived from such research. These concerns stem from the need for privacy about personally identifiable information (PII), commercial intellectual property, and other types of information. For example, businesses, researchers, and policymakers may benefit by analyzing aggregate information about markets, but individual companies may not be willing to reveal information about risks, strategies, and weaknesses that could be exploited by competitors. Extracting valuable utility from the new âbig dataâ economy demands new privacy technologies to overcome barriers that impede sensitive data from being aggregated and analyzed. Secure multiparty computation (MPC) is a collection of cryptographic technologies that can be used to effectively cope with some of these obstacles, and provide a new means of allowing researchers to coordinate and analyze sensitive data collections, obviating the need for data-owners to share the underlying data sets with other researchers or with each other. This paper outlines the findings that were made during interdisciplinary workshops that examined potential applications of MPC to data in the social and health sciences. The primary goals of this work are to describe the computational needs of these disciplines and to develop a specific roadmap for selecting efficient algorithms and protocols that can be used as a starting point for interdisciplinary projects between cryptographers and data scientists. Â© 2018 The Author(s).","In several domains, privacy presents a significant obstacle to scientific and analytic research, and limits the economic, social, health and scholastic benefits that could be derived from such research. These concerns stem from the need for privacy about personally identifiable information (PII), commercial intellectual property, and other types of information. For example, businesses, researchers, and policymakers may benefit by analyzing aggregate information about markets, but individual companies may not be willing to reveal information about risks, strategies, and weaknesses that could be exploited by competitors. Extracting valuable utility from the new big data economy demands new privacy technologies to overcome barriers that impede sensitive data from being aggregated and analyzed. Secure multiparty computation (MPC) is a collection of cryptographic technologies that can be used to effectively cope with some of these obstacles, and provide a new means of allowing researchers to coordinate and analyze sensitive data collections, obviating the need for data-owners to share the underlying data sets with other researchers or with each other. This paper outlines the findings that were made during interdisciplinary workshops that examined potential applications of MPC to data in the social and health sciences. The primary goals of this work are to describe the computational needs of these disciplines and to develop a specific roadmap for selecting efficient algorithms and protocols that can be used as a starting point for interdisciplinary projects between cryptographers and data scientists."
Retraction: A project-based case study of data science education (Data Science Journal (2016) 15: 5 DOI: 10.5334/dsj-2016-005),"It has come to the attention of the Data Science Journal Editors that the authors of the paper ""A Project-Based Case Study of Data Science Education"" (Turek, Suen and Clark, 2016) did not seek the necessary approval for research involving human subjects prior to conducting their study. In addition, they failed to obtain consent from research participants before publication. The article has therefore been retracted. In the interests of protecting the identity of the research participants, we have also withdrawn the contents of the article from the published record. Data Science Journal requires that all research involving human subjects is conducted in accordance with the Declaration of Helsinki (or equivalent framework) and, if appropriate, has been approved by the local institutional research ethics committee. Â© 2016 The Author(s).","It has come to the attention of the Data Science Journal Editors that the authors of the paper ""A Project-Based Case Study of Data Science Education"" (Turek, Suen and Clark, 2016) did not seek the necessary approval for research involving human subjects prior to conducting their study. In addition, they failed to obtain consent from research participants before publication. The article has therefore been retracted. In the interests of protecting the identity of the research participants, we have also withdrawn the contents of the article from the published record. Data Science Journal requires that all research involving human subjects is conducted in accordance with the Declaration of Helsinki (or equivalent framework) and, if appropriate, has been approved by the local institutional research ethics committee."
A FAIR-based approach to enhancing the discovery and re-use of transcriptomic data assets for nuclear receptor signaling pathways,"Public transcriptomic assets in the nuclear receptor (NR) signaling field hold considerable collective potential for exposing underappreciated aspects of NR regulation of gene expression. This potential is undermined however by a series of enduring informatic pain points that retard the routine re-use of these datasets. Here we describe a coordinated biocuration and web develop-ment approach to redress this situation that is closely aligned with ideals articulated in the FAIR (findable, accessible, interoperable, re-usable) principles on data stewardship. To improve findability, biocurators engage authors of studies in collaborating journals to secure datasets for deposition in public archives. Annotated derivatives of the archived datasets are assigned digital object identifiers and regulatory molecule identifiers that support persistent linkages between datasets and their associated research articles, integration in relevant records in gene and small molecule knowledgebases, and indexing by dataset search engines. To enhance their accessibility and interoperability, datasets are visualizable in responsively designed web pages, retrievable in machine-readable spreadsheets, or through an application programming interface. Re-use of the datasets is supported by their interrogation as a universe of data points through the Transcriptomine search engine, highlighting transcriptional intersections between NR signaling pathways, physiological processes and disease states. We illustrate the value of our approach in connecting disparate research communities using a use case of persistent interoperability between the Nuclear Receptor Signaling Atlas and the Pharmacogenomics Knowledgebase. Our FAIR-Aligned model demonstrates the enduring value of discovery-scale datasets that accrues from their systematic compilation, biocuration and distribution across the digital biomedical research enterprise.","Public transcriptomic assets in the nuclear receptor (NR) signaling field hold considerable collective potential for exposing underappreciated aspects of NR regulation of gene expression. This potential is undermined however by a series of enduring informatic pain points that retard the routine re-use of these datasets. Here we describe a coordinated biocuration and web develop-ment approach to redress this situation that is closely aligned with ideals articulated in the FAIR (findable, accessible, interoperable, re-usable) principles on data stewardship. To improve findability, biocurators engage authors of studies in collaborating journals to secure datasets for deposition in public archives. Annotated derivatives of the archived datasets are assigned digital object identifiers and regulatory molecule identifiers that support persistent linkages between datasets and their associated research articles, integration in relevant records in gene and small molecule knowledgebases, and indexing by dataset search engines. To enhance their accessibility and interoperability, datasets are visualizable in responsively designed web pages, retrievable in machine-readable spreadsheets, or through an application programming interface. Re-use of the datasets is supported by their interrogation as a universe of data points through the Transcriptomine search engine, highlighting transcriptional intersections between NR signaling pathways, physiological processes and disease states. We illustrate the value of our approach in connecting disparate research communities using a use case of persistent interoperability between the Nuclear Receptor Signaling Atlas and the Pharmacogenomics Knowledgebase. Our FAIR-Aligned model demonstrates the enduring value of discovery-scale datasets that accrues from their systematic compilation, biocuration and distribution across the digital biomedical research enterprise."
The Oklahoma Mesonet: A pilot study of environmental sensor data citations,"This pilot study of 110 scientific papers utilizing environmental sensor data from the Oklahoma Mesonet during its first two decades of operations demonstrates the diversity of potential purposes in scientific research for a robust, rigorously maintained, accessible source of environmental sensor data, as well as the challenges involved in identifying uses of that data within scientific papers. The study authors selected three publication years (1995, 2005, 2015) from an extensive corpus of peer-reviewed journal publications, identified each paperâs specific citation of and uses of the Mesonetâs environmental sensor data, and derived a typology of those usages (assimilation, experimentation, observation, simulation, utilization, validation) found to be most common. The rapid increase in data assimilation research projects today is discussed in terms of the difficulty and importance of correct attribution to individual data sources in these complex research projects. The study examines the possible role played by highly-cited papers that describe the quality assurance procedures in sensor data sources, which may serve as surrogates to signal the quality of the data provided by such sources, and which may also provide a useful contribution towards understanding data citation as a special form of scholarly citation. Â© 2017 The Author(s).","This pilot study of 110 scientific papers utilizing environmental sensor data from the Oklahoma Mesonet during its first two decades of operations demonstrates the diversity of potential purposes in scientific research for a robust, rigorously maintained, accessible source of environmental sensor data, as well as the challenges involved in identifying uses of that data within scientific papers. The study authors selected three publication years (1995, 2005, 2015) from an extensive corpus of peer-reviewed journal publications, identified each papers specific citation of and uses of the Mesonets environmental sensor data, and derived a typology of those usages (assimilation, experimentation, observation, simulation, utilization, validation) found to be most common. The rapid increase in data assimilation research projects today is discussed in terms of the difficulty and importance of correct attribution to individual data sources in these complex research projects. The study examines the possible role played by highly-cited papers that describe the quality assurance procedures in sensor data sources, which may serve as surrogates to signal the quality of the data provided by such sources, and which may also provide a useful contribution towards understanding data citation as a special form of scholarly citation."
A triad percolation method for detecting communities in social networks,"For the purpose of detecting communities in social networks, a triad percolation method is Â­proposed, which first locates all close-triads and open-triads from a social network, then a Â­specified close-triad or open-triad is selected as the seed to expand by utilizing the triad Â­percolation method, such that a community is found when this expanding process meet a Â­particular threshold. This approach can efficiently detect communities not only from a densely social network, but also from the sparsely one. Experimental results performing on real-world social benchmark networks and artificially simulated networks give a satisfactory Â­correspondence. Â© 2018 The Author(s).","For the purpose of detecting communities in social networks, a triad percolation method is proposed, which first locates all close-triads and open-triads from a social network, then a specified close-triad or open-triad is selected as the seed to expand by utilizing the triad percolation method, such that a community is found when this expanding process meet a particular threshold. This approach can efficiently detect communities not only from a densely social network, but also from the sparsely one. Experimental results performing on real-world social benchmark networks and artificially simulated networks give a satisfactory correspondence."
Persistence statements: Describing digital stickiness,"In this paper we present a draft vocabulary for making âpersistence statements.â These are simple tools for pragmatically addressing the concern that anyone feels upon experiencing a broken web link. Scholars increasingly use scientific and cultural assets in digital form, but choosing which among many objects to cite for the long term can be difficult. There are few well-defined terms to describe the various kinds and qualities of persistence that object repositories and identifier resolvers do or donât provide. Given an objectâs identifier, one should be able to query a provider to retrieve human- and machine-readable information to help judge the level of service to expect and help gauge whether the identifier is durable enough, as a sort of long-term bet, to include in a citation. The vocabulary should enable providers to articulate persistence policies and set user expectations. Â© 2017 The Author(s).","In this paper we present a draft vocabulary for making persistence statements. These are simple tools for pragmatically addressing the concern that anyone feels upon experiencing a broken web link. Scholars increasingly use scientific and cultural assets in digital form, but choosing which among many objects to cite for the long term can be difficult. There are few well-defined terms to describe the various kinds and qualities of persistence that object repositories and identifier resolvers do or dont provide. Given an objects identifier, one should be able to query a provider to retrieve human- and machine-readable information to help judge the level of service to expect and help gauge whether the identifier is durable enough, as a sort of long-term bet, to include in a citation. The vocabulary should enable providers to articulate persistence policies and set user expectations."
20 Years of Persistent Identifiers - Which Systems are Here to Stay?,"Web-based persistent identifiers have been around for more than 20 years, a period long enough for us to start observing patterns of success and failure. Persistent identifiers were invented to address challenges arising from the distributed and disorganised nature of the internet, which often resulted in URLs to internet endpoints becoming invalid. Over the years several different persistent identifier systems have been applied to the identification of research data, not all with the same level of success in terms of uptake and sustainability. We investigate the uptake of persistent identifier systems and discuss the factors that might determine the stability and longevity of these systems. Persistent identifiers have become essential elements of global research data infrastructures. Understanding the factors that influence the stability and longevity of persistent identifier systems will help us guide the future development of this important element of research data infrastructures and will make it easier to adapt to future technological and organisational changes.","Web-based persistent identifiers have been around for more than 20 years, a period long enough for us to start observing patterns of success and failure. Persistent identifiers were invented to address challenges arising from the distributed and disorganised nature of the internet, which often resulted in URLs to internet endpoints becoming invalid. Over the years several different persistent identifier systems have been applied to the identification of research data, not all with the same level of success in terms of uptake and sustainability. We investigate the uptake of persistent identifier systems and discuss the factors that might determine the stability and longevity of these systems. Persistent identifiers have become essential elements of global research data infrastructures. Understanding the factors that influence the stability and longevity of persistent identifier systems will help us guide the future development of this important element of research data infrastructures and will make it easier to adapt to future technological and organisational changes."
Fostering data openness by enabling science: A proposal for micro-funding,"In recent years, the promotion of data sharing has come with the recognition that not all scientists around the world are equally placed to partake in such activities. Notably, those within developing countries are sometimes regarded as experiencing hardware infrastructure challenges and data management skill shortages. Proposed remedies often focus on the provision of information and communication technology as well as enhanced data management training. Building on prior empirical social research undertaken in sub-Sahara Africa, this article provides a complementary but alternative proposal; namely, fostering data openness by enabling research. Towards this end, the underlying rationale is outlined for a âbottom-upâ system of research support that addresses the day-to-day demands in low-resourced environments. This approach draws on lessons from development financial assistance programs in recent decades. In doing so, this article provides an initial framework for science funding that call for holding together concerns for ensuring research can be undertaken in low-resourced laboratory environments with concerns about the data generated in such settings can be shared. Â© 2017 The Author(s).","In recent years, the promotion of data sharing has come with the recognition that not all scientists around the world are equally placed to partake in such activities. Notably, those within developing countries are sometimes regarded as experiencing hardware infrastructure challenges and data management skill shortages. Proposed remedies often focus on the provision of information and communication technology as well as enhanced data management training. Building on prior empirical social research undertaken in sub-Sahara Africa, this article provides a complementary but alternative proposal; namely, fostering data openness by enabling research. Towards this end, the underlying rationale is outlined for a bottom-up system of research support that addresses the day-to-day demands in low-resourced environments. This approach draws on lessons from development financial assistance programs in recent decades. In doing so, this article provides an initial framework for science funding that call for holding together concerns for ensuring research can be undertaken in low-resourced laboratory environments with concerns about the data generated in such settings can be shared."
Statistical inference in missing data by MCMC and non-MCMC multiple imputation algorithms: Assessing the effects of between-imputation iterations,"Incomplete data are ubiquitous in social sciences; as a consequence, available data are inefficient (ineffective) and often biased. In the literature, multiple imputation is known to be the standard method to handle missing data. While the theory of multiple imputation has been known for decades, the implementation is difficult due to the complicated nature of random draws from the posterior distribution. Thus, there are several computational algorithms in software: Data Augmentation (DA), Fully Conditional Specification (FCS), and Expectation-Maximization with Bootstrapping (EMB). Although the literature is full of comparisons between joint modeling (DA, EMB) and conditional modeling (FCS), little is known about the relative superiority between the MCMC algorithms (DA, FCS) and the non-MCMC algorithm (EMB), where MCMC stands for Markov chain Monte Carlo. Based on simulation experiments, the current study contends that EMB is a confidence proper (confidence-supporting) multiple imputation algorithm without between-imputation iterations; thus, EMB is more user-friendly than DA and FCS. Â© 2017 The Author(s).","Incomplete data are ubiquitous in social sciences; as a consequence, available data are inefficient (ineffective) and often biased. In the literature, multiple imputation is known to be the standard method to handle missing data. While the theory of multiple imputation has been known for decades, the implementation is difficult due to the complicated nature of random draws from the posterior distribution. Thus, there are several computational algorithms in software: Data Augmentation (DA), Fully Conditional Specification (FCS), and Expectation-Maximization with Bootstrapping (EMB). Although the literature is full of comparisons between joint modeling (DA, EMB) and conditional modeling (FCS), little is known about the relative superiority between the MCMC algorithms (DA, FCS) and the non-MCMC algorithm (EMB), where MCMC stands for Markov chain Monte Carlo. Based on simulation experiments, the current study contends that EMB is a confidence proper (confidence-supporting) multiple imputation algorithm without between-imputation iterations; thus, EMB is more user-friendly than DA and FCS."
Research data management in research institutions in Zimbabwe,"The research was aimed at evaluating how research data are being managed in research institutions in Zimbabwe. The study also sought to assess the challenges that are faced in research data management by research institutions in Zimbabwe. Twenty five institutions of higher learning and other organisations that deal with research were selected using purposive sampling to participate in the study. An online questionnaire on SurveyMonkey was sent to the selected participants and telephone interviews were done to follow up on participants who failed to respond on time. Data that were collected using interviews were entered manually into SurveyMonkey for easy analysis. It was found out that proper research data management is not being done. Researchers were managing their own research data. Most of the research data were in textual and spreadsheet format. Graphical, audio, video, database, structured text formats and software applications research data were also available. Lack of guidelines on good practice, inadequate human resources, technological obsolescence, insecure infrastructure, use of different vocabulary between librarians and researchers, inadequate financial resources, absence of research data management policies and lack of support by institutional authorities and researchers negatively impacted on research data management. Authors recommend the establishment of research data repositories and use of existing research data repositories that are registered with the Registry of Research Data Repositories to ensure that research data standards are adhered to when doing research. Â© 2017 The Author(s).","The research was aimed at evaluating how research data are being managed in research institutions in Zimbabwe. The study also sought to assess the challenges that are faced in research data management by research institutions in Zimbabwe. Twenty five institutions of higher learning and other organisations that deal with research were selected using purposive sampling to participate in the study. An online questionnaire on SurveyMonkey was sent to the selected participants and telephone interviews were done to follow up on participants who failed to respond on time. Data that were collected using interviews were entered manually into SurveyMonkey for easy analysis. It was found out that proper research data management is not being done. Researchers were managing their own research data. Most of the research data were in textual and spreadsheet format. Graphical, audio, video, database, structured text formats and software applications research data were also available. Lack of guidelines on good practice, inadequate human resources, technological obsolescence, insecure infrastructure, use of different vocabulary between librarians and researchers, inadequate financial resources, absence of research data management policies and lack of support by institutional authorities and researchers negatively impacted on research data management. Authors recommend the establishment of research data repositories and use of existing research data repositories that are registered with the Registry of Research Data Repositories to ensure that research data standards are adhered to when doing research."
The northern voice: Listening to indigenous and northern perspectives on management of data in Canada,"The Canadian Cryospheric Information Network and Polar Data Catalogue (CCIN/PDC) provide: (1) a trusted archive to store data from Canadian cryospheric research and (2) a public access portal to this information. The CCIN/PDC has since expanded its collection to include data from health, ecological, social, and other sciences. Since its inception, CCIN/PDC has engaged Indigenous and northern Canadians to understand and meet their information needs. This paper describes three instances of such engagement and next steps for enhanced interaction and support. First, feedback from northern and Indigenous communities led to the development of PDC Lite. Compared to the full-featured online PDC Search application, PDC Lite accommodates slower Internet speeds and allows one to search by particular northern communities. PDC Lite continues to be improved by input from the people that it serves. Next, to facilitate discussion and strengthen collaborative relationships within the polar data community, CCIN/PDC co-hosted two major meetings in 2015. Emerging from both these events was a need to prioritize what has been termed human interoperability and the need to have Indigenous and northern community involvement at all levels of data management. Future plans for CCIN/PDC include more effective partnerships in which we work with and listen to northern and Indigenous Canadians to better understand their requirements for data management services and expertise. Our ultimate goal is to provide, through collaboration with partners, data, information, and expertise that facilitate northern and Indigenous Canadiansâ access to publicly-archived data and enable and support management of their own data and resources. Â© 2017 The Author(s).","The Canadian Cryospheric Information Network and Polar Data Catalogue (CCIN/PDC) provide: a trusted archive to store data from Canadian cryospheric research and a public access portal to this information. The CCIN/PDC has since expanded its collection to include data from health, ecological, social, and other sciences. Since its inception, CCIN/PDC has engaged Indigenous and northern Canadians to understand and meet their information needs. This paper describes three instances of such engagement and next steps for enhanced interaction and support. First, feedback from northern and Indigenous communities led to the development of PDC Lite. Compared to the full-featured online PDC Search application, PDC Lite accommodates slower Internet speeds and allows one to search by particular northern communities. PDC Lite continues to be improved by input from the people that it serves. Next, to facilitate discussion and strengthen collaborative relationships within the polar data community, CCIN/PDC co-hosted two major meetings in 2015. Emerging from both these events was a need to prioritize what has been termed human interoperability and the need to have Indigenous and northern community involvement at all levels of data management. Future plans for CCIN/PDC include more effective partnerships in which we work with and listen to northern and Indigenous Canadians to better understand their requirements for data management services and expertise. Our ultimate goal is to provide, through collaboration with partners, data, information, and expertise that facilitate northern and Indigenous Canadians access to publicly-archived data and enable and support management of their own data and resources."
All or nothing: The false promise of anonymity,"In early 2016, the International Committee of Medical Journal Editors (ICMJE) proposed that responsible sharing of de-identified individual-level data be required for clinical trials published in their affiliated journals. There would be a delay in implementing this policy to allow for the necessary informed consents to work their way through ethical review. Meanwhile, some researchers and policy makers have conflated the notions of de-identification and anonymity. The former is a process that seeks to mitigate disclosure risk though careful application of rules and statistical analysis, while the latter is an absolute state. The consequence of confusing the process and the state is profound. Extensions to the ICMJE proposal based on the presumed anonymity of data include: sharing unconsented data; sharing data without managing access, as Open Data; and proposals to sell data. This essay aims to show that anonymity (the state) cannot be guaranteed by de-identification (the process), and so these extensions to the ICMJE proposal should be rejected on governance grounds, if no other. This is not as negative a po-i tion as it might seem, as other disciplines have been aware of these limitations and concomitant responsibilities for many years. The essay concludes with an example from social science of managed access strategies that could be adopted by the medical field. Â© 2017 The Author(s).","In early 2016, the International Committee of Medical Journal Editors (ICMJE) proposed that responsible sharing of de-identified individual-level data be required for clinical trials published in their affiliated journals. There would be a delay in implementing this policy to allow for the necessary informed consents to work their way through ethical review. Meanwhile, some researchers and policy makers have conflated the notions of de-identification and anonymity. The former is a process that seeks to mitigate disclosure risk though careful application of rules and statistical analysis, while the latter is an absolute state. The consequence of confusing the process and the state is profound. Extensions to the ICMJE proposal based on the presumed anonymity of data include: sharing unconsented data; sharing data without managing access, as Open Data; and proposals to sell data. This essay aims to show that anonymity (the state) cannot be guaranteed by de-identification (the process), and so these extensions to the ICMJE proposal should be rejected on governance grounds, if no other. This is not as negative a po-i tion as it might seem, as other disciplines have been aware of these limitations and concomitant responsibilities for many years. The essay concludes with an example from social science of managed access strategies that could be adopted by the medical field."
CMIP6 data citation of evolving data,"Data citations have become widely accepted. Technical infrastructures as well as principles and recommendations for data citation are in place but best practices or guidelines for their implementation are not yet available. On the other hand, the scientific climate community requests early citations on evolving data for credit, e.g. for CMIP6 (Coupled Model Intercomparison Project Phase 6). The data citation concept for CMIP6 is presented. The main challenges lie in limited resources, a strict project timeline and the dependency on changes of the data dissemination infrastructure ESGF (Earth System Grid Federation) to meet the data citation requirements. Therefore a pragmatic, flexible and extendible approach for the CMIP6 data citation service was developed, consisting of a citation for the full evolving data superset and a data cart approach for citing the concrete used data subset. This two citation approach can be implemented according to the RDA recommendations for evolving data. Because of resource constraints and missing project policies, the implementation of the second part of the citation concept is postponed to CMIP7. Â© 2017 The Author(s).","Data citations have become widely accepted. Technical infrastructures as well as principles and recommendations for data citation are in place but best practices or guidelines for their implementation are not yet available. On the other hand, the scientific climate community requests early citations on evolving data for credit, for CMIP6 (Coupled Model Intercomparison Project Phase 6). The data citation concept for CMIP6 is presented. The main challenges lie in limited resources, a strict project timeline and the dependency on changes of the data dissemination infrastructure ESGF (Earth System Grid Federation) to meet the data citation requirements. Therefore a pragmatic, flexible and extendible approach for the CMIP6 data citation service was developed, consisting of a citation for the full evolving data superset and a data cart approach for citing the concrete used data subset. This two citation approach can be implemented according to the RDA recommendations for evolving data. Because of resource constraints and missing project policies, the implementation of the second part of the citation concept is postponed to CMIP7."
LABEX L-IPSL Arctic metadata Portal,"The Institut Pierre Simon Laplace (IPSL) encompasses a wide diversity of projects that focus on the Arctic. From these observations the IPSL has generated a large number of datasets gathering Arctic observations. These observations include measurements on atmospheric chemical composition, snow micro-physical properties or ocean measurements. However, some of these datasets remain locally stored and there is a lack of public awareness regarding these resources, which has hindered their visualisation and sharing. This motivated the creation of the LABEX L-IPSL Arctic metadata Portal (http://climserv.ipsl.polytechnique.fr/arcticportal/), presented here, which improves the visibility of the variety of observations collected within the institute as well as the evaluation of numerical models. The LABEX L-IPSL Arctic metadata Portal will also promote new avenues in Arctic research within the IPSL and with other collaborating institutions. Â© 2016 by the authors.","The Institut Pierre Simon Laplace (IPSL) encompasses a wide diversity of projects that focus on the Arctic. From these observations the IPSL has generated a large number of datasets gathering Arctic observations. These observations include measurements on atmospheric chemical composition, snow micro-physical properties or ocean measurements. However, some of these datasets remain locally stored and there is a lack of public awareness regarding these resources, which has hindered their visualisation and sharing. This motivated the creation of the LABEX L-IPSL Arctic metadata Portal (http://climserv.ipsl.polytechnique.fr/arcticportal/), presented here, which improves the visibility of the variety of observations collected within the institute as well as the evaluation of numerical models. The LABEX L-IPSL Arctic metadata Portal will also promote new avenues in Arctic research within the IPSL and with other collaborating institutions."
Big data and insurance: Advantageous selection in european markets,"Rothschild and Stiglitz (1976) argued that people signal their risk profile through their insurance demand, i.e. individuals with a high risk profile would buy insurance as much as they can, while people who are not going to buy any insurance are the ones with a lower risk profile. This issue is commonly known as adverse selection. Even if their prediction seems to work quite well in a lot of different markets, Cutler et al. (2008) proved that there exist some insurance markets in United States in which the expected result is completely different. In the wake of this study, we provide empirical evidences that there are some European insurance markets in which the low risk profile agents are the ones who buy more insurance. Â© 2017 The Author(s).","Rothschild and Stiglitz argued that people signal their risk profile through their insurance demand, individuals with a high risk profile would buy insurance as much as they can, while people who are not going to buy any insurance are the ones with a lower risk profile. This issue is commonly known as adverse selection. Even if their prediction seems to work quite well in a lot of different markets, Cutler et al. proved that there exist some insurance markets in United States in which the expected result is completely different. In the wake of this study, we provide empirical evidences that there are some European insurance markets in which the low risk profile agents are the ones who buy more insurance."
A project-based case study of data science education,"The discipline of data science has emerged over the past decade as a convergence of high-power computing, data visualization and analysis, and data-driven application domains. Prominent research institutions and private sector industry have embraced data science, but foundations for effective tertiary-level data science education remain absent. This is nothing new, however, as the university has an established tradition of developing its educational mission hand-in-hand with the development of novel methods for human understanding (Feingold, 1991). Thus, it is natural that universities ""figure out"" data science concurrent with the development of needed pedagogy. We consider data science education with respect to recent trends in interdisciplinary and experiential educational methodologies. The first iteration of the Berkeley Institute for Data Science (BIDS) Collaborative, which took place at the University of California, Berkeley in the Spring of 2015, is used as a case study. From this, we draw lessons learned regarding the necessary components of effective tertiary data science education, which range from a complete end-to-end workflow, technological tools for development and team communications, and appropriate motivation and incentives. Our findings will be tested and revised in subsequent iterations of the BIDS Collaborative as we continue our study of data science education, research, and social impact. Â© 2016 by the authors.","The discipline of data science has emerged over the past decade as a convergence of high-power computing, data visualization and analysis, and data-driven application domains. Prominent research institutions and private sector industry have embraced data science, but foundations for effective tertiary-level data science education remain absent. This is nothing new, however, as the university has an established tradition of developing its educational mission hand-in-hand with the development of novel methods for human understanding (Feingold, 1991). Thus, it is natural that universities ""figure out"" data science concurrent with the development of needed pedagogy. We consider data science education with respect to recent trends in interdisciplinary and experiential educational methodologies. The first iteration of the Berkeley Institute for Data Science (BIDS) Collaborative, which took place at the University of California, Berkeley in the Spring of 2015, is used as a case study. From this, we draw lessons learned regarding the necessary components of effective tertiary data science education, which range from a complete end-to-end workflow, technological tools for development and team communications, and appropriate motivation and incentives. Our findings will be tested and revised in subsequent iterations of the BIDS Collaborative as we continue our study of data science education, research, and social impact."
Weather forecasts for pastoralism in a changing climate: Navigating the data space in North Eastern Uganda,"Efforts to support the building of resilient pastoralism have been stepped up in Uganda through a number of activities. One of the activity is the provision of seasonal and medium-range climate forecasts to enable decisions concerning livestock herding. Seasonal weather forecasts are critical but there are challenges of timeliness and usability of the forecasts. The challenges are associated with the multiplicity of information sources, methods for data integration and dissemination channels. Institutions including public and Civil Society Organizations usually invest in collecting weather and other data which should be accessible. Often times this data remains hoarded necessitating other organizations to collect similar data. The inter-institutional relations notwithstanding, the lack of data sharing leads to minimal data available for open access. This paper illustrates that this challenge can be addressed by using combined multiple methods to elicit data on weather and other biophysical conditions for pastoralism in Karamoja. In this paper we additionally analyse the opportunities and challenges of using multiple sources of pastoral-relevant data to couple with weather information in support of herding decisions. Building resilient pastoralism that utilizes pasture and water availability will have to utilize available data. It is evident that more robust approaches for data sharing at global, regional and local levels are needed to understand how pastoralists can respond to climate shocks and changes. The paper illustrates the use of a multifaceted-methods approach including open data to develop climate forecast information for risk-reduction oriented information for decision-making. Integration of this data provides insights on how pastoralists have long adapted to a variable and changing climate, the methods and processes of adaptation to losses and damages from the climate shocks. Â© 2017 The Author(s).","Efforts to support the building of resilient pastoralism have been stepped up in Uganda through a number of activities. One of the activity is the provision of seasonal and medium-range climate forecasts to enable decisions concerning livestock herding. Seasonal weather forecasts are critical but there are challenges of timeliness and usability of the forecasts. The challenges are associated with the multiplicity of information sources, methods for data integration and dissemination channels. Institutions including public and Civil Society Organizations usually invest in collecting weather and other data which should be accessible. Often times this data remains hoarded necessitating other organizations to collect similar data. The inter-institutional relations notwithstanding, the lack of data sharing leads to minimal data available for open access. This paper illustrates that this challenge can be addressed by using combined multiple methods to elicit data on weather and other biophysical conditions for pastoralism in Karamoja. In this paper we additionally analyse the opportunities and challenges of using multiple sources of pastoral-relevant data to couple with weather information in support of herding decisions. Building resilient pastoralism that utilizes pasture and water availability will have to utilize available data. It is evident that more robust approaches for data sharing at global, regional and local levels are needed to understand how pastoralists can respond to climate shocks and changes. The paper illustrates the use of a multifaceted-methods approach including open data to develop climate forecast information for risk-reduction oriented information for decision-making. Integration of this data provides insights on how pastoralists have long adapted to a variable and changing climate, the methods and processes of adaptation to losses and damages from the climate shocks."
NASA EOSDIS data identifiers: Approach and system,"NASA's Earth Science Data and Information System (ESDIS) Project began investigating the use of Digital Object Identifiers (DOIs) in 2010 with the goal of assigning DOIs to various data products. These Earth science research data products produced using Earth observations and models are archived and distributed by twelve Distributed Active Archive Centers (DAACs) located across the United States. Each data center serves a different Earth science discipline user community and, accordingly, has a unique approach and process for generating and archiving a variety of data products. These varied approaches present a challenge for developing a DOI solution. To address this challenge, the ESDIS Project has developed processes, guidelines, and several models for creating and assigning DOIs. Initially the DOI assignment and registration process was started as a prototype but now it is fully operational. In February 2012, the ESDIS Project started using the California Digital Library (CDL) EZID for registering DOIs. The DOI assignments were initially labor-intensive. The system is now automated, and the assignments are progressing rapidly. As of February 28, 2017, over 50% of the data products at the DAACs had been assigned DOIs. Citations using the DOIs increased from about 100 to over 370 between 2015 and 2016. Â© 2017 The Author(s).","NASA's Earth Science Data and Information System (ESDIS) Project began investigating the use of Digital Object Identifiers (DOIs) in 2010 with the goal of assigning DOIs to various data products. These Earth science research data products produced using Earth observations and models are archived and distributed by twelve Distributed Active Archive Centers (DAACs) located across the United States. Each data center serves a different Earth science discipline user community and, accordingly, has a unique approach and process for generating and archiving a variety of data products. These varied approaches present a challenge for developing a DOI solution. To address this challenge, the ESDIS Project has developed processes, guidelines, and several models for creating and assigning DOIs. Initially the DOI assignment and registration process was started as a prototype but now it is fully operational. In February 2012, the ESDIS Project started using the California Digital Library EZID for registering DOIs. The DOI assignments were initially labor-intensive. The system is now automated, and the assignments are progressing rapidly. As of February 28, 2017, over 50% of the data products at the DAACs had been assigned DOIs. Citations using the DOIs increased from about 100 to over 370 between 2015 and 2016."
Understanding perspectives on sharing neutron data at oak ridge national laboratory,"Even though the importance of sharing data is frequently discussed, data sharing appears to be limited to a few fields, and practices within those fields are not well understood. This study examines perspectives on sharing neutron data collected at Oak Ridge National Laboratoryâs neutron sources. Operation at user facilities has traditionally focused on making data accessible to those who create them. The recent emphasis on open data is shifting the focus to ensure that the data produced are reusable by others. This mixed methods research study included a series of surveys and focus group interviews in which 13 data consumers, data managers, and data producers answered questions about their perspectives on sharing neutron data. Data consumers reported interest in reusing neutron data for comparison/verification of results against their own measurements and testing new theories using existing data. They also stressed the importance of establishing context for data, including how data are produced, how samples are prepared, units of measurement, and how temperatures are determined. Data managers expressed reservations about reusing othersâ data because they were not always sure if they could trust whether the people responsible for interpreting data did so correctly. Data producers described concerns about their data being misused, competing with other users, and over-reliance on data producers to understand data. We present the Consumers Managers Producers (CMP) Model for understanding the interplay of each group regarding data sharing. We conclude with policy and system recommendations and discuss directions for future research. Â© 2017 The Author(s).","Even though the importance of sharing data is frequently discussed, data sharing appears to be limited to a few fields, and practices within those fields are not well understood. This study examines perspectives on sharing neutron data collected at Oak Ridge National Laboratorys neutron sources. Operation at user facilities has traditionally focused on making data accessible to those who create them. The recent emphasis on open data is shifting the focus to ensure that the data produced are reusable by others. This mixed methods research study included a series of surveys and focus group interviews in which 13 data consumers, data managers, and data producers answered questions about their perspectives on sharing neutron data. Data consumers reported interest in reusing neutron data for comparison/verification of results against their own measurements and testing new theories using existing data. They also stressed the importance of establishing context for data, including how data are produced, how samples are prepared, units of measurement, and how temperatures are determined. Data managers expressed reservations about reusing others data because they were not always sure if they could trust whether the people responsible for interpreting data did so correctly. Data producers described concerns about their data being misused, competing with other users, and over-reliance on data producers to understand data. We present the Consumers Managers Producers (CMP) Model for understanding the interplay of each group regarding data sharing. We conclude with policy and system recommendations and discuss directions for future research."
Distributed persistent identifiers system design,"The need to identify both digital and physical objects is ubiquitous in our society. Past and present persistent identifier (PID) systems, of which there is a great variety in terms of technical and social implementation, have evolved with the advent of the Internet, which has allowed for globally unique and globally resolvable identifiers. PID systems have, by in large, catered for identifier uniqueness, integrity, and persistence, regardless of the identifierâs application domain. Trustworthiness of these systems has been measured by the criteria first defined by BÃ¼tikofer (2009) and further elaborated by Golodoniuc et al. (2016) and Car et al. (2017). Since many PID systems have been largely conceived and developed by a single organisation they faced challenges for widespread adoption and, most importantly, the ability to survive change of technology. We believe that a cause of PID systems that were once successful fading away is the centralisation of support infrastructure â both organisational and computing and data storage systems. In this paper, we propose a PID system design that implements the pillars of a trustworthy system â ensuring identifiersâ independence of any particular technology or organisation, implementation of core PID system functions, separation from data delivery, and enabling the system to adapt for future change. We propose decentralisation at all levels â persistent identifiers and information objects registration, resolution, and data delivery â using Distributed Hash Tables and traditional peer-to-peer networks with information replication and caching mechanisms, thus eliminating the need for a central PID data store. This will increase overall system fault tolerance thus ensuring its trustworthiness. We also discuss important aspects of the distributed systemâs governance, such as the notion of the authoritative source and data integrity. Â© 2017 The Author(s).","The need to identify both digital and physical objects is ubiquitous in our society. Past and present persistent identifier (PID) systems, of which there is a great variety in terms of technical and social implementation, have evolved with the advent of the Internet, which has allowed for globally unique and globally resolvable identifiers. PID systems have, by in large, catered for identifier uniqueness, integrity, and persistence, regardless of the identifiers application domain. Trustworthiness of these systems has been measured by the criteria first defined by Btikofer and further elaborated by Golodoniuc et al. and Car et al. . Since many PID systems have been largely conceived and developed by a single organisation they faced challenges for widespread adoption and, most importantly, the ability to survive change of technology. We believe that a cause of PID systems that were once successful fading away is the centralisation of support infrastructure both organisational and computing and data storage systems. In this paper, we propose a PID system design that implements the pillars of a trustworthy system ensuring identifiers independence of any particular technology or organisation, implementation of core PID system functions, separation from data delivery, and enabling the system to adapt for future change. We propose decentralisation at all levels persistent identifiers and information objects registration, resolution, and data delivery using Distributed Hash Tables and traditional peer-to-peer networks with information replication and caching mechanisms, thus eliminating the need for a central PID data store. This will increase overall system fault tolerance thus ensuring its trustworthiness. We also discuss important aspects of the distributed systems governance, such as the notion of the authoritative source and data integrity."
Crystallography and databases,"Crystallographic databases have existed as electronic resources for over 50 years, and have provided comprehensive archives of crystal structures of inorganic, organic, metalâorganic and biological macromolecular compounds of immense value to a wide range of structural sciences. They thus serve a variety of scientific disciplines, but are all driven by considerations of accuracy, precise characterization, and potential for search, analysis and reuse. They also serve a variety of end-users in academia and industry, and have evolved through different funding and licensing models. The diversity of their operational mechanisms combined with their undisputed value as scientific research tools gives rise to a rich ecosystem. A session at SciDataCon2016 gave an overview of the largest extant crystallographic databases and their current activities and plans for the future. This review summarizes these presentations and considers them alongside other players in the field, demonstrating their variety, versatility and focus on quality and usefulness. Â© 2017 The Author(s).","Crystallographic databases have existed as electronic resources for over 50 years, and have provided comprehensive archives of crystal structures of inorganic, organic, metalorganic and biological macromolecular compounds of immense value to a wide range of structural sciences. They thus serve a variety of scientific disciplines, but are all driven by considerations of accuracy, precise characterization, and potential for search, analysis and reuse. They also serve a variety of end-users in academia and industry, and have evolved through different funding and licensing models. The diversity of their operational mechanisms combined with their undisputed value as scientific research tools gives rise to a rich ecosystem. A session at SciDataCon2016 gave an overview of the largest extant crystallographic databases and their current activities and plans for the future. This review summarizes these presentations and considers them alongside other players in the field, demonstrating their variety, versatility and focus on quality and usefulness."
Persistent identifier practice for big data management at NCI,"The National Computational Infrastructure (NCI) manages over 10 PB research data, which is co-located with the high performance computer (Raijin) and an HPC class 3000 core OpenStack cloud system (Tenjin). In support of this integrated High Performance Computing/High Performance Data (HPC/HPD) infrastructure, NCIâs data management practices includes building catalogues, DOI minting, data curation, data publishing, and data delivery through a variety of data services. The metadata catalogues, DOIs, THREDDS, and Vocabularies, all use different Uniform Resource Locator (URL) styles. A Persistent IDentifier (PID) service provides an important utility to manage URLs in a consistent, controlled and monitored manner to support the robustness of our national âBig Dataâ infrastructure. In this paper we demonstrate NCIâs approach of utilising the NCIâs PID Service to consistently manage its persistent identifiers with various applications. Â© 2017 The Author(s).","The National Computational Infrastructure (NCI) manages over 10 PB research data, which is co-located with the high performance computer (Raijin) and an HPC class 3000 core OpenStack cloud system (Tenjin). In support of this integrated High Performance Computing/High Performance Data (HPC/HPD) infrastructure, NCIs data management practices includes building catalogues, DOI minting, data curation, data publishing, and data delivery through a variety of data services. The metadata catalogues, DOIs, THREDDS, and Vocabularies, all use different Uniform Resource Locator (URL) styles. A Persistent IDentifier (PID) service provides an important utility to manage URLs in a consistent, controlled and monitored manner to support the robustness of our national Big Data infrastructure. In this paper we demonstrate NCIs approach of utilising the NCIs PID Service to consistently manage its persistent identifiers with various applications."
"Building a disciplinary, world-wide data infrastructure","Sharing scientific data with the objective of making it discoverable, accessible, reusable, and interoperable requires work and presents challenges being faced at the disciplinary level to define in particular how the data should be formatted and described. This paper represents the Proceedings of a session held at SciDataCon 2016 (Denver, 12-13 September 2016). It explores the way a range of disciplines, namely materials science, crystallography, astronomy, earth sciences, humanities and linguistics, get organized at the international level to address those challenges. The disciplinary culture with respect to data sharing, science drivers, organization, lessons learnt and the elements of the data infrastructure which are or could be shared with others are briefly described. Commonalities and differences are assessed. Common key elements for success are identified: data sharing should be science driven; defining the disciplinary part of the interdisciplinary standards is mandatory but challenging; sharing of applications should accompany data sharing. Incentives such as journal and funding agency requirements are also similar. For all, social aspects are more challenging than technological ones. Governance is more diverse, often specific to the discipline organization. Being problem-driven is also a key factor of success for building bridges to enable interdisciplinary research. Several international data organizations such as CODATA, RDA and WDS can facilitate the establishment of disciplinary interoperability frameworks. As a spin-off of the session, a RDA Disciplinary Interoperability Interest Group is proposed to bring together representatives across disciplines to better organize and drive the discussion for prioritizing, harmonizing and efficiently articulating disciplinary needs. Â© 2017 The Author(s).","Sharing scientific data with the objective of making it discoverable, accessible, reusable, and interoperable requires work and presents challenges being faced at the disciplinary level to define in particular how the data should be formatted and described. This paper represents the Proceedings of a session held at SciDataCon 2016 (Denver, 12-13 September 2016). It explores the way a range of disciplines, namely materials science, crystallography, astronomy, earth sciences, humanities and linguistics, get organized at the international level to address those challenges. The disciplinary culture with respect to data sharing, science drivers, organization, lessons learnt and the elements of the data infrastructure which are or could be shared with others are briefly described. Commonalities and differences are assessed. Common key elements for success are identified: data sharing should be science driven; defining the disciplinary part of the interdisciplinary standards is mandatory but challenging; sharing of applications should accompany data sharing. Incentives such as journal and funding agency requirements are also similar. For all, social aspects are more challenging than technological ones. Governance is more diverse, often specific to the discipline organization. Being problem-driven is also a key factor of success for building bridges to enable interdisciplinary research. Several international data organizations such as CODATA, RDA and WDS can facilitate the establishment of disciplinary interoperability frameworks. As a spin-off of the session, a RDA Disciplinary Interoperability Interest Group is proposed to bring together representatives across disciplines to better organize and drive the discussion for prioritizing, harmonizing and efficiently articulating disciplinary needs."
Frequent itemset mining for big data using greatest common divisor technique,The discovery of frequent itemsets is one of the very important topics in data mining. Frequent itemset discovery techniques help in generating qualitative knowledge which gives business insight and helps the decision makers. In the Big Data era the need for a customizable algorithm to work with big data sets in a reasonable time becomes a necessity. In this paper we propose a new algorithm for frequent itemset discovery that could work in distributed manner with big datasets. Our approach is based on the original Buddy Prima algorithm and the Greatest Common Divisor (GCD) calculation between itemsets which exist in the transaction database. The proposed algorithm introduces a new method to parallelize the frequent itemset mining without the need to generate candidate itemsets and also it avoids any communication over-head between the participated nodes. It explores the parallelism abilities in the hardware in case of single node operation. The proposed approach could be implemented using map-reduce technique or Spark. It was successfully applied on different size transactions DBs and compared with two well-known algorithms: FP-Growth and Parallel Apriori with different support levels. The experiments showed that the proposed algorithm achieves major time improvement over both algorithms especially with datasets having huge number of items. Â© 2017 The Author(s).,The discovery of frequent itemsets is one of the very important topics in data mining. Frequent itemset discovery techniques help in generating qualitative knowledge which gives business insight and helps the decision makers. In the Big Data era the need for a customizable algorithm to work with big data sets in a reasonable time becomes a necessity. In this paper we propose a new algorithm for frequent itemset discovery that could work in distributed manner with big datasets. Our approach is based on the original Buddy Prima algorithm and the Greatest Common Divisor (GCD) calculation between itemsets which exist in the transaction database. The proposed algorithm introduces a new method to parallelize the frequent itemset mining without the need to generate candidate itemsets and also it avoids any communication over-head between the participated nodes. It explores the parallelism abilities in the hardware in case of single node operation. The proposed approach could be implemented using map-reduce technique or Spark. It was successfully applied on different size transactions DBs and compared with two well-known algorithms: FP-Growth and Parallel Apriori with different support levels. The experiments showed that the proposed algorithm achieves major time improvement over both algorithms especially with datasets having huge number of items.
Technology transfer and true transformation: Implications for open data,"When considering the âopennessâ of data it is unsurprising that most conversations focus on the online environment - how data is collated, moved and recombined for multiple purposes. Nonetheless, it is important to recognize that the movements online are only part of the data lifecycle. Indeed, considering where and how data are created - namely, the research setting - are of key importance to Open Data initiatives. In particular, such insights offer key understandings of how and why scientists engage with in practices of openness, and how data transitions from personal control to public ownership. This paper examines research settings in low/middle-income countries (LMIC) to better understand how resource limitations influence Open Data buy-in. Using empirical fieldwork in Kenyan and South African laboratories it draws attention to some key issues currently overlooked in Open Data discussions. First, that many of the hesitations raised by the scientists about sharing data were as much tied to the speed of their research as to any other factor. Thus, it would seem that the longer it takes for individual scientists to create data, the more hesitant they are about sharing it. Second, that the pace of research is a multifaceted bind involving many different challenges relating to laboratory equipment and infrastructure. Indeed, it is unlikely that one single solution (such as equipment donation) will ameliorate these âbinds of paceâ. Third, that these âbinds of paceâ were used by the scientists to construct ânarratives of exclusionâ through which they remove themselves from responsibility for data sharing. Using an adapted model of technology first proposed by Elihu Gerson, the paper then offers key ways in which these critical âbinds of paceâ can be addressed in Open Data discourse. In particular, it calls for an expanded understanding of laboratory equipment and research speed to include all aspects of the research environment. It also advocates for better engagement with LMIC scientists regarding these challenges and the adoption of frugal/responsible design principles in future Open Data initiatives. Â© 2017 The Author(s).","When considering the openness of data it is unsurprising that most conversations focus on the online environment - how data is collated, moved and recombined for multiple purposes. Nonetheless, it is important to recognize that the movements online are only part of the data lifecycle. Indeed, considering where and how data are created - namely, the research setting - are of key importance to Open Data initiatives. In particular, such insights offer key understandings of how and why scientists engage with in practices of openness, and how data transitions from personal control to public ownership. This paper examines research settings in low/middle-income countries to better understand how resource limitations influence Open Data buy-in. Using empirical fieldwork in Kenyan and South African laboratories it draws attention to some key issues currently overlooked in Open Data discussions. First, that many of the hesitations raised by the scientists about sharing data were as much tied to the speed of their research as to any other factor. Thus, it would seem that the longer it takes for individual scientists to create data, the more hesitant they are about sharing it. Second, that the pace of research is a multifaceted bind involving many different challenges relating to laboratory equipment and infrastructure. Indeed, it is unlikely that one single solution (such as equipment donation) will ameliorate these binds of pace. Third, that these binds of pace were used by the scientists to construct narratives of exclusion through which they remove themselves from responsibility for data sharing. Using an adapted model of technology first proposed by Elihu Gerson, the paper then offers key ways in which these critical binds of pace can be addressed in Open Data discourse. In particular, it calls for an expanded understanding of laboratory equipment and research speed to include all aspects of the research environment. It also advocates for better engagement with LMIC scientists regarding these challenges and the adoption of frugal/responsible design principles in future Open Data initiatives."
"Build it, but will they come? A geoscience cyberinfrastructure baseline analysis","Understanding the earth as a system requires integrating many forms of data from multiple fields. Builders and funders of the cyberinfrastructure designed to enable open data sharing in the geosciences risk a key failure mode: What if geoscientists do not use the cyberinfrastructure to share, discover and reuse data? In this study, we report a baseline assessment of engagement with the NSF EarthCube initiative, an open cyberinfrastructure effort for the geosciences. We find scientists perceive the need for cross-disciplinary engagement and engage where there is organizational or institutional support. However, we also find a possibly imbalanced involvement between cyber and geoscience communities at the outset, with the former showing more interest than the latter. This analysis highlights the importance of examining fields and disciplines as stakeholders to investments in the cyberinfrastructure supporting science. Â© 2016 The Author(s).","Understanding the earth as a system requires integrating many forms of data from multiple fields. Builders and funders of the cyberinfrastructure designed to enable open data sharing in the geosciences risk a key failure mode: What if geoscientists do not use the cyberinfrastructure to share, discover and reuse data? In this study, we report a baseline assessment of engagement with the NSF EarthCube initiative, an open cyberinfrastructure effort for the geosciences. We find scientists perceive the need for cross-disciplinary engagement and engage where there is organizational or institutional support. However, we also find a possibly imbalanced involvement between cyber and geoscience communities at the outset, with the former showing more interest than the latter. This analysis highlights the importance of examining fields and disciplines as stakeholders to investments in the cyberinfrastructure supporting science."
Afraid of scooping â Case study on researcher strategies against fear of scooping in the context of open science,"The risk of scooping is often used as a counter argument for open science, especially open data. In this case study I have examined openness strategies, practices and attitudes in two open collaboration research projects created by Finnish researchers, in order to understand what made them resistant to the fear of scooping. The radically open approach of the projects includes open by default funding proposals, co-authorship and community membership. Primary sources used are interviews of the projectsâ founding members. The analysis indicates that openness requires trust in close peers, but not necessarily in research community or society at large. Based on the case study evidence, focusing on intrinsic goals, like new knowledge and bringing about ethical reform, instead of external goals such as publications, supports openness. Understanding fundaments of science, philosophy of science and research ethics, can also have a beneficial effect on willingness to share. Whether there are aspects in open sharing that makes it seem riskier from the point of view of certain demographical groups within research community, such as women, could be worth closer inspection. Â© 2017 The Author(s).","The risk of scooping is often used as a counter argument for open science, especially open data. In this case study I have examined openness strategies, practices and attitudes in two open collaboration research projects created by Finnish researchers, in order to understand what made them resistant to the fear of scooping. The radically open approach of the projects includes open by default funding proposals, co-authorship and community membership. Primary sources used are interviews of the projects founding members. The analysis indicates that openness requires trust in close peers, but not necessarily in research community or society at large. Based on the case study evidence, focusing on intrinsic goals, like new knowledge and bringing about ethical reform, instead of external goals such as publications, supports openness. Understanding fundaments of science, philosophy of science and research ethics, can also have a beneficial effect on willingness to share. Whether there are aspects in open sharing that makes it seem riskier from the point of view of certain demographical groups within research community, such as women, could be worth closer inspection."
An analysis of federal policy on public access to scientific research data,"The 2013 Office of Science and Technology Policy (OSTP) Memo on federally-funded research directed agencies with research and development budgets above $100 million to develop and release plans to increase and broaden access to research results, both published literature and data. The agency responses have generated discussion and interest but are yet to be analyzed and compared. In this paper, we examine how 19 federal agencies responded to the memo, written by John Holdren, on issues of scientific data and the extent of their compliance to the directives outlined in the memo. We present a varied picture of the readiness of federal science agencies to comply with the memo through a comparative analysis and close reading of the contents of these responses. While some agencies, particularly those with a long history of supporting and conducting science, scored well, other responses indicate that some agencies have only taken a few steps towards implementing policies that comply with the memo. These results are of interest to the data curation community as they reveal how different agencies across the federal government approach their responsibilities for research data management, and how new policies and requirements might continue to affect scientists and research communities. Â© 2017 The Author(s).","The 2013 Office of Science and Technology Policy (OSTP) Memo on federally-funded research directed agencies with research and development budgets above $100 million to develop and release plans to increase and broaden access to research results, both published literature and data. The agency responses have generated discussion and interest but are yet to be analyzed and compared. In this paper, we examine how 19 federal agencies responded to the memo, written by John Holdren, on issues of scientific data and the extent of their compliance to the directives outlined in the memo. We present a varied picture of the readiness of federal science agencies to comply with the memo through a comparative analysis and close reading of the contents of these responses. While some agencies, particularly those with a long history of supporting and conducting science, scored well, other responses indicate that some agencies have only taken a few steps towards implementing policies that comply with the memo. These results are of interest to the data curation community as they reveal how different agencies across the federal government approach their responsibilities for research data management, and how new policies and requirements might continue to affect scientists and research communities."
Automated hardware and software system for monitoring the earth's magnetic environment,"The continuous growth of geophysical observations requires adequate methods for their processing and analysis. This becomes one of the most important and widely discussed issues in the data science community. The system analysis methods and data mining techniques are able to sustain the solution of this problem. This paper presents an innovative holistic hardware/software system (HSS) developed for efficient management and intellectual analysis of geomagnetic data, registered by Russian geomagnetic observatories and international satellites. Geomagnetic observatories that comprise the International Real-time Magnetic Observatory Network (INTERMAGNET) produce preliminary (raw) and definitive (corrected) geomagnetic data of the highest quality. The designed system automates and accelerates routine production of definitive data from the preliminary magnetograms, obtained by Russian observatories, due to implemented algorithms that involve artificial intelligence elements. The HSS is the first system that provides sophisticated automatic detection and multi-criteria classification of extreme geomagnetic conditions, which may be hazardous for technological infrastructure and economic activity in Russia. It enables the online access to digital geomagnetic data, its processing results and modelling calculations along with their visualization on conventional and spherical screens. The concept of the presented system agrees with the accepted 'four Vs' paradigm of Big Data. The HSS can increase significantly the 'velocity' and 'veracity' features of the INTERMAGNET system. It also provides fusion of large sets of ground-based and satellite geomagnetic data, thus facilitating the 'volume' and 'variety' of handled data. Â© 2016 The Author(s).","The continuous growth of geophysical observations requires adequate methods for their processing and analysis. This becomes one of the most important and widely discussed issues in the data science community. The system analysis methods and data mining techniques are able to sustain the solution of this problem. This paper presents an innovative holistic hardware/software system (HSS) developed for efficient management and intellectual analysis of geomagnetic data, registered by Russian geomagnetic observatories and international satellites. Geomagnetic observatories that comprise the International Real-time Magnetic Observatory Network (INTERMAGNET) produce preliminary (raw) and definitive (corrected) geomagnetic data of the highest quality. The designed system automates and accelerates routine production of definitive data from the preliminary magnetograms, obtained by Russian observatories, due to implemented algorithms that involve artificial intelligence elements. The HSS is the first system that provides sophisticated automatic detection and multi-criteria classification of extreme geomagnetic conditions, which may be hazardous for technological infrastructure and economic activity in Russia. It enables the online access to digital geomagnetic data, its processing results and modelling calculations along with their visualization on conventional and spherical screens. The concept of the presented system agrees with the accepted 'four Vs' paradigm of Big Data. The HSS can increase significantly the 'velocity' and 'veracity' features of the INTERMAGNET system. It also provides fusion of large sets of ground-based and satellite geomagnetic data, thus facilitating the 'volume' and 'variety' of handled data."
Design and implementation of information resource co-construction and sharing system in electronic library,"With the continuous development of Internet technology, the era of big data has come. The traditional library resource sharing which still at the mode of information island is unable to maximize the use of resources. In order to solve this problem, this study designed a library information resources co-construction and sharing system based on Squid reverse proxy technology. After the system was completed, the login function, user opening function and resource sharing function were tested. Functions of the system were basically normal, but some small details affected usersâ experience. Therefore, it is necessary to optimize the system code before putting it into use. Moreover in the aspect of resource sharing, the system realized the login of the same user information into different library databases, and users can search same resources in different libraries. Â© 2018 The Author(s).","With the continuous development of Internet technology, the era of big data has come. The traditional library resource sharing which still at the mode of information island is unable to maximize the use of resources. In order to solve this problem, this study designed a library information resources co-construction and sharing system based on Squid reverse proxy technology. After the system was completed, the login function, user opening function and resource sharing function were tested. Functions of the system were basically normal, but some small details affected users experience. Therefore, it is necessary to optimize the system code before putting it into use. Moreover in the aspect of resource sharing, the system realized the login of the same user information into different library databases, and users can search same resources in different libraries."
RIDAL -A language for research information definition argumentation,"Information about the research process is gaining importance for research documentation and evaluation. With the increased usage of such research information, the requirements for data quality and interpretation consistency are increasing. An agreed understanding of the concepts of research information is therefore crucial for fair science evaluation and science policy. Initiatives like euroCRIS and CASRAI address this by standardising research information definitions. In this paper, we present an approach to systematically develop and document not only definitions of research information, but also discussed alternatives and related arguments. With that we aim to support existing RI standardisation initatives with a flexible and scalable way of documenting and communicating the standardisation process in order to increase acceptance for the resulting definitions. Our contribution is threefold: Based on the widely used IBIS notation for argumentation modelling, we first introduce semantic rules for defining research information. Secondly, a transformation algorithm is provided to reduce the complexity of those argumentations - without the loss of information -And in turn improve readability of the diagrams. Thirdly, the semantic rules of the resulting less complex RIDAL notation are provided. The presented modelling notations are evaluated in the case setting of the standardisation project for research information of the German science system ""Core Research Dataset"". Â© 2017 The Author(s).","Information about the research process is gaining importance for research documentation and evaluation. With the increased usage of such research information, the requirements for data quality and interpretation consistency are increasing. An agreed understanding of the concepts of research information is therefore crucial for fair science evaluation and science policy. Initiatives like euroCRIS and CASRAI address this by standardising research information definitions. In this paper, we present an approach to systematically develop and document not only definitions of research information, but also discussed alternatives and related arguments. With that we aim to support existing RI standardisation initatives with a flexible and scalable way of documenting and communicating the standardisation process in order to increase acceptance for the resulting definitions. Our contribution is threefold: Based on the widely used IBIS notation for argumentation modelling, we first introduce semantic rules for defining research information. Secondly, a transformation algorithm is provided to reduce the complexity of those argumentations - without the loss of information -And in turn improve readability of the diagrams. Thirdly, the semantic rules of the resulting less complex RIDAL notation are provided. The presented modelling notations are evaluated in the case setting of the standardisation project for research information of the German science system ""Core Research Dataset""."
Perseids: Experimenting with infrastructure for creating and sharing research data in the digital humanities,"The Perseids project provides a platform for creating, publishing, and sharing research data, in the form of textual transcriptions, annotations and analyses. An offshoot and collaborator of the Perseus Digital Library (PDL), Perseids is also an experiment in reusing and extending existing infrastructure, tools, and services. This paper discusses infrastructure in the domain of digital humanities (DH). It outlines some general approaches to facilitating data sharing in this domain, and the specific choices we made in developing Perseids to serve that goal. It concludes by identifying lessons we have learned about sustainability in the process of building Perseids, noting some critical gaps in infrastructure for the digital humanities, and suggesting some implications for the wider community. Â© 2017 The Author(s).","The Perseids project provides a platform for creating, publishing, and sharing research data, in the form of textual transcriptions, annotations and analyses. An offshoot and collaborator of the Perseus Digital Library (PDL), Perseids is also an experiment in reusing and extending existing infrastructure, tools, and services. This paper discusses infrastructure in the domain of digital humanities (DH). It outlines some general approaches to facilitating data sharing in this domain, and the specific choices we made in developing Perseids to serve that goal. It concludes by identifying lessons we have learned about sustainability in the process of building Perseids, noting some critical gaps in infrastructure for the digital humanities, and suggesting some implications for the wider community."
The study of time series using the DMA methods and geophysical applications,"The discrete mathematical analysis (DMA) is a series of algorithms aimed at the solution of basic problems of data analysis: clustering and tracing in multidimensional arrays, morphological analysis of reliefs, search for anomalies and trends in records etc. All the DMA algorithms are of universal nature, joined by the same formal foundation, based, in its turn, on fuzzy logic (FL) and fuzzy mathematics (FM). The current study finalizes the search for the anomalies in one-dimensional time series within the scope of DMA: here the initial concept of an interpreter's logic gets its additional development. First, the formal expert's opinions are more fully expressed, and this is realized with the more complex measures of activity (the concept of straightenings (Gvishiani et al. 2003; Gvishiani et al. 2004; Zlotnicki et al. 2005) is replaced by the measures of activity which come to the fore): second, for the junction of anomalies, a recently created DPS (Discrete Perfect Sets) algorithm is used DPS (Discrete Perfect Sets) (Agayan et al. 2011; Agayan et al. 2014). Â© 2016 The Author(s).","The discrete mathematical analysis (DMA) is a series of algorithms aimed at the solution of basic problems of data analysis: clustering and tracing in multidimensional arrays, morphological analysis of reliefs, search for anomalies and trends in records etc. All the DMA algorithms are of universal nature, joined by the same formal foundation, based, in its turn, on fuzzy logic (FL) and fuzzy mathematics (FM). The current study finalizes the search for the anomalies in one-dimensional time series within the scope of DMA: here the initial concept of an interpreter's logic gets its additional development. First, the formal expert's opinions are more fully expressed, and this is realized with the more complex measures of activity (the concept of straightenings (Gvishiani et al. 2003; Gvishiani et al. 2004; Zlotnicki et al. 2005) is replaced by the measures of activity which come to the fore): second, for the junction of anomalies, a recently created DPS (Discrete Perfect Sets) algorithm is used DPS (Discrete Perfect Sets) (Agayan et al. 2011; Agayan et al. 2014)."
The challenge of ensuring persistency of identifier systems in the world of ever-changing technology,"The identification of information objects has always been important with library collections with indexes having been created in the most ancient times. Since the digital age, many specialised and generic persistent identifier (PID) systems have been used to identify digital objects. Just as many ancient indexes have died over time, so too PID systems have had a lifecycle from inception to active phase to paralysis, and eventually a fall into oblivion. Where the indexes within the Great Library at Alexandria finally succumbed to fire, technology change has been the destroyer of more recent digital indexes. We distil four PID system design principles from observations over the years that we think should be implemented by PID system architects to ensure that their systems survive change. The principles: describe how to ensure identifiers' system and organisation independence; codify the delivery of essential PID system functions; mandate a separation of PID functions from data delivery mechanisms; and require generation of policies detailing how change is handled. In addition to suggesting specific items for each principle, we propose that a platform-independent model (PIM) be established for persistent identifiers - of any sort and with any resolver technology - in order to enable transition between present and future systems and the preservation of the identifiers' functioning. We detail our PID system-The PID Service-That implements the proposed principles and a data model to some extent and we describe an implementation case study of an organisation's implementation of PID systems that implement the Pillars further but still not completely. Penultimately, we describe in a Future Work section, an opportunity for the use of both the Pillars and the PIM; that of the World Wide Web Consortium's Permanent Identifier Community Group who is seeking to ""set up and maintain a secure permanent, URL re-direction service for the web"". Â© 2017 The Author(s).","The identification of information objects has always been important with library collections with indexes having been created in the most ancient times. Since the digital age, many specialised and generic persistent identifier (PID) systems have been used to identify digital objects. Just as many ancient indexes have died over time, so too PID systems have had a lifecycle from inception to active phase to paralysis, and eventually a fall into oblivion. Where the indexes within the Great Library at Alexandria finally succumbed to fire, technology change has been the destroyer of more recent digital indexes. We distil four PID system design principles from observations over the years that we think should be implemented by PID system architects to ensure that their systems survive change. The principles: describe how to ensure identifiers' system and organisation independence; codify the delivery of essential PID system functions; mandate a separation of PID functions from data delivery mechanisms; and require generation of policies detailing how change is handled. In addition to suggesting specific items for each principle, we propose that a platform-independent model (PIM) be established for persistent identifiers - of any sort and with any resolver technology - in order to enable transition between present and future systems and the preservation of the identifiers' functioning. We detail our PID system-The PID Service-That implements the proposed principles and a data model to some extent and we describe an implementation case study of an organisation's implementation of PID systems that implement the Pillars further but still not completely. Penultimately, we describe in a Future Work section, an opportunity for the use of both the Pillars and the PIM; that of the World Wide Web Consortium's Permanent Identifier Community Group who is seeking to ""set up and maintain a secure permanent, URL re-direction service for the web""."
Data integration and analysis system (DIAS) as a platform for data and model integration: Cases in the field of water resources management and disaster risk reduction,"The development of data and model integration platforms has furthered scientific inquiry and helped to solve pressing social and environmental problems. While several e-infrastructure platforms have been developed, the concept of data and model integration remains obscure, and these platforms have produced few firm results. This article investigates data and model integration on the Data Integration and Analysis System (DIAS) platform, using three case projects from water-related fields. We provide concrete examples of data and model integration by analyzing the data transfer and analysis process, and demonstrate what platform functions are needed to promote the advantages of data and model integration. In addition, we introduce the Digital Object Identifier (DOI), a valuable tool for promoting data and model integration and open science. Our investigation reveals that DIAS advances data and model integration in five main ways: it is a âsophisticated and robust integration platformâ; has ârich APIs, including a metadata management system, for high-quality data archive and utilizationâ; functions as a âcore hydrological modelâ; and promotes a âcollaborative R&D communityâ and âopen science and data repositoriesâ. This article will appeal especially to researchers interested in new methods of analysis, and information technology experts responsible for developing e-infrastructure systems to support environmental and scientific research. Â© 2018 The Author(s).","The development of data and model integration platforms has furthered scientific inquiry and helped to solve pressing social and environmental problems. While several e-infrastructure platforms have been developed, the concept of data and model integration remains obscure, and these platforms have produced few firm results. This article investigates data and model integration on the Data Integration and Analysis System (DIAS) platform, using three case projects from water-related fields. We provide concrete examples of data and model integration by analyzing the data transfer and analysis process, and demonstrate what platform functions are needed to promote the advantages of data and model integration. In addition, we introduce the Digital Object Identifier (DOI), a valuable tool for promoting data and model integration and open science. Our investigation reveals that DIAS advances data and model integration in five main ways: it is a sophisticated and robust integration platform; has rich APIs, including a metadata management system, for high-quality data archive and utilization; functions as a core hydrological model; and promotes a collaborative R&D community and open science and data repositories. This article will appeal especially to researchers interested in new methods of analysis, and information technology experts responsible for developing e-infrastructure systems to support environmental and scientific research."
Rethinking data sharing and human participant protection in social science research: Applications from the qualitative realm,"While data sharing is becoming increasingly common in quantitative social inquiry, qualitative data are rarely shared. One factor inhibiting data sharing is a concern about human participant protections and privacy. Protecting the confidentiality and safety of research participants is a concern for both quantitative and qualitative researchers, but it raises specific concerns within the epistemic context of qualitative research. Thus, the applicability of emerging protection models from the quantitative realm must be carefully evaluated for application to the qualitative realm. At the same time, qualitative scholars already employ a variety of strategies for human-participant protection implicitly or informally during the research process. In this practice paper, we assess available strategies for protecting human participants and how they can be deployed. We describe a spectrum of possible data management options, such as de-identification and applying access controls, including some already employed by the Qualitative Data Repository (QDR) in tandem with its pilot depositors. Throughout the discussion, we consider the tension between modifying data or restricting access to them, and retaining their analytic value. We argue that developing explicit guidelines for sharing qualitative data generated through interaction with humans will allow scholars to address privacy concerns and increase the secondary use of their data. Â© 2017 The Author(s).","While data sharing is becoming increasingly common in quantitative social inquiry, qualitative data are rarely shared. One factor inhibiting data sharing is a concern about human participant protections and privacy. Protecting the confidentiality and safety of research participants is a concern for both quantitative and qualitative researchers, but it raises specific concerns within the epistemic context of qualitative research. Thus, the applicability of emerging protection models from the quantitative realm must be carefully evaluated for application to the qualitative realm. At the same time, qualitative scholars already employ a variety of strategies for human-participant protection implicitly or informally during the research process. In this practice paper, we assess available strategies for protecting human participants and how they can be deployed. We describe a spectrum of possible data management options, such as de-identification and applying access controls, including some already employed by the Qualitative Data Repository (QDR) in tandem with its pilot depositors. Throughout the discussion, we consider the tension between modifying data or restricting access to them, and retaining their analytic value. We argue that developing explicit guidelines for sharing qualitative data generated through interaction with humans will allow scholars to address privacy concerns and increase the secondary use of their data."
Modeling citable textual analyses for the Homer Multitext,"The Homer Multitext project (HMT) is documenting the language and structure of Greek epic poetry, and the ancient tradition of commentary on it. The project's primary data consist of editions of Greek texts; automated and manually created readings analyze the texts across historical and thematic axes. This paper describes an abstract model we follow in documenting an open-ended body of diverse analyses. The analyses apply to passages of texts at different levels of granularity; they may refer to overlapping or mutually exclusive passages of text; and they may apply to non-contiguous passages of text. All are recorded in with explicit, concise, machine-actionable canonical citation of both text passage and analysis in a scheme aligning all analyses to a common notional text. We cite our texts with urns that capture a passage's position in an Ordered Hierarchy of Citation Objects (OHCO2). Analyses are modeled as data-objects with five properties. We create collections of 'analytical objects', each uniquely identified by its own URN and each aligned to a particular edition of a text by a URN citation. We can view these analytical objects as an extension of the edition's citation hierarchy; since they are explicitly ordered by their alignment with the edition they analyze, each collection of analyses meets satisfies the (OHCO2) model of a citable text. We call these texts that are derived from and aligned to an edition 'analytical exemplars'. Â© 2016 The Author(s).","The Homer Multitext project (HMT) is documenting the language and structure of Greek epic poetry, and the ancient tradition of commentary on it. The project's primary data consist of editions of Greek texts; automated and manually created readings analyze the texts across historical and thematic axes. This paper describes an abstract model we follow in documenting an open-ended body of diverse analyses. The analyses apply to passages of texts at different levels of granularity; they may refer to overlapping or mutually exclusive passages of text; and they may apply to non-contiguous passages of text. All are recorded in with explicit, concise, machine-actionable canonical citation of both text passage and analysis in a scheme aligning all analyses to a common notional text. We cite our texts with urns that capture a passage's position in an Ordered Hierarchy of Citation Objects (OHCO2). Analyses are modeled as data-objects with five properties. We create collections of 'analytical objects', each uniquely identified by its own URN and each aligned to a particular edition of a text by a URN citation. We can view these analytical objects as an extension of the edition's citation hierarchy; since they are explicitly ordered by their alignment with the edition they analyze, each collection of analyses meets satisfies the (OHCO2) model of a citable text. We call these texts that are derived from and aligned to an edition 'analytical exemplars'."
A semantic cross-species derived data management application,"Managing dynamic information in large multi-site, multi-species, and multi-discipline consortia is a challenging task for data management applications. Often in academic research studies the goals for informatics teams are to build applications that provide extract-transform-load (ETL) functionality to archive and catalog source data that has been collected by the research teams. In consortia that cross species and methodological or scientific domains, building interfaces which supply data in a usable fashion and make intuitive sense to scientists from dramatically different backgrounds increases the complexity for developers. Further, reusing source data from outside oneâs scientific domain is fraught with ambiguities in understanding the data types, analysis methodologies, and how to combine the data with those from other research teams. We report on the design, implementation, and performance of a semantic data management application to support the NIMH funded Conte Center at the University of California, Irvine. The Center is testing a theory of the consequences of âfragmentedâ (unpredictable, high entropy) early-life experiences on adolescent cognitive and emotional outcomes in both humans and rodents. It employs cross-species neuroimaging, epigenomic, molecular, and neuroanatomical approaches in humans and rodents to assess the potential consequences of fragmented unpredictable experience on brain structure and circuitry. To address this multi-technology, multi-species approach, the system uses semantic web techniques based on the Neuroimaging Data Model (NIDM) to facilitate data ETL functionality. We find this approach enables a low-cost, easy to maintain, and semantically meaningful information management system, enabling the diverse research teams to access and use the data. Â© 2017 The Author(s).","Managing dynamic information in large multi-site, multi-species, and multi-discipline consortia is a challenging task for data management applications. Often in academic research studies the goals for informatics teams are to build applications that provide extract-transform-load (ETL) functionality to archive and catalog source data that has been collected by the research teams. In consortia that cross species and methodological or scientific domains, building interfaces which supply data in a usable fashion and make intuitive sense to scientists from dramatically different backgrounds increases the complexity for developers. Further, reusing source data from outside ones scientific domain is fraught with ambiguities in understanding the data types, analysis methodologies, and how to combine the data with those from other research teams. We report on the design, implementation, and performance of a semantic data management application to support the NIMH funded Conte Center at the University of California, Irvine. The Center is testing a theory of the consequences of fragmented (unpredictable, high entropy) early-life experiences on adolescent cognitive and emotional outcomes in both humans and rodents. It employs cross-species neuroimaging, epigenomic, molecular, and neuroanatomical approaches in humans and rodents to assess the potential consequences of fragmented unpredictable experience on brain structure and circuitry. To address this multi-technology, multi-species approach, the system uses semantic web techniques based on the Neuroimaging Data Model (NIDM) to facilitate data ETL functionality. We find this approach enables a low-cost, easy to maintain, and semantically meaningful information management system, enabling the diverse research teams to access and use the data."
The implementation and evolution of STAR/CIF ontologies: Interoperability and preservation of structured data,"The global application of the Crystallographic Information Framework (CIF) to the molecular structure domain has been successful over the past 20 years. It is used widely by molecular science journals and databases for submission, validation and deposition. This paper will give an overview of the CIF implementation, highlighting its particular successes and occasional failures. It will also recommend criteria for the application of an ontology-based data management system to any information domain. The paper will conclude with some details of the latest STAR data definition language and the importance of methods to the preservation of derivative data items. Â© 2016 by the authors.","The global application of the Crystallographic Information Framework (CIF) to the molecular structure domain has been successful over the past 20 years. It is used widely by molecular science journals and databases for submission, validation and deposition. This paper will give an overview of the CIF implementation, highlighting its particular successes and occasional failures. It will also recommend criteria for the application of an ontology-based data management system to any information domain. The paper will conclude with some details of the latest STAR data definition language and the importance of methods to the preservation of derivative data items."
Enhancing interoperability and capabilities of earth science data using the Observations Data Model 2 (ODM2),"Earth Science researchers require access to integrated, cross-disciplinary data in order to answer critical research questions. Partially due to these science drivers, it is common for disciplinary data systems to expand from their original scope in order to accommodate collaborative research. The result is multiple disparate databases with overlapping but incompatible data. In order to enable more complete data integration and analysis, the Observations Data Model Version 2 (ODM2) was developed to be a general information model, with one of its major goals to integrate data collected by in situ sensors with those by ex-situ analyses of field specimens. Four use cases with different science drivers and disciplines have adopted ODM2 because of benefits to their users. The disciplines behind the four cases are diverse - hydrology, rock geochemistry, soil geochemistry, and biogeochemistry. For each case, we outline the benefits, challenges, and rationale for adopting ODM2. In each case, the decision to implement ODM2 was made to increase interoperability and expand data and metadata capabilities. One of the common benefits was the ability to use the flexible handling and comprehensive description of specimens and data collection sites in ODM2's sampling feature concept. We also summarize best practices for implementing ODM2 based on the experience of these initial adopters. The descriptions here should help other potential adopters of ODM2 implement their own instances or to modify ODM2 to suit their needs. Â© 2017 The Author(s).","Earth Science researchers require access to integrated, cross-disciplinary data in order to answer critical research questions. Partially due to these science drivers, it is common for disciplinary data systems to expand from their original scope in order to accommodate collaborative research. The result is multiple disparate databases with overlapping but incompatible data. In order to enable more complete data integration and analysis, the Observations Data Model Version 2 (ODM2) was developed to be a general information model, with one of its major goals to integrate data collected by in situ sensors with those by ex-situ analyses of field specimens. Four use cases with different science drivers and disciplines have adopted ODM2 because of benefits to their users. The disciplines behind the four cases are diverse - hydrology, rock geochemistry, soil geochemistry, and biogeochemistry. For each case, we outline the benefits, challenges, and rationale for adopting ODM2. In each case, the decision to implement ODM2 was made to increase interoperability and expand data and metadata capabilities. One of the common benefits was the ability to use the flexible handling and comprehensive description of specimens and data collection sites in ODM2's sampling feature concept. We also summarize best practices for implementing ODM2 based on the experience of these initial adopters. The descriptions here should help other potential adopters of ODM2 implement their own instances or to modify ODM2 to suit their needs."
Data and metadata brokering - Theory and practice from the BCube project,"EarthCube is a U.S. National Science Foundation initiative that aims to create a cyberinfrastructure (CI) for all the geosciences. An initial set of ""building blocks"" was funded to develop potential components of that CI. The Brokering Building Block (BCube) created a brokering framework to demonstrate cross-disciplinary data access based on a set of use cases developed by scientists from the domains of hydrology, oceanography, polar science and climate/weather. While some successes were achieved, considerable challenges were encountered. We present a synopsis of the processes and outcomes of the BCube experiment. Â© 2017 The Author(s).","EarthCube is a National Science Foundation initiative that aims to create a cyberinfrastructure for all the geosciences. An initial set of ""building blocks"" was funded to develop potential components of that The Brokering Building Block (BCube) created a brokering framework to demonstrate cross-disciplinary data access based on a set of use cases developed by scientists from the domains of hydrology, oceanography, polar science and climate/weather. While some successes were achieved, considerable challenges were encountered. We present a synopsis of the processes and outcomes of the BCube experiment."
Process Materials Scientific Data for Intelligent Service Using a Dataspace Model,"Nowadays, materials scientific data come from lab experiments, simulations, individual archives, enterprise and internet in all scales and formats. The data flood has outpaced our capability to process, manage, analyze, and provide intelligent services. Extracting valuable information from the huge data ocean is necessary for improving the quality of domain services. The most acute information management challenges today stem from organizations relying on amounts of diverse, interrelated data sources, but having no way to manage the dataspaces in an integrated, user-demand driven and services convenient way. Thus, we proposed the model of Virtual Data-Space (VDS) in materials science field to organize multi-source and heterogeneous data resources and offer services on the data in place without losing context information. First, the concept and theoretical analysis are described for the model. Then the methods for construction of the model is proposed based on users' interests. Furthermore, the dynamic evolution algorithm of VDS is analyzed using the user feedback mechanism. Finally, we showed its efficiency for intelligent, real-time, on-demand services in the field of materials engineering.","Nowadays, materials scientific data come from lab experiments, simulations, individual archives, enterprise and internet in all scales and formats. The data flood has outpaced our capability to process, manage, analyze, and provide intelligent services. Extracting valuable information from the huge data ocean is necessary for improving the quality of domain services. The most acute information management challenges today stem from organizations relying on amounts of diverse, interrelated data sources, but having no way to manage the dataspaces in an integrated, user-demand driven and services convenient way. Thus, we proposed the model of Virtual Data-Space (VDS) in materials science field to organize multi-source and heterogeneous data resources and offer services on the data in place without losing context information. First, the concept and theoretical analysis are described for the model. Then the methods for construction of the model is proposed based on users' interests. Furthermore, the dynamic evolution algorithm of VDS is analyzed using the user feedback mechanism. Finally, we showed its efficiency for intelligent, real-time, on-demand services in the field of materials engineering."
Utilizing the international geo sample number concept in continental scientific drilling during ICDP expedition COSC-1,"The International Geo Sample Number (IGSN) is a globally unique persistent identifier (PID) for physical samples that provides discovery functionality of digital sample descriptions via the internet. In this article we describe the implementation of a registration service for IGSNs of the Helmholtz Centre Potsdam - GFZ German Research Centre for Geosciences. This includes the adaption of the metadata schema developed within the context of the System for Earth Sample Registration (SESAR1) to better describe the complex sample hierarchy of drilling cores, core sections and samples of scientific drilling projects. Our case study is the COSC-1 expedition2 (Collisional Orogeny in the Scandinavian Caledonides) supported by the International Continental Scientific Drilling Program3 (ICDP). COSC-1 prompted for the first time in ICDP's history to assign and register IGSNs during an on-going drilling campaign preserving the original parent-child relationship of the sample objects. IGSN-associated data and metadata are distributed and shared with the world wide community through novel web portals, one of which is currently evolving as part of ICDP's collaborative efforts within the GFZ Potsdam and researchers from ICDP's COSC clientele. Thus, COSC-1 can be considered as a 'Prime-Example' for ICDP projects to further improve the quality of scientific research output through a transparent process of producing and managing large quantities of data as they are normally acquired during a typical scientific drilling operation. The IGSN is an important new player in the general publication landscape that can be cited in scholarly literature and also cross-referenced in DOI-bearing scholarly and data publications. Â© 2017 The Author(s).","The International Geo Sample Number (IGSN) is a globally unique persistent identifier (PID) for physical samples that provides discovery functionality of digital sample descriptions via the internet. In this article we describe the implementation of a registration service for IGSNs of the Helmholtz Centre Potsdam - GFZ German Research Centre for Geosciences. This includes the adaption of the metadata schema developed within the context of the System for Earth Sample Registration (SESAR1) to better describe the complex sample hierarchy of drilling cores, core sections and samples of scientific drilling projects. Our case study is the COSC-1 expedition2 (Collisional Orogeny in the Scandinavian Caledonides) supported by the International Continental Scientific Drilling Program3 (ICDP). COSC-1 prompted for the first time in ICDP's history to assign and register IGSNs during an on-going drilling campaign preserving the original parent-child relationship of the sample objects. IGSN-associated data and metadata are distributed and shared with the world wide community through novel web portals, one of which is currently evolving as part of ICDP's collaborative efforts within the GFZ Potsdam and researchers from ICDP's COSC clientele. Thus, COSC-1 can be considered as a 'Prime-Example' for ICDP projects to further improve the quality of scientific research output through a transparent process of producing and managing large quantities of data as they are normally acquired during a typical scientific drilling operation. The IGSN is an important new player in the general publication landscape that can be cited in scholarly literature and also cross-referenced in DOI-bearing scholarly and data publications."
Scalable data-oriented replication with flexible consistency in real-time data systems,"Scalability is an increasingly important target for distributed real-time databases. Replication is widely applied to improve the scalability and availability of data. With full replication, database systems cannot scale well, since all updates must be replicated to all nodes, whether or not they are needed there. With virtual Full Replication, all nodes have an image of a fully replicated database and the system manages the knowledge of what is needed for each node to adapt to the actual needs, so that the system can be more scalable. This work proposes a scalable and consistent replication protocol using an adaptive clustering technique that dynamically detects the new data requirements. Because time is critical in such systems, the clustering technique must take into account both the communication time cost and the timing properties of the data. The proposed protocol also proposes a new updated method for addressing the temporal inconsistency problem by skipping unnecessary operations. It allows many database nodes to update their data concurrently, without any need for distributed synchronization. It uses state-transfer propagation with on-demand integration techniques to reduce the temporal inconsistency. The experimental results show the ability of the proposed protocol to reduce the system resources consumed and improves system scalability while maintaining consistency. Â© 2016 by the authors.","Scalability is an increasingly important target for distributed real-time databases. Replication is widely applied to improve the scalability and availability of data. With full replication, database systems cannot scale well, since all updates must be replicated to all nodes, whether or not they are needed there. With virtual Full Replication, all nodes have an image of a fully replicated database and the system manages the knowledge of what is needed for each node to adapt to the actual needs, so that the system can be more scalable. This work proposes a scalable and consistent replication protocol using an adaptive clustering technique that dynamically detects the new data requirements. Because time is critical in such systems, the clustering technique must take into account both the communication time cost and the timing properties of the data. The proposed protocol also proposes a new updated method for addressing the temporal inconsistency problem by skipping unnecessary operations. It allows many database nodes to update their data concurrently, without any need for distributed synchronization. It uses state-transfer propagation with on-demand integration techniques to reduce the temporal inconsistency. The experimental results show the ability of the proposed protocol to reduce the system resources consumed and improves system scalability while maintaining consistency."
Applying the canonical text services model to the Coptic SCRIPTORIUM,"Coptic SCRIPTORIUM is a platform for interdisciplinary and computational research in Coptic texts and linguistics. The purpose of this project was to research and implement a system of stable identification for the texts and linguistic data objects in Coptic SCRIPTORIUM to facilitate their citation and reuse. We began the project with a preferred solution, the Canonical Text Services URN model, which we validated for suitability for the corpus and compared it to other approaches, including HTTP URLs and Handles. The process of applying the CTS model to Coptic SCRIPTORIUM required an in-depth analysis that took into account the domain-specific scholarly research and citation practices, the structure of the textual data, and the data management workflow. Â© 2016 The Author(s).","Coptic SCRIPTORIUM is a platform for interdisciplinary and computational research in Coptic texts and linguistics. The purpose of this project was to research and implement a system of stable identification for the texts and linguistic data objects in Coptic SCRIPTORIUM to facilitate their citation and reuse. We began the project with a preferred solution, the Canonical Text Services URN model, which we validated for suitability for the corpus and compared it to other approaches, including HTTP URLs and Handles. The process of applying the CTS model to Coptic SCRIPTORIUM required an in-depth analysis that took into account the domain-specific scholarly research and citation practices, the structure of the textual data, and the data management workflow."
"A robust, format-agnostic scientific data transfer framework","The olog approach of Spivak and Kent (PLoS ONE 7, 1 (2012) p e24274) is applied to the practical development of data transfer frameworks, yielding simple rules for construction and assessment of data transfer standards. The simplicity, extensibility and modularity of such descriptions allows discipline experts unfamiliar with complex ontological constructs or toolsets to synthesise multiple pre-existing standards, potentially including a variety of file formats, into a single overarching ontology. These ontologies nevertheless capture all scientifically-relevant prior knowledge, and when expressed in machine-readable form are sufficiently expressive to mediate translation between legacy and modern data formats. A format-independent programming interface informed by this ontology consists of six functions, of which only two handle data. Demonstration software implementing this interface is used to translate between two common diffraction image formats using such an ontology in place of an intermediate format. Â© 2016 The Author(s).","The olog approach of Spivak and Kent (PLoS ONE 7, 1 p e24274) is applied to the practical development of data transfer frameworks, yielding simple rules for construction and assessment of data transfer standards. The simplicity, extensibility and modularity of such descriptions allows discipline experts unfamiliar with complex ontological constructs or toolsets to synthesise multiple pre-existing standards, potentially including a variety of file formats, into a single overarching ontology. These ontologies nevertheless capture all scientifically-relevant prior knowledge, and when expressed in machine-readable form are sufficiently expressive to mediate translation between legacy and modern data formats. A format-independent programming interface informed by this ontology consists of six functions, of which only two handle data. Demonstration software implementing this interface is used to translate between two common diffraction image formats using such an ontology in place of an intermediate format."
Data as social capital and the gift culture in research,"The value of making research data available is broadly accepted. Policies concerning the open access to research data try to implement new norms calling for researchers to make their data more openly available. These policies either appeal to the common good or focus on publication and citation as an incentive to bring about a cultural change in how researchers share their data with their peers. But when we compare the total number of publications in the fields of science, technology and medicine with the number data publications from the same time period, the number of openly available datasets is rather small. This indicates that current policies on data sharing are not effective in changing behaviours and bringing about the wanted cultural change. By looking at research communities that are more open to data sharing we can study the social patterns that influence data sharing and point us to possible points for intervention and change. Â© 2017 The Author(s).","The value of making research data available is broadly accepted. Policies concerning the open access to research data try to implement new norms calling for researchers to make their data more openly available. These policies either appeal to the common good or focus on publication and citation as an incentive to bring about a cultural change in how researchers share their data with their peers. But when we compare the total number of publications in the fields of science, technology and medicine with the number data publications from the same time period, the number of openly available datasets is rather small. This indicates that current policies on data sharing are not effective in changing behaviours and bringing about the wanted cultural change. By looking at research communities that are more open to data sharing we can study the social patterns that influence data sharing and point us to possible points for intervention and change."
Motivation and strategies for implementing digital object identifiers (DOIs) at NCAR's earth observing laboratory - Past progress and future collaborations,"In an effort to lead our community in following modern data citation practices by formally citing data used in published research and implementing standards to facilitate reproducible research results and data, while also producing meaningful metrics that help assess the impact of our services, the National Center for Atmospheric Research (NCAR) Earth Observing Laboratory (EOL) has implemented the use of Digital Object Identifiers (DOIs) (DataCite 2017) for both physical objects (e.g., research platforms and instruments) and datasets. We discuss why this work is important and timely, and review the development of guidelines for the use of DOIs at EOL by focusing on how decisions were made. We discuss progress in assigning DOIs to physical objects and datasets, summarize plans to cite software, describe a current collaboration to develop community tools to display citations on websites, and touch on future plans to cite workflows that document dataset processing and quality control. Finally, we will review the status of efforts to engage our scientific community in the process of using DOIs in their research publications. Â© 2017 The Author(s).","In an effort to lead our community in following modern data citation practices by formally citing data used in published research and implementing standards to facilitate reproducible research results and data, while also producing meaningful metrics that help assess the impact of our services, the National Center for Atmospheric Research (NCAR) Earth Observing Laboratory (EOL) has implemented the use of Digital Object Identifiers (DOIs) (DataCite 2017) for both physical objects (, research platforms and instruments) and datasets. We discuss why this work is important and timely, and review the development of guidelines for the use of DOIs at EOL by focusing on how decisions were made. We discuss progress in assigning DOIs to physical objects and datasets, summarize plans to cite software, describe a current collaboration to develop community tools to display citations on websites, and touch on future plans to cite workflows that document dataset processing and quality control. Finally, we will review the status of efforts to engage our scientific community in the process of using DOIs in their research publications."
Global data quality assessment and the situated nature of âbestâ research practices in biology,"This paper reflects on the relation between international debates around data quality assessment and the diversity characterising research practices, goals and environments within the life sciences. Since the emergence of molecular approaches, many biologists have focused their research, and related methods and instruments for data production, on the study of genes and genomes. While this trend is now shifting, prominent institutions and companies with stakes in molecular biology continue to set standards for what counts as âgood scienceâ worldwide, resulting in the use of specific data production technologies as proxy for assessing data quality. This is problematic considering (1) the variability in research cultures, goals and the very characteristics of biological systems, which can give rise to countless different approaches to knowledge production; and (2) the existence of research environments that produce high-quality, significant datasets despite not availing themselves of the latest technologies. Ethnographic research carried out in such environments evidences a widespread fear among researchers that providing extensive information about their experimental set-up will affect the perceived quality of their data, making their findings vulnerable to criticisms by better-resourced peers. These fears can make scientists resistant to sharing data or describing their provenance. To counter this, debates around Open Data need to include critical reflection on how data quality is evaluated, and the extent to which that evaluation requires a localised assessment of the needs, means and goals of each research environment. Â© 2017 The Author(s).","This paper reflects on the relation between international debates around data quality assessment and the diversity characterising research practices, goals and environments within the life sciences. Since the emergence of molecular approaches, many biologists have focused their research, and related methods and instruments for data production, on the study of genes and genomes. While this trend is now shifting, prominent institutions and companies with stakes in molecular biology continue to set standards for what counts as good science worldwide, resulting in the use of specific data production technologies as proxy for assessing data quality. This is problematic considering the variability in research cultures, goals and the very characteristics of biological systems, which can give rise to countless different approaches to knowledge production; and the existence of research environments that produce high-quality, significant datasets despite not availing themselves of the latest technologies. Ethnographic research carried out in such environments evidences a widespread fear among researchers that providing extensive information about their experimental set-up will affect the perceived quality of their data, making their findings vulnerable to criticisms by better-resourced peers. These fears can make scientists resistant to sharing data or describing their provenance. To counter this, debates around Open Data need to include critical reflection on how data quality is evaluated, and the extent to which that evaluation requires a localised assessment of the needs, means and goals of each research environment."
"Earth science data analytics: Definitions, techniques and skills","The continuous evolution of data management systems affords great opportunities for the enhancement of knowledge and advancement of science research. To capitalize on these opportunities, it is essential to understand and develop methods that enable data relationships to be examined and information to be manipulated. Earth Science Data Analytics (ESDA) comprises the techniques and skills needed to holistically extract information and knowledge from all sources of available, often heterogeneous, data sets. This paper reports on the ground breaking efforts of the Earth Science Information Partners (ESIP) ESDA Cluster in defining ESDA and identifying ESDA methodologies. As a result of the void of Earth science data analytics in the literature the ESIP ESDA definition and goals serve as an initial framework for a common understanding of techniques and skills that are available, as well as those still needed to support ESDA. Through the acquisition of Earth science research use cases and categorization of ESDA result oriented research goals, ESDA techniques/skills have been assembled. The resulting ESDA techniques/skills provide the community with a definition for ESDA that is useful in articulating data management and research needs, as well as a working list of techniques and skills relevant to the different types of ESDA. Â© 2017 The Author(s).","The continuous evolution of data management systems affords great opportunities for the enhancement of knowledge and advancement of science research. To capitalize on these opportunities, it is essential to understand and develop methods that enable data relationships to be examined and information to be manipulated. Earth Science Data Analytics (ESDA) comprises the techniques and skills needed to holistically extract information and knowledge from all sources of available, often heterogeneous, data sets. This paper reports on the ground breaking efforts of the Earth Science Information Partners (ESIP) ESDA Cluster in defining ESDA and identifying ESDA methodologies. As a result of the void of Earth science data analytics in the literature the ESIP ESDA definition and goals serve as an initial framework for a common understanding of techniques and skills that are available, as well as those still needed to support ESDA. Through the acquisition of Earth science research use cases and categorization of ESDA result oriented research goals, ESDA techniques/skills have been assembled. The resulting ESDA techniques/skills provide the community with a definition for ESDA that is useful in articulating data management and research needs, as well as a working list of techniques and skills relevant to the different types of ESDA."
Legal and ethical issues around incorporating traditional knowledge in polar data infrastructures,"Human knowledge of the polar region is a unique blend of Western scientific knowledge and local and indigenous knowledge. It is increasingly recognized that to exclude Traditional Knowledge from repositories of polar data would both limit the value of such repositories and perpetuate colonial legacies of exclusion and exploitation. However, the inclusion of Traditional Knowledge within repositories that are conceived and designed for Western scientific knowledge raises its own unique challenges. There is increasing acceptance of the need to make these two knowledge systems interoperable but in addition to the technical challenge there are legal and ethical issues involved. These relate to 'ownership' or custodianship of the knowledge; obtaining appropriate consent to gather, use and incorporate this knowledge; being sensitive to potentially different norms regarding access to and sharing of some types of knowledge; and appropriate acknowledgement for data contributors. In some cases, respectful incorporation of Traditional Knowledge may challenge standard conceptions regarding the sharing of data, including through open data licensing. These issues have not been fully addressed in the existing literature on legal interoperability which does not adequately deal with Traditional Knowledge. In this paper we identify legal and ethical norms regarding the use of Traditional Knowledge and explore their application in the particular context of polar data. Drawing upon our earlier work on cybercartography and Traditional Knowledge we identify the elements required in the development of a framework for the inclusion of Traditional Knowledge within data infrastructures. Â© 2017 The Author(s).","Human knowledge of the polar region is a unique blend of Western scientific knowledge and local and indigenous knowledge. It is increasingly recognized that to exclude Traditional Knowledge from repositories of polar data would both limit the value of such repositories and perpetuate colonial legacies of exclusion and exploitation. However, the inclusion of Traditional Knowledge within repositories that are conceived and designed for Western scientific knowledge raises its own unique challenges. There is increasing acceptance of the need to make these two knowledge systems interoperable but in addition to the technical challenge there are legal and ethical issues involved. These relate to 'ownership' or custodianship of the knowledge; obtaining appropriate consent to gather, use and incorporate this knowledge; being sensitive to potentially different norms regarding access to and sharing of some types of knowledge; and appropriate acknowledgement for data contributors. In some cases, respectful incorporation of Traditional Knowledge may challenge standard conceptions regarding the sharing of data, including through open data licensing. These issues have not been fully addressed in the existing literature on legal interoperability which does not adequately deal with Traditional Knowledge. In this paper we identify legal and ethical norms regarding the use of Traditional Knowledge and explore their application in the particular context of polar data. Drawing upon our earlier work on cybercartography and Traditional Knowledge we identify the elements required in the development of a framework for the inclusion of Traditional Knowledge within data infrastructures."
Redesigning the DOE data explorer to embed dataset relationships at the point of search and to reflect landing page organization,"Scientific research is producing ever-increasing amounts of data. Organizing and reflecting relationships across data collections, datasets, publications, and other research objects are essential functionalities of the modern science environment, yet challenging to implement. Landing pages are often used for providing 'big picture' contextual frameworks for datasets and data collections, and many large-volume data holders are utilizing them in thoughtful, creative ways. The benefits of their organizational efforts, however, are not realized unless the user eventually sees the landing page at the end point of their search. What if that organization and 'big picture' context could benefit the user at the beginning of the search? That is a challenging approach, but The Department of Energy's (DOE) Office of Scientific and Technical Information (OSTI) is redesigning the database functionality of the DOE Data Explorer (DDE) with that goal in mind. Phase I is focused on redesigning the DDE database to leverage relationships between two existing distinct populations in DDE, data Projects and individual Datasets, and then adding a third intermediate population, data Collections. Mapped, structured linkages, designed to show user relationships, will allow users to make informed search choices. These linkages will be sustainable and scalable, created automatically with the use of new metadata fields and existing authorities. Phase II will study selected DOE Data ID Service clients, analyzing how their landing pages are organized, and how that organization might be used to improve DDE search capabilities. At the heart of both phases is the realization that adding more metadata information for cross-referencing may require additional effort for data scientists. OSTI's approach seeks to leverage existing metadata and landing page intelligence without imposing an additional burden on the data creators. Â© 2017 The Author(s).","Scientific research is producing ever-increasing amounts of data. Organizing and reflecting relationships across data collections, datasets, publications, and other research objects are essential functionalities of the modern science environment, yet challenging to implement. Landing pages are often used for providing 'big picture' contextual frameworks for datasets and data collections, and many large-volume data holders are utilizing them in thoughtful, creative ways. The benefits of their organizational efforts, however, are not realized unless the user eventually sees the landing page at the end point of their search. What if that organization and 'big picture' context could benefit the user at the beginning of the search? That is a challenging approach, but The Department of Energy's (DOE) Office of Scientific and Technical Information (OSTI) is redesigning the database functionality of the DOE Data Explorer (DDE) with that goal in mind. Phase I is focused on redesigning the DDE database to leverage relationships between two existing distinct populations in DDE, data Projects and individual Datasets, and then adding a third intermediate population, data Collections. Mapped, structured linkages, designed to show user relationships, will allow users to make informed search choices. These linkages will be sustainable and scalable, created automatically with the use of new metadata fields and existing authorities. Phase II will study selected DOE Data ID Service clients, analyzing how their landing pages are organized, and how that organization might be used to improve DDE search capabilities. At the heart of both phases is the realization that adding more metadata information for cross-referencing may require additional effort for data scientists. OSTI's approach seeks to leverage existing metadata and landing page intelligence without imposing an additional burden on the data creators."
Three dimensional (3D) lumbar vertebrae data set,"3D modelling can be used for a variety of purposes, including biomedical modelling for orthopaedic or anatomical applications. Low back pain is prevalent in society yet few validated 3D models of the lumbar spine exist to facilitate assessment. We therefore created a 3D surface data set for lumbar vertebrae from human vertebrae. Models from 86 lumbar vertebrae were constructed using an inexpensive method involving image capture by digital camera and reconstruction of 3D models via an image-based technique. The reconstruction method was validated using a laser-based arm scanner and measurements derived from real vertebrae using electronic callipers. Results show a mean relative error of 5.2% between image-based models and real vertebrae, a mean relative error of 4.7% between image-based and arm scanning models and 95% of vertices' errors are less than 3.5 millimetres with a median of 1.1 millimetres. The accuracy of the method indicates that the generated models could be useful for biomechanical modelling or 3D visualisation of the spine.","3D modelling can be used for a variety of purposes, including biomedical modelling for orthopaedic or anatomical applications. Low back pain is prevalent in society yet few validated 3D models of the lumbar spine exist to facilitate assessment. We therefore created a 3D surface data set for lumbar vertebrae from human vertebrae. Models from 86 lumbar vertebrae were constructed using an inexpensive method involving image capture by digital camera and reconstruction of 3D models via an image-based technique. The reconstruction method was validated using a laser-based arm scanner and measurements derived from real vertebrae using electronic callipers. Results show a mean relative error of 5.2% between image-based models and real vertebrae, a mean relative error of 4.7% between image-based and arm scanning models and 95% of vertices' errors are less than 3.5 millimetres with a median of 1.1 millimetres. The accuracy of the method indicates that the generated models could be useful for biomechanical modelling or 3D visualisation of the spine."
Post-disaster supply chain interdependent critical infrastructure system restoration: A review of data necessary and available for modeling,"The majority of restoration strategies in the wake of large-scale disasters have focused on short-term emergency response solutions. Few consider medium- to long-term restoration strategies to reconnect urban areas to national supply chain interdependent critical infrastructure systems (SCICI). These SCICI promote the effective flow of goods, services, and information vital to the economic vitality of an urban environment. To re-establish the connectivity that has been broken during a disaster between the different SCICI, relationships between these systems must be identified, formulated, and added to a common framework to form a system-level restoration plan. To accomplish this goal, a considerable collection of SCICI data is necessary. The aim of this paper is to review what data are required for model construction, the accessibility of these data, and their integration with each other. While a review of publically available data reveals a dearth of real-time data to assist modeling long-term recovery following an extreme event, a significant amount of static data does exist and these data can be used to model the complex interdependencies needed. For the sake of illustration, a particular SCICI (transportation) is used to highlight the challenges of determining the interdependencies and creating models capable of describing the complexity of an urban environment with the data publically available. Integration of such data as is derived from public domain sources is readily achieved in a geospatial environment, after all geospatial infrastructure data are the most abundant data source and while significant quantities of data can be acquired through public sources, a significant effort is still required to gather, develop, and integrate these data from multiple sources to build a complete model. Therefore, while continued availability of high quality, public information is essential for modeling efforts in academic as well as government communities, a more streamlined approach to a real-time acquisition and integration of these data is essential. Â© 2016 by the authors.","The majority of restoration strategies in the wake of large-scale disasters have focused on short-term emergency response solutions. Few consider medium- to long-term restoration strategies to reconnect urban areas to national supply chain interdependent critical infrastructure systems (SCICI). These SCICI promote the effective flow of goods, services, and information vital to the economic vitality of an urban environment. To re-establish the connectivity that has been broken during a disaster between the different SCICI, relationships between these systems must be identified, formulated, and added to a common framework to form a system-level restoration plan. To accomplish this goal, a considerable collection of SCICI data is necessary. The aim of this paper is to review what data are required for model construction, the accessibility of these data, and their integration with each other. While a review of publically available data reveals a dearth of real-time data to assist modeling long-term recovery following an extreme event, a significant amount of static data does exist and these data can be used to model the complex interdependencies needed. For the sake of illustration, a particular SCICI (transportation) is used to highlight the challenges of determining the interdependencies and creating models capable of describing the complexity of an urban environment with the data publically available. Integration of such data as is derived from public domain sources is readily achieved in a geospatial environment, after all geospatial infrastructure data are the most abundant data source and while significant quantities of data can be acquired through public sources, a significant effort is still required to gather, develop, and integrate these data from multiple sources to build a complete model. Therefore, while continued availability of high quality, public information is essential for modeling efforts in academic as well as government communities, a more streamlined approach to a real-time acquisition and integration of these data is essential."
Developing criteria to establish trusted digital repositories,"This paper details the drivers, methods, and outcomes of the U.S. Geological Surveyâs quest to establish criteria by which to judge its own digital preservation resources as Trusted Ãigital Repositories. Drivers included recent U.S. legislation focused on data and asset management conducted by federal agencies spending $100M USD or more annually on research activities. The methods entailed seeking existing evaluation criteria from national and international organizations such as International Standards Organization (ISO), U.S. Library of Congress, and Data Seal of Approval upon which to model USGS repository evaluations. Certification, complexity, cost, and usability of existing evaluation models were key considerations. The selected evaluation method was derived to allow the repository evaluation process to be transparent, understandable, and defensible; factors that are critical for judging competing, internal units. Implementing the chosen evaluation criteria involved establishing a cross-agency, multi-disciplinary team that interfaced across the organization. Â© 2017 The Author(s).","This paper details the drivers, methods, and outcomes of the Geological Surveys quest to establish criteria by which to judge its own digital preservation resources as Trusted igital Repositories. Drivers included recent legislation focused on data and asset management conducted by federal agencies spending $100M USD or more annually on research activities. The methods entailed seeking existing evaluation criteria from national and international organizations such as International Standards Organization (ISO), Library of Congress, and Data Seal of Approval upon which to model USGS repository evaluations. Certification, complexity, cost, and usability of existing evaluation models were key considerations. The selected evaluation method was derived to allow the repository evaluation process to be transparent, understandable, and defensible; factors that are critical for judging competing, internal units. Implementing the chosen evaluation criteria involved establishing a cross-agency, multi-disciplinary team that interfaced across the organization."
Quality control of observation data by the Geomagnetic Network of China,Data quality is an important guarantee for scientific research. The Geomagnetic Network of China (GNC) controls data quality for all observatories under the jurisdiction of China Earthquake Administration. This paper presents the quality control content and methods; quality evaluation and feedback procedures; and the process for publishing data sets via the GNC website. Technical challenges and proposed quality assurance procedures for future GNC data sets are described. Â© 2016 The Author(s).,Data quality is an important guarantee for scientific research. The Geomagnetic Network of China (GNC) controls data quality for all observatories under the jurisdiction of China Earthquake Administration. This paper presents the quality control content and methods; quality evaluation and feedback procedures; and the process for publishing data sets via the GNC website. Technical challenges and proposed quality assurance procedures for future GNC data sets are described.
Data integration and analysis system (DIAS) contributing to climate change analysis and disaster risk reduction,"In 2015, global attempts were made to reconcile the relationship between development and environmental issues. This led to the adoption of key agreements such as the Sustainable Development Goals. In this regard, it is important to identify and evaluate under-recognized disaster risks that hinder sustainable development: measures to mitigate climate change are the same as those that build resilience against climate-related disasters. To do this we need to advance scientific and technical knowledge, build data infrastructure that allows us to predict events with greater accuracy, and develop data archives. For this reason we have developed the Data Integration and Analysis System (DIAS). DIAS incorporates analysis, data and models from many fields and disciplines. It collects and stores data from satellites, ground observation stations and numerical weather prediction models; integrates this data with geographical and socio-economic information; then generates results for crisis management of global environmental issues. This article gives an overview of DIAS and summarizes its application to climate change analysis and disaster risk reduction. As the article shows, DIAS aims to initiate cooperation between different stakeholders, and contribute to the creation of scientific knowledge. DIAS provides a model for sharing transdisciplinary research data that is essential for achieving the goal of sustainable development. Â© 2017 The Author(s).","In 2015, global attempts were made to reconcile the relationship between development and environmental issues. This led to the adoption of key agreements such as the Sustainable Development Goals. In this regard, it is important to identify and evaluate under-recognized disaster risks that hinder sustainable development: measures to mitigate climate change are the same as those that build resilience against climate-related disasters. To do this we need to advance scientific and technical knowledge, build data infrastructure that allows us to predict events with greater accuracy, and develop data archives. For this reason we have developed the Data Integration and Analysis System (DIAS). DIAS incorporates analysis, data and models from many fields and disciplines. It collects and stores data from satellites, ground observation stations and numerical weather prediction models; integrates this data with geographical and socio-economic information; then generates results for crisis management of global environmental issues. This article gives an overview of DIAS and summarizes its application to climate change analysis and disaster risk reduction. As the article shows, DIAS aims to initiate cooperation between different stakeholders, and contribute to the creation of scientific knowledge. DIAS provides a model for sharing transdisciplinary research data that is essential for achieving the goal of sustainable development."
Starting from the end: What to do when restricted data is released,"Repository managers can never be one hundred percent sure of the security of hosted research data. Even assuming that human errors and technical faults will never happen, repositories can be subject to hacking attacks. Therefore, repositories accepting personal/sensitive data (or other forms of restricted data) should have workflows in place with defined procedures to be followed should things go wrong and restricted data is inappropriately released. In this paper we will report on our considerations and procedures when restricted data from our institution was inappropriately released. Â© 2017 The Author(s).","Repository managers can never be one hundred percent sure of the security of hosted research data. Even assuming that human errors and technical faults will never happen, repositories can be subject to hacking attacks. Therefore, repositories accepting personal/sensitive data (or other forms of restricted data) should have workflows in place with defined procedures to be followed should things go wrong and restricted data is inappropriately released. In this paper we will report on our considerations and procedures when restricted data from our institution was inappropriately released."
Open data for research and strategic monitoring in the pharmaceutical and biotech industry,"Open data is considered the new oil. As oil can be used to produce fertilisers, pesticides, lubricants, plastics and many other derivatives, so data is considered the commodity to use and re-use to create value. The number of initiatives supporting free access to data has increased in the last years and open data is becoming the norm in the public sector; the approach empowers stakeholders and nurtures the economy. Even if at early stage, private companies also are adapting to the open data market. A survey was conducted to which thirteen companies of different size (from micro enterprises to world-leading pharmas) in the pharmaceutical and biotech sector and representing four business models archetypes of companies exploiting open data (aggregators, developers, enrichers and enablers) participated. The information collected provides a snapshot of the use of open data by the pharmaceutical and biotech industry in 2015-2016. The companies interviewed use open data to complement proprietary data for research purposes, to implement licensing-in/licensing-out strategies, to map partnerships and connections among players or to identify key expertise and hire staff. Pharmaceutical and biotech companies have made of the protection of knowledge a dogma at the foundation of their business models, but using and contributing to the open data movement may change their approach to intellectual property and innovation. Â© 2017 The Author(s).","Open data is considered the new oil. As oil can be used to produce fertilisers, pesticides, lubricants, plastics and many other derivatives, so data is considered the commodity to use and re-use to create value. The number of initiatives supporting free access to data has increased in the last years and open data is becoming the norm in the public sector; the approach empowers stakeholders and nurtures the economy. Even if at early stage, private companies also are adapting to the open data market. A survey was conducted to which thirteen companies of different size (from micro enterprises to world-leading pharmas) in the pharmaceutical and biotech sector and representing four business models archetypes of companies exploiting open data (aggregators, developers, enrichers and enablers) participated. The information collected provides a snapshot of the use of open data by the pharmaceutical and biotech industry in 2015-2016. The companies interviewed use open data to complement proprietary data for research purposes, to implement licensing-in/licensing-out strategies, to map partnerships and connections among players or to identify key expertise and hire staff. Pharmaceutical and biotech companies have made of the protection of knowledge a dogma at the foundation of their business models, but using and contributing to the open data movement may change their approach to intellectual property and innovation."
"Research funding and citations in papers of Nobel Laureates in Physics, Chemistry and Medicine, 2019-2020","Purpose: The goal of this study is a comparative analysis of the relation between funding (a main driver for scientific research) and citations in papers of Nobel Laureates in physics, chemistry and medicine over 2019-2020 and the same relation in these research fields as a whole. Design/Methodology/Approach: This study utilizes a power law model to explore the relationship between research funding and citations of related papers. The study here analyzes 3,539 recorded documents by Nobel Laureates in physics, chemistry and medicine and a broader dataset of 183,016 documents related to the fields of physics, medicine, and chemistry recorded in the Web of Science database. Findings: Results reveal that in chemistry and medicine, funded researches published in papers of Nobel Laureates have higher citations than unfunded studies published in articles; vice versa high citations of Nobel Laureates in physics are for unfunded studies published in papers. Instead, when overall data of publications and citations in physics, chemistry and medicine are analyzed, all papers based on funded researches show higher citations than unfunded ones. Originality/Value: Results clarify the driving role of research funding for science diffusion that are systematized in general properties: a) articles concerning funded researches receive more citations than (un)funded studies published in papers of physics, chemistry and medicine sciences, generating a high Matthew effect (a higher growth of citations with the increase in the number of papers); b) research funding increases the citations of articles in fields oriented to applied research (e.g., chemistry and medicine) more than fields oriented towards basic research (e.g., physics). Practical Implications: The results here explain some characteristics of scientific development and diffusion, highlighting the critical role of research funding in fostering citations and the expansion of scientific knowledge. This finding can support decisionmaking of policymakers and R&D managers to improve the effectiveness in allocating financial resources in science policies to generate a higher positive scientific and societal impact.  Â© 2024 Mario Coccia et al., published by Sciendo.","The goal of this study is a comparative analysis of the relation between funding (a main driver for scientific research) and citations in papers of Nobel Laureates in physics, chemistry and medicine over 2019-2020 and the same relation in these research fields as a whole. This study utilizes a power law model to explore the relationship between research funding and citations of related papers. The study here analyzes 3,539 recorded documents by Nobel Laureates in physics, chemistry and medicine and a broader dataset of 183,016 documents related to the fields of physics, medicine, and chemistry recorded in the Web of Science database. Results reveal that in chemistry and medicine, funded researches published in papers of Nobel Laureates have higher citations than unfunded studies published in articles; vice versa high citations of Nobel Laureates in physics are for unfunded studies published in papers. Instead, when overall data of publications and citations in physics, chemistry and medicine are analyzed, all papers based on funded researches show higher citations than unfunded ones. Results clarify the driving role of research funding for science diffusion that are systematized in general properties: a) articles concerning funded researches receive more citations than (un)funded studies published in papers of physics, chemistry and medicine sciences, generating a high Matthew effect (a higher growth of citations with the increase in the number of papers); b) research funding increases the citations of articles in fields oriented to applied research (, chemistry and medicine) more than fields oriented towards basic research (, physics). The results here explain some characteristics of scientific development and diffusion, highlighting the critical role of research funding in fostering citations and the expansion of scientific knowledge. This finding can support decisionmaking of policymakers and R&D managers to improve the effectiveness in allocating financial resources in science policies to generate a higher positive scientific and societal impact."
Learning Context-based Embeddings for Knowledge Graph Completion,"Purpose: Due to the incompleteness nature of knowledge graphs (KGs), the task of predicting missing links between entities becomes important. Many previous approaches are static, this posed a notable problem that all meanings of a polysemous entity share one embedding vector. This study aims to propose a polysemous embedding approach, named KG embedding under relational contexts (ContE for short), for missing link prediction. Design/methodology/approach: ContE models and infers different relationship patterns by considering the context of the relationship, which is implicit in the local neighborhood of the relationship. The forward and backward impacts of the relationship in ContE are mapped to two different embedding vectors, which represent the contextual information of the relationship. Then, according to the position of the entity, the entity's polysemous representation is obtained by adding its static embedding vector to the corresponding context vector of the relationship. Findings: ContE is a fully expressive, that is, given any ground truth over the triples, there are embedding assignments to entities and relations that can precisely separate the true triples from false ones. ContE is capable of modeling four connectivity patterns such as symmetry, antisymmetry, inversion and composition. Research limitations: ContE needs to do a grid search to find best parameters to get best performance in practice, which is a time-consuming task. Sometimes, it requires longer entity vectors to get better performance than some other models. Practical implications: ContE is a bilinear model, which is a quite simple model that could be applied to large-scale KGs. By considering contexts of relations, ContE can distinguish the exact meaning of an entity in different triples so that when performing compositional reasoning, it is capable to infer the connectivity patterns of relations and achieves good performance on link prediction tasks. Originality/value: ContE considers the contexts of entities in terms of their positions in triples and the relationships they link to. It decomposes a relation vector into two vectors, namely, forward impact vector and backward impact vector in order to capture the relational contexts. ContE has the same low computational complexity as TransE. Therefore, it provides a new approach for contextualized knowledge graph embedding.  Â© 2022 Fei Pu et al., published by Sciendo.","Due to the incompleteness nature of knowledge graphs (KGs), the task of predicting missing links between entities becomes important. Many previous approaches are static, this posed a notable problem that all meanings of a polysemous entity share one embedding vector. This study aims to propose a polysemous embedding approach, named KG embedding under relational contexts (ContE for short), for missing link prediction. ContE models and infers different relationship patterns by considering the context of the relationship, which is implicit in the local neighborhood of the relationship. The forward and backward impacts of the relationship in ContE are mapped to two different embedding vectors, which represent the contextual information of the relationship. Then, according to the position of the entity, the entity's polysemous representation is obtained by adding its static embedding vector to the corresponding context vector of the relationship. ContE is a fully expressive, that is, given any ground truth over the triples, there are embedding assignments to entities and relations that can precisely separate the true triples from false ones. ContE is capable of modeling four connectivity patterns such as symmetry, antisymmetry, inversion and composition. ContE needs to do a grid search to find best parameters to get best performance in practice, which is a time-consuming task. Sometimes, it requires longer entity vectors to get better performance than some other models. ContE is a bilinear model, which is a quite simple model that could be applied to large-scale KGs. By considering contexts of relations, ContE can distinguish the exact meaning of an entity in different triples so that when performing compositional reasoning, it is capable to infer the connectivity patterns of relations and achieves good performance on link prediction tasks. ContE considers the contexts of entities in terms of their positions in triples and the relationships they link to. It decomposes a relation vector into two vectors, namely, forward impact vector and backward impact vector in order to capture the relational contexts. ContE has the same low computational complexity as TransE. Therefore, it provides a new approach for contextualized knowledge graph embedding."
Contribution of the Open Access Modality to the Impact of Hybrid Journals Controlling by Field and Time Effects,"Purpose: Researchers are more likely to read and cite papers to which they have access than those that they cannot obtain. Thus, the objective of this work is to analyze the contribution of the Open Access (OA) modality to the impact of hybrid journals. Design/methodology/approach: The ""research articles""in the year 2017 from 200 hybrid journals in four subject areas, and the citations received by such articles in the period 2017-2020 in the Scopus database, were analyzed. The hybrid OA papers were compared with the paywalled ones. The journals were randomly selected from those with share of OA papers higher than some minimal value. More than 60 thousand research articles were analyzed in the sample, of which 24% under the OA modality. Findings: We obtain at journal level that cites per article in both hybrid modalities (OA and paywalled) strongly correlate. However, there is no correlation between the OA prevalence and cites per article. There is OA citation advantage in 80% of hybrid journals. Moreover, the OA citation advantage is consistent across fields and held in time. We obtain an OA citation advantage of 50% in average, and higher than 37% in half of the hybrid journals. Finally, the OA citation advantage is higher in Humanities than in Science and Social Science. Research limitations: Some of the citation advantage is likely due to more access allows more people to read and hence cite articles they otherwise would not. However, causation is difficult to establish and there are many possible bias. Several factors can affect the observed differences in citation rates. Funder mandates can be one of them. Funders are likely to have OA requirement, and well-funded studies are more likely to receive more citations than poorly funded studies. Another discussed factor is the selection bias postulate, which suggests that authors choose only their most impactful studies to be open access. Practical implications: For hybrid journals, the open access modality is positive, in the sense that it provides a greater number of potential readers. This in turn translates into a greater number of citations and an improvement in the position of the journal in the rankings by impact factor. For researchers it is also positive because it increases the potential number of readers and citations received. Originality/value: Our study refines previous results by comparing documents more similar to each other. Although it does not examine the cause of the observed citation advantage, we find that it exists in a very large sample.  Â© 2022 Pablo Dorta-GonzÃ¡lez et al., published by Sciendo.","Researchers are more likely to read and cite papers to which they have access than those that they cannot obtain. Thus, the objective of this work is to analyze the contribution of the Open Access (OA) modality to the impact of hybrid journals. The ""research articles""in the year 2017 from 200 hybrid journals in four subject areas, and the citations received by such articles in the period 2017-2020 in the Scopus database, were analyzed. The hybrid OA papers were compared with the paywalled ones. The journals were randomly selected from those with share of OA papers higher than some minimal value. More than 60 thousand research articles were analyzed in the sample, of which 24% under the OA modality. We obtain at journal level that cites per article in both hybrid modalities (OA and paywalled) strongly correlate. However, there is no correlation between the OA prevalence and cites per article. There is OA citation advantage in 80% of hybrid journals. Moreover, the OA citation advantage is consistent across fields and held in time. We obtain an OA citation advantage of 50% in average, and higher than 37% in half of the hybrid journals. Finally, the OA citation advantage is higher in Humanities than in Science and Social Science. Some of the citation advantage is likely due to more access allows more people to read and hence cite articles they otherwise would not. However, causation is difficult to establish and there are many possible bias. Several factors can affect the observed differences in citation rates. Funder mandates can be one of them. Funders are likely to have OA requirement, and well-funded studies are more likely to receive more citations than poorly funded studies. Another discussed factor is the selection bias postulate, which suggests that authors choose only their most impactful studies to be open access. For hybrid journals, the open access modality is positive, in the sense that it provides a greater number of potential readers. This in turn translates into a greater number of citations and an improvement in the position of the journal in the rankings by impact factor. For researchers it is also positive because it increases the potential number of readers and citations received. Our study refines previous results by comparing documents more similar to each other. Although it does not examine the cause of the observed citation advantage, we find that it exists in a very large sample."
"What factors influence research impact? An empirical study on the interplay of research, publications, researchers, institutions, and national conditions","Purpose: This study investigates key factors contributing to research impact and their interactions with the Research Impact Quintuple Helix Model by Arsalan et al. (2024). Design/methodology/approach: Using data from a global survey of 630 scientists across diverse disciplines, genders, regions, and experience levels, Structural Equation Modelling (SEM) was employed to assess the influence of 29 factors related to researcher characteristics, research attributes, publication strategies, institutional support, and national roles. Findings: The study validated the Quintuple Helix Model, uncovering complex interdependencies. Institutional support significantly affects research impact by covering leadership, resources, recognition, and funding. Researcher attributes, including academic experience and domain knowledge, also play a crucial role. National socioeconomic conditions indirectly influence research impact by supporting institutions, underscoring the importance of conducive national frameworks. Research limitations: While the study offers valuable insights, it has limitations. Although statistically sufficient, the response rate was below 10%, suggesting that the findings may not fully represent the entire global research community. The reliance on self-reported data may also introduce bias, as perceptions of impact can be subjective. Practical implications: The findings have a significant impact on researchers aiming to enhance their work's societal, economic, and cultural significance, institutions seeking supportive environments, and policymakers interested in creating favourable national conditions for impactful research. The study advocates for a strategic alignment among national policies, institutional practices, and individual researcher efforts to maximise research impact and effectively address global challenges. Originality/value: By empirically validating the Research Impact Quintuple Helix Model, this study offers a holistic framework for understanding the synergy of factors that drive impactful research. Â© 2024 Sciendo. All rights reserved.","This study investigates key factors contributing to research impact and their interactions with the Research Impact Quintuple Helix Model by Arsalan et al. . Using data from a global survey of 630 scientists across diverse disciplines, genders, regions, and experience levels, Structural Equation Modelling (SEM) was employed to assess the influence of 29 factors related to researcher characteristics, research attributes, publication strategies, institutional support, and national roles. The study validated the Quintuple Helix Model, uncovering complex interdependencies. Institutional support significantly affects research impact by covering leadership, resources, recognition, and funding. Researcher attributes, including academic experience and domain knowledge, also play a crucial role. National socioeconomic conditions indirectly influence research impact by supporting institutions, underscoring the importance of conducive national frameworks. While the study offers valuable insights, it has limitations. Although statistically sufficient, the response rate was below 10%, suggesting that the findings may not fully represent the entire global research community. The reliance on self-reported data may also introduce bias, as perceptions of impact can be subjective. The findings have a significant impact on researchers aiming to enhance their work's societal, economic, and cultural significance, institutions seeking supportive environments, and policymakers interested in creating favourable national conditions for impactful research. The study advocates for a strategic alignment among national policies, institutional practices, and individual researcher efforts to maximise research impact and effectively address global challenges. By empirically validating the Research Impact Quintuple Helix Model, this study offers a holistic framework for understanding the synergy of factors that drive impactful research."
Gauging scholarsâ acceptance of Open Access journals by examining the relationship between perceived quality and citation impact,"Purpose: For a set of 1,561 Open Access (OA) and non-OA journals in business and economics, this study evaluates the relationships between four citation metricsâfive-year Impact Factor (5IF), CiteScore, Article Influence (AI) score, and SCImago Journal Rank (SJR)âand the journal ratings assigned by expert reviewers. We expect that the OA journals will have especially high citation impact relative to their perceived quality (reputation). Design/methodology/approach: Regression is used to estimate the ratings assigned by expert reviewers for the 2021 CABS (Chartered Association of Business Schools) journal assessment exercise. The independent variables are the four citation metrics, evaluated separately, and a dummy variable representing the OA/non-OA status of each journal. Findings: Regardless of the citation metric used, OA journals in business and economics have especially high citation impact relative to their perceived quality (reputation). That is, they have especially low perceived quality (reputation) relative to their citation impact. Research limitations: These results are specific to the CABS journal ratings and the four citation metrics. However, there is strong evidence that CABS is closely related to several other expert ratings, and that 5IF, CiteScore, AI, and SJR are representative of the other citation metrics that might have been chosen. Practical implications: There are at least two possible explanations for these results: (1) expert evaluators are biased against OA journals, and (2) OA journals have especially high citation impact due to their increased accessibility. Although this study does not allow us to determine which of these explanations are supported, the results suggest that authors should consider publishing in OA journals whenever overall readership and citation impact are more important than journal reputation within a particular field. Moreover, the OA coefficients provide a useful indicator of the extent to which anti-OA bias (or the citation advantage of OA journals) is diminishing over time. Originality/value: This is apparently the first study to investigate the impact of OA status on the relationships between expert journal ratings and journal citation metrics. Copyright: Â© 2025 William H. Walters.","For a set of 1,561 Open Access (OA) and non-OA journals in business and economics, this study evaluates the relationships between four citation metricsfive-year Impact Factor (5IF), CiteScore, Article Influence (AI) score, and SCImago Journal Rank (SJR)and the journal ratings assigned by expert reviewers. We expect that the OA journals will have especially high citation impact relative to their perceived quality (reputation). Regression is used to estimate the ratings assigned by expert reviewers for the 2021 CABS (Chartered Association of Business Schools) journal assessment exercise. The independent variables are the four citation metrics, evaluated separately, and a dummy variable representing the OA/non-OA status of each journal. Regardless of the citation metric used, OA journals in business and economics have especially high citation impact relative to their perceived quality (reputation). That is, they have especially low perceived quality (reputation) relative to their citation impact. These results are specific to the CABS journal ratings and the four citation metrics. However, there is strong evidence that CABS is closely related to several other expert ratings, and that 5IF, CiteScore, AI, and SJR are representative of the other citation metrics that might have been chosen. There are at least two possible explanations for these results: expert evaluators are biased against OA journals, and OA journals have especially high citation impact due to their increased accessibility. Although this study does not allow us to determine which of these explanations are supported, the results suggest that authors should consider publishing in OA journals whenever overall readership and citation impact are more important than journal reputation within a particular field. Moreover, the OA coefficients provide a useful indicator of the extent to which anti-OA bias (or the citation advantage of OA journals) is diminishing over time. This is apparently the first study to investigate the impact of OA status on the relationships between expert journal ratings and journal citation metrics."
Identifying grey-rhino in eminent technologies via patent analysis,"Purpose: Following the typical features of the grey-rhino event as predictability and profound influence, we attempt to find a special pattern called the grey-rhino in eminent technologies via patent analysis. Design/methodology/approach: We propose to combine triadic patent families and technology life cycle to define the grey-rhino model. Firstly, we design the indicator rhino-index Rh = ST/SP and descriptor sequence {Rh}, where ST and SP are the accumulative number of triadic patent families and all patent families respectively for a specific technology. Secondly, according to the two typical features of the grey-rhino event, a grey-rhino is defined as a technology that meets both qualitative and quantitative conditions. Qualitatively, this technology has a profound influence. Quantitatively, in the emerging stage, Rh â¥ Rae, where Rae is the average level of the proportion of triadic patent families. Finally, this model is verified in three datasets, namely Encyclopedia Britannica's list for the greatest inventions (EB technologies for short), MIT breakthrough technologies (MIT technologies) and Derwent Manual Code technologies (MAN technologies). Findings: The result shows that there are 64.71% EB technologies and 50.00% MIT technologies meeting the quantitative standard of the grey-rhino model, but only 14.71% MAN technologies fit the quantitative standard. This falling trend indicates the quantitative standard of the grey-rhino model is reasonable. EB technologies and MIT technologies have profound influence on society, which means they satisfy the qualitative standard of the grey-rhino model. Hence, 64.71% EB technologies and 50.00% MIT technologies are grey-rhinos. In 14.71% MAN technologies meeting the quantitative standard, we make some qualitative judgments and deem U11-A01A, U12-A01A1A, and W01-A01A as grey-rhino technologies. In addition, grey-rhinos and non-grey-rhinos have some differences. Rh values of grey-rhinos have a downward trend, while Rh values of non-grey-rhinos have a contrary trend. Rh values of grey-rhinos are scattered relatively in the early stage and centralize gradually, but non-grey-rhinos do not have this feature. Research limitations: There are four main limitations. First, if a technology satisfies the quantitative standard of the model, it is likely to be a grey-rhino but expert judgments are necessary. Second, we don't know why it will be eminent, which involves technical contents. Thirdly, we did not consider the China National Intellectual Property Administration (CNIPA) and the German Patent and Trademark Office (DPMA) which also play important roles in worldwide patents, so we hope to expand our study to the CNIPA and the DPMA. Furthermore, we did not compare the rhino-index with other patent indicators. Practical implications: If a technology meets the quantitative standard, this can be seen as early warning signals and the technology may become a grey-rhino in the future, which can catch people's attention in the emerging stage and make people seize the technical opportunity early. Originality/value: We define and verify a new pattern called the grey-rhino model in eminent technologies.  Â© 2023 Shelia X. Wei et al., published by Sciendo.","Following the typical features of the grey-rhino event as predictability and profound influence, we attempt to find a special pattern called the grey-rhino in eminent technologies via patent analysis. We propose to combine triadic patent families and technology life cycle to define the grey-rhino model. Firstly, we design the indicator rhino-index Rh = ST/SP and descriptor sequence {Rh}, where ST and SP are the accumulative number of triadic patent families and all patent families respectively for a specific technology. Secondly, according to the two typical features of the grey-rhino event, a grey-rhino is defined as a technology that meets both qualitative and quantitative conditions. Qualitatively, this technology has a profound influence. Quantitatively, in the emerging stage, Rh Rae, where Rae is the average level of the proportion of triadic patent families. Finally, this model is verified in three datasets, namely Encyclopedia Britannica's list for the greatest inventions (EB technologies for short), MIT breakthrough technologies (MIT technologies) and Derwent Manual Code technologies (MAN technologies). The result shows that there are 64.71% EB technologies and 50.00% MIT technologies meeting the quantitative standard of the grey-rhino model, but only 14.71% MAN technologies fit the quantitative standard. This falling trend indicates the quantitative standard of the grey-rhino model is reasonable. EB technologies and MIT technologies have profound influence on society, which means they satisfy the qualitative standard of the grey-rhino model. Hence, 64.71% EB technologies and 50.00% MIT technologies are grey-rhinos. In 14.71% MAN technologies meeting the quantitative standard, we make some qualitative judgments and deem U11-A01A, U12-A01A1A, and W01-A01A as grey-rhino technologies. In addition, grey-rhinos and non-grey-rhinos have some differences. Rh values of grey-rhinos have a downward trend, while Rh values of non-grey-rhinos have a contrary trend. Rh values of grey-rhinos are scattered relatively in the early stage and centralize gradually, but non-grey-rhinos do not have this feature. There are four main limitations. First, if a technology satisfies the quantitative standard of the model, it is likely to be a grey-rhino but expert judgments are necessary. Second, we don't know why it will be eminent, which involves technical contents. Thirdly, we did not consider the China National Intellectual Property Administration (CNIPA) and the German Patent and Trademark Office (DPMA) which also play important roles in worldwide patents, so we hope to expand our study to the CNIPA and the DPMA. Furthermore, we did not compare the rhino-index with other patent indicators. If a technology meets the quantitative standard, this can be seen as early warning signals and the technology may become a grey-rhino in the future, which can catch people's attention in the emerging stage and make people seize the technical opportunity early. We define and verify a new pattern called the grey-rhino model in eminent technologies."
"Global trends in international research collaboration, 1980-2021","Purpose: The aim of this study is to analyze the evolution of international research collaboration from 1980 to 2021. The study examines the main global patterns as well as those specific to individual countries, country groups, and different areas of research. Design/methodology/approach: The study is based on the Web of Science Core collection database. More than 50 million publications are analyzed using co-authorship data. International collaboration is defined as publications having authors affiliated with institutions located in more than one country. Findings: At the global level, the share of publications representing international collaboration has gradually increased from 4.7% in 1980 to 25.7% in 2021. The proportion of such publications within each country is higher and, in 2021, varied from less than 30% to more than 90%. There are notable disparities in the temporal trends, indicating that the process of internationalization has impacted countries in different ways. Several factors such as country size, income level, and geopolitics may explain the variance. Research limitations: Not all international research collaboration results in joint co-authored scientific publications. International co-authorship is a partial indicator of such collaboration. Another limitation is that the applied full counting method does not take into account the number of authors representing in each country in the publication. Practical implications: The study provides global averages, indicators, and concepts that can provide a useful framework of reference for further comparative studies of international research collaboration. Originality/value: Long-term macro-level studies of international collaboration are rare, and as a novelty, this study includes an analysis by the World Bank's division of countries into four income groups.  Â© 2023 Dag W. Aksnes et al., published by Sciendo.","The aim of this study is to analyze the evolution of international research collaboration from 1980 to 2021. The study examines the main global patterns as well as those specific to individual countries, country groups, and different areas of research. The study is based on the Web of Science Core collection database. More than 50 million publications are analyzed using co-authorship data. International collaboration is defined as publications having authors affiliated with institutions located in more than one country. At the global level, the share of publications representing international collaboration has gradually increased from 4.7% in 1980 to 25.7% in 2021. The proportion of such publications within each country is higher and, in 2021, varied from less than 30% to more than 90%. There are notable disparities in the temporal trends, indicating that the process of internationalization has impacted countries in different ways. Several factors such as country size, income level, and geopolitics may explain the variance. Not all international research collaboration results in joint co-authored scientific publications. International co-authorship is a partial indicator of such collaboration. Another limitation is that the applied full counting method does not take into account the number of authors representing in each country in the publication. The study provides global averages, indicators, and concepts that can provide a useful framework of reference for further comparative studies of international research collaboration. Long-term macro-level studies of international collaboration are rare, and as a novelty, this study includes an analysis by the World Bank's division of countries into four income groups."
Early identification of scientific breakthroughs through outlier analysis based on research entities,"Purpose: To address the âanomaliesâ that occur when scientific breakthroughs emerge, this study focuses on identifying early signs and nascent stages of breakthrough innovations from the perspective of outliers, aiming to achieve early identification of scientific breakthroughs in papers. Design/methodology/approach: This study utilizes semantic technology to extract research entities from the titles and abstracts of papers to represent each paperâs research content. Outlier detection methods are then employed to measure and analyze the anomalies in breakthrough papers during their early stages. The development and evolution process are traced using literature time tags. Finally, a case study is conducted using the key publications of the 2021 Nobel Prize laureates in Physiology or Medicine. Findings: Through manual analysis of all identified outlier papers, the effectiveness of the proposed method for early identifying potential scientific breakthroughs is verified. Research limitations: The studyâs applicability has only been empirically tested in the biomedical field. More data from various fields are needed to validate the robustness and generalizability of the method. Practical implications: This study provides a valuable supplement to current methods for research entities early identification of scientific breakthroughs, effectively supporting technological intelligence decision-making and services. Originality/value: The study introduces a novel approach to early identification of scientific breakthroughs by leveraging outlier analysis of research entities, offering a more sensitive, precise, and fine-grained alternative method compared to traditional citation-based evaluations, which enhances the ability to identify nascent breakthrough innovations. Â© 2024 Yang Zhao, Mengting Zhang, Xiaoli Chen, Zhixiong Zhang.","To address the anomalies that occur when scientific breakthroughs emerge, this study focuses on identifying early signs and nascent stages of breakthrough innovations from the perspective of outliers, aiming to achieve early identification of scientific breakthroughs in papers. This study utilizes semantic technology to extract research entities from the titles and abstracts of papers to represent each papers research content. Outlier detection methods are then employed to measure and analyze the anomalies in breakthrough papers during their early stages. The development and evolution process are traced using literature time tags. Finally, a case study is conducted using the key publications of the 2021 Nobel Prize laureates in Physiology or Medicine. Through manual analysis of all identified outlier papers, the effectiveness of the proposed method for early identifying potential scientific breakthroughs is verified. The studys applicability has only been empirically tested in the biomedical field. More data from various fields are needed to validate the robustness and generalizability of the method. This study provides a valuable supplement to current methods for research entities early identification of scientific breakthroughs, effectively supporting technological intelligence decision-making and services. The study introduces a novel approach to early identification of scientific breakthroughs by leveraging outlier analysis of research entities, offering a more sensitive, precise, and fine-grained alternative method compared to traditional citation-based evaluations, which enhances the ability to identify nascent breakthrough innovations."
The need to develop tailored tools for improving the quality of thematic bibliometric analyses: Evidence from papers published in Sustainability and Scientometrics,"Purpose: The aim of this article is to explore up to seven parameters related to the methodological quality and reproducibility of thematic bibliometric research published in the two most productive journals in bibliometrics, Sustainability (a journal outside the discipline) and Scientometrics, the flagship journal in the field. Design/methodology/approach: The study identifies the need for developing tailored tools for improving the quality of thematic bibliometric analyses, and presents a framework that can guide the development of such tools. A total of 508 papers are analysed, 77& #x00025; of Sustainability, and 23& #x00025; published in Scientometrics, for the 2019-2021 period. Findings: An average of 2.6 shortcomings per paper was found for the whole sample, with an almost identical number of flaws in both journals. Sustainability has more flaws than Scientometrics in four of the seven parameters studied, while Scientometrics has more shortcomings in the remaining three variables. Research limitations: The first limitation of this work is that it is a study of two scientific journals, so the results cannot be directly extrapolated to the set of thematic bibliometric analyses published in journals from all fields. Practical implications: We propose the adoption of protocols, guidelines, and other similar tools, adapted to bibliometric practice, which could increase the thoroughness, transparency, and reproducibility of this type of research. Originality/value: These results show considerable room for improvement in terms of the adequate use and breakdown of methodological procedures in thematic bibliometric research, both in journals in the Information Science area and journals outside the discipline. & amp; copy; 2023 Alvaro Cabezas-Clavijo et al., published by Sciendo. Â© 2023 Sciendo. All rights reserved.","The aim of this article is to explore up to seven parameters related to the methodological quality and reproducibility of thematic bibliometric research published in the two most productive journals in bibliometrics, Sustainability (a journal outside the discipline) and Scientometrics, the flagship journal in the field. The study identifies the need for developing tailored tools for improving the quality of thematic bibliometric analyses, and presents a framework that can guide the development of such tools. A total of 508 papers are analysed, 77& #x00025; of Sustainability, and 23& #x00025; published in Scientometrics, for the 2019-2021 period. An average of 2.6 shortcomings per paper was found for the whole sample, with an almost identical number of flaws in both journals. Sustainability has more flaws than Scientometrics in four of the seven parameters studied, while Scientometrics has more shortcomings in the remaining three variables. The first limitation of this work is that it is a study of two scientific journals, so the results cannot be directly extrapolated to the set of thematic bibliometric analyses published in journals from all fields. We propose the adoption of protocols, guidelines, and other similar tools, adapted to bibliometric practice, which could increase the thoroughness, transparency, and reproducibility of this type of research. These results show considerable room for improvement in terms of the adequate use and breakdown of methodological procedures in thematic bibliometric research, both in journals in the Information Science area and journals outside the discipline. & amp; copy; 2023 Alvaro Cabezas-Clavijo et al., published by Sciendo."
Confidence Intervals for Relative Intensity of Collaboration (RIC) Indicators,"Purpose: We aim to extend our investigations related to the Relative Intensity of Collaboration (RIC) indicator, by constructing a confidence interval for the obtained values. Design/methodology/approach: We use Mantel-Haenszel statistics as applied recently by Smolinsky, Klingenberg, and Marx. Findings: We obtain confidence intervals for the RIC indicator Research limitations: It is not obvious that data obtained from the Web of Science (or any other database) can be considered a random sample. Practical implications: We explain how to calculate confidence intervals. Bibliometric indicators are more often than not presented as precise values instead of an approximation depending on the database and the time of measurement. Our approach presents a suggestion to solve this problem. Originality/value: Our approach combines the statistics of binary categorical data and bibliometric studies of collaboration.  Â© 2022 Joel Emanuel Fuchs et al., published by Sciendo.","We aim to extend our investigations related to the Relative Intensity of Collaboration (RIC) indicator, by constructing a confidence interval for the obtained values. We use Mantel-Haenszel statistics as applied recently by Smolinsky, Klingenberg, and Marx. We obtain confidence intervals for the RIC indicator It is not obvious that data obtained from the Web of Science (or any other database) can be considered a random sample. We explain how to calculate confidence intervals. Bibliometric indicators are more often than not presented as precise values instead of an approximation depending on the database and the time of measurement. Our approach presents a suggestion to solve this problem. Our approach combines the statistics of binary categorical data and bibliometric studies of collaboration."
Mapping the geography of editors-in-chief,"Purpose: This study aims to explore the geography of editors-in-chief to demonstrate which countries exercise the highest-level decision-making in scholarly communication. In addition, the study seeks to investigate the potential relationships between the origin and nationality of academic publishers and the geography of editors-in-chief. Design/methodology/approach: The analysis involves 11,915 journals listed in Web of Science's Social Sciences Citation Index (SSCI) and Science Citation Index Expanded (SCIE). These journals employ 15,795 scholars as editors-in-chief. The geographical locations of the institutions the editors-in-chief are affiliated with were identified; then, the data were aggregated at the country level. Findings: The results show that most editors-in-chief are located in countries of the Anglosphere, primarily the United States and the United Kingdom. In addition, most academic publishers and professional organizations that publish academic journals were found to be based in the United States and the United Kingdom, where most editors-in-chief are also based. Research limitations: The analysis involves journals indexed in the Web of Science's SCIE/SSCI databases, which are demonstrably biased toward the English language. Furthermore, the study only takes a snapshot of the geography of editors-in-chief for the year 2022, but it does not investigate trends. Research implications: The study maps the highest-level decision-making in scholarly communication. Originality/value: The study explores and maps the geography of editors-in-chief by using a massive dataset. Copyright: Â© 2024 GyÃ¶rgy CsomÃ³s. Published under a Creative Commons Attribution 4.0 International (CC BY 4.0) license.","This study aims to explore the geography of editors-in-chief to demonstrate which countries exercise the highest-level decision-making in scholarly communication. In addition, the study seeks to investigate the potential relationships between the origin and nationality of academic publishers and the geography of editors-in-chief. The analysis involves 11,915 journals listed in Web of Science's Social Sciences Citation Index (SSCI) and Science Citation Index Expanded (SCIE). These journals employ 15,795 scholars as editors-in-chief. The geographical locations of the institutions the editors-in-chief are affiliated with were identified; then, the data were aggregated at the country level. The results show that most editors-in-chief are located in countries of the Anglosphere, primarily the United States and the United Kingdom. In addition, most academic publishers and professional organizations that publish academic journals were found to be based in the United States and the United Kingdom, where most editors-in-chief are also based. The analysis involves journals indexed in the Web of Science's SCIE/SSCI databases, which are demonstrably biased toward the English language. Furthermore, the study only takes a snapshot of the geography of editors-in-chief for the year 2022, but it does not investigate trends. Research implications: The study maps the highest-level decision-making in scholarly communication. The study explores and maps the geography of editors-in-chief by using a massive dataset."
I'm Nervous about Sharing This Secret with You: Youtube Influencers Generate Strong Parasocial Interactions by Discussing Personal Issues,"Purpose: Performers may generate loyalty partly through eliciting illusory personal connections with their audience, parasocial relationships (PSRs), and individual illusory exchanges, parasocial interactions (PSIs). On social media, semi-PSIs are real but imbalanced exchanges with audiences, including through comments on influencers' videos, and strong semi-PSIs are those that occur within PSRs. This article introduces and assesses an automatic method to detect videos with strong PSI potential. Design/methodology/approach: Strong semi-PSIs were hypothesized to occur when commenters used a variant of the pronoun ""you"", typically addressing the influencer. Comments on the videos of UK female influencer channels were used to test whether the proportion of you pronoun comments could be an automated indicator of strong PSI potential, and to find factors associating with the strong PSI potential of influencer videos. The highest and lowest strong PSI potential videos for 117 influencers were classified with content analysis for strong PSI potential and evidence of factors that might elicit PSIs. Findings: The you pronoun proportion was effective at indicating video strong PSI potential, the first automated method to detect any type of PSI. Gazing at the camera, head and shoulders framing, discussing personal issues, and focusing on the influencer associated with higher strong PSI potential for influencer videos. New social media factors found include requesting feedback and discussing the channel itself. Research limitations: Only one country, genre and social media platform was analysed. Practical implications: The method can be used to automatically detect YouTube videos with strong PSI potential, helping influencers to monitor their performance. Originality/value: This is the first automatic method to detect any aspect of PSI or PSR.  Â© 2022 Mike Thelwall et al., published by Sciendo.","Performers may generate loyalty partly through eliciting illusory personal connections with their audience, parasocial relationships (PSRs), and individual illusory exchanges, parasocial interactions (PSIs). On social media, semi-PSIs are real but imbalanced exchanges with audiences, including through comments on influencers' videos, and strong semi-PSIs are those that occur within PSRs. This article introduces and assesses an automatic method to detect videos with strong PSI potential. Strong semi-PSIs were hypothesized to occur when commenters used a variant of the pronoun ""you"", typically addressing the influencer. Comments on the videos of UK female influencer channels were used to test whether the proportion of you pronoun comments could be an automated indicator of strong PSI potential, and to find factors associating with the strong PSI potential of influencer videos. The highest and lowest strong PSI potential videos for 117 influencers were classified with content analysis for strong PSI potential and evidence of factors that might elicit PSIs. The you pronoun proportion was effective at indicating video strong PSI potential, the first automated method to detect any type of PSI. Gazing at the camera, head and shoulders framing, discussing personal issues, and focusing on the influencer associated with higher strong PSI potential for influencer videos. New social media factors found include requesting feedback and discussing the channel itself. Only one country, genre and social media platform was analysed. The method can be used to automatically detect YouTube videos with strong PSI potential, helping influencers to monitor their performance. This is the first automatic method to detect any aspect of PSI or PSR."
Research misconduct in hospitals is spreading: A bibliometric analysis of retracted papers from Chinese university-affiliated hospitals,"Purpose: The number of retracted papers from Chinese university-affiliated hospitals is increasing, which has raised much concern. The aim of this study is to analyze the retracted papers from university-affiliated hospitals in mainland China from 2000 to 2021. Design/methodology/approach: Data for 1,031 retracted papers were identified from the Web of Science Core collection database. The information of the hospitals involved was obtained from their official websites. We analyzed the chronological changes, journal distribution, discipline distribution and retraction reasons for the retracted papers. The grade and geographic locations of the hospitals involved were explored as well. Findings: We found a rapid increase in the number of retracted papers, while the retraction time interval is decreasing. The main reasons for retraction are plagiarism/self-plagiarism (n=255), invalid data/images/conclusions (n=212), fake peer review (n=175) and honesty error(n=163). The disciplines are mainly distributed in oncology (n=320), pharmacology & amp; amp; pharmacy (n=198) and research & amp; amp; experimental medicine (n=166). About 43.8& #x00025; of the retracted papers were from hospitals affiliated with prestigious universities. Research limitations: This study fails to differentiate between retractions due to honest error and retractions due to research misconduct. We believe that there is a fundamental difference between honest error retractions and misconduct retractions. Another limitation is that authors of the retracted papers have not been analyzed in this study. Practical implications: This study provides a reference for addressing research misconduct in Chinese university-affiliated hospitals. It is our recommendation that universities and hospitals should educate all their staff about the basic norms of research integrity, punish authors of scientific misconduct retracted papers, and reform the unreasonable evaluation system. Originality/value: Based on the analysis of retracted papers, this study further analyzes the characteristics of institutions of retracted papers, which may deepen the research on retracted papers and provide a new perspective to understand the retraction phenomenon. & amp; copy; 2023 Zi-han Yuan et al., published by Sciendo. Â© 2023 Sciendo. All rights reserved.","The number of retracted papers from Chinese university-affiliated hospitals is increasing, which has raised much concern. The aim of this study is to analyze the retracted papers from university-affiliated hospitals in mainland China from 2000 to 2021. Data for 1,031 retracted papers were identified from the Web of Science Core collection database. The information of the hospitals involved was obtained from their official websites. We analyzed the chronological changes, journal distribution, discipline distribution and retraction reasons for the retracted papers. The grade and geographic locations of the hospitals involved were explored as well. We found a rapid increase in the number of retracted papers, while the retraction time interval is decreasing. The main reasons for retraction are plagiarism/self-plagiarism (n=255), invalid data/images/conclusions (n=212), fake peer review (n=175) and honesty error(n=163). The disciplines are mainly distributed in oncology (n=320), pharmacology & amp; amp; pharmacy (n=198) and research & amp; amp; experimental medicine (n=166). About 43.8& #x00025; of the retracted papers were from hospitals affiliated with prestigious universities. This study fails to differentiate between retractions due to honest error and retractions due to research misconduct. We believe that there is a fundamental difference between honest error retractions and misconduct retractions. Another limitation is that authors of the retracted papers have not been analyzed in this study. This study provides a reference for addressing research misconduct in Chinese university-affiliated hospitals. It is our recommendation that universities and hospitals should educate all their staff about the basic norms of research integrity, punish authors of scientific misconduct retracted papers, and reform the unreasonable evaluation system. Based on the analysis of retracted papers, this study further analyzes the characteristics of institutions of retracted papers, which may deepen the research on retracted papers and provide a new perspective to understand the retraction phenomenon. & amp; copy; 2023 Zi-han Yuan et al., published by Sciendo."
General laws of funding for scientific citations: how citations change in funded and unfunded research between basic and applied sciences,"Purpose: The goal of this study is to analyze the relationship between funded and unfunded papers and their citations in both basic and applied sciences. Design/methodology/approach: A power law model analyzes the relationship between research funding and citations of papers using 831,337 documents recorded in the Web of Science database. Findings: The original results reveal general characteristics of the diffusion of science in research fields: a) Funded articles receive higher citations compared to unfunded papers in journals; b) Funded articles exhibit a super-linear growth in citations, surpassing the increase seen in unfunded articles. This finding reveals a higher diffusion of scientific knowledge in funded articles. Moreover, c) funded articles in both basic and applied sciences demonstrate a similar expected change in citations, equivalent to about 1.23%, when the number of funded papers increases by 1% in journals. This result suggests, for the first time, that funding effect of scientific research is an invariant driver, irrespective of the nature of the basic or applied sciences. Originality/value: This evidence suggests empirical laws of funding for scientific citations that explain the importance of robust funding mechanisms for achieving impactful research outcomes in science and society. These findings here also highlight that funding for scientific research is a critical driving force in supporting citations and the dissemination of scientific knowledge in recorded documents in both basic and applied sciences. Practical implications: This comprehensive result provides a holistic view of the relationship between funding and citation performance in science to guide policymakers and R&D managers with science policies by directing funding to research in promoting the scientific development and higher diffusion of results for the progress of human society. Â© 2024 Mario Coccia, Saeed Roshani.","The goal of this study is to analyze the relationship between funded and unfunded papers and their citations in both basic and applied sciences. A power law model analyzes the relationship between research funding and citations of papers using 831,337 documents recorded in the Web of Science database. The original results reveal general characteristics of the diffusion of science in research fields: a) Funded articles receive higher citations compared to unfunded papers in journals; b) Funded articles exhibit a super-linear growth in citations, surpassing the increase seen in unfunded articles. This finding reveals a higher diffusion of scientific knowledge in funded articles. Moreover, c) funded articles in both basic and applied sciences demonstrate a similar expected change in citations, equivalent to about 1.23%, when the number of funded papers increases by 1% in journals. This result suggests, for the first time, that funding effect of scientific research is an invariant driver, irrespective of the nature of the basic or applied sciences. This evidence suggests empirical laws of funding for scientific citations that explain the importance of robust funding mechanisms for achieving impactful research outcomes in science and society. These findings here also highlight that funding for scientific research is a critical driving force in supporting citations and the dissemination of scientific knowledge in recorded documents in both basic and applied sciences. This comprehensive result provides a holistic view of the relationship between funding and citation performance in science to guide policymakers and R&D managers with science policies by directing funding to research in promoting the scientific development and higher diffusion of results for the progress of human society."
Multimodal sentiment analysis for social media contents during public emergencies,"Purpose: Nowadays, public opinions during public emergencies involve not only textual contents but also contain images. However, the existing works mainly focus on textual contents and they do not provide a satisfactory accuracy of sentiment analysis, lacking the combination of multimodal contents. In this paper, we propose to combine texts and images generated in the social media to perform sentiment analysis. Design/methodology/approach: We propose a Deep Multimodal Fusion Model (DMFM), which combines textual and visual sentiment analysis. We first train word2vec model on a large-scale public emergency corpus to obtain semantic-rich word vectors as the input of textual sentiment analysis. BiLSTM is employed to generate encoded textual embeddings. To fully excavate visual information from images, a modified pretrained VGG16-based sentiment analysis network is used with the best-performed fine-tuning strategy. A multimodal fusion method is implemented to fuse textual and visual embeddings completely, producing predicted labels. Findings: We performed extensive experiments on Weibo and Twitter public emergency datasets, to evaluate the performance of our proposed model. Experimental results demonstrate that the DMFM provides higher accuracy compared with baseline models. The introduction of images can boost the performance of sentiment analysis during public emergencies. Research limitations: In the future, we will test our model in a wider dataset. We will also consider a better way to learn the multimodal fusion information. Practical implications: We build an efficient multimodal sentiment analysis model for the social media contents during public emergencies. Originality/value: We consider the images posted by online users during public emergencies on social platforms. The proposed method can present a novel scope for sentiment analysis during public emergencies and provide the decision support for the government when formulating policies in public emergencies.  Â© 2023 Tao Fan et al., published by Sciendo.","Nowadays, public opinions during public emergencies involve not only textual contents but also contain images. However, the existing works mainly focus on textual contents and they do not provide a satisfactory accuracy of sentiment analysis, lacking the combination of multimodal contents. In this paper, we propose to combine texts and images generated in the social media to perform sentiment analysis. We propose a Deep Multimodal Fusion Model (DMFM), which combines textual and visual sentiment analysis. We first train word2vec model on a large-scale public emergency corpus to obtain semantic-rich word vectors as the input of textual sentiment analysis. BiLSTM is employed to generate encoded textual embeddings. To fully excavate visual information from images, a modified pretrained VGG16-based sentiment analysis network is used with the best-performed fine-tuning strategy. A multimodal fusion method is implemented to fuse textual and visual embeddings completely, producing predicted labels. We performed extensive experiments on Weibo and Twitter public emergency datasets, to evaluate the performance of our proposed model. Experimental results demonstrate that the DMFM provides higher accuracy compared with baseline models. The introduction of images can boost the performance of sentiment analysis during public emergencies. In the future, we will test our model in a wider dataset. We will also consider a better way to learn the multimodal fusion information. We build an efficient multimodal sentiment analysis model for the social media contents during public emergencies. We consider the images posted by online users during public emergencies on social platforms. The proposed method can present a novel scope for sentiment analysis during public emergencies and provide the decision support for the government when formulating policies in public emergencies."
Navigating interdisciplinary research: Historical progression and contemporary challenges,"Interdisciplinary research plays a crucial role in addressing complex problems by integrating knowledge from multiple disciplines. This integration fosters innovative solutions and enhances understanding across various fields. This study explores the historical and sociological development of interdisciplinary research and maps its evolution through three distinct phases: pre-disciplinary, disciplinary, and post-disciplinary. It identifies key internal dynamics, such as disciplinary diversification, reorganization, and innovation, as primary drivers of this evolution. Additionally, this study highlights how external factors, particularly the urgency of World War II and the subsequent political and economic changes, have accelerated its advancement. The rise of interdisciplinary research has significantly reshaped traditional educational paradigms, promoting its integration across different educational levels. However, the inherent contradictions within interdisciplinary research present cognitive, emotional, and institutional challenges for researchers. Meanwhile, finding a balance between the breadth and depth of knowledge remains a critical challenge in interdisciplinary education.  Â© 2024 Xiaoqiang Li et al., published by Sciendo.","Interdisciplinary research plays a crucial role in addressing complex problems by integrating knowledge from multiple disciplines. This integration fosters innovative solutions and enhances understanding across various fields. This study explores the historical and sociological development of interdisciplinary research and maps its evolution through three distinct phases: pre-disciplinary, disciplinary, and post-disciplinary. It identifies key internal dynamics, such as disciplinary diversification, reorganization, and innovation, as primary drivers of this evolution. Additionally, this study highlights how external factors, particularly the urgency of World War II and the subsequent political and economic changes, have accelerated its advancement. The rise of interdisciplinary research has significantly reshaped traditional educational paradigms, promoting its integration across different educational levels. However, the inherent contradictions within interdisciplinary research present cognitive, emotional, and institutional challenges for researchers. Meanwhile, finding a balance between the breadth and depth of knowledge remains a critical challenge in interdisciplinary education."
A comparison of model choice strategies for logistic regression,"Purpose: The purpose of this study is to develop and compare model choice strategies in context of logistic regression. Model choice means the choice of the covariates to be included in the model. Design/methodology/approach: The study is based on Monte Carlo simulations. The methods are compared in terms of three measures of accuracy: specificity and two kinds of sensitivity. A loss function combining sensitivity and specificity is introduced and used for a final comparison. Findings: The choice of method depends on how much the users emphasize sensitivity against specificity. It also depends on the sample size. For a typical logistic regression setting with a moderate sample size and a small to moderate effect size, either BIC, BICc or Lasso seems to be optimal. Research limitations: Numerical simulations cannot cover the whole range of data-generating processes occurring with real-world data. Thus, more simulations are needed. Practical implications: Researchers can refer to these results if they believe that their data-generating process is somewhat similar to some of the scenarios presented in this paper. Alternatively, they could run their own simulations and calculate the loss function. Originality/value: This is a systematic comparison of model choice algorithms and heuristics in context of logistic regression. The distinction between two types of sensitivity and a comparison based on a loss function are methodological novelties. Copyright: Â© 2024 Markku Karhunen. Published under a Creative Commons Attribution 4.0 International (CC BY 4.0) license.","The purpose of this study is to develop and compare model choice strategies in context of logistic regression. Model choice means the choice of the covariates to be included in the model. The study is based on Monte Carlo simulations. The methods are compared in terms of three measures of accuracy: specificity and two kinds of sensitivity. A loss function combining sensitivity and specificity is introduced and used for a final comparison. The choice of method depends on how much the users emphasize sensitivity against specificity. It also depends on the sample size. For a typical logistic regression setting with a moderate sample size and a small to moderate effect size, either BIC, BICc or Lasso seems to be optimal. Numerical simulations cannot cover the whole range of data-generating processes occurring with real-world data. Thus, more simulations are needed. Researchers can refer to these results if they believe that their data-generating process is somewhat similar to some of the scenarios presented in this paper. Alternatively, they could run their own simulations and calculate the loss function. This is a systematic comparison of model choice algorithms and heuristics in context of logistic regression. The distinction between two types of sensitivity and a comparison based on a loss function are methodological novelties."
"An explorative study on document type assignment of review articles in Web of Science, Scopus and journalsâ websites","Purpose: Accurately assigning the document type of review articles in citation index databases like Web of Science(WoS) and Scopus is important. This study aims to investigate the document type assignation of review articles in Web of Science, Scopus and Publisherâs websites on a large scale. Design/methodology/approach: 27,616 papers from 160 journals from 10 review journal series indexed in SCI are analyzed. The document types of these papers labeled on journalsâ websites, and assigned by WoS and Scopus are retrieved and compared to determine the assigning accuracy and identify the possible reasons for wrongly assigning. For the document type labeled on the website, we further differentiate them into explicit review and implicit review based on whether the website directly indicates it is a review or not. Findings: Overall, WoS and Scopus performed similarly, with an average precision of about 99% and recall of about 80%. However, there were some differences between WoS and Scopus across different journal series and within the same journal series. The assigning accuracy of WoS and Scopus for implicit reviews dropped significantly, especially for Scopus. Research limitations: The document types we used as the gold standard were based on the journal websitesâ labeling which were not manually validated one by one. We only studied the labeling performance for review articles published during 2017-2018 in review journals. Whether this conclusion can be extended to review articles published in non-review journals and most current situation is not very clear. Practical implications: This study provides a reference for the accuracy of document type assigning of review articles in WoS and Scopus, and the identified pattern for assigning implicit reviews may be helpful to better labeling on websites, WoS and Scopus. Originality/value: This study investigated the assigning accuracy of document type of reviews and identified the some patterns of wrong assignments. Copyright: Â© 2024 Manman Zhu, Xinyue Lu, Fuyou Chen, Liying Yang, Zhesi Shen. Published under a Creative Commons Attribution 4.0 International (CC BY 4.0) license.","Accurately assigning the document type of review articles in citation index databases like Web of Science(WoS) and Scopus is important. This study aims to investigate the document type assignation of review articles in Web of Science, Scopus and Publishers websites on a large scale. 27,616 papers from 160 journals from 10 review journal series indexed in SCI are analyzed. The document types of these papers labeled on journals websites, and assigned by WoS and Scopus are retrieved and compared to determine the assigning accuracy and identify the possible reasons for wrongly assigning. For the document type labeled on the website, we further differentiate them into explicit review and implicit review based on whether the website directly indicates it is a review or not. Overall, WoS and Scopus performed similarly, with an average precision of about 99% and recall of about 80%. However, there were some differences between WoS and Scopus across different journal series and within the same journal series. The assigning accuracy of WoS and Scopus for implicit reviews dropped significantly, especially for Scopus. The document types we used as the gold standard were based on the journal websites labeling which were not manually validated one by one. We only studied the labeling performance for review articles published during 2017-2018 in review journals. Whether this conclusion can be extended to review articles published in non-review journals and most current situation is not very clear. This study provides a reference for the accuracy of document type assigning of review articles in WoS and Scopus, and the identified pattern for assigning implicit reviews may be helpful to better labeling on websites, WoS and Scopus. This study investigated the assigning accuracy of document type of reviews and identified the some patterns of wrong assignments."
Enhancing emerging technology discovery in nanomedicine by integrating innovative sentences using BERT and NLDA,"Purpose: Nanomedicine has significant potential to revolutionize biomedicine and healthcare through innovations in diagnostics, therapeutics, and regenerative medicine. This study aims to develop a novel framework that integrates advanced natural language processing, noise-free topic modeling, and multidimensional bibliometrics to systematically identify emerging nanomedicine technology topics from scientific literature. Design/methodology/approach: The framework involves collecting full-text articles from PubMed Central and nanomedicine-related metrics from the Web of Science for the period 2013â2023. A fine-tuned BERT model is employed to extract key informative sentences. Noiseless Latent Dirichlet Allocation (NLDA) is applied to model interpretable topics from the cleaned corpus. Additionally, we develop and apply metrics for novelty, innovation, growth, impact, and intensity to quantify the emergence of novel technological topics. Findings: By applying this methodology to nanomedical publications, we identify an increasing emphasis on research aligned with global health priorities, particularly inflammation and biomaterial interactions in disease research. This methodology provides deeper insights through full-text analysis and leading to a more robust discovery of emerging technologies. Research limitations: One limitation of this study is its reliance on the existing scientific literature, which may introduce publication biases and language constraints. Additionally, manual annotation of the dataset, while thorough, is subject to subjectivity and can be time-consuming. Future research could address these limitations by incorporating more diverse data sources, and automating the annotation process. Practical implications: The methodology presented can be adapted to explore emerging technologies in other scientific domains. It allows for tailored assessment criteria based on specific contexts and objectives, enabling more precise analysis and decision-making in various fields. Originality/value: This study offers a comprehensive framework for identifying emerging technologies in nanomedicine, combining theoretical insights and practical applications. Its potential for adaptation across scientific disciplines enhances its value for future research and decision-making in technology discovery. Â© 2024 Sciendo. All rights reserved.","Nanomedicine has significant potential to revolutionize biomedicine and healthcare through innovations in diagnostics, therapeutics, and regenerative medicine. This study aims to develop a novel framework that integrates advanced natural language processing, noise-free topic modeling, and multidimensional bibliometrics to systematically identify emerging nanomedicine technology topics from scientific literature. The framework involves collecting full-text articles from PubMed Central and nanomedicine-related metrics from the Web of Science for the period 20132023. A fine-tuned BERT model is employed to extract key informative sentences. Noiseless Latent Dirichlet Allocation (NLDA) is applied to model interpretable topics from the cleaned corpus. Additionally, we develop and apply metrics for novelty, innovation, growth, impact, and intensity to quantify the emergence of novel technological topics. By applying this methodology to nanomedical publications, we identify an increasing emphasis on research aligned with global health priorities, particularly inflammation and biomaterial interactions in disease research. This methodology provides deeper insights through full-text analysis and leading to a more robust discovery of emerging technologies. One limitation of this study is its reliance on the existing scientific literature, which may introduce publication biases and language constraints. Additionally, manual annotation of the dataset, while thorough, is subject to subjectivity and can be time-consuming. Future research could address these limitations by incorporating more diverse data sources, and automating the annotation process. The methodology presented can be adapted to explore emerging technologies in other scientific domains. It allows for tailored assessment criteria based on specific contexts and objectives, enabling more precise analysis and decision-making in various fields. This study offers a comprehensive framework for identifying emerging technologies in nanomedicine, combining theoretical insights and practical applications. Its potential for adaptation across scientific disciplines enhances its value for future research and decision-making in technology discovery."
Amend: An integrated platform of retracted papers and concerned papers,"Purpose: The notable increase in retraction papers has attracted considerable attention from diverse stakeholders. Various sources are now offering information related to research integrity, including concerns voiced on social media, disclosed lists of paper mills, and retraction notices accessible through journal websites. However, despite the availability of such resources, there remains a lack of a unified platform to consolidate this information, thereby hindering efficient searching and cross-referencing. Thus, it is imperative to develop a comprehensive platform for retracted papers and related concerns. This article aims to introduce ""Amend,""a platform designed to integrate information on research integrity from diverse sources. Design/methodology/approach: The Amend platform consolidates concerns and lists of problematic articles sourced from social media platforms (e.g., PubPeer, For Better Science), retraction notices from journal websites, and citation databases (e.g., Web of Science, CrossRef). Moreover, Amend includes investigation and punishment announcements released by administrative agencies (e.g., NSFC, MOE, MOST, CAS). Each related paper is marked and can be traced back to its information source via a provided link. Furthermore, the Amend database incorporates various attributes of retracted articles, including citation topics, funding details, open access status, and more. The reasons for retraction are identified and classified as either academic misconduct or honest errors, with detailed subcategories provided for further clarity. Findings: Within the Amend platform, a total of 32,515 retracted papers indexed in SCI, SSCI, and ESCI between 1980 and 2023 were identified. Of these, 26,620 (81.87%) were associated with academic misconduct. The retraction rate stands at 6.64 per 10,000 articles. Notably, the retraction rate for non-gold open access articles significantly differs from that for gold open access articles, with this disparity progressively widening over the years. Furthermore, the reasons for retractions have shifted from traditional individual behaviors like falsification, fabrication, plagiarism, and duplication to more organized large-scale fraudulent practices, including Paper Mills, Fake Peer-review, and Artificial Intelligence Generated Content (AIGC). Research limitations: The Amend platform may not fully capture all retracted and concerning papers, thereby impacting its comprehensiveness. Additionally, inaccuracies in retraction notices may lead to errors in tagged reasons. Practical implications: Amend provides an integrated platform for stakeholders to enhance monitoring, analysis, and research on academic misconduct issues. Ultimately, the Amend database can contribute to upholding scientific integrity. Originality/value: This study introduces a globally integrated platform for retracted and concerning papers, along with a preliminary analysis of the evolutionary trends in retracted papers.  Â© 2024 Menghui Li et al., published by Sciendo.","The notable increase in retraction papers has attracted considerable attention from diverse stakeholders. Various sources are now offering information related to research integrity, including concerns voiced on social media, disclosed lists of paper mills, and retraction notices accessible through journal websites. However, despite the availability of such resources, there remains a lack of a unified platform to consolidate this information, thereby hindering efficient searching and cross-referencing. Thus, it is imperative to develop a comprehensive platform for retracted papers and related concerns. This article aims to introduce ""Amend,""a platform designed to integrate information on research integrity from diverse sources. The Amend platform consolidates concerns and lists of problematic articles sourced from social media platforms (, PubPeer, For Better Science), retraction notices from journal websites, and citation databases (, Web of Science, CrossRef). Moreover, Amend includes investigation and punishment announcements released by administrative agencies (, NSFC, MOE, MOST, CAS). Each related paper is marked and can be traced back to its information source via a provided link. Furthermore, the Amend database incorporates various attributes of retracted articles, including citation topics, funding details, open access status, and more. The reasons for retraction are identified and classified as either academic misconduct or honest errors, with detailed subcategories provided for further clarity. Within the Amend platform, a total of 32,515 retracted papers indexed in SCI, SSCI, and ESCI between 1980 and 2023 were identified. Of these, 26,620 (81.87%) were associated with academic misconduct. The retraction rate stands at 6.64 per 10,000 articles. Notably, the retraction rate for non-gold open access articles significantly differs from that for gold open access articles, with this disparity progressively widening over the years. Furthermore, the reasons for retractions have shifted from traditional individual behaviors like falsification, fabrication, plagiarism, and duplication to more organized large-scale fraudulent practices, including Paper Mills, Fake Peer-review, and Artificial Intelligence Generated Content (AIGC). The Amend platform may not fully capture all retracted and concerning papers, thereby impacting its comprehensiveness. Additionally, inaccuracies in retraction notices may lead to errors in tagged reasons. Amend provides an integrated platform for stakeholders to enhance monitoring, analysis, and research on academic misconduct issues. Ultimately, the Amend database can contribute to upholding scientific integrity. This study introduces a globally integrated platform for retracted and concerning papers, along with a preliminary analysis of the evolutionary trends in retracted papers."
Academic Collaborator Recommendation Based on Attributed Network Embedding,"Abstract Purpose Based on real-world academic data, this study aims to use network embedding technology to mining academic relationships, and investigate the effectiveness of the proposed embedding model on academic collaborator recommendation tasks. Design/methodology/approach We propose an academic collaborator recommendation model based on attributed network embedding (ACR-ANE), which can get enhanced scholar embedding and take full advantage of the topological structure of the network and multi-type scholar attributes. The non-local neighbors for scholars are defined to capture strong relationships among scholars. A deep auto-encoder is adopted to encode the academic collaboration network structure and scholar attributes into a low-dimensional representation space. Findings 1. The proposed non-local neighbors can better describe the relationships among scholars in the real world than the first-order neighbors. 2. It is important to consider the structure of the academic collaboration network and scholar attributes when recommending collaborators for scholars simultaneously. Research limitations The designed method works for static networks, without taking account of the network dynamics. Practical implications The designed model is embedded in academic collaboration network structure and scholarly attributes, which can be used to help scholars recommend potential collaborators. Originality/value Experiments on two real-world scholarly datasets, Aminer and APS, show that our proposed method performs better than other baselines. Â© 2022 Ouxia Du et al., published by Sciendo","Abstract Purpose Based on real-world academic data, this study aims to use network embedding technology to mining academic relationships, and investigate the effectiveness of the proposed embedding model on academic collaborator recommendation tasks. Design/methodology/approach We propose an academic collaborator recommendation model based on attributed network embedding (ACR-ANE), which can get enhanced scholar embedding and take full advantage of the topological structure of the network and multi-type scholar attributes. The non-local neighbors for scholars are defined to capture strong relationships among scholars. A deep auto-encoder is adopted to encode the academic collaboration network structure and scholar attributes into a low-dimensional representation space. Findings 1. The proposed non-local neighbors can better describe the relationships among scholars in the real world than the first-order neighbors. 2. It is important to consider the structure of the academic collaboration network and scholar attributes when recommending collaborators for scholars simultaneously. Research limitations The designed method works for static networks, without taking account of the network dynamics. Practical implications The designed model is embedded in academic collaboration network structure and scholarly attributes, which can be used to help scholars recommend potential collaborators. Originality/value Experiments on two real-world scholarly datasets, Aminer and APS, show that our proposed method performs better than other baselines."
Public Reaction to Scientific Research via Twitter Sentiment Prediction,"Purpose: Social media users share their ideas, thoughts, and emotions with other users. However, it is not clear how online users would respond to new research outcomes. This study aims to predict the nature of the emotions expressed by Twitter users toward scientific publications. Additionally, we investigate what features of the research articles help in such prediction. Identifying the sentiments of research articles on social media will help scientists gauge a new societal impact of their research articles. Design/methodology/approach: Several tools are used for sentiment analysis, so we applied five sentiment analysis tools to check which are suitable for capturing a tweet's sentiment value and decided to use NLTK VADER and TextBlob. We segregated the sentiment value into negative, positive, and neutral. We measure the mean and median of tweets' sentiment value for research articles with more than one tweet. We next built machine learning models to predict the sentiments of tweets related to scientific publications and investigated the essential features that controlled the prediction models. Findings: We found that the most important feature in all the models was the sentiment of the research article title followed by the author count. We observed that the tree-based models performed better than other classification models, with Random Forest achieving 89% accuracy for binary classification and 73% accuracy for three-label classification. Research limitations: In this research, we used state-of-the-art sentiment analysis libraries. However, these libraries might vary at times in their sentiment prediction behavior. Tweet sentiment may be influenced by a multitude of circumstances and is not always immediately tied to the paper's details. In the future, we intend to broaden the scope of our research by employing word2vec models. Practical implications: Many studies have focused on understanding the impact of science on scientists or how science communicators can improve their outcomes. Research in this area has relied on fewer and more limited measures, such as citations and user studies with small datasets. There is currently a critical need to find novel methods to quantify and evaluate the broader impact of research. This study will help scientists better comprehend the emotional impact of their work. Additionally, the value of understanding the public's interest and reactions helps science communicators identify effective ways to engage with the public and build positive connections between scientific communities and the public. Originality/value: This study will extend work on public engagement with science, sociology of science, and computational social science. It will enable researchers to identify areas in which there is a gap between public and expert understanding and provide strategies by which this gap can be bridged.  Â© 2022 Murtuza Shahzad et al., published by Sciendo.","Social media users share their ideas, thoughts, and emotions with other users. However, it is not clear how online users would respond to new research outcomes. This study aims to predict the nature of the emotions expressed by Twitter users toward scientific publications. Additionally, we investigate what features of the research articles help in such prediction. Identifying the sentiments of research articles on social media will help scientists gauge a new societal impact of their research articles. Several tools are used for sentiment analysis, so we applied five sentiment analysis tools to check which are suitable for capturing a tweet's sentiment value and decided to use NLTK VADER and TextBlob. We segregated the sentiment value into negative, positive, and neutral. We measure the mean and median of tweets' sentiment value for research articles with more than one tweet. We next built machine learning models to predict the sentiments of tweets related to scientific publications and investigated the essential features that controlled the prediction models. We found that the most important feature in all the models was the sentiment of the research article title followed by the author count. We observed that the tree-based models performed better than other classification models, with Random Forest achieving 89% accuracy for binary classification and 73% accuracy for three-label classification. In this research, we used state-of-the-art sentiment analysis libraries. However, these libraries might vary at times in their sentiment prediction behavior. Tweet sentiment may be influenced by a multitude of circumstances and is not always immediately tied to the paper's details. In the future, we intend to broaden the scope of our research by employing word2vec models. Many studies have focused on understanding the impact of science on scientists or how science communicators can improve their outcomes. Research in this area has relied on fewer and more limited measures, such as citations and user studies with small datasets. There is currently a critical need to find novel methods to quantify and evaluate the broader impact of research. This study will help scientists better comprehend the emotional impact of their work. Additionally, the value of understanding the public's interest and reactions helps science communicators identify effective ways to engage with the public and build positive connections between scientific communities and the public. This study will extend work on public engagement with science, sociology of science, and computational social science. It will enable researchers to identify areas in which there is a gap between public and expert understanding and provide strategies by which this gap can be bridged."
Regression discontinuity design and its applications to Science of Science: A survey,"Purpose: With the availability of large-scale scholarly datasets, scientists from various domains hope to understand the underlying mechanisms behind science, forming a vibrant area of inquiry in the emerging ""science of science""field. As the results from the science of science often has strong policy implications, understanding the causal relationships between variables becomes prominent. However, the most credible quasi-experimental method among all causal inference methods, and a highly valuable tool in the empirical toolkit, Regression Discontinuity Design (RDD) has not been fully exploited in the field of science of science. In this paper, we provide a systematic survey of the RDD method, and its practical applications in the science of science. Design/methodology/approach: First, we introduce the basic assumptions, mathematical notations, and two types of RDD, i.e., sharp and fuzzy RDD. Second, we use the Web of Science and the Microsoft Academic Graph datasets to study the evolution and citation patterns of RDD papers. Moreover, we provide a systematic survey of the applications of RDD methodologies in various scientific domains, as well as in the science of science. Finally, we demonstrate a case study to estimate the effect of Head Start Funding Proposals on child mortality. Findings: RDD was almost neglected for 30 years after it was first introduced in 1960. Afterward, scientists used mathematical and economic tools to develop the RDD methodology. After 2010, RDD methods showed strong applications in various domains, including medicine, psychology, political science and environmental science. However, we also notice that the RDD method has not been well developed in science of science research. Research Limitations: This work uses a keyword search to obtain RDD papers, which may neglect some related work. Additionally, our work does not aim to develop rigorous mathematical and technical details of RDD but rather focuses on its intuitions and applications. Practical implications: This work proposes how to use the RDD method in science of science research. Originality/value: This work systematically introduces the RDD, and calls for the awareness of using such a method in the field of science of science.  Â© 2023 Meiling Li et al., published by Sciendo.","With the availability of large-scale scholarly datasets, scientists from various domains hope to understand the underlying mechanisms behind science, forming a vibrant area of inquiry in the emerging ""science of science""field. As the results from the science of science often has strong policy implications, understanding the causal relationships between variables becomes prominent. However, the most credible quasi-experimental method among all causal inference methods, and a highly valuable tool in the empirical toolkit, Regression Discontinuity Design (RDD) has not been fully exploited in the field of science of science. In this paper, we provide a systematic survey of the RDD method, and its practical applications in the science of science. First, we introduce the basic assumptions, mathematical notations, and two types of RDD, , sharp and fuzzy RDD. Second, we use the Web of Science and the Microsoft Academic Graph datasets to study the evolution and citation patterns of RDD papers. Moreover, we provide a systematic survey of the applications of RDD methodologies in various scientific domains, as well as in the science of science. Finally, we demonstrate a case study to estimate the effect of Head Start Funding Proposals on child mortality. RDD was almost neglected for 30 years after it was first introduced in 1960. Afterward, scientists used mathematical and economic tools to develop the RDD methodology. After 2010, RDD methods showed strong applications in various domains, including medicine, psychology, political science and environmental science. However, we also notice that the RDD method has not been well developed in science of science research. This work uses a keyword search to obtain RDD papers, which may neglect some related work. Additionally, our work does not aim to develop rigorous mathematical and technical details of RDD but rather focuses on its intuitions and applications. This work proposes how to use the RDD method in science of science research. This work systematically introduces the RDD, and calls for the awareness of using such a method in the field of science of science."
Parameterless Pruning Algorithms for Similarity-Weight Network and Its Application in Extracting the Backbone of Global Value Chain,"Purpose: With the availability and utilization of Inter-Country Input-Output (ICIO) tables, it is possible to construct quantitative indices to assess its impact on the Global Value Chain (GVC). For the sake of visualization, ICIO networks with tremendous low- weight edges are too dense to show the substantial structure. These redundant edges, inevitably make the network data full of noise and eventually exert negative effects on Social Network Analysis (SNA). In this case, we need a method to filter such edges and obtain a sparser network with only the meaningful connections. Design/methodology/approach: In this paper, we propose two parameterless pruning algorithms from the global and local perspectives respectively, then the performance of them is examined using the ICIO table from different databases. Findings: The Searching Paths (SP) method extracts the strongest association paths from the global perspective, while Filtering Edges (FE) method captures the key links according to the local weight ratio. The results show that the FE method can basically include the SP method and become the best solution for the ICIO networks. Research limitations: There are still two limitations in this research. One is that the computational complexity may increase rapidly while processing the large-scale networks, so the proposed method should be further improved. The other is that much more empirical networks should be introduced to testify the scientificity and practicability of our methodology. Practical implications: The network pruning methods we proposed will promote the analysis of the ICIO network, in terms of community detection, link prediction, and spatial econometrics, etc. Also, they can be applied to many other complex networks with similar characteristics. Originality/value: This paper improves the existing research from two aspects, namely, considering the heterogeneity of weights and avoiding the interference of parameters. Therefore, it provides a new idea for the research of network backbone extraction.  Â© 2022 Lizhi Xing et al., published by Sciendo.","With the availability and utilization of Inter-Country Input-Output (ICIO) tables, it is possible to construct quantitative indices to assess its impact on the Global Value Chain (GVC). For the sake of visualization, ICIO networks with tremendous low- weight edges are too dense to show the substantial structure. These redundant edges, inevitably make the network data full of noise and eventually exert negative effects on Social Network Analysis (SNA). In this case, we need a method to filter such edges and obtain a sparser network with only the meaningful connections. In this paper, we propose two parameterless pruning algorithms from the global and local perspectives respectively, then the performance of them is examined using the ICIO table from different databases. The Searching Paths (SP) method extracts the strongest association paths from the global perspective, while Filtering Edges (FE) method captures the key links according to the local weight ratio. The results show that the FE method can basically include the SP method and become the best solution for the ICIO networks. There are still two limitations in this research. One is that the computational complexity may increase rapidly while processing the large-scale networks, so the proposed method should be further improved. The other is that much more empirical networks should be introduced to testify the scientificity and practicability of our methodology. The network pruning methods we proposed will promote the analysis of the ICIO network, in terms of community detection, link prediction, and spatial econometrics, etc. Also, they can be applied to many other complex networks with similar characteristics. This paper improves the existing research from two aspects, namely, considering the heterogeneity of weights and avoiding the interference of parameters. Therefore, it provides a new idea for the research of network backbone extraction."
A quantitative study of disruptive technology policy texts: An example of China's artificial intelligence policy,"Purpose: The transformative impact of disruptive technologies on the restructuring of the times has attracted widespread global attention. This study aims to analyze the characteristics and shortcomings of China's artificial intelligence (AI) disruptive technology policy, and to put forward suggestions for optimizing China's AI disruptive technology policy. Design/methodology/approach: Develop a three-dimensional analytical framework for ""policy tools-policy actors-policy themes""and apply policy tools, social network analysis, and LDA topic model to conduct a comprehensive analysis of the utilization of policy tools, cooperative relationships among policy actors, and the trends in policy theme settings within China's innovative AI technology policy. Findings: We find that the collaborative relationship among the policy actors of AI disruptive technology in China is insufficiently close. Marginal subjects exhibit low participation in the cooperation network and overly rely on central subjects, forming a ""center-periphery""network structure. Policy tool usage is predominantly focused on supply and environmental types, with a severe inadequacy in demand-side policy tool utilization. Policy themes are diverse, encompassing topics such as ""Intelligent Services""""Talent Cultivation""""Information Security""and ""Technological Innovation"", which will remain focal points. Under the themes of ""Intelligent Services""and ""Intelligent Governance"", policy tool usage is relatively balanced, with close collaboration among policy entities. However, the theme of ""AI Theoretical System""lacks a comprehensive understanding of tool usage and necessitates enhanced cooperation with other policy entities. Research limitations: The data sources and experimental scope are subject to certain limitations, potentially introducing biases and imperfections into the research results, necessitating further validation and refinement. Practical implications: The study introduces a three-dimensional analysis framework for disruptive technology policy texts, which is significant for formulating and enhancing disruptive technology policies. Originality/value: This study utilizes text mining and content analysis techniques to quantitatively analyze disruptive technology policy texts. It systematically evaluates China's AI policies quantitatively, focusing on policy tools, policy actors, policy themes. The study uncovers the characteristics and deficiencies of current AI policies, offering recommendations for formulating and enhancing disruptive technology policies.  Â© 2024 Ying Zhou et al., published by Sciendo.","The transformative impact of disruptive technologies on the restructuring of the times has attracted widespread global attention. This study aims to analyze the characteristics and shortcomings of China's artificial intelligence (AI) disruptive technology policy, and to put forward suggestions for optimizing China's AI disruptive technology policy. Develop a three-dimensional analytical framework for ""policy tools-policy actors-policy themes""and apply policy tools, social network analysis, and LDA topic model to conduct a comprehensive analysis of the utilization of policy tools, cooperative relationships among policy actors, and the trends in policy theme settings within China's innovative AI technology policy. We find that the collaborative relationship among the policy actors of AI disruptive technology in China is insufficiently close. Marginal subjects exhibit low participation in the cooperation network and overly rely on central subjects, forming a ""center-periphery""network structure. Policy tool usage is predominantly focused on supply and environmental types, with a severe inadequacy in demand-side policy tool utilization. Policy themes are diverse, encompassing topics such as ""Intelligent Services""""Talent Cultivation""""Information Security""and ""Technological Innovation"", which will remain focal points. Under the themes of ""Intelligent Services""and ""Intelligent Governance"", policy tool usage is relatively balanced, with close collaboration among policy entities. However, the theme of ""AI Theoretical System""lacks a comprehensive understanding of tool usage and necessitates enhanced cooperation with other policy entities. The data sources and experimental scope are subject to certain limitations, potentially introducing biases and imperfections into the research results, necessitating further validation and refinement. The study introduces a three-dimensional analysis framework for disruptive technology policy texts, which is significant for formulating and enhancing disruptive technology policies. This study utilizes text mining and content analysis techniques to quantitatively analyze disruptive technology policy texts. It systematically evaluates China's AI policies quantitatively, focusing on policy tools, policy actors, policy themes. The study uncovers the characteristics and deficiencies of current AI policies, offering recommendations for formulating and enhancing disruptive technology policies."
The notion of dominant terminology in bibliometric research,"In this opinion paper, we introduce the expressions of dominant terminology and dominant term in the quantitative studies of science in analogy to the notion of dominant design in product development and innovation. & copy; 2023 Yves Fassin et al., published by Sciendo. Â© 2023 Sciendo. All rights reserved.","In this opinion paper, we introduce the expressions of dominant terminology and dominant term in the quantitative studies of science in analogy to the notion of dominant design in product development and innovation. & copy; 2023 Yves Fassin et al., published by Sciendo."
Scientific Value Weights more than Being Open or Toll Access: An analysis of the OA advantage in Nature and Science,"Purpose: We attempt to find out whether OA or TA really affects the dissemination of scientific discoveries. Design/methodology/approach: We design the indicators, hot-degree, and R-index to indicate a topic OA or TA advantages. First, according to the OA classification of the Web of Science (WoS), we collect data from the WoS by downloading OA and TA articles, letters, and reviews published in Nature and Science during 2010-2019. These papers are divided into three broad disciplines, namely biomedicine, physics, and others. Then, taking a discipline in a journal and using the classical Latent Dirichlet Allocation (LDA) to cluster 100 topics of OA and TA papers respectively, we apply the Pearson correlation coefficient to match the topics of OA and TA, and calculate the hot-degree and R-index of every OA-TA topic pair. Finally, characteristics of the discipline can be presented. In qualitative comparison, we choose some high-quality papers which belong to Nature remarkable papers or Science breakthroughs, and analyze the relations between OA/TA and citation numbers. Findings: The result shows that OA hot-degree in biomedicine is significantly greater than that of TA, but significantly less than that of TA in physics. Based on the R-index, it is found that OA advantages exist in biomedicine and TA advantages do in physics. Therefore, the dissemination of average scientific discoveries in all fields is not necessarily affected by OA or TA. However, OA promotes the spread of important scientific discoveries in high-quality papers. Research limitations: We lost some citations by ignoring other open sources such as arXiv and bioArxiv. Another limitation came from that Nature employs some strong measures for access-promoting subscription-based articles, on which the boundary between OA and TA became fuzzy. Practical implications: It is useful to select hot topics in a set of publications by the hot-degree index. The finding comprehensively reflects the differences of OA and TA in different disciplines, which is a useful reference when researchers choose the publishing way as OA or TA. Originality/value: We propose a new method, including two indicators, to explore and measure OA or TA advantages.  Â© 2021 Howell Y. Wang et al., published by Sciendo.","We attempt to find out whether OA or TA really affects the dissemination of scientific discoveries. We design the indicators, hot-degree, and R-index to indicate a topic OA or TA advantages. First, according to the OA classification of the Web of Science (WoS), we collect data from the WoS by downloading OA and TA articles, letters, and reviews published in Nature and Science during 2010-2019. These papers are divided into three broad disciplines, namely biomedicine, physics, and others. Then, taking a discipline in a journal and using the classical Latent Dirichlet Allocation (LDA) to cluster 100 topics of OA and TA papers respectively, we apply the Pearson correlation coefficient to match the topics of OA and TA, and calculate the hot-degree and R-index of every OA-TA topic pair. Finally, characteristics of the discipline can be presented. In qualitative comparison, we choose some high-quality papers which belong to Nature remarkable papers or Science breakthroughs, and analyze the relations between OA/TA and citation numbers. The result shows that OA hot-degree in biomedicine is significantly greater than that of TA, but significantly less than that of TA in physics. Based on the R-index, it is found that OA advantages exist in biomedicine and TA advantages do in physics. Therefore, the dissemination of average scientific discoveries in all fields is not necessarily affected by OA or TA. However, OA promotes the spread of important scientific discoveries in high-quality papers. We lost some citations by ignoring other open sources such as arXiv and bioArxiv. Another limitation came from that Nature employs some strong measures for access-promoting subscription-based articles, on which the boundary between OA and TA became fuzzy. It is useful to select hot topics in a set of publications by the hot-degree index. The finding comprehensively reflects the differences of OA and TA in different disciplines, which is a useful reference when researchers choose the publishing way as OA or TA. We propose a new method, including two indicators, to explore and measure OA or TA advantages."
Identifying multidisciplinary problems from scientific publications based on a text generation method,"Purpose: A text generation based multidisciplinary problem identification method is proposed, which does not rely on a large amount of data annotation. Design/methodology/approach: The proposed method first identifies the research objective types and disciplinary labels of papers using a text classification technique; second, it generates abstractive titles for each paper based on abstract and research objective types using a generative pre-trained language model; third, it extracts problem phrases from generated titles according to regular expression rules; fourth, it creates problem relation networks and identifies the same problems by exploiting a weighted community detection algorithm; finally, it identifies multidisciplinary problems based on the disciplinary labels of papers. Findings: Experiments in the ""Carbon Peaking and Carbon Neutrality""field show that the proposed method can effectively identify multidisciplinary research problems. The disciplinary distribution of the identified problems is consistent with our understanding of multidisciplinary collaboration in the field. Research limitations: It is necessary to use the proposed method in other multidisciplinary fields to validate its effectiveness. Practical implications: Multidisciplinary problem identification helps to gather multidisciplinary forces to solve complex real-world problems for the governments, fund valuable multidisciplinary problems for research management authorities, and borrow ideas from other disciplines for researchers. Originality/value: This approach proposes a novel multidisciplinary problem identification method based on text generation, which identifies multidisciplinary problems based on generative abstractive titles of papers without data annotation required by standard sequence labeling techniques.  Â© 2024 Ziyan Xu et al., published by Sciendo.","A text generation based multidisciplinary problem identification method is proposed, which does not rely on a large amount of data annotation. The proposed method first identifies the research objective types and disciplinary labels of papers using a text classification technique; second, it generates abstractive titles for each paper based on abstract and research objective types using a generative pre-trained language model; third, it extracts problem phrases from generated titles according to regular expression rules; fourth, it creates problem relation networks and identifies the same problems by exploiting a weighted community detection algorithm; finally, it identifies multidisciplinary problems based on the disciplinary labels of papers. Experiments in the ""Carbon Peaking and Carbon Neutrality""field show that the proposed method can effectively identify multidisciplinary research problems. The disciplinary distribution of the identified problems is consistent with our understanding of multidisciplinary collaboration in the field. It is necessary to use the proposed method in other multidisciplinary fields to validate its effectiveness. Multidisciplinary problem identification helps to gather multidisciplinary forces to solve complex real-world problems for the governments, fund valuable multidisciplinary problems for research management authorities, and borrow ideas from other disciplines for researchers. This approach proposes a novel multidisciplinary problem identification method based on text generation, which identifies multidisciplinary problems based on generative abstractive titles of papers without data annotation required by standard sequence labeling techniques."
Does Success Breed Success? A Study on the Correlation between Impact Factor and Quantity in Chinese Academic Journals,"Purpose: This paper studies the relationship between the impact factor (IF) and the number of journal papers in Chinese publishing system. Design/methodology/approach: The method proposed by Huang (2016) is used whereas to analysis the data of Chinese journals in this study. Findings: Based on the analysis, we find the following. (1) The average impact factor (AIF) of journals in all disciplines maintained a growth trend from 2007 to 2017. Whether before or after removing outlier journals that may garner publication fees, the IF and its growth rate for most social sciences disciplines are larger than those of most natural sciences disciplines, and the number of journal papers on social sciences disciplines decreased while that of natural sciences disciplines increased from 2007 to 2017. (2) The removal of outlier journals has a greater impact on the relationship between the IF and the number of journal papers in some disciplines such as Geosciences because there may be journals that publish many papers to garner publication fees. (3) The success-breeds-success (SBS) principle is applicable in Chinese journals on natural sciences disciplines but not in Chinese journals on social sciences disciplines, and the relationship is the reverse of the SBS principle in Economics and Education & Educational Research. (4) Based on interviews and surveys, the difference in the relationship between the IF and the number of journal papers for Chinese natural sciences disciplines and Chinese social sciences disciplines may be due to the influence of the international publishing system. Chinese natural sciences journals are losing their academic power while Chinese social sciences journals that are less influenced by the international publishing system are in fierce competition. Research limitation: More implications could be found if long-term tracking and comparing the international publishing system with Chinese publishing system are taken. Practical implications: It is suggested that researchers from different countries study natural science and social sciences journals in their languages and observe the influence of the international publishing system. Originality/value: This paper presents an overview of the relationship between IF and the number of journal papers in Chinese publishing system from 2007 to 2017, provides insights into the relationship in different disciplines in Chinese publishing system, and points out the similarities and differences between Chinese publishing system and international publishing system.  Â© 2021 Kun Chen et al., published by Sciendo.","This paper studies the relationship between the impact factor (IF) and the number of journal papers in Chinese publishing system. The method proposed by Huang is used whereas to analysis the data of Chinese journals in this study. Based on the analysis, we find the following. The average impact factor (AIF) of journals in all disciplines maintained a growth trend from 2007 to 2017. Whether before or after removing outlier journals that may garner publication fees, the IF and its growth rate for most social sciences disciplines are larger than those of most natural sciences disciplines, and the number of journal papers on social sciences disciplines decreased while that of natural sciences disciplines increased from 2007 to 2017. The removal of outlier journals has a greater impact on the relationship between the IF and the number of journal papers in some disciplines such as Geosciences because there may be journals that publish many papers to garner publication fees. The success-breeds-success (SBS) principle is applicable in Chinese journals on natural sciences disciplines but not in Chinese journals on social sciences disciplines, and the relationship is the reverse of the SBS principle in Economics and Education & Educational Research. Based on interviews and surveys, the difference in the relationship between the IF and the number of journal papers for Chinese natural sciences disciplines and Chinese social sciences disciplines may be due to the influence of the international publishing system. Chinese natural sciences journals are losing their academic power while Chinese social sciences journals that are less influenced by the international publishing system are in fierce competition. Research limitation: More implications could be found if long-term tracking and comparing the international publishing system with Chinese publishing system are taken. It is suggested that researchers from different countries study natural science and social sciences journals in their languages and observe the influence of the international publishing system. This paper presents an overview of the relationship between IF and the number of journal papers in Chinese publishing system from 2007 to 2017, provides insights into the relationship in different disciplines in Chinese publishing system, and points out the similarities and differences between Chinese publishing system and international publishing system."
A comparative study on characteristics of retracted publications across different open access levels,"Purpose: Recently, global science has shown an increasing open trend, however, the characteristics of research integrity of open access (OA) publications have rarely been studied. The aim of this study is to compare the characteristics of retracted articles across different OA levels and discover whether OA level influences the characteristics of retracted articles. Design/methodology/approach: The research conducted an analysis of 6,005 retracted publications between 2001 and 2020 from the Web of Science and Retraction Watch databases. These publications were categorized based on their OA levels, including Gold OA, Green OA, and non-OA. The study explored retraction rates, time lags and reasons within these categories. Findings: The findings of this research revealed distinct patterns in retraction rates among different OA levels. Publications with Gold OA demonstrated the highest retraction rate, followed by Green OA and non-OA. A comparison of retraction reasons between Gold OA and non-OA categories indicated similar proportions, while Green OA exhibited a higher proportion due to falsification and manipulation issues, along with a lower occurrence of plagiarism and authorship issues. The retraction time lag was shortest for Gold OA, followed by non-OA, and longest for Green OA. The prolonged retraction time for Green OA could be attributed to an atypical distribution of retraction reasons. Research limitations: There is no exploration of a wider range of OA levels, such as Hybrid OA and Bronze OA. Practical implications: The outcomes of this study suggest the need for increased attention to research integrity within the OA publications. The occurrences of falsification, manipulation, and ethical concerns within Green OA publications warrant attention from the scientific community. Originality/value: This study contributes to the understanding of research integrity in the realm of OA publications, shedding light on retraction patterns and reasons across different OA levels.  Â© 2024 Er-Te Zheng et al., published by Sciendo.","Recently, global science has shown an increasing open trend, however, the characteristics of research integrity of open access (OA) publications have rarely been studied. The aim of this study is to compare the characteristics of retracted articles across different OA levels and discover whether OA level influences the characteristics of retracted articles. The research conducted an analysis of 6,005 retracted publications between 2001 and 2020 from the Web of Science and Retraction Watch databases. These publications were categorized based on their OA levels, including Gold OA, Green OA, and non-OA. The study explored retraction rates, time lags and reasons within these categories. The findings of this research revealed distinct patterns in retraction rates among different OA levels. Publications with Gold OA demonstrated the highest retraction rate, followed by Green OA and non-OA. A comparison of retraction reasons between Gold OA and non-OA categories indicated similar proportions, while Green OA exhibited a higher proportion due to falsification and manipulation issues, along with a lower occurrence of plagiarism and authorship issues. The retraction time lag was shortest for Gold OA, followed by non-OA, and longest for Green OA. The prolonged retraction time for Green OA could be attributed to an atypical distribution of retraction reasons. There is no exploration of a wider range of OA levels, such as Hybrid OA and Bronze OA. The outcomes of this study suggest the need for increased attention to research integrity within the OA publications. The occurrences of falsification, manipulation, and ethical concerns within Green OA publications warrant attention from the scientific community. This study contributes to the understanding of research integrity in the realm of OA publications, shedding light on retraction patterns and reasons across different OA levels."
Citation distributions and research evaluations: The impossibility of formulating a universal indicator,"Purpose: To analyze the diversity of citation distributions to publications in different research topics to investigate the accuracy of size-independent, rank-based indicators. The top percentile-based indicators are the most common indicators of this type, and the evaluations of Japan are the most evident misjudgments. Design/methodology/approach: The distributions of citations to publications from countries and journals in several research topics were analyzed along with the corresponding global publications using histograms with logarithmic binning, double rank plots, and normal probability plots of log-transformed numbers of citations. Findings: Size-independent, top percentile-based indicators are accurate when the global ranks of local publications fit a power law, but deviations in the least cited papers are frequent in countries and occur in all journals with high impact factors. In these cases, a single indicator is misleading. Comparisons of the proportions of uncited papers are the best way to predict these deviations. Research limitations: This study is fundamentally analytical, and its results describe mathematical facts that are self-evident. Practical implications: Respectable institutions, such as the OECD, the European Commission, and the U.S. National Science Board, produce research country rankings and individual evaluations using size-independent percentile indicators that are misleading in many countries. These misleading evaluations should be discontinued because they can cause confusion among research policymakers and lead to incorrect research policies. Originality/value: Studies linking the lower tail of citation distribution, including uncited papers, to percentile research indicators have not been performed previously. The present results demonstrate that studies of this type are necessary to find reliable procedures for research assessments. Â© 2024 Alonso RodrÃ­guez-Navarro.","To analyze the diversity of citation distributions to publications in different research topics to investigate the accuracy of size-independent, rank-based indicators. The top percentile-based indicators are the most common indicators of this type, and the evaluations of Japan are the most evident misjudgments. The distributions of citations to publications from countries and journals in several research topics were analyzed along with the corresponding global publications using histograms with logarithmic binning, double rank plots, and normal probability plots of log-transformed numbers of citations. Size-independent, top percentile-based indicators are accurate when the global ranks of local publications fit a power law, but deviations in the least cited papers are frequent in countries and occur in all journals with high impact factors. In these cases, a single indicator is misleading. Comparisons of the proportions of uncited papers are the best way to predict these deviations. This study is fundamentally analytical, and its results describe mathematical facts that are self-evident. Respectable institutions, such as the OECD, the European Commission, and the National Science Board, produce research country rankings and individual evaluations using size-independent percentile indicators that are misleading in many countries. These misleading evaluations should be discontinued because they can cause confusion among research policymakers and lead to incorrect research policies. Studies linking the lower tail of citation distribution, including uncited papers, to percentile research indicators have not been performed previously. The present results demonstrate that studies of this type are necessary to find reliable procedures for research assessments."
How Has Covid-19 Affected Published Academic Research? A Content Analysis of Journal Articles Mentioning the Virus,"Purpose: Methods to tackle Covid-19 have been developed by a wave of biomedical research but the pandemic has also influenced many aspects of society, generating a need for research into its consequences, and potentially changing the way existing topics are investigated. This article investigates the nature of this influence on the wider academic research mission. Design/methodology/approach: This article reports an inductive content analysis of 500 randomly selected journal articles mentioning Covid-19, as recorded by the Dimensions scholarly database on 19 March 2021. Covid-19 mentions were coded for the influence of the disease on the research. Findings: Whilst two thirds of these articles were about biomedicine (e.g. treatments, vaccines, virology), or health services in response to Covid-19, others covered the pandemic economy, society, safety, or education. In addition, some articles were not about the pandemic but stated that Covid-19 had increased or decreased the value of the reported research or changed the context in which it was conducted. Research limitations: The findings relate only to Covid-19 influences declared in published journal articles. Practical implications: Research managers and funders should consider whether their current procedures are effective in supporting researchers to address the evolving demands of pandemic societies, particularly in terms of timeliness. Originality/value: The results show that although health research dominates the academic response to Covid-19, it is more widely disrupting academic research with new demands and challenges.  Â© 2021 Mike Thelwall et al., published by Sciendo.","Methods to tackle Covid-19 have been developed by a wave of biomedical research but the pandemic has also influenced many aspects of society, generating a need for research into its consequences, and potentially changing the way existing topics are investigated. This article investigates the nature of this influence on the wider academic research mission. This article reports an inductive content analysis of 500 randomly selected journal articles mentioning Covid-19, as recorded by the Dimensions scholarly database on 19 March 2021. Covid-19 mentions were coded for the influence of the disease on the research. Whilst two thirds of these articles were about biomedicine ( treatments, vaccines, virology), or health services in response to Covid-19, others covered the pandemic economy, society, safety, or education. In addition, some articles were not about the pandemic but stated that Covid-19 had increased or decreased the value of the reported research or changed the context in which it was conducted. The findings relate only to Covid-19 influences declared in published journal articles. Research managers and funders should consider whether their current procedures are effective in supporting researchers to address the evolving demands of pandemic societies, particularly in terms of timeliness. The results show that although health research dominates the academic response to Covid-19, it is more widely disrupting academic research with new demands and challenges."
Is big team research fair in national research assessments? the case of the UK Research Excellence Framework 2021,"Collaborative research causes problems for research assessments because of the difficulty in fairly crediting its authors. Whilst splitting the rewards for an article amongst its authors has the greatest surface-level fairness, many important evaluations assign full credit to each author, irrespective of team size. The underlying rationales for this are labour reduction and the need to incentivise collaborative work because it is necessary to solve many important societal problems. This article assesses whether full counting changes results compared to fractional counting in the case of the UK's Research Excellence Framework (REF) 2021. For this assessment, fractional counting reduces the number of journal articles to as little as 10% of the full counting value, depending on the Unit of Assessment (UoA). Despite this large difference, allocating an overall grade point average (GPA) based on full counting or fractional counting gives results with a median Pearson correlation within UoAs of 0.98. The largest changes are for Archaeology (r=0.84) and Physics (r=0.88). There is a weak tendency for higher scoring institutions to lose from fractional counting, with the loss being statistically significant in 5 of the 34 UoAs. Thus, whilst the apparent over-weighting of contributions to collaboratively authored outputs does not seem too problematic from a fairness perspective overall, it may be worth examining in the few UoAs in which it makes the most difference.  Â© 2023 Mike Thelwall et al., published by Sciendo.","Collaborative research causes problems for research assessments because of the difficulty in fairly crediting its authors. Whilst splitting the rewards for an article amongst its authors has the greatest surface-level fairness, many important evaluations assign full credit to each author, irrespective of team size. The underlying rationales for this are labour reduction and the need to incentivise collaborative work because it is necessary to solve many important societal problems. This article assesses whether full counting changes results compared to fractional counting in the case of the UK's Research Excellence Framework (REF) 2021. For this assessment, fractional counting reduces the number of journal articles to as little as 10% of the full counting value, depending on the Unit of Assessment (UoA). Despite this large difference, allocating an overall grade point average (GPA) based on full counting or fractional counting gives results with a median Pearson correlation within UoAs of 0.98. The largest changes are for Archaeology (r=0.84) and Physics (r=0.88). There is a weak tendency for higher scoring institutions to lose from fractional counting, with the loss being statistically significant in 5 of the 34 UoAs. Thus, whilst the apparent over-weighting of contributions to collaboratively authored outputs does not seem too problematic from a fairness perspective overall, it may be worth examining in the few UoAs in which it makes the most difference."
Research evolution of metal organic frameworks: A scientometric approach with human-in-the-loop,"Purpose: This paper reports on a scientometric analysis bolstered by human-in-the-loop, domain experts, to examine the field of metal-organic frameworks (MOFs) research. Scientometric analyses reveal the intellectual landscape of a field. The study engaged MOF scientists in the design and review of our research workflow. MOF materials are an essential component in next-generation renewable energy storage and biomedical technologies. The research approach demonstrates how engaging experts, via human-in-the-loop processes, can help develop a comprehensive view of a field's research trends, influential works, and specialized topics. Design/methodology/approach: A scientometric analysis was conducted, integrating natural language processing (NLP), topic modeling, and network analysis methods. The analytical approach was enhanced through a human-in-the-loop iterative process involving MOF research scientists at selected intervals. MOF researcher feedback was incorporated into our method. The data sample included 65,209 MOF research articles. Python3 and software tool VOSviewer were used to perform the analysis. Findings: The findings demonstrate the value of including domain experts in research workflows, refinement, and interpretation of results. At each stage of the analysis, the MOF researchers contributed to interpreting the results and method refinements targeting our focus on MOF research. This study identified influential works and their themes. Our findings also underscore four main MOF research directions and applications. Research limitations: This study is limited by the sample (articles identified and referenced by the Cambridge Structural Database) that informed our analysis. Practical implications: Our findings contribute to addressing the current gap in fully mapping out the comprehensive landscape of MOF research. Additionally, the results will help domain scientists target future research directions. Originality/value: To the best of our knowledge, the number of publications collected for analysis exceeds those of previous studies. This enabled us to explore a more extensive body of MOF research compared to previous studies. Another contribution of our work is the iterative engagement of domain scientists, who brought in-depth, expert interpretation to the data analysis, helping hone the study.  Â© 2024 Xintong Zhao et al., published by Sciendo.","This paper reports on a scientometric analysis bolstered by human-in-the-loop, domain experts, to examine the field of metal-organic frameworks (MOFs) research. Scientometric analyses reveal the intellectual landscape of a field. The study engaged MOF scientists in the design and review of our research workflow. MOF materials are an essential component in next-generation renewable energy storage and biomedical technologies. The research approach demonstrates how engaging experts, via human-in-the-loop processes, can help develop a comprehensive view of a field's research trends, influential works, and specialized topics. A scientometric analysis was conducted, integrating natural language processing (NLP), topic modeling, and network analysis methods. The analytical approach was enhanced through a human-in-the-loop iterative process involving MOF research scientists at selected intervals. MOF researcher feedback was incorporated into our method. The data sample included 65,209 MOF research articles. Python3 and software tool VOSviewer were used to perform the analysis. The findings demonstrate the value of including domain experts in research workflows, refinement, and interpretation of results. At each stage of the analysis, the MOF researchers contributed to interpreting the results and method refinements targeting our focus on MOF research. This study identified influential works and their themes. Our findings also underscore four main MOF research directions and applications. This study is limited by the sample (articles identified and referenced by the Cambridge Structural Database) that informed our analysis. Our findings contribute to addressing the current gap in fully mapping out the comprehensive landscape of MOF research. Additionally, the results will help domain scientists target future research directions. To the best of our knowledge, the number of publications collected for analysis exceeds those of previous studies. This enabled us to explore a more extensive body of MOF research compared to previous studies. Another contribution of our work is the iterative engagement of domain scientists, who brought in-depth, expert interpretation to the data analysis, helping hone the study."
Feature and Tendency of Technology Transfer in Z-Park Patent Cooperation Network: From the Perspective of Global Optimal Path,"Purpose: This study aims to provide a new framework for analyzing the path of technology diffusion in the innovation network at the regional level and industrial level respectively, which is conducive to the integration of innovation resources, the coordinated development of innovative subjects, and the improvement of innovation abilities. Design/methodology/approach: Based on the Z-Park patent cooperation data, we establish Inter-Enterprise Technology Transfer Network model and apply the concept of Pivotability to describe the key links of technology diffusion and quantify the importance of innovative partnerships. By measuring the topologically structural characteristics in the levels of branch park and the technosphere, this paper demonstrates how technology spreads and promotes overall innovation activities within the innovation network. Findings: The results indicate that: (1) Patent cooperation network of the Z-Park displays heterogeneity and the connections between the innovative subjects distribute extremely uneven. (2) Haidian park owns the highest pivotability in the IETTN model, yet the related inter-enterprise patent cooperation is mainly concentrated in its internal, failing to facilitate the technology diffusion across multiple branch parks. (3) Such fields as ""electronics and information""and ""advanced manufacturing""are prominent in the cross-technosphere cooperation, while fields such as ""new energy""and ""environmental protection technology""can better promote industrial integration. Research limitations: Only the part of the joint patent application is taken into account while establishing the patent cooperation network. The other factors that influence the mechanism of technology diffusion in the innovation network need to be further studied, such as financial capital, market competition, and personnel mobility, etc. Practical implications: The findings of this paper will provide useful information and suggestions for the administration and policy-making of high-tech parks. Originality/value: The value of this paper is to build a bridge between the massive amount of patent data and the nature of technology diffusion, and to develop a set of tools to analyze the nonlinear relations between innovative subjects.  Â© 2021 Jun Guan et al., published by Sciendo.","This study aims to provide a new framework for analyzing the path of technology diffusion in the innovation network at the regional level and industrial level respectively, which is conducive to the integration of innovation resources, the coordinated development of innovative subjects, and the improvement of innovation abilities. Based on the Z-Park patent cooperation data, we establish Inter-Enterprise Technology Transfer Network model and apply the concept of Pivotability to describe the key links of technology diffusion and quantify the importance of innovative partnerships. By measuring the topologically structural characteristics in the levels of branch park and the technosphere, this paper demonstrates how technology spreads and promotes overall innovation activities within the innovation network. The results indicate that: Patent cooperation network of the Z-Park displays heterogeneity and the connections between the innovative subjects distribute extremely uneven. Haidian park owns the highest pivotability in the IETTN model, yet the related inter-enterprise patent cooperation is mainly concentrated in its internal, failing to facilitate the technology diffusion across multiple branch parks. Such fields as ""electronics and information""and ""advanced manufacturing""are prominent in the cross-technosphere cooperation, while fields such as ""new energy""and ""environmental protection technology""can better promote industrial integration. Only the part of the joint patent application is taken into account while establishing the patent cooperation network. The other factors that influence the mechanism of technology diffusion in the innovation network need to be further studied, such as financial capital, market competition, and personnel mobility, etc. The findings of this paper will provide useful information and suggestions for the administration and policy-making of high-tech parks. The value of this paper is to build a bridge between the massive amount of patent data and the nature of technology diffusion, and to develop a set of tools to analyze the nonlinear relations between innovative subjects."
The physics of citation growth,"Purpose: This study investigates the physics of annual fractional citation growth and its impact on journal bibliographic metrics, focusing on the interplay between journal publication growth and citation dynamics. Design/methodology/approach: We analyze bibliometric data from three prominent fluids journalsâPhysics of Fluids, Journal of Fluid Mechanics, and Physical Review Fluidsâover the period 1999â2023. The analysis examines the relations among annual fractional journal publication growth, citation growth, and bibliographic metric suppressions. Findings: Our findings reveal that the suppression of impact factor growth is significantly influenced by annual fractional journal publication growth rather than citation growth. All three journals exhibit similar responses to publication growth with minimal scatter, following a consistent functional relation. We also identify narrow, nearly Gaussian distributions for annual fractional journal publication growth. Furthermore, we introduce a new growth-independent dimensionless bibliometric metric, journal urgency, the ratio of annual fractional citation growth to the 4-year running average immediacy index. This metric captures effectively the dependency of citation growth on urgency and reveals consistent distributions across the journals analyzed. Research limitations: The study is limited to three major fluids journals and to the availability of bibliometric data from 1999 to 2023. Future work could extend the analysis to other disciplines and journals. Practical implications: Understanding the relation between publication growth and bibliometric suppressions can inform editorial and strategic decisions in journal management. The proposed journal urgency metric offers a novel tool for assessing and comparing journal performance independent of growth rates. Originality/value: This study introduces a new bibliometric metricâjournal urgencyâthat provides fresh insights into citation dynamics and bibliographic metric behavior. It highlights the critical role of publication growth in shaping journal impact factors and CiteScores, offering a unified framework applicable across multiple journals. Copyright: Â© 2025 Alan J. Giacomin, Martin Zatloukal, Mona A. Kanso, Nhan Phan-Thien.","This study investigates the physics of annual fractional citation growth and its impact on journal bibliographic metrics, focusing on the interplay between journal publication growth and citation dynamics. We analyze bibliometric data from three prominent fluids journalsPhysics of Fluids, Journal of Fluid Mechanics, and Physical Review Fluidsover the period 19992023. The analysis examines the relations among annual fractional journal publication growth, citation growth, and bibliographic metric suppressions. Our findings reveal that the suppression of impact factor growth is significantly influenced by annual fractional journal publication growth rather than citation growth. All three journals exhibit similar responses to publication growth with minimal scatter, following a consistent functional relation. We also identify narrow, nearly Gaussian distributions for annual fractional journal publication growth. Furthermore, we introduce a new growth-independent dimensionless bibliometric metric, journal urgency, the ratio of annual fractional citation growth to the 4-year running average immediacy index. This metric captures effectively the dependency of citation growth on urgency and reveals consistent distributions across the journals analyzed. The study is limited to three major fluids journals and to the availability of bibliometric data from 1999 to 2023. Future work could extend the analysis to other disciplines and journals. Understanding the relation between publication growth and bibliometric suppressions can inform editorial and strategic decisions in journal management. The proposed journal urgency metric offers a novel tool for assessing and comparing journal performance independent of growth rates. This study introduces a new bibliometric metricjournal urgencythat provides fresh insights into citation dynamics and bibliographic metric behavior. It highlights the critical role of publication growth in shaping journal impact factors and CiteScores, offering a unified framework applicable across multiple journals."
Causal inference using regression-based statistical control: Confusion in Econometrics,"Regression is a widely used econometric tool in research. In observational studies, based on a number of assumptions, regression-based statistical control methods attempt to analyze the causation between treatment and outcome by adding control variables. However, this approach may not produce reliable estimates of causal effects. In addition to the shortcomings of the method, this lack of confidence is mainly related to ambiguous formulations in econometrics, such as the definition of selection bias, selection of core control variables, and method of testing for robustness. Within the framework of the causal models, we clarify the assumption of causal inference using regression-based statistical controls, as described in econometrics, and discuss how to select core control variables to satisfy this assumption and conduct robustness tests for regression estimates.  Â© 2023 Fan Chao et al., published by Sciendo.","Regression is a widely used econometric tool in research. In observational studies, based on a number of assumptions, regression-based statistical control methods attempt to analyze the causation between treatment and outcome by adding control variables. However, this approach may not produce reliable estimates of causal effects. In addition to the shortcomings of the method, this lack of confidence is mainly related to ambiguous formulations in econometrics, such as the definition of selection bias, selection of core control variables, and method of testing for robustness. Within the framework of the causal models, we clarify the assumption of causal inference using regression-based statistical controls, as described in econometrics, and discuss how to select core control variables to satisfy this assumption and conduct robustness tests for regression estimates."
A Morphology-Driven Method for Measuring Technology Complementarity: Empirical Study Involving Alzheimer's Disease,"Purpose: Measuring the exact technology complementarity between different institutions is necessary to obtain complementary technology resources for R&D cooperation. Design/methodology/approach: This study constructs a morphology-driven method for measuring technology complementarity, taking medical field as an example. First, we calculate semantic similarities between subjects (S and S) and action-objects (AO and AO) based on the Metathesaurus, forming clusters of S and AO based on a semantic similarity matrix. Second, we identify key technology issues and methods based on clusters of S and AO. Third, a technology morphology matrix of several dimensions is constructed using morphology analysis, and the matrix is filled with subjects-action-objects (SAO) structures according to corresponding key technology issues and methods for different institutions. Finally, the technology morphology matrix is used to measure the technology complementarity between different institutions based on SAO. Findings: The improved technology complementarity method based on SAO is more of a supplementary and refined framework for the traditional IPC method. Research limitations: In future studies we will reprocess and identify the SAO structures which were not in the technology morphology matrix, and find other methods to characterize key technical issues and methods. Furthermore, we will add the comparison between proposed method and traditional and mostly used complementarity measurement method based on industry chain and industry code. Practical implications: This study takes medical field as an example. The morphology-driven method for measuring technology complementarity can be migrated and applied for any given field. Originality/value: From the perspective of complementary technology resources, this study develops and tests a more accurate morphology-driven method for technology complementarity measurement.  Â© 2022 Xuefeng Wang et al., published by Sciendo.","Measuring the exact technology complementarity between different institutions is necessary to obtain complementary technology resources for R&D cooperation. This study constructs a morphology-driven method for measuring technology complementarity, taking medical field as an example. First, we calculate semantic similarities between subjects (S and S) and action-objects (AO and AO) based on the Metathesaurus, forming clusters of S and AO based on a semantic similarity matrix. Second, we identify key technology issues and methods based on clusters of S and AO. Third, a technology morphology matrix of several dimensions is constructed using morphology analysis, and the matrix is filled with subjects-action-objects (SAO) structures according to corresponding key technology issues and methods for different institutions. Finally, the technology morphology matrix is used to measure the technology complementarity between different institutions based on SAO. The improved technology complementarity method based on SAO is more of a supplementary and refined framework for the traditional IPC method. In future studies we will reprocess and identify the SAO structures which were not in the technology morphology matrix, and find other methods to characterize key technical issues and methods. Furthermore, we will add the comparison between proposed method and traditional and mostly used complementarity measurement method based on industry chain and industry code. This study takes medical field as an example. The morphology-driven method for measuring technology complementarity can be migrated and applied for any given field. From the perspective of complementary technology resources, this study develops and tests a more accurate morphology-driven method for technology complementarity measurement."
A new evolutional model for institutional field knowledge flow network,"Purpose: This paper aims to address the limitations in existing research on the evolution of knowledge flow networks by proposing a meso-level institutional field knowledge flow network evolution model (IKM). The purpose is to simulate the construction process of a knowledge flow network using knowledge organizations as units and to investigate its effectiveness in replicating institutional field knowledge flow networks. Design/Methodology/Approach: The IKM model enhances the preferential attachment and growth observed in scale-free BA networks, while incorporating three adjustment parameters to simulate the selection of connection targets and the types of nodes involved in the network evolution process Using the PageRank algorithm to calculate the significance of nodes within the knowledge flow network. To compare its performance, the BA and DMS models are also employed for simulating the network. Pearson coefficient analysis is conducted on the simulated networks generated by the IKM, BA and DMS models, as well as on the actual network. Findings: The research findings demonstrate that the IKM model outperforms the BA and DMS models in replicating the institutional field knowledge flow network. It provides comprehensive insights into the evolution mechanism of knowledge flow networks in the scientific research realm. The model also exhibits potential applicability to other knowledge networks that involve knowledge organizations as node units. Research Limitations: This study has some limitations. Firstly, it primarily focuses on the evolution of knowledge flow networks within the field of physics, neglecting other fields. Additionally, the analysis is based on a specific set of data, which may limit the generalizability of the findings. Future research could address these limitations by exploring knowledge flow networks in diverse fields and utilizing broader datasets. Practical Implications: The proposed IKM model offers practical implications for the construction and analysis of knowledge flow networks within institutions. It provides a valuable tool for understanding and managing knowledge exchange between knowledge organizations. The model can aid in optimizing knowledge flow and enhancing collaboration within organizations. Originality/value: This research highlights the significance of meso-level studies in understanding knowledge organization and its impact on knowledge flow networks. The IKM model demonstrates its effectiveness in replicating institutional field knowledge flow networks and offers practical implications for knowledge management in institutions. Moreover, the model has the potential to be applied to other knowledge networks, which are formed by knowledge organizations as node units. Copyright: Â© 2024 Jinzhong Guo, Kai Wang, Xueqin Liao, Xiaoling Liu.","This paper aims to address the limitations in existing research on the evolution of knowledge flow networks by proposing a meso-level institutional field knowledge flow network evolution model (IKM). The purpose is to simulate the construction process of a knowledge flow network using knowledge organizations as units and to investigate its effectiveness in replicating institutional field knowledge flow networks. The IKM model enhances the preferential attachment and growth observed in scale-free BA networks, while incorporating three adjustment parameters to simulate the selection of connection targets and the types of nodes involved in the network evolution process Using the PageRank algorithm to calculate the significance of nodes within the knowledge flow network. To compare its performance, the BA and DMS models are also employed for simulating the network. Pearson coefficient analysis is conducted on the simulated networks generated by the IKM, BA and DMS models, as well as on the actual network. The research findings demonstrate that the IKM model outperforms the BA and DMS models in replicating the institutional field knowledge flow network. It provides comprehensive insights into the evolution mechanism of knowledge flow networks in the scientific research realm. The model also exhibits potential applicability to other knowledge networks that involve knowledge organizations as node units. This study has some limitations. Firstly, it primarily focuses on the evolution of knowledge flow networks within the field of physics, neglecting other fields. Additionally, the analysis is based on a specific set of data, which may limit the generalizability of the findings. Future research could address these limitations by exploring knowledge flow networks in diverse fields and utilizing broader datasets. The proposed IKM model offers practical implications for the construction and analysis of knowledge flow networks within institutions. It provides a valuable tool for understanding and managing knowledge exchange between knowledge organizations. The model can aid in optimizing knowledge flow and enhancing collaboration within organizations. This research highlights the significance of meso-level studies in understanding knowledge organization and its impact on knowledge flow networks. The IKM model demonstrates its effectiveness in replicating institutional field knowledge flow networks and offers practical implications for knowledge management in institutions. Moreover, the model has the potential to be applied to other knowledge networks, which are formed by knowledge organizations as node units."
Progress and Knowledge Transfer from Science to Technology in the Research Frontier of CRISPR Based on the LDA Model,"Abstract Purpose This study explores the underlying research topics regarding CRISPR based on the LDA model and figures out trends in knowledge transfer from science to technology in this area over the latest 10 years. Design/methodology/approach We collected publications on CRISPR between 2011 and 2020 from the Web of Science, and traced all the patents citing them from lens.org. 15,904 articles and 18,985 patents in total are downloaded and analyzed. The LDA model was applied to identify underlying research topics in related research. In addition, some indicators were introduced to measure the knowledge transfer from research topics of scientific publications to IPC-4 classes of patents. Findings The emerging research topics on CRISPR were identified and their evolution over time displayed. Furthermore, a big picture of knowledge transition from research topics to technological classes of patents was presented. We found that for all topics on CRISPR, the average first transition year, the ratio of articles cited by patents, the NPR transition rate are respectively 1.08, 15.57%, and 1.19, extremely shorter and more intensive than those of general fields. Moreover, the transition patterns are different among research topics. Research limitations Our research is limited to publications retrieved from the Web of Science and their citing patents indexed in lens.org. A limitation inherent with LDA analysis is in the manual interpretation and labeling of âtopicsâ. Practical implications Our study provides good references for policy-makers on allocating scientific resources and regulating financial budgets to face challenges related to the transformative technology of CRISPR. Originality/value The LDA model here is applied to topic identification in the area of transformative researches for the first time, as exemplified on CRISPR. Additionally, the dataset of all citing patents in this area helps to provide a full picture to detect the knowledge transition between S&T. Â© 2022 Yushuang Lyu et al., published by Sciendo","Abstract Purpose This study explores the underlying research topics regarding CRISPR based on the LDA model and figures out trends in knowledge transfer from science to technology in this area over the latest 10 years. Design/methodology/approach We collected publications on CRISPR between 2011 and 2020 from the Web of Science, and traced all the patents citing them from lens.org. 15,904 articles and 18,985 patents in total are downloaded and analyzed. The LDA model was applied to identify underlying research topics in related research. In addition, some indicators were introduced to measure the knowledge transfer from research topics of scientific publications to IPC-4 classes of patents. Findings The emerging research topics on CRISPR were identified and their evolution over time displayed. Furthermore, a big picture of knowledge transition from research topics to technological classes of patents was presented. We found that for all topics on CRISPR, the average first transition year, the ratio of articles cited by patents, the NPR transition rate are respectively 1.08, 15.57%, and 1.19, extremely shorter and more intensive than those of general fields. Moreover, the transition patterns are different among research topics. Research limitations Our research is limited to publications retrieved from the Web of Science and their citing patents indexed in lens.org. A limitation inherent with LDA analysis is in the manual interpretation and labeling of topics. Practical implications Our study provides good references for policy-makers on allocating scientific resources and regulating financial budgets to face challenges related to the transformative technology of CRISPR. Originality/value The LDA model here is applied to topic identification in the area of transformative researches for the first time, as exemplified on CRISPR. Additionally, the dataset of all citing patents in this area helps to provide a full picture to detect the knowledge transition between S&"
Examining âSalami slicingâ publications as a side-effect of research performance evaluation: An empirical study,"Purpose: This study investigates whether publication-centric incentive systems, introduced through the National Scientific Accreditation (ASN: Abilitazione Scientifica Nazionale) for professorships in Italy in 2012, contribute to adopting âsalami publishingâ strategies among Italian academics. Design/methodology/approach: A longitudinal bibliometric analysis was conducted on the publication records of over 25,000 Italian science professors to examine changes in publication output and the originality of their work following the implementation of the ASN. Findings: The analysis revealed a significant increase in publication output after the ASN's introduction, along with a concurrent decline in the originality of publications. However, no evidence was found linking these trends to increased salami slicing practices among the observed researchers. Research limitations: Given the size of our observation field, we propose an innovative indirect approach based on the degree of originality of publications' bibliographies. We know that bibliographic coupling cannot capture salami publications per se, but only topically-related records. On the other hand, controlling for the author's specialization level in the period, we believe that a higher level of bibliographic coupling in his scientific output can signal a change in his strategy of disseminating the results of his research. The relatively low R-squared values in our models (0.3-0.4) reflect the complexity of the phenomenon under investigation, revealing the presence of unmeasured factors influencing the outcomes, and future research should explore additional variables or alternative models that might account for a greater proportion of the variability. Despite this limitation, the significant predictors identified in our analysis provide valuable insights into the key factors driving the observed outcomes. Practical implications: The results of the study support those who argue that quantitative research assessment frameworks have had very positive effects and should not be dismissed, contrary to the claims of those evoking the occurrence of side effects that do not appear in the empirical analyses. Originality/value: This study provides empirical evidence on the impact of the ASN on publication behaviors in a huge micro-level dataset, contributing to the broader discourse on the effects of quantitative research assessments on academic publishing practices. Copyright: Â© 2025 Ciriaco Andrea D'Angelo. Published under a Creative Commons Attribution 4.0 International (CC BY 4.0) license.","This study investigates whether publication-centric incentive systems, introduced through the National Scientific Accreditation (ASN: Abilitazione Scientifica Nazionale) for professorships in Italy in 2012, contribute to adopting salami publishing strategies among Italian academics. A longitudinal bibliometric analysis was conducted on the publication records of over 25,000 Italian science professors to examine changes in publication output and the originality of their work following the implementation of the ASN. The analysis revealed a significant increase in publication output after the ASN's introduction, along with a concurrent decline in the originality of publications. However, no evidence was found linking these trends to increased salami slicing practices among the observed researchers. Given the size of our observation field, we propose an innovative indirect approach based on the degree of originality of publications' bibliographies. We know that bibliographic coupling cannot capture salami publications per se, but only topically-related records. On the other hand, controlling for the author's specialization level in the period, we believe that a higher level of bibliographic coupling in his scientific output can signal a change in his strategy of disseminating the results of his research. The relatively low R-squared values in our models (0.3-0.4) reflect the complexity of the phenomenon under investigation, revealing the presence of unmeasured factors influencing the outcomes, and future research should explore additional variables or alternative models that might account for a greater proportion of the variability. Despite this limitation, the significant predictors identified in our analysis provide valuable insights into the key factors driving the observed outcomes. The results of the study support those who argue that quantitative research assessment frameworks have had very positive effects and should not be dismissed, contrary to the claims of those evoking the occurrence of side effects that do not appear in the empirical analyses. This study provides empirical evidence on the impact of the ASN on publication behaviors in a huge micro-level dataset, contributing to the broader discourse on the effects of quantitative research assessments on academic publishing practices."
Dimensionality reduction model based on integer planning for the analysis of key indicators affecting life expectancy,"Purpose: Exploring a dimensionality reduction model that can adeptly eliminate outliers and select the appropriate number of clusters is of profound theoretical and practical importance. Additionally, the interpretability of these models presents a persistent challenge. Design/methodology/approach: This paper proposes two innovative dimensionality reduction models based on integer programming (DRMBIP). These models assess compactness through the correlation of each indicator with its class center, while separation is evaluated by the correlation between different class centers. In contrast to DRMBIP-p, the DRMBIP-v considers the threshold parameter as a variable aiming to optimally balances both compactness and separation. Findings: This study, getting data from the Global Health Observatory (GHO), investigates 141 indicators that influence life expectancy. The findings reveal that DRMBIP-p effectively reduces the dimensionality of data, ensuring compactness. It also maintains compatibility with other models. Additionally, DRMBIP-v finds the optimal result, showing exceptional separation. Visualization of the results reveals that all classes have a high compactness. Research limitations: The DRMBIP-p requires the input of the correlation threshold parameter, which plays a pivotal role in the effectiveness of the final dimensionality reduction results. In the DRMBIP-v, modifying the threshold parameter to variable potentially emphasizes either separation or compactness. This necessitates an artificial adjustment to the overflow component within the objective function. Practical implications: The DRMBIP presented in this paper is adept at uncovering the primary geometric structures within high-dimensional indicators. Validated by life expectancy data, this paper demonstrates potential to assist data miners with the reduction of data dimensions. Originality/value: To our knowledge, this is the first time that integer programming has been used to build a dimensionality reduction model with indicator filtering. It not only has applications in life expectancy, but also has obvious advantages in data mining work that requires precise class centers.  Â© 2023 Wei Cui et al., published by Sciendo.","Exploring a dimensionality reduction model that can adeptly eliminate outliers and select the appropriate number of clusters is of profound theoretical and practical importance. Additionally, the interpretability of these models presents a persistent challenge. This paper proposes two innovative dimensionality reduction models based on integer programming (DRMBIP). These models assess compactness through the correlation of each indicator with its class center, while separation is evaluated by the correlation between different class centers. In contrast to DRMBIP-p, the DRMBIP-v considers the threshold parameter as a variable aiming to optimally balances both compactness and separation. This study, getting data from the Global Health Observatory (GHO), investigates 141 indicators that influence life expectancy. The findings reveal that DRMBIP-p effectively reduces the dimensionality of data, ensuring compactness. It also maintains compatibility with other models. Additionally, DRMBIP-v finds the optimal result, showing exceptional separation. Visualization of the results reveals that all classes have a high compactness. The DRMBIP-p requires the input of the correlation threshold parameter, which plays a pivotal role in the effectiveness of the final dimensionality reduction results. In the DRMBIP-v, modifying the threshold parameter to variable potentially emphasizes either separation or compactness. This necessitates an artificial adjustment to the overflow component within the objective function. The DRMBIP presented in this paper is adept at uncovering the primary geometric structures within high-dimensional indicators. Validated by life expectancy data, this paper demonstrates potential to assist data miners with the reduction of data dimensions. To our knowledge, this is the first time that integer programming has been used to build a dimensionality reduction model with indicator filtering. It not only has applications in life expectancy, but also has obvious advantages in data mining work that requires precise class centers."
Subject Area Risk Assessment of Four Hungarian Universities with a View to the QS University Rankings by Subject,"Purpose: The aim of our paper is to investigate the role of a mentor leading a research team in the overall scientific performance of an academic institution and the possible risks of their departure with a special attention to their publication output. Design/methodology/approach: By using SciVal subject area data, we composed a formula describing the level of vulnerability of any given university in the case of losing any of its leading mentors, identifying other risk factors by dividing their careers into separate stages. Findings: It turns out that the higher field-weighed citation impact is, the better position universities reach in the rankings by subject and the vulnerability of institutions highly depends on the mentors, especially in view of their contribution to the topic clusters. Research limitations: The analysis covers the publication output of leading researchers working at four Hungarian universities, the scope of the analysis is worth being extended. Practical implications: Our analysis has the potential to give an applicable systemic approach as well as a data collection scheme to university managements so as to formulate an inclusive and comprehensive research strategy involving the introduction of a reward system aimed at publications and further encouraging national and international research cooperation. Originality/value: The methodology and the principles of risk assessment laid down in our paper are not restricted to measuring the vulnerability level of a limited group of academic institutions, they can be appropriately used for investigating the role of mentors or leading researchers at every university across the globe.  Â© 2022 PÃ©ter SasvÃ¡ri et al., published by Sciendo.","The aim of our paper is to investigate the role of a mentor leading a research team in the overall scientific performance of an academic institution and the possible risks of their departure with a special attention to their publication output. By using SciVal subject area data, we composed a formula describing the level of vulnerability of any given university in the case of losing any of its leading mentors, identifying other risk factors by dividing their careers into separate stages. It turns out that the higher field-weighed citation impact is, the better position universities reach in the rankings by subject and the vulnerability of institutions highly depends on the mentors, especially in view of their contribution to the topic clusters. The analysis covers the publication output of leading researchers working at four Hungarian universities, the scope of the analysis is worth being extended. Our analysis has the potential to give an applicable systemic approach as well as a data collection scheme to university managements so as to formulate an inclusive and comprehensive research strategy involving the introduction of a reward system aimed at publications and further encouraging national and international research cooperation. The methodology and the principles of risk assessment laid down in our paper are not restricted to measuring the vulnerability level of a limited group of academic institutions, they can be appropriately used for investigating the role of mentors or leading researchers at every university across the globe."
Build neural network models to identify and correct news headlines exaggerating obesity-related scientific findings,"Purpose: Media exaggerations of health research may confuse readers' understanding, erode public trust in science and medicine, and cause disease mismanagement. This study built artificial intelligence (AI) models to automatically identify and correct news headlines exaggerating obesity-related research findings. Design/methodology/approach: We searched popular digital media outlets to collect 523 headlines exaggerating obesity-related research findings. The reasons for exaggerations include: inferring causality from observational studies, inferring human outcomes from animal research, inferring distant/end outcomes (e.g., obesity) from immediate/intermediate outcomes (e.g., calorie intake), and generalizing findings to the population from a subgroup or convenience sample. Each headline was paired with the title and abstract of the peer-reviewed journal publication covered by the news article. We drafted an exaggeration-free counterpart for each original headline and fined-Tuned a BERT model to differentiate between them. We further fine-Tuned three generative language models-BART, PEGASUS, and T5 to autogenerate exaggeration-free headlines based on a journal publication's title and abstract. Model performance was evaluated using the ROUGE metrics by comparing model-generated headlines with journal publication titles. Findings: The fine-Tuned BERT model achieved 92.5% accuracy in differentiating between exaggeration-free and original headlines. Baseline ROUGE scores averaged 0.311 for ROUGE-1, 0.113 for ROUGE-2, 0.253 for ROUGE-L, and 0.253 ROUGE-Lsum. PEGASUS, T5, and BART all outperformed the baseline. The best-performing BART model attained 0.447 for ROUGE-1, 0.221 for ROUGE-2, 0.402 for ROUGE-L, and 0.402 for ROUGE-Lsum. Originality/value: This study demonstrated the feasibility of leveraging AI to automatically identify and correct news headlines exaggerating obesity-related research findings.  Â© 2023 Ruopeng An et al., published by Sciendo.","Media exaggerations of health research may confuse readers' understanding, erode public trust in science and medicine, and cause disease mismanagement. This study built artificial intelligence (AI) models to automatically identify and correct news headlines exaggerating obesity-related research findings. We searched popular digital media outlets to collect 523 headlines exaggerating obesity-related research findings. The reasons for exaggerations include: inferring causality from observational studies, inferring human outcomes from animal research, inferring distant/end outcomes (, obesity) from immediate/intermediate outcomes (, calorie intake), and generalizing findings to the population from a subgroup or convenience sample. Each headline was paired with the title and abstract of the peer-reviewed journal publication covered by the news article. We drafted an exaggeration-free counterpart for each original headline and fined-Tuned a BERT model to differentiate between them. We further fine-Tuned three generative language models-BART, PEGASUS, and T5 to autogenerate exaggeration-free headlines based on a journal publication's title and abstract. Model performance was evaluated using the ROUGE metrics by comparing model-generated headlines with journal publication titles. The fine-Tuned BERT model achieved 92.5% accuracy in differentiating between exaggeration-free and original headlines. Baseline ROUGE scores averaged 0.311 for ROUGE-1, 0.113 for ROUGE-2, 0.253 for ROUGE-L, and 0.253 ROUGE-Lsum. PEGASUS, T5, and BART all outperformed the baseline. The best-performing BART model attained 0.447 for ROUGE-1, 0.221 for ROUGE-2, 0.402 for ROUGE-L, and 0.402 for ROUGE-Lsum. This study demonstrated the feasibility of leveraging AI to automatically identify and correct news headlines exaggerating obesity-related research findings."
Evaluating grant proposals: Lessons from using metrics as screening device,"Purpose: This study examines the effects of using publication-based metrics for the initial screening in the application process for a project leader. The key questions are whether formal policy affects the allocation of funds to researchers with a better publication record and how the previous academic performance of principal investigators is related to future project results. Design/methodology/approach: We compared two competitions, before and after the policy raised the publication threshold for the principal investigators. We analyzed 9,167 papers published by 332 winners in physics and the social sciences and humanities (SSH), and 11,253 publications resulting from each funded project. Findings: We found that among physicists, even in the first period, grants tended to be allocated to prolific authors publishing in high-quality journals. In contrast, the SSH project grantees had been less prolific in publishing internationally in both periods; however, in the second period, the selection of grant recipients yielded better results regarding awarding grants to more productive authors in terms of the quantity and quality of publications. There was no evidence that this better selection of grant recipients resulted in better publication records during grant realization. Originality: This study contributes to the discussion of formal policies that rely on metrics for the evaluation of grant proposals. The Russian case shows that such policy may have a profound effect on changing the supply side of applicants, especially in disciplines that are less suitable for metric-based evaluations. In spite of the criticism given to metrics, they might be a useful additional instrument in academic systems where professional expertise is corrupted and prevents allocation of funds to prolific researchers.  Â© 2023 Katerina Guba et al., published by Sciendo.","This study examines the effects of using publication-based metrics for the initial screening in the application process for a project leader. The key questions are whether formal policy affects the allocation of funds to researchers with a better publication record and how the previous academic performance of principal investigators is related to future project results. We compared two competitions, before and after the policy raised the publication threshold for the principal investigators. We analyzed 9,167 papers published by 332 winners in physics and the social sciences and humanities (SSH), and 11,253 publications resulting from each funded project. We found that among physicists, even in the first period, grants tended to be allocated to prolific authors publishing in high-quality journals. In contrast, the SSH project grantees had been less prolific in publishing internationally in both periods; however, in the second period, the selection of grant recipients yielded better results regarding awarding grants to more productive authors in terms of the quantity and quality of publications. There was no evidence that this better selection of grant recipients resulted in better publication records during grant realization. Originality: This study contributes to the discussion of formal policies that rely on metrics for the evaluation of grant proposals. The Russian case shows that such policy may have a profound effect on changing the supply side of applicants, especially in disciplines that are less suitable for metric-based evaluations. In spite of the criticism given to metrics, they might be a useful additional instrument in academic systems where professional expertise is corrupted and prevents allocation of funds to prolific researchers."
Practical operation and theoretical basis of difference-in-difference regression in science of science: The comparative trial on the scientific performance of Nobel laureates versus their coauthors,"Purpose: In recent decades, with the availability of large-scale scientific corpus datasets, difference-in-difference (DID) is increasingly used in the science of science and bibliometrics studies. DID method outputs the unbiased estimation on condition that several hypotheses hold, especially the common trend assumption. In this paper, we gave a systematic demonstration of DID in the science of science, and the potential ways to improve the accuracy of DID method. Design/methodology/approach: At first, we reviewed the statistical assumptions, the model specification, and the application procedures of DID method. Second, to improve the necessary assumptions before conducting DID regression and the accuracy of estimation, we introduced some matching techniques serving as the pre-selecting step for DID design by matching control individuals who are equivalent to those treated ones on observational variables before the intervention. Lastly, we performed a case study to estimate the effects of prizewinning on the scientific performance of Nobel laureates, by comparing the yearly citation impact after the prizewinning year between Nobel laureates and their prizewinning-work coauthors. Findings: We introduced the procedures to conduct a DID estimation and demonstrated the effectiveness to use matching method to improve the results. As a case study, we found that there are no significant increases in citations for Nobel laureates compared to their prizewinning coauthors. Research limitations: This study ignored the rigorous mathematical deduction parts of DID, while focused on the practical parts. Practical implications: This work gives experimental practice and potential guidelines to use DID method in science of science and bibliometrics studies. Originality/value: This study gains insights into the usage of econometric tools in science of science.  Â© 2023 Yurui Huang et al., published by Sciendo.","In recent decades, with the availability of large-scale scientific corpus datasets, difference-in-difference is increasingly used in the science of science and bibliometrics studies. DID method outputs the unbiased estimation on condition that several hypotheses hold, especially the common trend assumption. In this paper, we gave a systematic demonstration of DID in the science of science, and the potential ways to improve the accuracy of DID method. At first, we reviewed the statistical assumptions, the model specification, and the application procedures of DID method. Second, to improve the necessary assumptions before conducting DID regression and the accuracy of estimation, we introduced some matching techniques serving as the pre-selecting step for DID design by matching control individuals who are equivalent to those treated ones on observational variables before the intervention. Lastly, we performed a case study to estimate the effects of prizewinning on the scientific performance of Nobel laureates, by comparing the yearly citation impact after the prizewinning year between Nobel laureates and their prizewinning-work coauthors. We introduced the procedures to conduct a DID estimation and demonstrated the effectiveness to use matching method to improve the results. As a case study, we found that there are no significant increases in citations for Nobel laureates compared to their prizewinning coauthors. This study ignored the rigorous mathematical deduction parts of DID, while focused on the practical parts. This work gives experimental practice and potential guidelines to use DID method in science of science and bibliometrics studies. This study gains insights into the usage of econometric tools in science of science."
"Assessment of retracted papers, and their retraction notices, from a cancer journal associated with ""paper mills""","Cancer research is occasionally described as being in a reproducibility crisis. The cancer literature has ample papers retracted due to misconduct, including the use of paper mills, invalid authorship, or fake data. The objective of this paper was to gain an appreciation of the balance of retractions and associated retraction notices of 23 retracted Cancer Biotherapy and Radiopharmaceuticals papers associated with paper mills. By 23 March 2023, these retracted papers had already accumulated 287 citations according to Web of Science Core Collection, 253 according to Scopus, and 365 according to Google Scholar, i.e., metrically speaking, they were highly rewarded. All authors had an affiliation (71% being a hospital) in China. Most (12/21; 57%) of corresponding authors had emails with a @163.com suffix. Four of the retraction notices (i.e., 17%) explicitly indicated paper mills as a reason for retraction although, in general, the retraction notices lacked details and background that could assist readers' understanding of the retractions.  Â© 2023 Jaime A. Teixeira da Silva et al., published by Sciendo.","Cancer research is occasionally described as being in a reproducibility crisis. The cancer literature has ample papers retracted due to misconduct, including the use of paper mills, invalid authorship, or fake data. The objective of this paper was to gain an appreciation of the balance of retractions and associated retraction notices of 23 retracted Cancer Biotherapy and Radiopharmaceuticals papers associated with paper mills. By 23 March 2023, these retracted papers had already accumulated 287 citations according to Web of Science Core Collection, 253 according to Scopus, and 365 according to Google Scholar, , metrically speaking, they were highly rewarded. All authors had an affiliation (71% being a hospital) in China. Most (12/21; 57%) of corresponding authors had emails with a @163.com suffix. Four of the retraction notices (, 17%) explicitly indicated paper mills as a reason for retraction although, in general, the retraction notices lacked details and background that could assist readers' understanding of the retractions."
Detecting LLM-assisted writing in scientific communication: Are we there yet?,"Large Language Models (LLMs), exemplified by ChatGPT, have significantly reshaped text generation, particularly in the realm of writing assistance. While ethical considerations underscore the importance of transparently acknowledging LLM use, especially in scientific communication, genuine acknowledgment remains infrequent. A potential avenue to encourage accurate acknowledging of LLM-assisted writing involves employing automated detectors. Our evaluation of four cutting-edge LLM-generated text detectors reveals their suboptimal performance compared to a simple ad-hoc detector designed to identify abrupt writing style changes around the time of LLM proliferation. We contend that the development of specialized detectors exclusively dedicated to LLM-assisted writing detection is necessary. Such detectors could play a crucial role in fostering more authentic recognition of LLM involvement in scientific communication, addressing the current challenges in acknowledgment practices.  Â© 2024 Teddy Lazebnik et al., published by Sciendo.","Large Language Models (LLMs), exemplified by ChatGPT, have significantly reshaped text generation, particularly in the realm of writing assistance. While ethical considerations underscore the importance of transparently acknowledging LLM use, especially in scientific communication, genuine acknowledgment remains infrequent. A potential avenue to encourage accurate acknowledging of LLM-assisted writing involves employing automated detectors. Our evaluation of four cutting-edge LLM-generated text detectors reveals their suboptimal performance compared to a simple ad-hoc detector designed to identify abrupt writing style changes around the time of LLM proliferation. We contend that the development of specialized detectors exclusively dedicated to LLM-assisted writing detection is necessary. Such detectors could play a crucial role in fostering more authentic recognition of LLM involvement in scientific communication, addressing the current challenges in acknowledgment practices."
Beyond authorship: Analyzing contributions in PLOS ONE and the challenges of appropriate attribution,"Purpose: This study aims to evaluate the accuracy of authorship attributions in scientific publications, focusing on the fairness and precision of individual contributions within academic works. Design/methodology/approach: The study analyzes 81,823 publications from the journal PLOS ONE, covering the period from January 2018 to June 2023. It examines the authorship attributions within these publications to try and determine the prevalence of inappropriate authorship. It also investigates the demographic and professional profiles of affected authors, exploring trends and potential factors contributing to inaccuracies in authorship. Findings: Surprisingly, 9.14% of articles feature at least one author with inappropriate authorship, affecting over 14,000 individuals (2.56% of the sample). Inappropriate authorship is more concentrated in Asia, Africa, and specific European countries like Italy. Established researchers with significant publication records and those affiliated with companies or nonprofits show higher instances of potential monetary authorship. Research limitations: Our findings are based on contributions as declared by the authors, which implies a degree of trust in their transparency. However, this reliance on self-reporting may introduce biases or inaccuracies into the dataset. Further research could employ additional verification methods to enhance the reliability of the findings. Practical implications: These findings have significant implications for journal publishers, highlighting the necessity for robust control mechanisms to ensure the integrity of authorship attributions. Moreover, researchers must exercise discernment in determining when to acknowledge a contributor and when to include them in the author list. Addressing these issues is crucial for maintaining the credibility and fairness of academic publications. Originality/value: This study contributes to an understanding of critical issues within academic authorship, shedding light on the prevalence and impact of inappropriate authorship attributions. By calling for a nuanced approach to ensure accurate credit is given where it is due, the study underscores the importance of upholding ethical standards in scholarly publishing.  Â© 2024 Abdelghani Maddi et al., published by Sciendo.","This study aims to evaluate the accuracy of authorship attributions in scientific publications, focusing on the fairness and precision of individual contributions within academic works. The study analyzes 81,823 publications from the journal PLOS ONE, covering the period from January 2018 to June 2023. It examines the authorship attributions within these publications to try and determine the prevalence of inappropriate authorship. It also investigates the demographic and professional profiles of affected authors, exploring trends and potential factors contributing to inaccuracies in authorship. Surprisingly, 9.14% of articles feature at least one author with inappropriate authorship, affecting over 14,000 individuals (2.56% of the sample). Inappropriate authorship is more concentrated in Asia, Africa, and specific European countries like Italy. Established researchers with significant publication records and those affiliated with companies or nonprofits show higher instances of potential monetary authorship. Our findings are based on contributions as declared by the authors, which implies a degree of trust in their transparency. However, this reliance on self-reporting may introduce biases or inaccuracies into the dataset. Further research could employ additional verification methods to enhance the reliability of the findings. These findings have significant implications for journal publishers, highlighting the necessity for robust control mechanisms to ensure the integrity of authorship attributions. Moreover, researchers must exercise discernment in determining when to acknowledge a contributor and when to include them in the author list. Addressing these issues is crucial for maintaining the credibility and fairness of academic publications. This study contributes to an understanding of critical issues within academic authorship, shedding light on the prevalence and impact of inappropriate authorship attributions. By calling for a nuanced approach to ensure accurate credit is given where it is due, the study underscores the importance of upholding ethical standards in scholarly publishing."
What Does Information Science Offer for Data Science Research?: A Review of Data and Information Ethics Literature,"This paper reviews literature pertaining to the development of data science as a discipline, current issues with data bias and ethics, and the role that the discipline of information science may play in addressing these concerns. Information science research and researchers have much to offer for data science, owing to their background as transdisciplinary scholars who apply human-centered and social-behavioral perspectives to issues within natural science disciplines. Information science researchers have already contributed to a humanistic approach to data ethics within the literature and an emphasis on data science within information schools all but ensures that this literature will continue to grow in coming decades. This review article serves as a reference for the history, current progress, and potential future directions of data ethics research within the corpus of information science literature. Â© 2022 Brady Lund et al., published by Sciendo.","This paper reviews literature pertaining to the development of data science as a discipline, current issues with data bias and ethics, and the role that the discipline of information science may play in addressing these concerns. Information science research and researchers have much to offer for data science, owing to their background as transdisciplinary scholars who apply human-centered and social-behavioral perspectives to issues within natural science disciplines. Information science researchers have already contributed to a humanistic approach to data ethics within the literature and an emphasis on data science within information schools all but ensures that this literature will continue to grow in coming decades. This review article serves as a reference for the history, current progress, and potential future directions of data ethics research within the corpus of information science literature."
Tracking direct and indirect impact on technology and policy of transformative research via ego citation network,"Purpose: The disseminating of academic knowledge to nonacademic audiences partly relies on the transition of subsequent citing papers. This study aims to investigate direct and indirect impact on technology and policy originating from transformative research based on ego citation network. Design/methodology/approach: Key Nobel Prize-winning publications (NPs) in fields of gene engineering and astrophysics are regarded as a proxy for transformative research. In this contribution, we introduce a network-structural indicator of citing patents to measure technological impact of a target article and use policy citations as a preliminary tool for policy impact. Findings: The results show that the impact on technology and policy of NPs are higher than that of their subsequent citation generations in gene engineering but not in astrophysics. Research limitations: The selection of Nobel Prizes is not balanced and the database used in this study, Dimensions, suffers from incompleteness and inaccuracy of citation links. Practical implications: Our findings provide useful clues to better understand the characteristics of transformative research in technological and policy impact. Originality/value: This study proposes a new framework to explore the direct and indirect impact on technology and policy originating from transformative research.  Â© 2024 Xian Li et al., published by Sciendo.","The disseminating of academic knowledge to nonacademic audiences partly relies on the transition of subsequent citing papers. This study aims to investigate direct and indirect impact on technology and policy originating from transformative research based on ego citation network. Key Nobel Prize-winning publications (NPs) in fields of gene engineering and astrophysics are regarded as a proxy for transformative research. In this contribution, we introduce a network-structural indicator of citing patents to measure technological impact of a target article and use policy citations as a preliminary tool for policy impact. The results show that the impact on technology and policy of NPs are higher than that of their subsequent citation generations in gene engineering but not in astrophysics. The selection of Nobel Prizes is not balanced and the database used in this study, Dimensions, suffers from incompleteness and inaccuracy of citation links. Our findings provide useful clues to better understand the characteristics of transformative research in technological and policy impact. This study proposes a new framework to explore the direct and indirect impact on technology and policy originating from transformative research."
Extended Lorenz majorization and frequencies of distances in an undirected network,"Purpose: To contribute to the study of networks and graphs. Design/methodology/approach: We apply standard mathematical thinking. Findings: We show that the distance distribution in an undirected network Lorenz majorize the one of a chain. As a consequence, the average and median distances in any such networ are smaller than or equal to those of a chain. Research limitations: We restricted our investigations to undirected, unweighted networks Practical implications: We are convinced that these results are useful in the study of sma worlds and the so-called six degrees of separation property. Originality/value: To the best of our knowledge our research contains new network results especially those related to frequencies of distances. Copyright: Â© 2024 Leo Egghe.","To contribute to the study of networks and graphs. We apply standard mathematical thinking. We show that the distance distribution in an undirected network Lorenz majorize the one of a chain. As a consequence, the average and median distances in any such networ are smaller than or equal to those of a chain. We restricted our investigations to undirected, unweighted networks We are convinced that these results are useful in the study of sma worlds and the so-called six degrees of separation property. To the best of our knowledge our research contains new network results especially those related to frequencies of distances."
Substantiality: A Construct Indicating Research Excellence to Measure University Research Performance,"Purpose: The adequacy of research performance of universities or research institutes have often been evaluated and understood in two axes: ""quantity""(i.e. size or volume) and ""quality""(i.e. what we define here as a measure of excellence that is considered theoretically independent of size or volume, such as clarity in diamond grading). The purpose of this article is, however, to introduce a third construct named ""substantiality""(""ATSUMI""in Japanese) of research performance and to demonstrate its importance in evaluating/understanding research universities. Design/methodology/approach: We take a two-step approach to demonstrate the effectiveness of the proposed construct by showing that (1) some characteristics of research universities are not well captured by the conventional constructs (""quantity""and ""quality"")-based indicators, and (2) the ""substantiality""indicators can capture them. Furthermore, by suggesting that ""substantiality""indicators appear linked to the reputation that appeared in university reputation rankings by simple statistical analysis, we reveal additional benefits of the construct. Findings: We propose a new construct named ""substantiality""for measuring research performance. We show that indicators based on ""substantiality""can capture important characteristics of research institutes. ""Substantiality""indicators demonstrate their ""predictive powers""on research reputation. Research limitations: The concept of ""substantiality""originated from IGO game; therefore the ease/difficulty of accepting the concept is culturally dependent. In other words, while it is easily accepted by people from Japan and other East Asian countries and regions, it might be difficult for researchers from other cultural regions to accept it. Practical implications: There is no simple solution to the challenge of evaluating research universities' research performance. It is vital to combine different types of indicators to understand the excellence of research institutes. Substantiality indicators could be part of such a combination of indicators. Originality/value: The authors propose a new construct named substantiality for measuring research performance. They show that indicators based on this construct can capture the important characteristics of research institutes.  Â© 2021 Masashi Shirabe et al., published by Sciendo.","The adequacy of research performance of universities or research institutes have often been evaluated and understood in two axes: ""quantity""( size or volume) and ""quality""( what we define here as a measure of excellence that is considered theoretically independent of size or volume, such as clarity in diamond grading). The purpose of this article is, however, to introduce a third construct named ""substantiality""(""ATSUMI""in Japanese) of research performance and to demonstrate its importance in evaluating/understanding research universities. We take a two-step approach to demonstrate the effectiveness of the proposed construct by showing that some characteristics of research universities are not well captured by the conventional constructs (""quantity""and ""quality"")-based indicators, and the ""substantiality""indicators can capture them. Furthermore, by suggesting that ""substantiality""indicators appear linked to the reputation that appeared in university reputation rankings by simple statistical analysis, we reveal additional benefits of the construct. We propose a new construct named ""substantiality""for measuring research performance. We show that indicators based on ""substantiality""can capture important characteristics of research institutes. ""Substantiality""indicators demonstrate their ""predictive powers""on research reputation. The concept of ""substantiality""originated from IGO game; therefore the ease/difficulty of accepting the concept is culturally dependent. In other words, while it is easily accepted by people from Japan and other East Asian countries and regions, it might be difficult for researchers from other cultural regions to accept it. There is no simple solution to the challenge of evaluating research universities' research performance. It is vital to combine different types of indicators to understand the excellence of research institutes. Substantiality indicators could be part of such a combination of indicators. The authors propose a new construct named substantiality for measuring research performance. They show that indicators based on this construct can capture the important characteristics of research institutes."
Can ChatGPT evaluate research quality?,"Purpose: Assess whether ChatGPT 4.0 is accurate enough to perform research evaluations on journal articles to automate this time-consuming task. Design/methodology/approach: Test the extent to which ChatGPT-4 can assess the quality of journal articles using a case study of the published scoring guidelines of the UK Research Excellence Framework (REF) 2021 to create a research evaluation ChatGPT. This was applied to 51 of my own articles and compared against my own quality judgements. Findings: ChatGPT-4 can produce plausible document summaries and quality evaluation rationales that match the REF criteria. Its overall scores have weak correlations with my self-evaluation scores of the same documents (averaging r=0.281 over 15 iterations, with 8 being statistically significantly different from 0). In contrast, the average scores from the 15 iterations produced a statistically significant positive correlation of 0.509. Thus, averaging scores from multiple ChatGPT-4 rounds seems more effective than individual scores. The positive correlation may be due to ChatGPT being able to extract the author's significance, rigour, and originality claims from inside each paper. If my weakest articles are removed, then the correlation with average scores (r=0.200) falls below statistical significance, suggesting that ChatGPT struggles to make fine-grained evaluations. Research limitations: The data is self-evaluations of a convenience sample of articles from one academic in one field. Practical implications: Overall, ChatGPT does not yet seem to be accurate enough to be trusted for any formal or informal research quality evaluation tasks. Research evaluators, including journal editors, should therefore take steps to control its use. Originality/value: This is the first published attempt at post-publication expert review accuracy testing for ChatGPT.  Â© 2024 Mike Thelwall, published by Sciendo.","Assess whether ChatGPT 4.0 is accurate enough to perform research evaluations on journal articles to automate this time-consuming task. Test the extent to which ChatGPT-4 can assess the quality of journal articles using a case study of the published scoring guidelines of the UK Research Excellence Framework (REF) 2021 to create a research evaluation ChatGPT. This was applied to 51 of my own articles and compared against my own quality judgements. ChatGPT-4 can produce plausible document summaries and quality evaluation rationales that match the REF criteria. Its overall scores have weak correlations with my self-evaluation scores of the same documents (averaging r=0.281 over 15 iterations, with 8 being statistically significantly different from 0). In contrast, the average scores from the 15 iterations produced a statistically significant positive correlation of 0.509. Thus, averaging scores from multiple ChatGPT-4 rounds seems more effective than individual scores. The positive correlation may be due to ChatGPT being able to extract the author's significance, rigour, and originality claims from inside each paper. If my weakest articles are removed, then the correlation with average scores (r=0.200) falls below statistical significance, suggesting that ChatGPT struggles to make fine-grained evaluations. The data is self-evaluations of a convenience sample of articles from one academic in one field. Overall, ChatGPT does not yet seem to be accurate enough to be trusted for any formal or informal research quality evaluation tasks. Research evaluators, including journal editors, should therefore take steps to control its use. This is the first published attempt at post-publication expert review accuracy testing for ChatGPT."
A Novel Metric for Assessing National Strength in Scientific Research: Understanding China's Research Output in Quantum Technology through Collaboration,"Purpose: The 5th Plenary Session of the 19th Communist Party of China (CPC) Central Committee clearly states that developing science and technology through self-reliance and self-strengthening provides the strategic underpinning for China's development. Based on this background, this paper explores a metric model for assessing national scientific research strength through collaboration on research papers. Design/methodology/approach: We propose a novel metric model for assessing national scientific research strength, which sets two indicators, national scientific self-reliance (SR) and national academic contribution (CT), to reflect ""self-reliance""and ""self-strengthening""respectively. Taking the research papers in quantum technology as an example, this study analyzes the scientific research strength of various countries around the world, especially China in quantum technology. Findings: The results show that the research of quantum technology in China has always been relatively independent with fewer international collaboration papers and located in a more marginal position in cooperation networks. China's academic contribution (CT) to global quantum technology research is increasing and has been greater than that of the United States in 2020. Combining the two indicators, CT and SR, China's research strength in the quantum field closely follows the United States, and the United States is the most powerful with high research autonomy. Research limitations: This paper only reflects China's scientific research strength in quantum technology from collaboration on research papers and doesn't consider the segmentation of quantum technology and the industrial upstream and downstream aspects, which need further study. Practical implications: The model is helpful to better understand the national scientific research strength in a certain field from ""self-reliance""and ""self-strengthening"". Originality/value: We propose a novel metric model to measure the national scientific research strength from the perspective of ""self-reliance""and ""self-strengthening"", which provides a solid basis for the assessment of the strength level of scientific research in countries/regions and institutions. Â© 2022 Yuqi Wang et al., published by Sciendo.","The 5th Plenary Session of the 19th Communist Party of China (CPC) Central Committee clearly states that developing science and technology through self-reliance and self-strengthening provides the strategic underpinning for China's development. Based on this background, this paper explores a metric model for assessing national scientific research strength through collaboration on research papers. We propose a novel metric model for assessing national scientific research strength, which sets two indicators, national scientific self-reliance (SR) and national academic contribution (CT), to reflect ""self-reliance""and ""self-strengthening""respectively. Taking the research papers in quantum technology as an example, this study analyzes the scientific research strength of various countries around the world, especially China in quantum technology. The results show that the research of quantum technology in China has always been relatively independent with fewer international collaboration papers and located in a more marginal position in cooperation networks. China's academic contribution (CT) to global quantum technology research is increasing and has been greater than that of the United States in 2020. Combining the two indicators, CT and SR, China's research strength in the quantum field closely follows the United States, and the United States is the most powerful with high research autonomy. This paper only reflects China's scientific research strength in quantum technology from collaboration on research papers and doesn't consider the segmentation of quantum technology and the industrial upstream and downstream aspects, which need further study. The model is helpful to better understand the national scientific research strength in a certain field from ""self-reliance""and ""self-strengthening"". We propose a novel metric model to measure the national scientific research strength from the perspective of ""self-reliance""and ""self-strengthening"", which provides a solid basis for the assessment of the strength level of scientific research in countries/regions and institutions."
The Three-Step Workflow: A Pragmatic Approach to Allocating Academic Hospitalsâ Affiliations for Bibliometric Purposes,"Abstract Purpose A key question when ranking universities is whether or not to allocate the publication output of affiliated hospitals to universities. This paper presents a method for classifying the varying degrees of interdependency between academic hospitals and universities in the context of the Leiden Ranking. Design/methodology/approach Hospital nomenclatures vary worldwide to denote some form of collaboration with a university, however they do not correspond to universally standard definitions. Thus, rather than seeking a normative definition of academic hospitals, we propose a three-step workflow that aligns the university-hospital relationship with one of three general models: full integration of the hospital and the medical faculty into a single organization; health science centres in which hospitals and medical faculty remain separate entities albeit within the same governance structure; and structures in which universities and hospitals are separate entities which collaborate with one another. This classification system provides a standard through which publications which mention affiliations with academic hospitals can be better allocated. Findings In the paper we illustrate how the three-step workflow effectively translates the three above-mentioned models into two types of instrumental relationships for the assignation of publications: âassociateâ and âcomponentâ. When a hospital and a medical faculty are fully integrated or when a hospital is part of a health science centre, the relationship is classified as component. When a hospital follows the model of collaboration and support, the relationship is classified as associate. The compilation of data following these standards allows for a more uniform comparison between worldwide educational and research systems. Research limitations The workflow is resource intensive, depends heavily on the information provided by universities and hospitals, and is more challenging for languages that use non-Latin characters. Further, the application of the workflow demands a careful evaluation of different types of input which can result in ambiguity and makes it difficult to automatize. Practical implications Determining the type of affiliation an academic hospital has with a university can have a substantial impact on the publication counts for universities. This workflow can also aid in analysing collaborations among the two types of organizations. Originality/value The three-step workflow is a unique way to establish the type of relationship an academic hospital has with a university accounting for national and regional differences on nomenclature. Â© 2022 Andrea Reyes Elizondo et al., published by Sciendo","Abstract Purpose A key question when ranking universities is whether or not to allocate the publication output of affiliated hospitals to universities. This paper presents a method for classifying the varying degrees of interdependency between academic hospitals and universities in the context of the Leiden Ranking. Design/methodology/approach Hospital nomenclatures vary worldwide to denote some form of collaboration with a university, however they do not correspond to universally standard definitions. Thus, rather than seeking a normative definition of academic hospitals, we propose a three-step workflow that aligns the university-hospital relationship with one of three general models: full integration of the hospital and the medical faculty into a single organization; health science centres in which hospitals and medical faculty remain separate entities albeit within the same governance structure; and structures in which universities and hospitals are separate entities which collaborate with one another. This classification system provides a standard through which publications which mention affiliations with academic hospitals can be better allocated. Findings In the paper we illustrate how the three-step workflow effectively translates the three above-mentioned models into two types of instrumental relationships for the assignation of publications: associate and component. When a hospital and a medical faculty are fully integrated or when a hospital is part of a health science centre, the relationship is classified as component. When a hospital follows the model of collaboration and support, the relationship is classified as associate. The compilation of data following these standards allows for a more uniform comparison between worldwide educational and research systems. Research limitations The workflow is resource intensive, depends heavily on the information provided by universities and hospitals, and is more challenging for languages that use non-Latin characters. Further, the application of the workflow demands a careful evaluation of different types of input which can result in ambiguity and makes it difficult to automatize. Practical implications Determining the type of affiliation an academic hospital has with a university can have a substantial impact on the publication counts for universities. This workflow can also aid in analysing collaborations among the two types of organizations. Originality/value The three-step workflow is a unique way to establish the type of relationship an academic hospital has with a university accounting for national and regional differences on nomenclature."
A novel approach based on journal coupling to determine authors who are most likely to be part of the same invisible college,"Purpose: In this paper, we use author clustering based on journal coupling (i.e., shared academic journals) to determine researchers who have the same scientific interests and similar conceptual frameworks. The basic assumption is that authors who publish in the same academic journals are more likely to share similar conceptual frameworks and interests than those who never publish in the same venues. Therefore, they are more likely to be part of the same invisible college (i.e., authors in this subgroup contribute materially to research on the same topic and often publish their work in similar publication venues). Design/methodology/approach: Test in a controlled exercise the grouping of authors based on journal coupling to determine invisible colleges in a research field using a case study of 302 authors who had published in the Information Science and Library Science (IS&LS) category of the Web of Science Core Collection. For each author, we retrieved all the scientific journals in which this author had published his/her articles. We then used the cosine measure to calculate the similarity between authors (both first and second order). Findings: In this paper, using journal coupling of IS&LS authors, we found four main invisible colleges: âInformation Systemsâ, âBusiness and Information Managementâ, âQuantitative Information Scienceâ and âLibrary Science.â The main journals that determine the existence of these invisible colleges were Inform Syst Res, Inform Syst J, J Bus Res, J Knowl Manage, J Informetr, Pro Int Conf Sci Inf, Int J Geogr Inf Sci, J Am Med Inform Assn, and Learn Publ. However, the main journals that demonstrate that IS&LS determine a field were J Am Soc Inf Sci Tec/J Assoc Inf Sci Tech, Scientometrics, Inform Process Manag, and J Inf Sci. Research limitations: The results shown in this article are from a controlled exercise. The analysis performed using journal coupling excludes books, book chapters, and conference papers. In this article, only academic journals were used for the representation of research results. Practical implications: Our results may be of interest to IS&LS scholars. This is because these results provide a new lens for grouping authors, making use of the authors' journal publication profile and journal coupling. Furthermore, extending our approach to the study of the structure of other disciplines would possibly be of interest to historians of science as well as scientometricians. Originality/value: This is a novel approach based on journal coupling to determine authors who are most likely to be part of the same invisible college. Copyright: Â© 2025 Jose A. Garcia, Rosa Rodriguez-Sanchez, J. Fdez-Valdivia.","In this paper, we use author clustering based on journal coupling (, shared academic journals) to determine researchers who have the same scientific interests and similar conceptual frameworks. The basic assumption is that authors who publish in the same academic journals are more likely to share similar conceptual frameworks and interests than those who never publish in the same venues. Therefore, they are more likely to be part of the same invisible college (, authors in this subgroup contribute materially to research on the same topic and often publish their work in similar publication venues). Test in a controlled exercise the grouping of authors based on journal coupling to determine invisible colleges in a research field using a case study of 302 authors who had published in the Information Science and Library Science (IS&LS) category of the Web of Science Core Collection. For each author, we retrieved all the scientific journals in which this author had published his/her articles. We then used the cosine measure to calculate the similarity between authors (both first and second order). In this paper, using journal coupling of IS&LS authors, we found four main invisible colleges: Information Systems, Business and Information Management, Quantitative Information Science and Library Science. The main journals that determine the existence of these invisible colleges were Inform Syst Res, Inform Syst J, J Bus Res, J Knowl Manage, J Informetr, Pro Int Conf Sci Inf, Int J Geogr Inf Sci, J Am Med Inform Assn, and Learn Publ. However, the main journals that demonstrate that IS&LS determine a field were J Am Soc Inf Sci Tec/J Assoc Inf Sci Tech, Scientometrics, Inform Process Manag, and J Inf Sci. The results shown in this article are from a controlled exercise. The analysis performed using journal coupling excludes books, book chapters, and conference papers. In this article, only academic journals were used for the representation of research results. Our results may be of interest to IS&LS scholars. This is because these results provide a new lens for grouping authors, making use of the authors' journal publication profile and journal coupling. Furthermore, extending our approach to the study of the structure of other disciplines would possibly be of interest to historians of science as well as scientometricians. This is a novel approach based on journal coupling to determine authors who are most likely to be part of the same invisible college."
Text duplication of papers in four medical related fields,"Purpose: To reveal the typical features of text duplication in papers from four medical fields: basic medicine, health management, pharmacology and pharmacy, and public health and preventive medicine. To analyze the reasons for duplication and provide suggestions for the management of medical academic misconduct. Design/methodology/approach: In total, 2,469 representative Chinese journal papers were included in our research, which were submitted by researchers in 2020 and 2021. A plagiarism check was carried out using the Academic Misconduct Literature Check System (AMLC). We generated a corrected similarity index based on the AMLC general similarity index for further analysis. We compared the similarity indices of papers in four medical fields and revealed their trends over time; differences in similarity index between review and research articles were also analyzed according to the different fields. Further analysis of 143 papers suspected of plagiarism was also performed from the perspective of sections containing duplication and according to the field of research. Findings: Papers in the field of pharmacology and pharmacy had the highest similarity index (8.67 & plusmn; 5.92%), which was significantly higher than that in other fields, except health management. The similarity index of review articles (9.77 & plusmn; 10.28%) was significantly higher than that of research articles (7.41 & plusmn; 6.26%). In total, 143 papers were suspected of plagiarism (5.80%) with similarity indices & ge; 15%; most were papers on health management (78, 54.55%), followed by public health and preventive medicine (38, 26.58%); 90.21% of the 143 papers had duplication in multiple sections, while only 9.79% had duplication in a single section. The distribution of sections with duplication varied among different fields; papers in pharmacology and pharmacy were more likely to have duplication in the data/methods and introduction/background sections, however, papers in health management were more likely to contain duplication in the introduction/background or results/discussion sections. Different structures for papers in different fields may have caused these differences. Research limitations: There were three limitations to our research. Firstly, we observed that a small number of papers have been checked early. It is unknown who conducted the plagiarism check as this can be included in other evaluations, such as applications for Science and technology projects or awards. If the authors carried out the check, text with high similarity indices may have been excluded before submission, meaning the similarity index in our research may have been lower than the original value. Secondly, there were only four medical fields included in our research. Additional analysis on a wider scale is required in the future. Thirdly, only a general similarity index was calculated in our study; other similarity indices were not tested. Practical implications: A comprehensive analysis of similarity indices in four medical fields was performed. We made several recommendations for the supervision of medical academic misconduct and the formation of criteria for defining suspected plagiarism for medical papers, as well as for the improved accuracy of text duplication checks. Originality/value: We quantified the differences between the AMLC general similarity index and the corrected index, described the situation around text duplication and plagiarism in papers from four medical fields, and revealed differences in similarity indices between different article types. We also revealed differences in the sections containing duplication for papers with suspected plagiarism among different fields. & copy; 2023 Ping Ni et al., published by Sciendo. Â© 2023 Sciendo. All rights reserved.","To reveal the typical features of text duplication in papers from four medical fields: basic medicine, health management, pharmacology and pharmacy, and public health and preventive medicine. To analyze the reasons for duplication and provide suggestions for the management of medical academic misconduct. In total, 2,469 representative Chinese journal papers were included in our research, which were submitted by researchers in 2020 and 2021. A plagiarism check was carried out using the Academic Misconduct Literature Check System (AMLC). We generated a corrected similarity index based on the AMLC general similarity index for further analysis. We compared the similarity indices of papers in four medical fields and revealed their trends over time; differences in similarity index between review and research articles were also analyzed according to the different fields. Further analysis of 143 papers suspected of plagiarism was also performed from the perspective of sections containing duplication and according to the field of research. Papers in the field of pharmacology and pharmacy had the highest similarity index (8.67 & plusmn; 5.92%), which was significantly higher than that in other fields, except health management. The similarity index of review articles (9.77 & plusmn; 10.28%) was significantly higher than that of research articles (7.41 & plusmn; 6.26%). In total, 143 papers were suspected of plagiarism (5.80%) with similarity indices & ge; 15%; most were papers on health management (78, 54.55%), followed by public health and preventive medicine (38, 26.58%); 90.21% of the 143 papers had duplication in multiple sections, while only 9.79% had duplication in a single section. The distribution of sections with duplication varied among different fields; papers in pharmacology and pharmacy were more likely to have duplication in the data/methods and introduction/background sections, however, papers in health management were more likely to contain duplication in the introduction/background or results/discussion sections. Different structures for papers in different fields may have caused these differences. There were three limitations to our research. Firstly, we observed that a small number of papers have been checked early. It is unknown who conducted the plagiarism check as this can be included in other evaluations, such as applications for Science and technology projects or awards. If the authors carried out the check, text with high similarity indices may have been excluded before submission, meaning the similarity index in our research may have been lower than the original value. Secondly, there were only four medical fields included in our research. Additional analysis on a wider scale is required in the future. Thirdly, only a general similarity index was calculated in our study; other similarity indices were not tested. A comprehensive analysis of similarity indices in four medical fields was performed. We made several recommendations for the supervision of medical academic misconduct and the formation of criteria for defining suspected plagiarism for medical papers, as well as for the improved accuracy of text duplication checks. We quantified the differences between the AMLC general similarity index and the corrected index, described the situation around text duplication and plagiarism in papers from four medical fields, and revealed differences in similarity indices between different article types. We also revealed differences in the sections containing duplication for papers with suspected plagiarism among different fields. & copy; 2023 Ping Ni et al., published by Sciendo."
Extracting and Measuring Uncertain Biomedical Knowledge from Scientific Statements,"Purpose: Given the information overload of scientific literature, there is an increasing need for computable biomedical knowledge buried in free text. This study aimed to develop a novel approach to extracting and measuring uncertain biomedical knowledge from scientific statements. Design/methodology/approach: Taking cardiovascular research publications in China as a sample, we extracted subject-predicate-object triples (SPO triples) as knowledge units and unknown/hedging/conflicting uncertainties as the knowledge context. We introduced information entropy (IE) as potential metric to quantify the uncertainty of epistemic status of scientific knowledge represented at subject-object pairs (SO pairs) levels. Findings: The results indicated an extraordinary growth of cardiovascular publications in China while only a modest growth of the novel SPO triples. After evaluating the uncertainty of biomedical knowledge with IE, we identified the Top 10 SO pairs with highest IE, which implied the epistemic status pluralism. Visual presentation of the SO pairs overlaid with uncertainty provided a comprehensive overview of clusters of biomedical knowledge and contending topics in cardiovascular research. Research limitations: The current methods didn't distinguish the specificity and probabilities of uncertainty cue words. The number of sentences surrounding a given triple may also influence the value of IE. Practical implications: Our approach identified major uncertain knowledge areas such as diagnostic biomarkers, genetic polymorphism and co-existing risk factors related to cardiovascular diseases in China. These areas are suggested to be prioritized; new hypotheses need to be verified, while disputes, conflicts, and contradictions need to be settled. Originality/value: We provided a novel approach by combining natural language processing and computational linguistics with informetric methods to extract and measure uncertain knowledge from scientific statements.  Â© 2022 Xin Guo et al., published by Sciendo.","Given the information overload of scientific literature, there is an increasing need for computable biomedical knowledge buried in free text. This study aimed to develop a novel approach to extracting and measuring uncertain biomedical knowledge from scientific statements. Taking cardiovascular research publications in China as a sample, we extracted subject-predicate-object triples (SPO triples) as knowledge units and unknown/hedging/conflicting uncertainties as the knowledge context. We introduced information entropy (IE) as potential metric to quantify the uncertainty of epistemic status of scientific knowledge represented at subject-object pairs (SO pairs) levels. The results indicated an extraordinary growth of cardiovascular publications in China while only a modest growth of the novel SPO triples. After evaluating the uncertainty of biomedical knowledge with IE, we identified the Top 10 SO pairs with highest IE, which implied the epistemic status pluralism. Visual presentation of the SO pairs overlaid with uncertainty provided a comprehensive overview of clusters of biomedical knowledge and contending topics in cardiovascular research. The current methods didn't distinguish the specificity and probabilities of uncertainty cue words. The number of sentences surrounding a given triple may also influence the value of IE. Our approach identified major uncertain knowledge areas such as diagnostic biomarkers, genetic polymorphism and co-existing risk factors related to cardiovascular diseases in China. These areas are suggested to be prioritized; new hypotheses need to be verified, while disputes, conflicts, and contradictions need to be settled. We provided a novel approach by combining natural language processing and computational linguistics with informetric methods to extract and measure uncertain knowledge from scientific statements."
"Ranking academic institutions based on the productivity, impact, and quality of institutional scholars","Purpose: The quantitative rankings of over 55,000 institutions and their institutional programs are based on the individual rankings of approximately 30 million scholars determined by their productivity, impact, and quality. Design/methodology/approach: The institutional ranking process developed here considers all institutions in all countries and regions, thereby including those that are established, as well as those that are emerging in scholarly prowess. Rankings of individual scholars worldwide are first generated using the recently introduced, fully indexed ScholarGPS database. The rankings of individual scholars are extended here to determine the lifetime and last-five-year Top 20 rankings of academic institutions over all Fields of scholarly endeavor, in 14 individual Fields, in 177 Disciplines, and in approximately 350,000 unique Specialties. Rankings associated with five specific Fields (Medicine, Engineering & Computer Science, Life Sciences, Physical Sciences & Mathematics, and Social Sciences), and in two Disciplines (Chemistry, and Electrical & Computer Engineering) are presented as examples, and changes in the rankings over time are discussed. Findings: For the Fields considered here, the Top 20 institutional rankings in Medicine have undergone the least change (lifetime versus last five years), while the rankings in Engineering & Computer Science have exhibited significant change. The evolution of institutional rankings over time is largely attributed to the recent emergence of Chinese academic institutions, although this emergence is shown to be highly Field- and Discipline-dependent. Research limitations: The ScholarGPS database used here ranks institutions in the categories of: (i) all Fields, (ii) in 14 individual Fields, (iii) in 177 Disciplines, and (iv) in approximately 350,000 unique Specialties. A comprehensive investigation covering all categories is not practical. Practical implementations: Existing rankings of academic institutions have: (i) often been restricted to pre-selected institutions, clouding the potential discovery of scholarly activity in emerging institutions and countries; (ii) considered only broad areas of research, limiting the ability of university leadership to act on the assessments in a concrete manner, or in contrast; (iii) have considered only a narrow area of research for comparison, diminishing the broader applicability and impact of the assessment. In general, existing institutional rankings depend on which institutions are included in the ranking process, which areas of research are considered, the breadth (or granularity) of the research areas of interest, and the methodologies used to define and quantify research performance. In contrast, the methods presented here can provide important data over a broad range of granularity to allow responsible individuals to gauge the performance of any institution from the Overall (all Fields) level, to the level of the Specialty. The methods may also assist identification of the root causes of shifts in institution rankings, and how these shifts vary across hundreds of thousands of Fields, Disciplines, and Specialties of scholarly endeavor. Originality/value: This study provides the first ranking of all academic institutions worldwide over Fields, Disciplines, and Specialties based on a unique methodology that quantifies the productivity, impact, and quality of individual scholars.  Â© 2024 Amir Faghri et al., published by Sciendo.","The quantitative rankings of over 55,000 institutions and their institutional programs are based on the individual rankings of approximately 30 million scholars determined by their productivity, impact, and quality. The institutional ranking process developed here considers all institutions in all countries and regions, thereby including those that are established, as well as those that are emerging in scholarly prowess. Rankings of individual scholars worldwide are first generated using the recently introduced, fully indexed ScholarGPS database. The rankings of individual scholars are extended here to determine the lifetime and last-five-year Top 20 rankings of academic institutions over all Fields of scholarly endeavor, in 14 individual Fields, in 177 Disciplines, and in approximately 350,000 unique Specialties. Rankings associated with five specific Fields (Medicine, Engineering & Computer Science, Life Sciences, Physical Sciences & Mathematics, and Social Sciences), and in two Disciplines (Chemistry, and Electrical & Computer Engineering) are presented as examples, and changes in the rankings over time are discussed. For the Fields considered here, the Top 20 institutional rankings in Medicine have undergone the least change (lifetime versus last five years), while the rankings in Engineering & Computer Science have exhibited significant change. The evolution of institutional rankings over time is largely attributed to the recent emergence of Chinese academic institutions, although this emergence is shown to be highly Field- and Discipline-dependent. The ScholarGPS database used here ranks institutions in the categories of: all Fields, in 14 individual Fields, in 177 Disciplines, and in approximately 350,000 unique Specialties. A comprehensive investigation covering all categories is not practical. Practical implementations: Existing rankings of academic institutions have: often been restricted to pre-selected institutions, clouding the potential discovery of scholarly activity in emerging institutions and countries; considered only broad areas of research, limiting the ability of university leadership to act on the assessments in a concrete manner, or in contrast; have considered only a narrow area of research for comparison, diminishing the broader applicability and impact of the assessment. In general, existing institutional rankings depend on which institutions are included in the ranking process, which areas of research are considered, the breadth (or granularity) of the research areas of interest, and the methodologies used to define and quantify research performance. In contrast, the methods presented here can provide important data over a broad range of granularity to allow responsible individuals to gauge the performance of any institution from the Overall (all Fields) level, to the level of the Specialty. The methods may also assist identification of the root causes of shifts in institution rankings, and how these shifts vary across hundreds of thousands of Fields, Disciplines, and Specialties of scholarly endeavor. This study provides the first ranking of all academic institutions worldwide over Fields, Disciplines, and Specialties based on a unique methodology that quantifies the productivity, impact, and quality of individual scholars."
How does network intermediary affect collaborative innovation? Evidence from Chinese listed companies,"Purpose: This study aims to explore how network intermediaries influence collaborative innovation performance within inter-organizational technological collaboration networks. Design/methodology/approach: This study employs a mixed-method approach, combining quantitative social network analysis with regression techniques to investigate the role of network intermediaries in collaborative innovation performance. Using a patent dataset of Chinese industrial enterprises, the research constructs the collaboration networks and analyzes their structural positions, particularly focusing on their role as intermediaries, characterized by betweenness centrality. Negative binomial regression analysis is employed to assess how these network characteristics shape innovation outcomes. Findings: The study reveals that firms in intermediary positions enhance collaborative innovation performance, but this effect is nuanced. A key finding is that network clustering negatively moderates the intermediary-innovation relationship. Highly clustered networks, while fostering local collaboration, may limit the innovation potential of intermediaries. On the other hand, relationship strength, measured by collaboration intensity and trust among firms, positively moderates the intermediary-innovation link. Research limitations: This study has several limitations that present opportunities for further research. The reliance on quantitative social network analysis may overlook the complexity of intermediariesâ roles, and future studies could benefit from incorporating qualitative methods to better understand cultural and institutional factors. Additionally, cross-country comparisons are needed to assess the consistency of these dynamics in different contexts. Practical implications: The study offers practical insights for firms and policymakers. Organizations should strategically position themselves as network intermediaries to access diverse information and resources, thereby improving innovation performance. Building strong trust helps using network intermediary advantages. For firms in highly clustered networks, it is important to seek external partners to avoid limiting their exposure to new ideas and technologies. This research emphasizes the need to balance network diversity with relationship strength for sustained innovation. Originality/value: This research contributes to the literature by offering new insights into the role of network intermediaries, presenting a comprehensive framework for understanding the interaction between network dynamics and firm innovation. Â© 2024 Zhiwei Zhang, Wenhao Zhou.","This study aims to explore how network intermediaries influence collaborative innovation performance within inter-organizational technological collaboration networks. This study employs a mixed-method approach, combining quantitative social network analysis with regression techniques to investigate the role of network intermediaries in collaborative innovation performance. Using a patent dataset of Chinese industrial enterprises, the research constructs the collaboration networks and analyzes their structural positions, particularly focusing on their role as intermediaries, characterized by betweenness centrality. Negative binomial regression analysis is employed to assess how these network characteristics shape innovation outcomes. The study reveals that firms in intermediary positions enhance collaborative innovation performance, but this effect is nuanced. A key finding is that network clustering negatively moderates the intermediary-innovation relationship. Highly clustered networks, while fostering local collaboration, may limit the innovation potential of intermediaries. On the other hand, relationship strength, measured by collaboration intensity and trust among firms, positively moderates the intermediary-innovation link. This study has several limitations that present opportunities for further research. The reliance on quantitative social network analysis may overlook the complexity of intermediaries roles, and future studies could benefit from incorporating qualitative methods to better understand cultural and institutional factors. Additionally, cross-country comparisons are needed to assess the consistency of these dynamics in different contexts. The study offers practical insights for firms and policymakers. Organizations should strategically position themselves as network intermediaries to access diverse information and resources, thereby improving innovation performance. Building strong trust helps using network intermediary advantages. For firms in highly clustered networks, it is important to seek external partners to avoid limiting their exposure to new ideas and technologies. This research emphasizes the need to balance network diversity with relationship strength for sustained innovation. This research contributes to the literature by offering new insights into the role of network intermediaries, presenting a comprehensive framework for understanding the interaction between network dynamics and firm innovation."
Characterizing structure of cross-disciplinary impact of global disciplines: A perspective of the Hierarchy of Science,"Purpose: Interdisciplinary fields have become the driving force of modern science and a significant source of scientific innovation. However, there is still a paucity of analysis about the essential characteristics of disciplinesâ cross-disciplinary impact. Design/methodology/approach: In this study, we define cross-disciplinary impact on one discipline as its impact to other disciplines, and refer to a three-dimensional framework of variety-balance-disparity to characterize the structure of cross-disciplinary impact. The variety of cross-disciplinary impact of the discipline was defined as the proportion of the high cross-disciplinary impact publications, and the balance and disparity of cross-disciplinary impact were measured as well. To demonstrate the cross-disciplinary impact of the disciplines in science, we chose Microsoft Academic Graph (MAG) as the data source, and investigated the relationship between disciplinesâ cross-disciplinary impact and their positions in the Hierarchy of Science (HOS). Findings: Analytical results show that there is a significant correlation between the ranking of cross-disciplinary impact and the HOS structure, and that the discipline exerts a greater cross-disciplinary impact on its neighboring disciplines. Several bibliometric features that measure the hardness of a discipline, including the number of references, the number of cited disciplines, the citation distribution, and the Price index have a significant positive effect on the variety of cross-disciplinary impact. The number of references, the number of cited disciplines, and the citation distribution have significant positive and negative effects on balance and disparity, respectively. It is concluded that the less hard the discipline, the greater the cross-disciplinary impact, the higher balance and the lower disparity of cross-disciplinary impact. Research limitations: In the empirical analysis of HOS, we only included five broad disciplines. This study also has some biases caused by the data source and applied regression models. Practical implications: This study contributes to the formulation of discipline-specific policies and promotes the growth of interdisciplinary research, as well as offering fresh insights for predicting the cross-disciplinary impact of disciplines. Originality/value: This study provides a new perspective to properly understand the mechanisms of cross-disciplinary impact and disciplinary integration. Â© 2024 Ruolan Liu, Jin Mao, Gang Li, Yujie Cao. Published under a Creative Commons Attribution 4.0 International (CC BY 4.0) license.","Interdisciplinary fields have become the driving force of modern science and a significant source of scientific innovation. However, there is still a paucity of analysis about the essential characteristics of disciplines cross-disciplinary impact. In this study, we define cross-disciplinary impact on one discipline as its impact to other disciplines, and refer to a three-dimensional framework of variety-balance-disparity to characterize the structure of cross-disciplinary impact. The variety of cross-disciplinary impact of the discipline was defined as the proportion of the high cross-disciplinary impact publications, and the balance and disparity of cross-disciplinary impact were measured as well. To demonstrate the cross-disciplinary impact of the disciplines in science, we chose Microsoft Academic Graph (MAG) as the data source, and investigated the relationship between disciplines cross-disciplinary impact and their positions in the Hierarchy of Science (HOS). Analytical results show that there is a significant correlation between the ranking of cross-disciplinary impact and the HOS structure, and that the discipline exerts a greater cross-disciplinary impact on its neighboring disciplines. Several bibliometric features that measure the hardness of a discipline, including the number of references, the number of cited disciplines, the citation distribution, and the Price index have a significant positive effect on the variety of cross-disciplinary impact. The number of references, the number of cited disciplines, and the citation distribution have significant positive and negative effects on balance and disparity, respectively. It is concluded that the less hard the discipline, the greater the cross-disciplinary impact, the higher balance and the lower disparity of cross-disciplinary impact. In the empirical analysis of HOS, we only included five broad disciplines. This study also has some biases caused by the data source and applied regression models. This study contributes to the formulation of discipline-specific policies and promotes the growth of interdisciplinary research, as well as offering fresh insights for predicting the cross-disciplinary impact of disciplines. This study provides a new perspective to properly understand the mechanisms of cross-disciplinary impact and disciplinary integration."
A Topic Detection Method Based on Word-attention Networks,"Purpose: We proposed a method to represent scientific papers by a complex network, which combines the approaches of neural and complex networks. Design/methodology/approach: Its novelty is representing a paper by a word branch, which carries the sequential structure of words in sentences. The branches are generated by the attention mechanism in deep learning models. We connected those branches at the positions of their common words to generate networks, called word-attention networks, and then detect their communities, defined as topics. Findings: Those detected topics can carry the sequential structure of words in sentences, represent the intra- and inter-sentential dependencies among words, and reveal the roles of words playing in them by network indexes. Research limitations: The parameter setting of our method may depend on practical data. Thus it needs human experience to find proper settings. Practical implications: Our method is applied to the papers of the PNAS, where the discipline designations provided by authors are used as the golden labels of papers' topics. Originality/value: This empirical study shows that the proposed method outperforms the Latent Dirichlet Allocation and is more stable.  Â© 2021 Zheng Xie, published by Sciendo.","We proposed a method to represent scientific papers by a complex network, which combines the approaches of neural and complex networks. Its novelty is representing a paper by a word branch, which carries the sequential structure of words in sentences. The branches are generated by the attention mechanism in deep learning models. We connected those branches at the positions of their common words to generate networks, called word-attention networks, and then detect their communities, defined as topics. Those detected topics can carry the sequential structure of words in sentences, represent the intra- and inter-sentential dependencies among words, and reveal the roles of words playing in them by network indexes. The parameter setting of our method may depend on practical data. Thus it needs human experience to find proper settings. Our method is applied to the papers of the PNAS, where the discipline designations provided by authors are used as the golden labels of papers' topics. This empirical study shows that the proposed method outperforms the Latent Dirichlet Allocation and is more stable."
The Roles of Female Involvement and Risk Aversion in Open Access Publishing Patterns in Vietnamese Social Sciences and Humanities,"Purpose: The open-access (OA) publishing model can help improve researchers' outreach, thanks to its accessibility and visibility to the public. Therefore, the presentation of female researchers can benefit from the OA publishing model. Despite that, little is known about how gender affects OA practices. Thus, the current study explores the effects of female involvement and risk aversion on OA publishing patterns among Vietnamese social sciences and humanities. Design/methodology/approach: The study employed Bayesian Mindsponge Framework (BMF) on a dataset of 3,122 Vietnamese social sciences and humanities (SS&H) publications during 2008-2019. The Mindsponge mechanism was specifically used to construct theoretical models, while Bayesian inference was utilized for fitting models. Findings: The result showed a positive association between female participation and OA publishing probability. However, the positive effect of female involvement on OA publishing probability was negated by the high ratio of female researchers in a publication. OA status was negatively associated with the JIF of the journal in which the publication was published, but the relationship was moderated by the involvement of a female researcher(s). The findings suggested that Vietnamese female researchers might be more likely to publish under the OA model in journals with high JIF for avoiding the risk of public criticism. Research limitations: The study could only provide evidence on the association between female involvement and OA publishing probability. However, whether to publish under OA terms is often determined by the first or corresponding authors, but not necessarily gender-based. Practical implications: Systematically coordinated actions are suggested to better support women and promote the OA movement in Vietnam. Originality/value: The findings show the OA publishing patterns of female researchers in Vietnamese SS&H.  Â© 2022 Minh-Hoang Nguyen et al., published by Sciendo.","The open-access (OA) publishing model can help improve researchers' outreach, thanks to its accessibility and visibility to the public. Therefore, the presentation of female researchers can benefit from the OA publishing model. Despite that, little is known about how gender affects OA practices. Thus, the current study explores the effects of female involvement and risk aversion on OA publishing patterns among Vietnamese social sciences and humanities. The study employed Bayesian Mindsponge Framework (BMF) on a dataset of 3,122 Vietnamese social sciences and humanities (SS&H) publications during 2008-2019. The Mindsponge mechanism was specifically used to construct theoretical models, while Bayesian inference was utilized for fitting models. The result showed a positive association between female participation and OA publishing probability. However, the positive effect of female involvement on OA publishing probability was negated by the high ratio of female researchers in a publication. OA status was negatively associated with the JIF of the journal in which the publication was published, but the relationship was moderated by the involvement of a female researcher(s). The findings suggested that Vietnamese female researchers might be more likely to publish under the OA model in journals with high JIF for avoiding the risk of public criticism. The study could only provide evidence on the association between female involvement and OA publishing probability. However, whether to publish under OA terms is often determined by the first or corresponding authors, but not necessarily gender-based. Systematically coordinated actions are suggested to better support women and promote the OA movement in Vietnam. The findings show the OA publishing patterns of female researchers in Vietnamese SS&"
Publication behaviour and (dis)qualification of chief editors in Turkish national Social Sciences journals,"Purpose: This study investigated the publication behaviour of 573 chief editors managing 432 Social Sciences journals in Turkey. Direct inquiries into editorial qualifications are rare, and this research aims to shed light on editors' scientific leadership capabilities. Design/methodology/approach: This study contrasts insider publication behaviour in national journals with international articles in journals indexed by the Web of Science (WOS) and Scopus. It argues that editors demonstrating a consistent ability to publish in competitive WOS and Scopus indexed journals signal high qualifications, while editors with persistent insider behaviour and strong local orientation signal low qualification. Scientific leadership capability is measured by first-authored publications. Correlation and various regression tests are conducted to identify significant determinants of publication behaviour. Findings: International publications are rare and concentrated on a few individuals, while insider publications are endemic and constitute nearly 40% of all national articles. Editors publish 3.2 insider papers and 8.1 national papers for every SSCI article. 62% (58%) of the editors have no SSCI (Scopus) article, 53% (63%) do not have a single lead-authored WOS (Scopus) article, and 89% publish at least one insider paper. Only a minority consistently publish in international journals; a fifth of the editors have three or more SSCI publications, and a quarter have three or more Scopus articles. Editors with foreign Ph.D. degrees are the most qualified and internationally oriented, whereas non-mobile editors are the most underqualified and underperform other editors by every measure. Illustrating the overall lack of qualification, nearly half of the professor editors and the majority of the WOS and Scopus indexed journal editors have no record of SSCI or Scopus publications. Research limitations: This research relies on local settings that encourage national publications at the expense of international journals. Findings should be evaluated in light of this setting and bearing in mind that narrow localities are more prone to peer favouritism. Practical implications: Incompetent and nepotistic editors pose an imminent threat to Turkish national literature. A lasting solution would likely include the dismissal and replacement of unqualified editors, as well as delisting and closure of dozens of journals that operate in questionable ways and serve little scientific purpose. Originality/value: To my knowledge, this is the first study to document the publication behaviour of national journal chief editors.  Â© 2024 Lokman Tutuncu, published by Sciendo.","This study investigated the publication behaviour of 573 chief editors managing 432 Social Sciences journals in Turkey. Direct inquiries into editorial qualifications are rare, and this research aims to shed light on editors' scientific leadership capabilities. This study contrasts insider publication behaviour in national journals with international articles in journals indexed by the Web of Science (WOS) and Scopus. It argues that editors demonstrating a consistent ability to publish in competitive WOS and Scopus indexed journals signal high qualifications, while editors with persistent insider behaviour and strong local orientation signal low qualification. Scientific leadership capability is measured by first-authored publications. Correlation and various regression tests are conducted to identify significant determinants of publication behaviour. International publications are rare and concentrated on a few individuals, while insider publications are endemic and constitute nearly 40% of all national articles. Editors publish 3.2 insider papers and 8.1 national papers for every SSCI article. 62% (58%) of the editors have no SSCI (Scopus) article, 53% (63%) do not have a single lead-authored WOS (Scopus) article, and 89% publish at least one insider paper. Only a minority consistently publish in international journals; a fifth of the editors have three or more SSCI publications, and a quarter have three or more Scopus articles. Editors with foreign Ph. degrees are the most qualified and internationally oriented, whereas non-mobile editors are the most underqualified and underperform other editors by every measure. Illustrating the overall lack of qualification, nearly half of the professor editors and the majority of the WOS and Scopus indexed journal editors have no record of SSCI or Scopus publications. This research relies on local settings that encourage national publications at the expense of international journals. Findings should be evaluated in light of this setting and bearing in mind that narrow localities are more prone to peer favouritism. Incompetent and nepotistic editors pose an imminent threat to Turkish national literature. A lasting solution would likely include the dismissal and replacement of unqualified editors, as well as delisting and closure of dozens of journals that operate in questionable ways and serve little scientific purpose. To my knowledge, this is the first study to document the publication behaviour of national journal chief editors."
Perspectives from a publishing ethics and research integrity team for required improvements,"It is imperative that all stakeholders within the research ecosystem take responsibility to improve research integrity and reliability of published research. Based on the unique experiences of a specialist publishing ethics and research integrity team within a major publisher, this article provides insights into the observed trends of misconduct and how those have evolved over time, and addresses key actions needed to improve the interface between researchers, funders, institutions and publishers to collectively improve research integrity on a global scale.  Â© 2023 Sabina Alam et al., published by Sciendo.","It is imperative that all stakeholders within the research ecosystem take responsibility to improve research integrity and reliability of published research. Based on the unique experiences of a specialist publishing ethics and research integrity team within a major publisher, this article provides insights into the observed trends of misconduct and how those have evolved over time, and addresses key actions needed to improve the interface between researchers, funders, institutions and publishers to collectively improve research integrity on a global scale."
Citation and bibliographic coupling between authors in the field of social network analysis,"Purpose: We analyzed the structure of a community of authors working in the field of social network analysis (SNA) based on citation indicators: direct citation and bibliographic coupling metrics. We observed patterns at the micro, meso, and macro levels of analysis. Design/methodology/approach: We used bibliometric network analysis, including the âtemporal quantitiesâ approach proposed to study temporal networks. Using a two-mode network linking publications with authors and a one-mode network of citations between the works, we constructed and analyzed the networks of citation and bibliographic coupling among authors. We used an iterated saturation data collection approach. Findings: At the macro-level, we observed the global structural features of citations between authors, showing that 80% of authors have not more than 15 citations from other works. At the meso-level, we extracted the groups of authors citing each other and similar to each other according to their citation patterns. We have seen a division of authors in SNA into groups of social scientists and physicists, as well as into other groups of authors from different disciplines. We found some examples of brokerage between different groups that maintained the common identity of the field. At the micro-level, we extracted authors with extremely high values of received citations, who can be considered as the most prominent authors in the field. We examined the temporal properties of the most popular authors. Research limitations: The main challenge in this approach is the resolution of the authorâs name (synonyms and homonyms). We faced the author disambiguation, or âmultiple personalitiesâ (Harzing, 2015) problem. To remain consistent and comparable with our previously published articles, we used the same SNA data collected up to 2018. The analysis and conclusions on the activity, productivity, and visibility of the authors are relative only to the field of SNA. Practical implications: The proposed approach can be utilized for similar objectives and identifying key structures and characteristics in other disciplines. This may potentially inspire the application of network approaches in other research areas, creating more authors collaborating in the field of SNA. Originality/value: We identified and applied an innovative approach and methods to study the structure of scientific communities, which allowed us to get the findings going beyond those obtained with other methods. We used a new approach to temporal network analysis, which is an important addition to the analysis as it provides detailed information on different measures for the authors and pairs of authors over time. Â© 2024 Daria Maltseva et al., published by Sciendo.","We analyzed the structure of a community of authors working in the field of social network analysis (SNA) based on citation indicators: direct citation and bibliographic coupling metrics. We observed patterns at the micro, meso, and macro levels of analysis. We used bibliometric network analysis, including the temporal quantities approach proposed to study temporal networks. Using a two-mode network linking publications with authors and a one-mode network of citations between the works, we constructed and analyzed the networks of citation and bibliographic coupling among authors. We used an iterated saturation data collection approach. At the macro-level, we observed the global structural features of citations between authors, showing that 80% of authors have not more than 15 citations from other works. At the meso-level, we extracted the groups of authors citing each other and similar to each other according to their citation patterns. We have seen a division of authors in SNA into groups of social scientists and physicists, as well as into other groups of authors from different disciplines. We found some examples of brokerage between different groups that maintained the common identity of the field. At the micro-level, we extracted authors with extremely high values of received citations, who can be considered as the most prominent authors in the field. We examined the temporal properties of the most popular authors. The main challenge in this approach is the resolution of the authors name (synonyms and homonyms). We faced the author disambiguation, or multiple personalities (Harzing, 2015) problem. To remain consistent and comparable with our previously published articles, we used the same SNA data collected up to 2018. The analysis and conclusions on the activity, productivity, and visibility of the authors are relative only to the field of SNA. The proposed approach can be utilized for similar objectives and identifying key structures and characteristics in other disciplines. This may potentially inspire the application of network approaches in other research areas, creating more authors collaborating in the field of SNA. We identified and applied an innovative approach and methods to study the structure of scientific communities, which allowed us to get the findings going beyond those obtained with other methods. We used a new approach to temporal network analysis, which is an important addition to the analysis as it provides detailed information on different measures for the authors and pairs of authors over time."
Evaluating research quality with Large Language Models: An analysis of ChatGPT's effectiveness with different settings and inputs,"Purpose: Evaluating the quality of academic journal articles is a time consuming but critical task for national research evaluation exercises, appointments and promotion. It is therefore important to investigate whether Large Language Models (LLMs) can play a role in this process. Design/methodology/approach: This article assesses which ChatGPT inputs (full text without tables, figures, and references; title and abstract; title only) produce better quality score estimates, and the extent to which scores are affected by ChatGPT models and system prompts. Findings: The optimal input is the article title and abstract, with average ChatGPT scores based on these (30 iterations on a dataset of 51 papers) correlating at 0.67 with human scores, the highest ever reported. ChatGPT 4o is slightly better than 3.5-turbo (0.66), and 4o-mini (0.66). Research limitations: The data is a convenience sample of the work of a single author, it only includes one field, and the scores are self-evaluations. Practical implications: The results suggest that article full texts might confuse LLM research quality evaluations, even though complex system instructions for the task are more effective than simple ones. Thus, whilst abstracts contain insufficient information for a thorough assessment of rigour, they may contain strong pointers about originality and significance. Finally, linear regression can be used to convert the model scores into the human scale scores, which is 31% more accurate than guessing. Originality/value: This is the first systematic comparison of the impact of different prompts, parameters and inputs for ChatGPT research quality evaluations. Copyright: Â© 2025 Mike Thelwall. Published under a Creative Commons Attribution 4.0 International (CC BY 4.0) license.","Evaluating the quality of academic journal articles is a time consuming but critical task for national research evaluation exercises, appointments and promotion. It is therefore important to investigate whether Large Language Models (LLMs) can play a role in this process. This article assesses which ChatGPT inputs (full text without tables, figures, and references; title and abstract; title only) produce better quality score estimates, and the extent to which scores are affected by ChatGPT models and system prompts. The optimal input is the article title and abstract, with average ChatGPT scores based on these (30 iterations on a dataset of 51 papers) correlating at 0.67 with human scores, the highest ever reported. ChatGPT 4o is slightly better than 3.5-turbo (0.66), and 4o-mini (0.66). The data is a convenience sample of the work of a single author, it only includes one field, and the scores are self-evaluations. The results suggest that article full texts might confuse LLM research quality evaluations, even though complex system instructions for the task are more effective than simple ones. Thus, whilst abstracts contain insufficient information for a thorough assessment of rigour, they may contain strong pointers about originality and significance. Finally, linear regression can be used to convert the model scores into the human scale scores, which is 31% more accurate than guessing. This is the first systematic comparison of the impact of different prompts, parameters and inputs for ChatGPT research quality evaluations."
An author credit allocation method with improved distinguishability and robustness,"Purpose: The purpose of this study is to propose an improved credit allocation method that makes the leading author of the paper more distinguishable and makes the deification more robust under malicious manipulations. Design/methodology/approach: We utilize a modified Sigmoid function to handle the fat-tail distributed citation counts. We also remove the target paper in calculating the contribution of co-citations. Following previous studies, we use 30 Nobel Prize-winning papers and their citation networks based on the American Physical Society (APS) and the Microsoft Academic Graph (MAG) dataset to test the accuracy of our proposed method (NCCAS). In addition, we use 654,148 articles published in the field of computer science from 2000 to 2009 in the MAG dataset to validate the distinguishability and robustness of NCCAS. Finding: Compared with the state-of-the-art methods, NCCAS gives the most accurate prediction of Nobel laureates. Furthermore, the leading author of the paper identified by NCCAS is more distinguishable compared with other co-authors. The results by NCCAS are also more robust to malicious manipulation. Finally, we perform ablation studies to show the contribution of different components in our methods. Research limitations: Due to limited ground truth on the true leading author of a work, the accuracy of NCCAS and other related methods can only be tested in Nobel Physics Prize-winning papers. Practical implications: NCCAS is successfully applied to a large number of publications, demonstrating its potential in analyzing the relationship between the contribution and the recognition of authors with different by-line orders. Originality/value: Compared with existing methods, NCCAS not only identifies the leading author of a paper more accurately, but also makes the deification more distinguishable and more robust, providing a new tool for related studies.  Â© 2023 Yang Li et al., published by Sciendo.","The purpose of this study is to propose an improved credit allocation method that makes the leading author of the paper more distinguishable and makes the deification more robust under malicious manipulations. We utilize a modified Sigmoid function to handle the fat-tail distributed citation counts. We also remove the target paper in calculating the contribution of co-citations. Following previous studies, we use 30 Nobel Prize-winning papers and their citation networks based on the American Physical Society (APS) and the Microsoft Academic Graph (MAG) dataset to test the accuracy of our proposed method (NCCAS). In addition, we use 654,148 articles published in the field of computer science from 2000 to 2009 in the MAG dataset to validate the distinguishability and robustness of NCCAS. Finding: Compared with the state-of-the-art methods, NCCAS gives the most accurate prediction of Nobel laureates. Furthermore, the leading author of the paper identified by NCCAS is more distinguishable compared with other co-authors. The results by NCCAS are also more robust to malicious manipulation. Finally, we perform ablation studies to show the contribution of different components in our methods. Due to limited ground truth on the true leading author of a work, the accuracy of NCCAS and other related methods can only be tested in Nobel Physics Prize-winning papers. NCCAS is successfully applied to a large number of publications, demonstrating its potential in analyzing the relationship between the contribution and the recognition of authors with different by-line orders. Compared with existing methods, NCCAS not only identifies the leading author of a paper more accurately, but also makes the deification more distinguishable and more robust, providing a new tool for related studies."
Can first or last name uniqueness help to identify diaspora researchers from any country?,"Purpose: Diaspora researchers work in one country but have ancestral origins in another, either through moves during a research career (mobile diaspora researchers) or by starting research in the target country (embedded diaspora researchers). Whilst mobile researchers might be tracked through affiliation changes in bibliometric databases, embedded researchers cannot. This article reports an evidence-based discussion of which countries' diaspora researchers can be partially tracked using first or last names, addressing this limitation. Design/methodology/approach: A frequency analysis of first and last names of authors of all Scopus journal articles 2001-2021 for 200 countries or regions. Findings: There are great variations in the extent to which first or last names are uniquely national, from Monserrat (no unique first names) to Thailand (81% unique last names). Nevertheless, most countries have a subset of first or last names that are relatively unique. For the 50 countries with the most researchers, authors with relatively national names are always more likely to research their name-associated country, suggesting a continued national association. Lists of researchers' first and last name frequencies and proportions are provided for 200 countries/regions. Research limitations: Only one period is tracked (2001-2021) and no attempt was made to validate the ancestral origins of any researcher. Practical implications: Simple name heuristics can be used to identify the international spread of a sample of most countries' diaspora researchers, but some manual checks of individual names are needed to weed out false matches. This can supplement mobile researcher data from bibliometric databases. Originality/value: This is the first attempt to list name associations for the authors of all countries and large regions, and to identify the countries for which diaspora researchers could be tracked by name.  Â© 2023 Mike Thelwall, published by Sciendo.","Diaspora researchers work in one country but have ancestral origins in another, either through moves during a research career (mobile diaspora researchers) or by starting research in the target country (embedded diaspora researchers). Whilst mobile researchers might be tracked through affiliation changes in bibliometric databases, embedded researchers cannot. This article reports an evidence-based discussion of which countries' diaspora researchers can be partially tracked using first or last names, addressing this limitation. A frequency analysis of first and last names of authors of all Scopus journal articles 2001-2021 for 200 countries or regions. There are great variations in the extent to which first or last names are uniquely national, from Monserrat (no unique first names) to Thailand (81% unique last names). Nevertheless, most countries have a subset of first or last names that are relatively unique. For the 50 countries with the most researchers, authors with relatively national names are always more likely to research their name-associated country, suggesting a continued national association. Lists of researchers' first and last name frequencies and proportions are provided for 200 countries/regions. Only one period is tracked (2001-2021) and no attempt was made to validate the ancestral origins of any researcher. Simple name heuristics can be used to identify the international spread of a sample of most countries' diaspora researchers, but some manual checks of individual names are needed to weed out false matches. This can supplement mobile researcher data from bibliometric databases. This is the first attempt to list name associations for the authors of all countries and large regions, and to identify the countries for which diaspora researchers could be tracked by name."
Data-enhanced revealing of trends in Geoscience,"Purpose: This article presents an in-depth analysis of global research trends in Geosciences from 2014 to 2023. By integrating bibliometric analysis with expert insights from the Deeptime Digital Earth (DDE) initiative, this article identifies key emerging themes shaping the landscape of Earth Sciences-. Design/methodology/approach: The identification process involved a meticulous analysis of over 400,000 papers from 466 Geosciences journals and approximately 5,800 papers from 93 interdisciplinary journals sourced from the Web of Science and Dimensions database. To map relationships between articles, citation networks were constructed, and spectral clustering algorithms were then employed to identify groups of related research, resulting in 407 clusters. Relevant research terms were extracted using the Log-Likelihood Ratio (LLR) algorithm, followed by statistical analyses on the volume of papers, average publication year, and average citation count within each cluster. Additionally, expert knowledge from DDE Scientific Committee was utilized to select top 30 trends based on their representation, relevance, and impact within Geosciences, and finalize naming of these top trends with consideration of the content and implications of the associated research. This comprehensive approach in systematically delineating and characterizing the trends in a way which is understandable to geoscientists. Findings: Thirty significant trends were identified in the field of Geosciences, spanning five domains: deep space, deep time, deep Earth, habitable Earth, and big data. These topics reflect the latest trends and advancements in Geosciences and have the potential to address real-world problems that are closely related to society, science, and technology. Research limitations: The analyzed data of this study only contain those were included in the Web of Science. Practical implications: This study will strongly support the organizations and individual scientists to understand the modern frontier of earth science, especially on solid earth. The organizations such as the surveys or natural science fund could map out areas for future exploration and analyze the hot topics reference to this study. Originality/value: This paper integrates bibliometric analysis with expert insights to highlight the most significant trends on earth science and reach the individual scientist and public by global voting.  Â© 2024 Yu Zhao et al., published by Sciendo.","This article presents an in-depth analysis of global research trends in Geosciences from 2014 to 2023. By integrating bibliometric analysis with expert insights from the Deeptime Digital Earth (DDE) initiative, this article identifies key emerging themes shaping the landscape of Earth Sciences-. The identification process involved a meticulous analysis of over 400,000 papers from 466 Geosciences journals and approximately 5,800 papers from 93 interdisciplinary journals sourced from the Web of Science and Dimensions database. To map relationships between articles, citation networks were constructed, and spectral clustering algorithms were then employed to identify groups of related research, resulting in 407 clusters. Relevant research terms were extracted using the Log-Likelihood Ratio (LLR) algorithm, followed by statistical analyses on the volume of papers, average publication year, and average citation count within each cluster. Additionally, expert knowledge from DDE Scientific Committee was utilized to select top 30 trends based on their representation, relevance, and impact within Geosciences, and finalize naming of these top trends with consideration of the content and implications of the associated research. This comprehensive approach in systematically delineating and characterizing the trends in a way which is understandable to geoscientists. Thirty significant trends were identified in the field of Geosciences, spanning five domains: deep space, deep time, deep Earth, habitable Earth, and big data. These topics reflect the latest trends and advancements in Geosciences and have the potential to address real-world problems that are closely related to society, science, and technology. The analyzed data of this study only contain those were included in the Web of Science. This study will strongly support the organizations and individual scientists to understand the modern frontier of earth science, especially on solid earth. The organizations such as the surveys or natural science fund could map out areas for future exploration and analyze the hot topics reference to this study. This paper integrates bibliometric analysis with expert insights to highlight the most significant trends on earth science and reach the individual scientist and public by global voting."
Community detection on elite mathematiciansâ collaboration network,"Purpose: This study focuses on understanding the collaboration relationships among mathematicians, particularly those esteemed as elites, to reveal the structures of their communities and evaluate their impact on the field of mathematics. Design/methodology/approach: Two community detection algorithms, namely Greedy Modularity Maximization and Infomap, are utilized to examine collaboration patterns among mathematicians. We conduct a comparative analysis of mathematiciansâ centrality, emphasizing the influence of award-winning individuals in connecting network roles such as Betweenness, Closeness, and Harmonic centrality. Additionally, we investigate the distribution of elite mathematicians across communities and their relationships within different mathematical sub-fields. Findings: The study identifies the substantial influence exerted by award-winning mathematicians in connecting network roles. The elite distribution across the network is uneven, with a concentration within specific communities rather than being evenly dispersed. Secondly, the research identifies a positive correlation between distinct mathematical sub-fields and the communities, indicating collaborative tendencies among scientists engaged in related domains. Lastly, the study suggests that reduced research diversity within a community might lead to a higher concentration of elite scientists within that specific community. Research limitations: The studyâs limitations include its narrow focus on mathematicians, which may limit the applicability of the findings to broader scientific fields. Issues with manually collected data affect the reliability of conclusions about collaborative networks. Practical implications: This study offers valuable insights into how elite mathematicians collaborate and how knowledge is disseminated within mathematical circles. Understanding these collaborative behaviors could aid in fostering better collaboration strategies among mathematicians and institutions, potentially enhancing scientific progress in mathematics. Originality/value: The study adds value to understanding collaborative dynamics within the realm of mathematics, offering a unique angle for further exploration and research. Â© 2024 Yurui Huang, Zimo Wang, Chaolin Tian, Yifang Ma.","This study focuses on understanding the collaboration relationships among mathematicians, particularly those esteemed as elites, to reveal the structures of their communities and evaluate their impact on the field of mathematics. Two community detection algorithms, namely Greedy Modularity Maximization and Infomap, are utilized to examine collaboration patterns among mathematicians. We conduct a comparative analysis of mathematicians centrality, emphasizing the influence of award-winning individuals in connecting network roles such as Betweenness, Closeness, and Harmonic centrality. Additionally, we investigate the distribution of elite mathematicians across communities and their relationships within different mathematical sub-fields. The study identifies the substantial influence exerted by award-winning mathematicians in connecting network roles. The elite distribution across the network is uneven, with a concentration within specific communities rather than being evenly dispersed. Secondly, the research identifies a positive correlation between distinct mathematical sub-fields and the communities, indicating collaborative tendencies among scientists engaged in related domains. Lastly, the study suggests that reduced research diversity within a community might lead to a higher concentration of elite scientists within that specific community. The studys limitations include its narrow focus on mathematicians, which may limit the applicability of the findings to broader scientific fields. Issues with manually collected data affect the reliability of conclusions about collaborative networks. This study offers valuable insights into how elite mathematicians collaborate and how knowledge is disseminated within mathematical circles. Understanding these collaborative behaviors could aid in fostering better collaboration strategies among mathematicians and institutions, potentially enhancing scientific progress in mathematics. The study adds value to understanding collaborative dynamics within the realm of mathematics, offering a unique angle for further exploration and research."
New roles of research data infrastructure in research paradigm evolution,"Research data infrastructures form the cornerstone in both cyber and physical spaces, driving the progression of the data-intensive scientific research paradigm. This opinion paper presents an overview of global research data infrastructure, drawing insights from national roadmaps and strategic documents related to research data infrastructure. It emphasizes the pivotal role of research data infrastructures by delineating four new missions aimed at positioning them at the core of the current scientific research and communication ecosystem. The four new missions of research data infrastructures are: (1) as a pioneer, to transcend the disciplinary border and address complex, cutting-edge scientific and social challenges with problem- and data-oriented insights; (2) as an architect, to establish a digital, intelligent, flexible research and knowledge services environment; (3) as a platform, to foster the high-end academic communication; (4) as a coordinator, to balance scientific openness with ethics needs.  Â© 2024 Yizhan Li et al., published by Sciendo.","Research data infrastructures form the cornerstone in both cyber and physical spaces, driving the progression of the data-intensive scientific research paradigm. This opinion paper presents an overview of global research data infrastructure, drawing insights from national roadmaps and strategic documents related to research data infrastructure. It emphasizes the pivotal role of research data infrastructures by delineating four new missions aimed at positioning them at the core of the current scientific research and communication ecosystem. The four new missions of research data infrastructures are: as a pioneer, to transcend the disciplinary border and address complex, cutting-edge scientific and social challenges with problem- and data-oriented insights; as an architect, to establish a digital, intelligent, flexible research and knowledge services environment; as a platform, to foster the high-end academic communication; as a coordinator, to balance scientific openness with ethics needs."
Implications of Publication Requirements for the Research Output of Ukrainian Academics in Scopus in 1999-2019,"Purpose: This article explores the implications of publication requirements for the research output of Ukrainian academics in Scopus in 1999-2019. As such it contributes to the existing body of knowledge on quantitative and qualitative effects of research evaluation policies. Design/methodology/approach: Three metrics were chosen to analyse the implications of publication requirements for the quality of research output: publications in predatory journals, publications in local journals and publications per SNIP quartile from the disciplinary perspective. Findings: Study results highlight, that, firstly, publications of Ukrainian authors in predatory journals rose to 1% in 2019. Secondly, the share of publications in local journals reached the peak of 47.3% in 2015. In 2019 it fell to 31.8%. Thirdly, though the total number of publications has risen dramatically since 2011, but the share of Q3+Q4 has exceeded the share of Q1+Q2. To summarise, the study findings highligh, that research evaluation policies are required to contain not only quantitative but also qualitative criteria. Research limitation: The study does not explore in detail the effects of a particular type of publication requirements. Practical implications: The findings of the study have practical implications for policymakers and university managers aimed to develop research evaluation policies. Originality/value: This paper gains insights into the effects of publication requirements on the research output of Ukrainian academics in Scopus.  Â© 2022 Myroslava Hladchenko, published by Sciendo.","This article explores the implications of publication requirements for the research output of Ukrainian academics in Scopus in 1999-2019. As such it contributes to the existing body of knowledge on quantitative and qualitative effects of research evaluation policies. Three metrics were chosen to analyse the implications of publication requirements for the quality of research output: publications in predatory journals, publications in local journals and publications per SNIP quartile from the disciplinary perspective. Study results highlight, that, firstly, publications of Ukrainian authors in predatory journals rose to 1% in 2019. Secondly, the share of publications in local journals reached the peak of 47.3% in 2015. In 2019 it fell to 31.8%. Thirdly, though the total number of publications has risen dramatically since 2011, but the share of Q3+Q4 has exceeded the share of Q1+Q2. To summarise, the study findings highligh, that research evaluation policies are required to contain not only quantitative but also qualitative criteria. Research limitation: The study does not explore in detail the effects of a particular type of publication requirements. The findings of the study have practical implications for policymakers and university managers aimed to develop research evaluation policies. This paper gains insights into the effects of publication requirements on the research output of Ukrainian academics in Scopus."
Performance evaluation of seven multi-label classification methods on real-world patent and publication datasets,"Purpose: Many science, technology and innovation (STI) resources are attached with several different labels. To assign automatically the resulting labels to an interested instance, many approaches with good performance on the benchmark datasets have been proposed for multilabel classification task in the literature. Furthermore, several open-source tools implementing these approaches have also been developed. However, the characteristics of real-world multilabel patent and publication datasets are not completely in line with those of benchmark ones. Therefore, the main purpose of this paper is to evaluate comprehensively seven multi-label classification methods on real-world datasets. Design/methodology/approach: Three real-world datasets (Biological-Sciences, Health-Sciences, and USPTO) from SciGraph and USPTO database are constructed. Seven multilabel classification methods with tuned parameters (dependency-LDA, MLkNN, LabelPowerset, RAkEL, TextCNN, TexRNN, and TextRCNN) are comprehensively compared on these three real-world datasets. To evaluate the performance, the study adopts three classification-based metrics: Macro-F1, Micro-F1, and Hamming Loss. Findings: The TextCNN and TextRCNN models show obvious superiority on small-scale datasets with more complex hierarchical structure of labels and more balanced documentlabel distribution in terms of macro-F1, micro-F1 and Hamming Loss. The MLkNN method works better on the larger-scale dataset with more unbalanced document-label distribution. Research limitations: Three real-world datasets differ in the following aspects: statement, data quality, and purposes. Additionally, open-source tools designed for multi-label classification also have intrinsic differences in their approaches for data processing and feature selection, which in turn impacts the performance of a multi-label classification approach. In the near future, we will enhance experimental precision and reinforce the validity of conclusions by employing more rigorous control over variables through introducing expanded parameter settings. Practical implications: The observed Macro F1 and Micro F1 scores on real-world datasets typically fall short of those achieved on benchmark datasets, underscoring the complexity of real-world multi-label classification tasks. Approaches leveraging deep learning techniques offer promising solutions by accommodating the hierarchical relationships and interdependencies among labels. With ongoing enhancements in deep learning algorithms and large-scale models, it is expected that the efficacy of multi-label classification tasks will be significantly improved, reaching a level of practical utility in the foreseeable future. Originality/value: (1) Seven multi-label classification methods are comprehensively compared on three real-world datasets. (2) The TextCNN and TextRCNN models perform better on small-scale datasets with more complex hierarchical structure of labels and more balanced document-label distribution. (3) The MLkNN method works better on the larger-scale dataset with more unbalanced document-label distribution.  Â© 2024 Shuo Xu et al., published by Sciendo.","Many science, technology and innovation (STI) resources are attached with several different labels. To assign automatically the resulting labels to an interested instance, many approaches with good performance on the benchmark datasets have been proposed for multilabel classification task in the literature. Furthermore, several open-source tools implementing these approaches have also been developed. However, the characteristics of real-world multilabel patent and publication datasets are not completely in line with those of benchmark ones. Therefore, the main purpose of this paper is to evaluate comprehensively seven multi-label classification methods on real-world datasets. Three real-world datasets (Biological-Sciences, Health-Sciences, and USPTO) from SciGraph and USPTO database are constructed. Seven multilabel classification methods with tuned parameters (dependency-LDA, MLkNN, LabelPowerset, RAkEL, TextCNN, TexRNN, and TextRCNN) are comprehensively compared on these three real-world datasets. To evaluate the performance, the study adopts three classification-based metrics: Macro-F1, Micro-F1, and Hamming Loss. The TextCNN and TextRCNN models show obvious superiority on small-scale datasets with more complex hierarchical structure of labels and more balanced documentlabel distribution in terms of macro-F1, micro-F1 and Hamming Loss. The MLkNN method works better on the larger-scale dataset with more unbalanced document-label distribution. Three real-world datasets differ in the following aspects: statement, data quality, and purposes. Additionally, open-source tools designed for multi-label classification also have intrinsic differences in their approaches for data processing and feature selection, which in turn impacts the performance of a multi-label classification approach. In the near future, we will enhance experimental precision and reinforce the validity of conclusions by employing more rigorous control over variables through introducing expanded parameter settings. The observed Macro F1 and Micro F1 scores on real-world datasets typically fall short of those achieved on benchmark datasets, underscoring the complexity of real-world multi-label classification tasks. Approaches leveraging deep learning techniques offer promising solutions by accommodating the hierarchical relationships and interdependencies among labels. With ongoing enhancements in deep learning algorithms and large-scale models, it is expected that the efficacy of multi-label classification tasks will be significantly improved, reaching a level of practical utility in the foreseeable future. Seven multi-label classification methods are comprehensively compared on three real-world datasets. The TextCNN and TextRCNN models perform better on small-scale datasets with more complex hierarchical structure of labels and more balanced document-label distribution. The MLkNN method works better on the larger-scale dataset with more unbalanced document-label distribution."
A comprehensive review of existing corpora and methods for creating annotated corpora for event extraction tasks,"Purpose: The purpose of this study is to serve as a comprehensive review of the existing annotated corpora. This review study aims to provide information on the existing annotated corpora for event extraction, which are limited but essential for training and improving the existing event extraction algorithms. In addition to the primary goal of this study, it provides guidelines for preparing an annotated corpus and suggests suitable tools for the annotation task. Design/methodology/approach: This study employs an analytical approach to examine available corpus that is suitable for event extraction tasks. It offers an in-depth analysis of existing event extraction corpora and provides systematic guidelines for researchers to develop accurate, high-quality corpora. This ensures the reliability of the created corpus and its suitability for training machine learning algorithms. Findings: Our exploration reveals a scarcity of annotated corpora for event extraction tasks. In particular, the English corpora are mainly focused on the biomedical and general domains. Despite the issue of annotated corpora scarcity, there are several high-quality corpora available and widely used as benchmark datasets. However, access to some of these corpora might be limited owing to closed-access policies or discontinued maintenance after being initially released, rendering them inaccessible owing to broken links. Therefore, this study documents the available corpora for event extraction tasks. Research limitations: Our study focuses only on well-known corpora available in English and Chinese. Nevertheless, this study places a strong emphasis on the English corpora due to its status as a global lingua franca, making it widely understood compared to other languages. Practical implications: We genuinely believe that this study provides valuable knowledge that can serve as a guiding framework for preparing and accurately annotating events from text corpora. It provides comprehensive guidelines for researchers to improve the quality of corpus annotations, especially for event extraction tasks across various domains. Originality/value: This study comprehensively compiled information on the existing annotated corpora for event extraction tasks and provided preparation guidelines. Â© 2024 Mohd Hafizul Afifi Abdullah et al., published by Sciendo.","The purpose of this study is to serve as a comprehensive review of the existing annotated corpora. This review study aims to provide information on the existing annotated corpora for event extraction, which are limited but essential for training and improving the existing event extraction algorithms. In addition to the primary goal of this study, it provides guidelines for preparing an annotated corpus and suggests suitable tools for the annotation task. This study employs an analytical approach to examine available corpus that is suitable for event extraction tasks. It offers an in-depth analysis of existing event extraction corpora and provides systematic guidelines for researchers to develop accurate, high-quality corpora. This ensures the reliability of the created corpus and its suitability for training machine learning algorithms. Our exploration reveals a scarcity of annotated corpora for event extraction tasks. In particular, the English corpora are mainly focused on the biomedical and general domains. Despite the issue of annotated corpora scarcity, there are several high-quality corpora available and widely used as benchmark datasets. However, access to some of these corpora might be limited owing to closed-access policies or discontinued maintenance after being initially released, rendering them inaccessible owing to broken links. Therefore, this study documents the available corpora for event extraction tasks. Our study focuses only on well-known corpora available in English and Chinese. Nevertheless, this study places a strong emphasis on the English corpora due to its status as a global lingua franca, making it widely understood compared to other languages. We genuinely believe that this study provides valuable knowledge that can serve as a guiding framework for preparing and accurately annotating events from text corpora. It provides comprehensive guidelines for researchers to improve the quality of corpus annotations, especially for event extraction tasks across various domains. This study comprehensively compiled information on the existing annotated corpora for event extraction tasks and provided preparation guidelines."
Differences between journal and conference in computer science: a bibliometric view based on Bayesian network,"Purpose: This paper aims to investigate the differences between conference papers and journal papers in the field of computer science based on Bayesian network. Design/methodology/approach: This paper investigated the differences between conference papers and journal papers in the field of computer science based on Bayesian network, a knowledge-representative framework that can model relationships among all variables in the network. We defined the variables required for Bayesian networks modeling, calculated the values of each variable based Aminer dataset (a literature data set in the field of computer science), learned the Bayesian network and derived some findings based on network inference. Findings: The study found that conferences are more attractive to senior scholars, the academic impact of conference papers is slightly higher than journal papers, and it is uncertain whether conference papers are more innovative than journal papers. Research limitations: The study was limited to the field of computer science and employed Aminer dataset as the sample. Further studies involving more diverse datasets and different fields could provide a more complete picture of the matter. Practical implications: By demonstrating that Bayesian networks can effectively analyze issues in Scientometrics, the study offers valuable insights that may enhance researchers' understanding of the differences between journal and conference in computer science. Originality/value: Academic conferences play a crucial role in facilitating scholarly exchange and knowledge dissemination within the field of computer science. Several studies have been conducted to examine the distinctions between conference papers and journal papers in terms of various factors, such as authors, citations, h-index and others. Those studies were carried out from different (independent) perspectives, lacking a systematic examination of the connections and interactions between multiple perspectives. This paper supplements this deficiency based on Bayesian network modeling.  Â© 2023 Mingyue Sun et al., published by Sciendo.","This paper aims to investigate the differences between conference papers and journal papers in the field of computer science based on Bayesian network. This paper investigated the differences between conference papers and journal papers in the field of computer science based on Bayesian network, a knowledge-representative framework that can model relationships among all variables in the network. We defined the variables required for Bayesian networks modeling, calculated the values of each variable based Aminer dataset (a literature data set in the field of computer science), learned the Bayesian network and derived some findings based on network inference. The study found that conferences are more attractive to senior scholars, the academic impact of conference papers is slightly higher than journal papers, and it is uncertain whether conference papers are more innovative than journal papers. The study was limited to the field of computer science and employed Aminer dataset as the sample. Further studies involving more diverse datasets and different fields could provide a more complete picture of the matter. By demonstrating that Bayesian networks can effectively analyze issues in Scientometrics, the study offers valuable insights that may enhance researchers' understanding of the differences between journal and conference in computer science. Academic conferences play a crucial role in facilitating scholarly exchange and knowledge dissemination within the field of computer science. Several studies have been conducted to examine the distinctions between conference papers and journal papers in terms of various factors, such as authors, citations, h-index and others. Those studies were carried out from different (independent) perspectives, lacking a systematic examination of the connections and interactions between multiple perspectives. This paper supplements this deficiency based on Bayesian network modeling."
Editorial board publication strategy and acceptance rates in Turkish national journals,"Purpose: This study takes advantage of newly released journal metrics to investigate whether local journals with more qualified boards have lower acceptance rates, based on data from 219 Turkish national journals and 2,367 editorial board members. Design/methodology/approach: This study argues that journal editors can signal their scholarly quality by publishing in reputable journals. Conversely, editors publishing inside articles in affiliated national journals would send negative signals. The research predicts that high (low) quality editorial boards will conduct more (less) selective evaluation and their journals will have lower (higher) acceptance rates. Based on the publication strategy of editors, four measures of board quality are defined: Number of board inside publications per editor (INSIDER), number of board Social Sciences Citation Index publications per editor (SSCI), inside-to-SSCI article ratio (ISRA), and board citation per editor (CITATION). Predictions are tested by correlation and regression analysis. Findings: Low-quality board proxies (INSIDER, ISRA) are positively, and high-quality board proxies (SSCI, CITATION) are negatively associated with acceptance rates. Further, we find that receiving a larger number of submissions, greater women representation on boards, and Web of Science and Scopus (WOSS) coverage are associated with lower acceptance rates. Acceptance rates for journals range from 12% to 91%, with an average of 54% and a median of 53%. Law journals have significantly higher average acceptance rate (68%) than other journals, while WOSS journals have the lowest (43%). Findings indicate some of the highest acceptance rates in Social Sciences literature, including competitive Business and Economics journals that traditionally have low acceptance rates. Limitations: Research relies on local context to define publication strategy of editors. Findings may not be generalizable to mainstream journals and core science countries where emphasis on research quality is stronger and editorial selection is based on scientific merit. Practical implications: Results offer useful insights into editorial management of national journals and allow us to make sense of local editorial practices. The importance of scientific merit for selection to national journal editorial boards is particularly highlighted for sound editorial evaluation of submitted manuscripts. Originality/value: This is the first attempt to document a significant relation between acceptance rates and editorial board publication behavior. Â© 2023 Lokman Tutuncu, published by Sciendo.","This study takes advantage of newly released journal metrics to investigate whether local journals with more qualified boards have lower acceptance rates, based on data from 219 Turkish national journals and 2,367 editorial board members. This study argues that journal editors can signal their scholarly quality by publishing in reputable journals. Conversely, editors publishing inside articles in affiliated national journals would send negative signals. The research predicts that high (low) quality editorial boards will conduct more (less) selective evaluation and their journals will have lower (higher) acceptance rates. Based on the publication strategy of editors, four measures of board quality are defined: Number of board inside publications per editor (INSIDER), number of board Social Sciences Citation Index publications per editor (SSCI), inside-to-SSCI article ratio (ISRA), and board citation per editor (CITATION). Predictions are tested by correlation and regression analysis. Low-quality board proxies (INSIDER, ISRA) are positively, and high-quality board proxies (SSCI, CITATION) are negatively associated with acceptance rates. Further, we find that receiving a larger number of submissions, greater women representation on boards, and Web of Science and Scopus (WOSS) coverage are associated with lower acceptance rates. Acceptance rates for journals range from 12% to 91%, with an average of 54% and a median of 53%. Law journals have significantly higher average acceptance rate (68%) than other journals, while WOSS journals have the lowest (43%). Findings indicate some of the highest acceptance rates in Social Sciences literature, including competitive Business and Economics journals that traditionally have low acceptance rates. Limitations: Research relies on local context to define publication strategy of editors. Findings may not be generalizable to mainstream journals and core science countries where emphasis on research quality is stronger and editorial selection is based on scientific merit. Results offer useful insights into editorial management of national journals and allow us to make sense of local editorial practices. The importance of scientific merit for selection to national journal editorial boards is particularly highlighted for sound editorial evaluation of submitted manuscripts. This is the first attempt to document a significant relation between acceptance rates and editorial board publication behavior."
The Triple Helix of innovation as a double game involving domestic and foreign actors,"Purpose: The collaboration relationships between innovation actors at a geographic level may be considered as grouping two separate layers, the domestic and the foreign. At the level of each layer, the relationships and the actors involved constitute a Triple Helix game. The paper distinguished three levels of analysis: the global grouping together all actors, the domestic grouping together domestic actors, and the foreign related to only actors from partner countries. Design/methodology/approach: Bibliographic records data from the Web of Science for South Korea and West Africa breakdown per innovation actors and distinguishing domestic and international collaboration are analyzed with game theory. The core, the Shapley value, and the nucleolus are computed at the three levels to measure the synergy between actors. Findings: The synergy operates more in South Korea than in West Africa; the government is more present in West Africa than in South Korea; domestic actors create more synergy in South Korea, but foreign more in West Africa; South Korea can consume all the foreign synergy, which is not the case of West Africa. Research limitations: Research data are limited to publication records; techniques and methods used may be extended to other research outputs. Practical implications: West African governments should increase their investment in science, technology, and innovation to benefit more from the synergy their innovation actors contributed at the foreign level. However, the results of the current study may not be sufficient to prove that greater investment will yield benefits from foreign synergies. Originality/value: This paper uses game theory to assess innovation systems by computing the contribution of foreign actors to knowledge production at an area level. It proposes an indicator to this end. Copyright: Â© 2024 Eustache MÃªgnigbÃªto. Published under a Creative Commons Attribution 4.0 International (CC BY 4.0) license.","The collaboration relationships between innovation actors at a geographic level may be considered as grouping two separate layers, the domestic and the foreign. At the level of each layer, the relationships and the actors involved constitute a Triple Helix game. The paper distinguished three levels of analysis: the global grouping together all actors, the domestic grouping together domestic actors, and the foreign related to only actors from partner countries. Bibliographic records data from the Web of Science for South Korea and West Africa breakdown per innovation actors and distinguishing domestic and international collaboration are analyzed with game theory. The core, the Shapley value, and the nucleolus are computed at the three levels to measure the synergy between actors. The synergy operates more in South Korea than in West Africa; the government is more present in West Africa than in South Korea; domestic actors create more synergy in South Korea, but foreign more in West Africa; South Korea can consume all the foreign synergy, which is not the case of West Africa. Research data are limited to publication records; techniques and methods used may be extended to other research outputs. West African governments should increase their investment in science, technology, and innovation to benefit more from the synergy their innovation actors contributed at the foreign level. However, the results of the current study may not be sufficient to prove that greater investment will yield benefits from foreign synergies. This paper uses game theory to assess innovation systems by computing the contribution of foreign actors to knowledge production at an area level. It proposes an indicator to this end."
Peculiarities of gender disambiguation and ordering of non-English authors' names for Economic papers beyond core databases,"Purpose: To supplement the quantitative portrait of Ukrainian Economics discipline with the results of gender and author ordering analysis at the level of individual authors, special methods of working with bibliographic data with a predominant share of non-English authors are used. The properties of gender mixing, the likelihood of male and female authors occupying the first position in the authorship list, as well as the arrangements of names are studied. Design/methodology/approach: A data set containing bibliographic records related to Ukrainian journal publications in the field of Economics is constructed using Crossref metadata. Partial semi-automatic disambiguation of authors' names is performed. First names, along with gender-specific ethnic surnames, are used for gender disambiguation required for further comparative gender analysis. Random reshuffling of data is used to determine the impact of gender correlations. To assess the level of alphabetization for our data set, both Latin and Cyrillic versions of names are taken into account. Findings: The lack of well-structured metadata and the poor use of digital identifiers lead to numerous problems with automatization of bibliographic data pre-processing, especially in the case of publications by non-Western authors. The described stages for working with such specific data help to work at the level of authors and analyse, in particular, gender issues. Despite the larger number of female authors, gender equality is more likely to be reported at the individual level for the discipline of Ukrainian Economics. The tendencies towards collaborative or solo-publications and gender mixing patterns are found to be dependent on the journal: the differences for publications indexed in Scopus and/or Web of Science databases are found. It has also been found that Ukrainian Economics research is characterized by rather a non-alphabetical order of authors. Research limitations: Only partial authors' name disambiguation is performed in a semi-automatic way. Gender labels can be derived only for authors declared by full First names or gender-specific Last names. Practical implications: The typical features of Ukrainian Economic discipline can be used to perform a comparison with other countries and disciplines, to develop an informed-based assessment procedure at the national level. The proposed way of processing publication data can be borrowed to enrich metadata about other research disciplines, especially for non-English speaking countries. Originality/value: To our knowledge, this is the first large-scale quantitative study of Ukrainian Economic discipline. The results obtained are valuable not only at the national level, but also contribute to general knowledge about Economic research, gender issues, and authors' names ordering. An example of the use of Crossref data is provided, while this data source is still less used due to a number of drawbacks. Here, for the first time, attention is drawn to the explicit use of the features of the Slavic authors' names.  Â© 2023 Olesya Mryglod et al., published by Sciendo.","To supplement the quantitative portrait of Ukrainian Economics discipline with the results of gender and author ordering analysis at the level of individual authors, special methods of working with bibliographic data with a predominant share of non-English authors are used. The properties of gender mixing, the likelihood of male and female authors occupying the first position in the authorship list, as well as the arrangements of names are studied. A data set containing bibliographic records related to Ukrainian journal publications in the field of Economics is constructed using Crossref metadata. Partial semi-automatic disambiguation of authors' names is performed. First names, along with gender-specific ethnic surnames, are used for gender disambiguation required for further comparative gender analysis. Random reshuffling of data is used to determine the impact of gender correlations. To assess the level of alphabetization for our data set, both Latin and Cyrillic versions of names are taken into account. The lack of well-structured metadata and the poor use of digital identifiers lead to numerous problems with automatization of bibliographic data pre-processing, especially in the case of publications by non-Western authors. The described stages for working with such specific data help to work at the level of authors and analyse, in particular, gender issues. Despite the larger number of female authors, gender equality is more likely to be reported at the individual level for the discipline of Ukrainian Economics. The tendencies towards collaborative or solo-publications and gender mixing patterns are found to be dependent on the journal: the differences for publications indexed in Scopus and/or Web of Science databases are found. It has also been found that Ukrainian Economics research is characterized by rather a non-alphabetical order of authors. Only partial authors' name disambiguation is performed in a semi-automatic way. Gender labels can be derived only for authors declared by full First names or gender-specific Last names. The typical features of Ukrainian Economic discipline can be used to perform a comparison with other countries and disciplines, to develop an informed-based assessment procedure at the national level. The proposed way of processing publication data can be borrowed to enrich metadata about other research disciplines, especially for non-English speaking countries. To our knowledge, this is the first large-scale quantitative study of Ukrainian Economic discipline. The results obtained are valuable not only at the national level, but also contribute to general knowledge about Economic research, gender issues, and authors' names ordering. An example of the use of Crossref data is provided, while this data source is still less used due to a number of drawbacks. Here, for the first time, attention is drawn to the explicit use of the features of the Slavic authors' names."
Convergence of Impact Measures and Impact Bundles,"Purpose: A new point of view in the study of impact is introduced. Design/methodology/approach: Using fundamental theorems in real analysis we study the convergence of well-known impact measures. Findings: We show that pointwise convergence is maintained by all well-known impact bundles (such as the h-, g-, and R-bundle) and that the Î¼-bundle even maintains uniform convergence. Based on these results, a classification of impact bundles is given. Research limitations: As for all impact studies, it is just impossible to study all measures in depth. Practical implications: It is proposed to include convergence properties in the study of impact measures. Originality/value: This article is the first to present a bundle classification based on convergence properties of impact bundles.  Â© 2022 Leo Egghe, published by Sciendo.","A new point of view in the study of impact is introduced. Using fundamental theorems in real analysis we study the convergence of well-known impact measures. We show that pointwise convergence is maintained by all well-known impact bundles (such as the h-, g-, and R-bundle) and that the -bundle even maintains uniform convergence. Based on these results, a classification of impact bundles is given. As for all impact studies, it is just impossible to study all measures in depth. It is proposed to include convergence properties in the study of impact measures. This article is the first to present a bundle classification based on convergence properties of impact bundles."
A Use Case of Patent Classification Using Deep Learning with Transfer Learning,"Purpose: Patent classification is one of the areas in Intellectual Property Analytics (IPA), and a growing use case since the number of patent applications has been increasing worldwide. We propose using machine learning algorithms to classify Portuguese patents and evaluate the performance of transfer learning methodologies to solve this task. Design/methodology/approach: We applied three different approaches in this paper. First, we used a dataset available by INPI to explore traditional machine learning algorithms and ensemble methods. After preprocessing data by applying TF-IDF, FastText and Doc2Vec, the models were evaluated by cross-validation in 5 folds. In a second approach, we used two different Neural Networks architectures, a Convolutional Neural Network (CNN) and a bi-directional Long Short-Term Memory (BiLSTM). Finally, we used pre-trained BERT, DistilBERT, and ULMFiT models in the third approach. Findings: BERTTimbau, a BERT architecture model pre-trained on a large Portuguese corpus, presented the best results for the task, even though with a performance of only 4% superior to a LinearSVC model using TF-IDF feature engineering. Research limitations: The dataset was highly imbalanced, as usual in patent applications, so the classes with the lowest samples were expected to present the worst performance. That result happened in some cases, especially in classes with less than 60 training samples. Practical implications: Patent classification is challenging because of the hierarchical classification system, the context overlap, and the underrepresentation of the classes. However, the final model presented an acceptable performance given the size of the dataset and the task complexity. This model can support the decision and improve the time by proposing a category in the second level of ICP, which is one of the critical phases of the grant patent process. Originality/value: To our knowledge, the proposed models were never implemented for Portuguese patent classification. Â© 2022 Roberto Henriques et al., published by Sciendo.","Patent classification is one of the areas in Intellectual Property Analytics (IPA), and a growing use case since the number of patent applications has been increasing worldwide. We propose using machine learning algorithms to classify Portuguese patents and evaluate the performance of transfer learning methodologies to solve this task. We applied three different approaches in this paper. First, we used a dataset available by INPI to explore traditional machine learning algorithms and ensemble methods. After preprocessing data by applying TF-IDF, FastText and Doc2Vec, the models were evaluated by cross-validation in 5 folds. In a second approach, we used two different Neural Networks architectures, a Convolutional Neural Network (CNN) and a bi-directional Long Short-Term Memory (BiLSTM). Finally, we used pre-trained BERT, DistilBERT, and ULMFiT models in the third approach. BERTTimbau, a BERT architecture model pre-trained on a large Portuguese corpus, presented the best results for the task, even though with a performance of only 4% superior to a LinearSVC model using TF-IDF feature engineering. The dataset was highly imbalanced, as usual in patent applications, so the classes with the lowest samples were expected to present the worst performance. That result happened in some cases, especially in classes with less than 60 training samples. Patent classification is challenging because of the hierarchical classification system, the context overlap, and the underrepresentation of the classes. However, the final model presented an acceptable performance given the size of the dataset and the task complexity. This model can support the decision and improve the time by proposing a category in the second level of ICP, which is one of the critical phases of the grant patent process. To our knowledge, the proposed models were never implemented for Portuguese patent classification."
International visibility of Armenian domestic journals: The role of scientific diaspora,"Purpose: Nearly 122 scientific journals are currently being published in Armenia - of which only six are indexed by WoS and/or Scopus databases. The majority of the national journals are published in the Armenian language, solely possessing abstracts written in English, although there are also English-language and multi-language journals with articles not only in Armenian but also in other foreign languages. The aim of this article is to study the visibility of the (non-indexed) national Armenian journals in the WoS database through citation analysis. In consideration of the existence of a relevant Armenian ""diaspora""in the world, this article also attempts to estimate its impact in terms of citation statistics. Design/methodology/approach: For this end, we have identified citations to the national/domestic Armenian journals in the WoS database in comparison with the share of citations received from ""diaspora""researchers (researchers of Armenian origin born in foreign countries and those originally from Armenia who have emigrated to foreign countries). Findings: Among the 116 Armenian domestic journals analyzed (not indexed by WoS), only 47 were found to be cited in WoS. Of these journals, almost 12% are citations by ""diaspora""researchers, most of which concern Social Science and Humanities journals. Research limitations: Although the surnames of Armenians end with -i(y)an, sometimes, the Diaspora Armenians, surnames are changed or modified or they are not ending with -i(y)an, in this case we may fail to identify them. Practical implications: This study can help to build new, more deep and comprehensive relations with scientific diasporas. Originality/value: This study offers a new understanding of multifaced research collaboration with scientific diasporas and their role in internationalization of domestic journals.  Â© 2023 Edita Gzoyan et al., published by Sciendo.","Nearly 122 scientific journals are currently being published in Armenia - of which only six are indexed by WoS and/or Scopus databases. The majority of the national journals are published in the Armenian language, solely possessing abstracts written in English, although there are also English-language and multi-language journals with articles not only in Armenian but also in other foreign languages. The aim of this article is to study the visibility of the (non-indexed) national Armenian journals in the WoS database through citation analysis. In consideration of the existence of a relevant Armenian ""diaspora""in the world, this article also attempts to estimate its impact in terms of citation statistics. For this end, we have identified citations to the national/domestic Armenian journals in the WoS database in comparison with the share of citations received from ""diaspora""researchers (researchers of Armenian origin born in foreign countries and those originally from Armenia who have emigrated to foreign countries). Among the 116 Armenian domestic journals analyzed (not indexed by WoS), only 47 were found to be cited in WoS. Of these journals, almost 12% are citations by ""diaspora""researchers, most of which concern Social Science and Humanities journals. Although the surnames of Armenians end with -i(y)an, sometimes, the Diaspora Armenians, surnames are changed or modified or they are not ending with -i(y)an, in this case we may fail to identify them. This study can help to build new, more deep and comprehensive relations with scientific diasporas. This study offers a new understanding of multifaced research collaboration with scientific diasporas and their role in internationalization of domestic journals."
Bibliometric-based Study of Scientist Academic Genealogy,"This study aims to construct new models and methods of academic genealogy research based on bibliometrics. This study proposes an academic influence scale for academic genealogy, and introduces the w index for bibliometric scaling of the academic genealogy. We then construct a two-dimensional (academic fecundity versus academic influence) evaluation system of academic genealogy, and validate it on the academic genealogy of a famous Chinese geologist. The two-dimensional evaluation system can characterize the development and evolution of the academic genealogy, compare the academic influences of different genealogies, and evaluate individuals' contributions to the inheritance and evolution of the academic genealogy. Individual academic influence is mainly indicated by the w index (the improved h index), which overcomes the situation of repeated measurements and distortion of results in the academic genealogy. The two-dimensional evaluation system for the academic genealogy can better demonstrate the reproduction and the academic inheritance ability of a genealogy. It is not comprehensive to only use the w index to characterize academic influence. It should also include scholars' academic awards and academic part-timers and so on. In future work, we will integrate scholars' academic awards and academic part-timers into the w index for a comprehensive reflection of scholars' individual academic influences. This study constructs new models and methods of academic genealogy research based on bibliometrics, which improves the quantitative assessment of academic genealogy and enriches its research and evaluation methods.  Â© 2021 2021 Ruihua Lv et al., published by Sciendo.","This study aims to construct new models and methods of academic genealogy research based on bibliometrics. This study proposes an academic influence scale for academic genealogy, and introduces the w index for bibliometric scaling of the academic genealogy. We then construct a two-dimensional (academic fecundity versus academic influence) evaluation system of academic genealogy, and validate it on the academic genealogy of a famous Chinese geologist. The two-dimensional evaluation system can characterize the development and evolution of the academic genealogy, compare the academic influences of different genealogies, and evaluate individuals' contributions to the inheritance and evolution of the academic genealogy. Individual academic influence is mainly indicated by the w index (the improved h index), which overcomes the situation of repeated measurements and distortion of results in the academic genealogy. The two-dimensional evaluation system for the academic genealogy can better demonstrate the reproduction and the academic inheritance ability of a genealogy. It is not comprehensive to only use the w index to characterize academic influence. It should also include scholars' academic awards and academic part-timers and so on. In future work, we will integrate scholars' academic awards and academic part-timers into the w index for a comprehensive reflection of scholars' individual academic influences. This study constructs new models and methods of academic genealogy research based on bibliometrics, which improves the quantitative assessment of academic genealogy and enriches its research and evaluation methods."
The compound F2-index and the compound H-index as extension of the f2and h-indexes from a dynamic perspective,"Purpose: Elaboration of an indicator to include the dynamic aspect of citations in bibliometric indexes. Design/methodology/approach: A new bibliometric methodology-the f2-index-is applied at the career level and at the level of the recent 5 years to analyze the dynamic aspect of bibliometrics. The method is applied, as an illustration, to the field of corporate governance. Findings: The compound F2-index as an extension of the f2-index recognizes past achievements but also values new research work with potential. The method is extended to the h-index and the h2-index. An activity index is defined as the ratio between the recent h'-index to the career h-index. Research limitations: The compound F2and H-indexes are PAC, probably approximately correct, and depend on the selection and database. Practical implications: The F2-and H compound indexes allow identifying the rising stars of a field from a dynamic perspective. The activity ratio highlights the contribution of younger researchers. Originality/value: The new methodology demonstrates the underestimated dynamic capacity of bibliometric research. Â© 2020 Sciendo. All rights reserved.","Elaboration of an indicator to include the dynamic aspect of citations in bibliometric indexes. A new bibliometric methodology-the f2-index-is applied at the career level and at the level of the recent 5 years to analyze the dynamic aspect of bibliometrics. The method is applied, as an illustration, to the field of corporate governance. The compound F2-index as an extension of the f2-index recognizes past achievements but also values new research work with potential. The method is extended to the h-index and the h2-index. An activity index is defined as the ratio between the recent h'-index to the career h-index. The compound F2and H-indexes are PAC, probably approximately correct, and depend on the selection and database. The F2-and H compound indexes allow identifying the rising stars of a field from a dynamic perspective. The activity ratio highlights the contribution of younger researchers. The new methodology demonstrates the underestimated dynamic capacity of bibliometric research."
The Scientometric Measurement of Interdisciplinarity and Diversity in the Research Portfolios of Chinese Universities,"Purpose: Interdisciplinarity is a hot topic in science and technology policy. However, the concept of interdisciplinarity is both abstract and complex, and therefore difficult to measure using a single indicator. A variety of metrics for measuring the diversity and interdisciplinarity of articles, journals, and fields have been proposed in the literature. In this article, we ask whether institutions can be ranked in terms of their (inter-)disciplinary diversity. Design/methodology/approach: We developed a software application (interd_vb.exe) that outputs the values of relevant diversity indicators for any document set or network structure. The software is made available, free to the public, online. The indicators it considers include the advanced diversity indicators Rao-Stirling (RS) diversity and DIVâ, as well as standard measures of diversity, such as the Gini coefficient, Shannon entropy, and the Simpson Index. As an empirical demonstration of how the application works, we compared the research portfolios of 42 ""Double First-Class""Chinese universities across Web of Science Subject Categories (WCs). Findings: The empirical results suggest that DIVâ provides results that are more in line with one's intuitive impressions than RS, particularly when the results are based on sample-dependent disparity measures. Furthermore, the scores for diversity are more consistent when based on a global disparity matrix than on a local map. Research limitations: ""Interdisciplinarity""can be operationalized as bibliographic coupling among (sets of) documents with references to disciplines. At the institutional level, however, diversity may also indicate comprehensiveness. Unlike impact (e.g. citation), diversity and interdisciplinarity are context-specific and therefore provide a second dimension to the evaluation. Policy or practical implications: Operationalization and quantification make it necessary for analysts to make their choices and options clear. Although the equations used to calculate diversity are often mathematically transparent, the specification in terms of computer code helps the analyst to further precision in decisions. Although diversity is not necessarily a goal of universities, a high diversity score may inform potential policies concerning interdisciplinarity at the university level. Originality/value: This article introduces a non-commercial online application to the public domain that allows researchers and policy analysts to measure ""diversity""and ""interdisciplinarity""using the various indicators as encompassing as possible for any document set or network structure (e.g. a network of co-authors). Insofar as we know, such a professional computing tool for evaluating data sets using diversity indicators has not yet been made available online.  Â© 2021 Lin Zhang et al., published by Sciendo.","Interdisciplinarity is a hot topic in science and technology policy. However, the concept of interdisciplinarity is both abstract and complex, and therefore difficult to measure using a single indicator. A variety of metrics for measuring the diversity and interdisciplinarity of articles, journals, and fields have been proposed in the literature. In this article, we ask whether institutions can be ranked in terms of their (inter-)disciplinary diversity. We developed a software application (interd_vb.exe) that outputs the values of relevant diversity indicators for any document set or network structure. The software is made available, free to the public, online. The indicators it considers include the advanced diversity indicators Rao-Stirling (RS) diversity and DIV, as well as standard measures of diversity, such as the Gini coefficient, Shannon entropy, and the Simpson Index. As an empirical demonstration of how the application works, we compared the research portfolios of 42 ""Double First-Class""Chinese universities across Web of Science Subject Categories (WCs). The empirical results suggest that DIV provides results that are more in line with one's intuitive impressions than RS, particularly when the results are based on sample-dependent disparity measures. Furthermore, the scores for diversity are more consistent when based on a global disparity matrix than on a local map. ""Interdisciplinarity""can be operationalized as bibliographic coupling among (sets of) documents with references to disciplines. At the institutional level, however, diversity may also indicate comprehensiveness. Unlike impact ( citation), diversity and interdisciplinarity are context-specific and therefore provide a second dimension to the evaluation. Policy or Operationalization and quantification make it necessary for analysts to make their choices and options clear. Although the equations used to calculate diversity are often mathematically transparent, the specification in terms of computer code helps the analyst to further precision in decisions. Although diversity is not necessarily a goal of universities, a high diversity score may inform potential policies concerning interdisciplinarity at the university level. This article introduces a non-commercial online application to the public domain that allows researchers and policy analysts to measure ""diversity""and ""interdisciplinarity""using the various indicators as encompassing as possible for any document set or network structure ( a network of co-authors). Insofar as we know, such a professional computing tool for evaluating data sets using diversity indicators has not yet been made available online."
Lone Geniuses or One among Many? An Explorative Study of Contemporary Highly Cited Researchers,"The ranking lists of highly cited researchers receive much public attention. In common interpretations, highly cited researchers are perceived to have made extraordinary contributions to science. Thus, the metrics of highly cited researchers are often linked to notions of breakthroughs, scientific excellence, and lone geniuses. In this study, we analyze a sample of individuals who appear on Clarivate Analytics' Highly Cited Researchers list. The main purpose is to juxtapose the characteristics of their research performance against the claim that the list captures a small fraction of the researcher population that contributes disproportionately to extending the frontier and gaining - on behalf of society - knowledge and innovations that make the world healthier, richer, sustainable, and more secure. The study reveals that the highly cited articles of the selected individuals generally have a very large number of authors. Thus, these papers seldom represent individual contributions but rather are the result of large collective research efforts conducted in research consortia. This challenges the common perception of highly cited researchers as individual geniuses who can be singled out for their extraordinary contributions. Moreover, the study indicates that a few of the individuals have not even contributed to highly cited original research but rather to reviews or clinical guidelines. Finally, the large number of authors of the papers implies that the ranking list is very sensitive to the specific method used for allocating papers and citations to individuals. In the ""whole count""methodology applied by Clarivate Analytics, each author gets full credit of the papers regardless of the number of additional co-authors. The study shows that the ranking list would look very different using an alternative fractionalised methodology. The study is based on a limited part of the total population of highly cited researchers. It is concluded that ""excellence""understood as highly cited encompasses very different types of research and researchers of which many do not fit with dominant preconceptions. The study develops further knowledge on highly cited researchers, addressing questions such as who becomes highly cited and the type of research that benefits by defining excellence in terms of citation scores and specific counting methods.  Â© 2021 2021 Dag W. Aksnes et al., published by Sciendo.","The ranking lists of highly cited researchers receive much public attention. In common interpretations, highly cited researchers are perceived to have made extraordinary contributions to science. Thus, the metrics of highly cited researchers are often linked to notions of breakthroughs, scientific excellence, and lone geniuses. In this study, we analyze a sample of individuals who appear on Clarivate Analytics' Highly Cited Researchers list. The main purpose is to juxtapose the characteristics of their research performance against the claim that the list captures a small fraction of the researcher population that contributes disproportionately to extending the frontier and gaining - on behalf of society - knowledge and innovations that make the world healthier, richer, sustainable, and more secure. The study reveals that the highly cited articles of the selected individuals generally have a very large number of authors. Thus, these papers seldom represent individual contributions but rather are the result of large collective research efforts conducted in research consortia. This challenges the common perception of highly cited researchers as individual geniuses who can be singled out for their extraordinary contributions. Moreover, the study indicates that a few of the individuals have not even contributed to highly cited original research but rather to reviews or clinical guidelines. Finally, the large number of authors of the papers implies that the ranking list is very sensitive to the specific method used for allocating papers and citations to individuals. In the ""whole count""methodology applied by Clarivate Analytics, each author gets full credit of the papers regardless of the number of additional co-authors. The study shows that the ranking list would look very different using an alternative fractionalised methodology. The study is based on a limited part of the total population of highly cited researchers. It is concluded that ""excellence""understood as highly cited encompasses very different types of research and researchers of which many do not fit with dominant preconceptions. The study develops further knowledge on highly cited researchers, addressing questions such as who becomes highly cited and the type of research that benefits by defining excellence in terms of citation scores and specific counting methods."
"""sparking"" and ""igniting"" Key Publications of 2020 Nobel Prize Laureates","This article aims to determine the percentage of ""Sparking""articles among the work of this year's Nobel Prize winners in medicine, physics, and chemistry. We focus on under-cited influential research among the key publications as mentioned by the Nobel Prize Committee for the 2020 Noble Prize laureates. Specifically, we extracted data from the Web of Science, and calculated the Sparking Indices using the formulas as proposed by Hu and Rousseau in 2016 and 2017. In addition, we identified another type of igniting articles based on the notion in 2017. In the fields of medicine and physics, the proportions of articles with sparking characteristics share 78.571% and 68.75% respectively, yet, in chemistry 90% articles characterized by ""igniting"". Moreover, the two types of articles share more than 93% in the work of the Nobel Prize included in this study. Our research did not cover the impact of topic, socio-political, and author's reputation on the Sparking Indices. Our study shows that the Sparking Indices truly reflect influence of the best research work, so it can be used to detect under-cited influential articles, as well as identifying fundamental work. Our findings suggest that the Sparking Indices have good applicability for research evaluation.  Â© 2021 2021 Fangjie Xi et al., published by Sciendo.","This article aims to determine the percentage of ""Sparking""articles among the work of this year's Nobel Prize winners in medicine, physics, and chemistry. We focus on under-cited influential research among the key publications as mentioned by the Nobel Prize Committee for the 2020 Noble Prize laureates. Specifically, we extracted data from the Web of Science, and calculated the Sparking Indices using the formulas as proposed by Hu and Rousseau in 2016 and 2017. In addition, we identified another type of igniting articles based on the notion in 2017. In the fields of medicine and physics, the proportions of articles with sparking characteristics share 78.571% and 68.75% respectively, yet, in chemistry 90% articles characterized by ""igniting"". Moreover, the two types of articles share more than 93% in the work of the Nobel Prize included in this study. Our research did not cover the impact of topic, socio-political, and author's reputation on the Sparking Indices. Our study shows that the Sparking Indices truly reflect influence of the best research work, so it can be used to detect under-cited influential articles, as well as identifying fundamental work. Our findings suggest that the Sparking Indices have good applicability for research evaluation."
National Lists of Scholarly Publication Channels: An Overview and Recommendations for Their Construction and Maintenance,"This paper presents an overview of different kinds of lists of scholarly publication channels and of experiences related to the construction and maintenance of national lists supporting performance-based research funding systems. It also contributes with a set of recommendations for the construction and maintenance of national lists of journals and book publishers. The study is based on analysis of previously published studies, policy papers, and reported experiences related to the construction and use of lists of scholarly publication channels. Several countries have systems for research funding and/or evaluation, that involve the use of national lists of scholarly publication channels (mainly journals and publishers). Typically, such lists are selective (do not include all scholarly or non-scholarly channels) and differentiated (distinguish between channels of different levels and quality). At the same time, most lists are embedded in a system that encompasses multiple or all disciplines. This raises the question how such lists can be organized and maintained to ensure that all relevant disciplines and all types of research are adequately represented. The conclusions and recommendations of the study are based on the authors' interpretation of a complex and sometimes controversial process with many different stakeholders involved. The recommendations and the related background information provided in this paper enable mutual learning that may feed into improvements in the construction and maintenance of national and other lists of scholarly publication channels in any geographical context. This may foster a development of responsible evaluation practices. This paper presents the first general overview and typology of different kinds of publication channel lists, provides insights on expert-based versus metrics-based evaluation, and formulates a set of recommendations for the responsible construction and maintenance of publication channel lists.  Â© 2021 2021 Janne PÃ¶lÃ¶nen et al., published by Sciendo.","This paper presents an overview of different kinds of lists of scholarly publication channels and of experiences related to the construction and maintenance of national lists supporting performance-based research funding systems. It also contributes with a set of recommendations for the construction and maintenance of national lists of journals and book publishers. The study is based on analysis of previously published studies, policy papers, and reported experiences related to the construction and use of lists of scholarly publication channels. Several countries have systems for research funding and/or evaluation, that involve the use of national lists of scholarly publication channels (mainly journals and publishers). Typically, such lists are selective (do not include all scholarly or non-scholarly channels) and differentiated (distinguish between channels of different levels and quality). At the same time, most lists are embedded in a system that encompasses multiple or all disciplines. This raises the question how such lists can be organized and maintained to ensure that all relevant disciplines and all types of research are adequately represented. The conclusions and recommendations of the study are based on the authors' interpretation of a complex and sometimes controversial process with many different stakeholders involved. The recommendations and the related background information provided in this paper enable mutual learning that may feed into improvements in the construction and maintenance of national and other lists of scholarly publication channels in any geographical context. This may foster a development of responsible evaluation practices. This paper presents the first general overview and typology of different kinds of publication channel lists, provides insights on expert-based versus metrics-based evaluation, and formulates a set of recommendations for the responsible construction and maintenance of publication channel lists."
Delayed recognition: Recent developments and a proposal to study this phenomenon as a fuzzy concept,"New developments in the study of delayed recognition are discussed. Based on these new developments a method is proposed to characterize delayed recognition as a fuzzy concept. A benchmark value of 0.333 corresponding with linear growth is obtained. Moreover, a case is discovered in which an expert found delayed recognition several years before citation analysis could discover this phenomenon. As all citation studies also this one is database dependent. Delayed recognition is turned into a fuzzy concept. The article presents a new way of studying delayed recognition. Â© 2017 2018 Ronald Rousseau, published by Sciendo.","New developments in the study of delayed recognition are discussed. Based on these new developments a method is proposed to characterize delayed recognition as a fuzzy concept. A benchmark value of 0.333 corresponding with linear growth is obtained. Moreover, a case is discovered in which an expert found delayed recognition several years before citation analysis could discover this phenomenon. As all citation studies also this one is database dependent. Delayed recognition is turned into a fuzzy concept. The article presents a new way of studying delayed recognition."
Why open government data? The case of a Swedish municipality,"Purpose: The purpose of this exploratory study is to provide modern local governments with potential use cases for their open data, in order to help inform related future policies and decision-making. The concrete context was that of the VÃ¤xjÃ¶ municipality located in southeastern Sweden. Design/methodology/approach: The methodology was two-fold: 1) a survey of potential end users (n=151) from a local university; and, 2) analysis of survey results using a theoretical model regarding local strategies for implementing open government data. Findings: Most datasets predicted to be useful were on: sustainability and environment; preschool and school; municipality and politics. The use context given is primarily research and development, informing policies and decision making; but also education, informing personal choices, informing citizens and creating services based on open data. Not the least, the need for educating target user groups on data literacy emerged. A tentative pattern comprising a technical perspective on open data and a social perspective on open government was identified. Research limitations: In line with available funding, the nature of the study was exploratory and implemented as an anonymous web-based survey of employees and students at the local university. Further research involving (qualitative) surveys with all stakeholders would allow for creating a more complete picture of the matter. Practical implications: The study determines potential use cases and use contexts for open government data, in order to help inform related future policies and decision-making. Originality/value: Modern local governments, and especially in Sweden, are faced with a challenge of how to make their data open, how to learn about which types of data will be most relevant for their end users and what will be different societal purposes. The paper contributes to knowledge that modern local governments can resort to when it comes to attitudes of local citizens to open government data in the context of an open government data perspective. Â© 2021 Sciendo. All rights reserved.","The purpose of this exploratory study is to provide modern local governments with potential use cases for their open data, in order to help inform related future policies and decision-making. The concrete context was that of the Vxj municipality located in southeastern Sweden. The methodology was two-fold: 1) a survey of potential end users (n=151) from a local university; and, 2) analysis of survey results using a theoretical model regarding local strategies for implementing open government data. Most datasets predicted to be useful were on: sustainability and environment; preschool and school; municipality and politics. The use context given is primarily research and development, informing policies and decision making; but also education, informing personal choices, informing citizens and creating services based on open data. Not the least, the need for educating target user groups on data literacy emerged. A tentative pattern comprising a technical perspective on open data and a social perspective on open government was identified. In line with available funding, the nature of the study was exploratory and implemented as an anonymous web-based survey of employees and students at the local university. Further research involving (qualitative) surveys with all stakeholders would allow for creating a more complete picture of the matter. The study determines potential use cases and use contexts for open government data, in order to help inform related future policies and decision-making. Modern local governments, and especially in Sweden, are faced with a challenge of how to make their data open, how to learn about which types of data will be most relevant for their end users and what will be different societal purposes. The paper contributes to knowledge that modern local governments can resort to when it comes to attitudes of local citizens to open government data in the context of an open government data perspective."
A Metric Approach to Hot Topics in Biomedicine via Keyword Co-occurrence,"To reveal the research hotpots and relationship among three research hot topics in biomedicine, namely CRISPR, i PS (induced Pluripotent Stem) cell and Synthetic biology. We set up their keyword co-occurrence networks with using three indicators and information visualization for metric analysis. The results reveal the main research hotspots in the three topics are different, but the overlapping keywords in the three topics indicate that they are mutually integrated and interacted each other. All analyses use keywords, without any other forms. We try to find the information distribution and structure of these three hot topics for revealing their research status and interactions, and for promoting biomedical developments. We chose the core keywords in three research hot topics in biomedicine by using h-index. Â© 2019 2019 Jane H. Qin, Jean J. Wang, Fred Y. Ye, published by Sciendo.","To reveal the research hotpots and relationship among three research hot topics in biomedicine, namely CRISPR, i PS (induced Pluripotent Stem) cell and Synthetic biology. We set up their keyword co-occurrence networks with using three indicators and information visualization for metric analysis. The results reveal the main research hotspots in the three topics are different, but the overlapping keywords in the three topics indicate that they are mutually integrated and interacted each other. All analyses use keywords, without any other forms. We try to find the information distribution and structure of these three hot topics for revealing their research status and interactions, and for promoting biomedical developments. We chose the core keywords in three research hot topics in biomedicine by using h-index."
Scientometric Implosion that Leads to Explosion: Case Study of Armenian Journals,"The purpose of this study is to introduce a new concept and term into the scientometric discourse and research-scientometric implosion- A nd test the idea on the example of the Armenian journals. The article argues that the existence of a compressed scientific area in the country makes pressure on the journals and after some time this pressure makes one or several journals explode-break the limited national scientific area and move to the international arena. As soon as one of the local journals breaks through this compressed space and appears at an international level, further explosion happens, which makes the other journals follow the same path. Our research is based on three international scientific databases-WoS, Scopus, and RISC CC, from where we have retrieved information about the Armenian journals indexed there and citations received by those journals and one national database-the Armenian Science Citation Index. Armenian Journal Impact Factor (ArmJIF) was calculated for the local Armenian journals based on the general impact factor formula. Journals were classified according to GlÃ¤nzel and Schubert (2003). Our results show that the science policy developed by the scientific authorities of Armenia and the introduction of ArmJIF have made the Armenian journals comply with international standards and resulted in some local journals to break the national scientific territory and be indexed in international scientific databases of RISC, Scopus, and WoS. Apart from complying with technical requirements, the journals start publishing articles also in foreign languages. Although nearly half of the local journals are in the fields of social sciences and humanities, only one journal from that field is indexed in international scientific databases. One of the limitations of the study is that it was performed on the example of only one state and the second one is that more time passage is needed to firmly evaluate the results. However, the introduction of the concept can inspire other similar case study. The new term and relevant model offered in the article can practically be used for the development of national journals. The article proposes a new term and a concept in scientometrics.  Â© 2020 2020 Shushanik Sargsyan et al., published by Sciendo.","The purpose of this study is to introduce a new concept and term into the scientometric discourse and research-scientometric implosion- A nd test the idea on the example of the Armenian journals. The article argues that the existence of a compressed scientific area in the country makes pressure on the journals and after some time this pressure makes one or several journals explode-break the limited national scientific area and move to the international arena. As soon as one of the local journals breaks through this compressed space and appears at an international level, further explosion happens, which makes the other journals follow the same path. Our research is based on three international scientific databases-WoS, Scopus, and RISC CC, from where we have retrieved information about the Armenian journals indexed there and citations received by those journals and one national database-the Armenian Science Citation Index. Armenian Journal Impact Factor (ArmJIF) was calculated for the local Armenian journals based on the general impact factor formula. Journals were classified according to Glnzel and Schubert . Our results show that the science policy developed by the scientific authorities of Armenia and the introduction of ArmJIF have made the Armenian journals comply with international standards and resulted in some local journals to break the national scientific territory and be indexed in international scientific databases of RISC, Scopus, and WoS. Apart from complying with technical requirements, the journals start publishing articles also in foreign languages. Although nearly half of the local journals are in the fields of social sciences and humanities, only one journal from that field is indexed in international scientific databases. One of the limitations of the study is that it was performed on the example of only one state and the second one is that more time passage is needed to firmly evaluate the results. However, the introduction of the concept can inspire other similar case study. The new term and relevant model offered in the article can practically be used for the development of national journals. The article proposes a new term and a concept in scientometrics."
A Discrimination Index Based on Jain's Fairness Index to Differentiate Researchers with Identical H-index Values,"This paper proposes a discrimination index method based on the Jain's fairness index to distinguish researchers with the same H-index. A validity test is used to measure the correlation of D-offset with the parameters, i.e. H-index, the number of cited papers, the total number of citations, the number of indexed papers, and the number of uncited papers. The correlation test is based on the Saphiro-Wilk method and Pearson's product-moment correlation. The result from the discrimination index calculation is a two-digit decimal value called the discrimination-offset (D-offset), with a range of D-offset from 0.00 to 0.99. The result of the correlation value between the D-offset and the number of uncited papers is 0.35, D-offset with the number of indexed papers is 0.24, and the number of cited papers is 0.27. The test provides the result that it is very unlikely that there exists no relationship between the parameters. For this reason, D-offset is proposed as an additional parameter for H-index to differentiate researchers with the same H-index. The H-index for researchers can be written with the format of ""H-index: D-offset"". D-offset is worthy to be considered as a complement value to add the H-index value. If the D-offset is added in the H-index value, the H-index will have more discrimination power to differentiate the rank of the researchers who have the same H-index.  Â© 2020 2020 Adian Fatchur Rochim et al., published by Sciendo.","This paper proposes a discrimination index method based on the Jain's fairness index to distinguish researchers with the same H-index. A validity test is used to measure the correlation of D-offset with the parameters, H-index, the number of cited papers, the total number of citations, the number of indexed papers, and the number of uncited papers. The correlation test is based on the Saphiro-Wilk method and Pearson's product-moment correlation. The result from the discrimination index calculation is a two-digit decimal value called the discrimination-offset (D-offset), with a range of D-offset from 0.00 to 0.99. The result of the correlation value between the D-offset and the number of uncited papers is 0.35, D-offset with the number of indexed papers is 0.24, and the number of cited papers is 0.27. The test provides the result that it is very unlikely that there exists no relationship between the parameters. For this reason, D-offset is proposed as an additional parameter for H-index to differentiate researchers with the same H-index. The H-index for researchers can be written with the format of ""H-index: D-offset"". D-offset is worthy to be considered as a complement value to add the H-index value. If the D-offset is added in the H-index value, the H-index will have more discrimination power to differentiate the rank of the researchers who have the same H-index."
Evolution of the Socio-cognitive Structure of Knowledge Management (1986-2015): An Author Co-citation Analysis,"Purpose: The evolution of the socio-cognitive structure of the field of knowledge management (KM) during the period 1986-2015 is described. Design/methodology/approach: Records retrieved from Web of Science were submitted to author co-citation analysis (ACA) following a longitudinal perspective as of the following time slices: 1986-1996, 1997-2006, and 2007-2015. The top 10% of most cited first authors by sub-periods were mapped in bibliometric networks in order to interpret the communities formed and their relationships. Findings: KM is a homogeneous field as indicated by networks results. Nine classical authors are identified since they are highly co-cited in each sub-period, highlighting Ikujiro Nonaka as the most influential authors in the field. The most significant communities in KM are devoted to strategic management, KM foundations, organisational learning and behaviour, and organisational theories. Major trends in the evolution of the intellectual structure of KM evidence a technological influence in 1986-1996, a strategic influence in 1997-2006, and finally a sociological influence in 2007-2015. Research limitations: Describing a field from a single database can offer biases in terms of output coverage. Likewise, the conference proceedings and books were not used and the analysis was only based on first authors. However, the results obtained can be very useful to understand the evolution of KM research. Practical implications: These results might be useful for managers and academicians to understand the evolution of KM field and to (re)define research activities and organisational projects. Originality/value: The novelty of this paper lies in considering ACA as a bibliometric technique to study KM research. In addition, our investigation has a wider time coverage than earlier articles. Â© 2019 Carlos Luis GonzÃ¡lez-Valiente, Magda LeÃ³n Santos, Ricardo Arencibia-Jorge.","The evolution of the socio-cognitive structure of the field of knowledge management (KM) during the period 1986-2015 is described. Records retrieved from Web of Science were submitted to author co-citation analysis (ACA) following a longitudinal perspective as of the following time slices: 1986-1996, 1997-2006, and 2007-2015. The top 10% of most cited first authors by sub-periods were mapped in bibliometric networks in order to interpret the communities formed and their relationships. KM is a homogeneous field as indicated by networks results. Nine classical authors are identified since they are highly co-cited in each sub-period, highlighting Ikujiro Nonaka as the most influential authors in the field. The most significant communities in KM are devoted to strategic management, KM foundations, organisational learning and behaviour, and organisational theories. Major trends in the evolution of the intellectual structure of KM evidence a technological influence in 1986-1996, a strategic influence in 1997-2006, and finally a sociological influence in 2007-2015. Describing a field from a single database can offer biases in terms of output coverage. Likewise, the conference proceedings and books were not used and the analysis was only based on first authors. However, the results obtained can be very useful to understand the evolution of KM research. These results might be useful for managers and academicians to understand the evolution of KM field and to (re)define research activities and organisational projects. The novelty of this paper lies in considering ACA as a bibliometric technique to study KM research. In addition, our investigation has a wider time coverage than earlier articles."
"Discipline Impact Factor: Some of Its History, Some of the Author's Experience of Its Application, the Continuing Reasons for Its Use and... Next beyond","This work aims to consider the role and some of the 42-year history of the discipline impact factor (DIF) in evaluation of serial publications. Also, the original ""symmetric""indicator called the ""discipline susceptibility factor""is to be presented. In accordance with the purpose of the work, the methods are analytical interpretation of the scientific literature related to this problem as well as speculative explanations. The information base of the research is bibliometric publications dealing with impact, impact factor, discipline impact factor, and discipline susceptibility factor. Examples of the DIF application and modification of the indicator are given. It is shown why research and university libraries need to use the DIF to evaluate serials in conditions of scarce funding for subscription to serial publications, even if open access is available. The role of the DIF for evaluating journals by authors of scientific papers when choosing a good and right journal for submitting a paper is also briefly discussed. An original indicator ""symmetrical""to the DIF (the ""discipline susceptibility factor"") and its differences from the DIF in terms of content and purpose of evaluation are also briefly presented. The selection of publications for the information base of the research did not include those in which the DIF was only mentioned, used partially or not for its original purpose. Restrictions on the length of the article to be submitted in this special issue of the JDIS also caused exclusion even a number of completely relevant publications. Consideration of the DIF is not placed in the context of describing other derivatives from the Garfield impact factor. An underrated bibliometric indicator, viz. the discipline impact factor is being promoted for the practical application. An original indicator ""symmetrical""to DIF has been proposed in order of searching serial publications representing the external research fields that might fit for potential applications of the results of scientific activities obtained within the framework of the specific research field represented by the cited specialized journals. Both can be useful in research and university libraries in their endeavors to improve scientific information services. Also, both can be used for evaluating journals by authors of scientific papers when choosing a journal to submit a paper. The article substantiates the need to evaluate scientific serial publications in library activities-even in conditions of access to huge and convenient databases (subscription packages) and open access to a large number of serial publications. It gives a mini-survey of the history of one of the methods of such evaluation, and offers an original method for evaluating scientific serial publications.  Â© 2020 2020 Vladimir S. Lazarev, published by Sciendo.","This work aims to consider the role and some of the 42-year history of the discipline impact factor (DIF) in evaluation of serial publications. Also, the original ""symmetric""indicator called the ""discipline susceptibility factor""is to be presented. In accordance with the purpose of the work, the methods are analytical interpretation of the scientific literature related to this problem as well as speculative explanations. The information base of the research is bibliometric publications dealing with impact, impact factor, discipline impact factor, and discipline susceptibility factor. Examples of the DIF application and modification of the indicator are given. It is shown why research and university libraries need to use the DIF to evaluate serials in conditions of scarce funding for subscription to serial publications, even if open access is available. The role of the DIF for evaluating journals by authors of scientific papers when choosing a good and right journal for submitting a paper is also briefly discussed. An original indicator ""symmetrical""to the DIF (the ""discipline susceptibility factor"") and its differences from the DIF in terms of content and purpose of evaluation are also briefly presented. The selection of publications for the information base of the research did not include those in which the DIF was only mentioned, used partially or not for its original purpose. Restrictions on the length of the article to be submitted in this special issue of the JDIS also caused exclusion even a number of completely relevant publications. Consideration of the DIF is not placed in the context of describing other derivatives from the Garfield impact factor. An underrated bibliometric indicator, viz. the discipline impact factor is being promoted for the practical application. An original indicator ""symmetrical""to DIF has been proposed in order of searching serial publications representing the external research fields that might fit for potential applications of the results of scientific activities obtained within the framework of the specific research field represented by the cited specialized journals. Both can be useful in research and university libraries in their endeavors to improve scientific information services. Also, both can be used for evaluating journals by authors of scientific papers when choosing a journal to submit a paper. The article substantiates the need to evaluate scientific serial publications in library activities-even in conditions of access to huge and convenient databases (subscription packages) and open access to a large number of serial publications. It gives a mini-survey of the history of one of the methods of such evaluation, and offers an original method for evaluating scientific serial publications."
A Multi-match Approach to the Author Uncertainty Problem,"The ability to identify the scholarship of individual authors is essential for performance evaluation. A number of factors hinder this endeavor. Common and similarly spelled surnames make it difficult to isolate the scholarship of individual authors indexed on large databases. Variations in name spelling of individual scholars further complicates matters. Common family names in scientific powerhouses like China make it problematic to distinguish between authors possessing ubiquitous and/or anglicized surnames (as well as the same or similar first names). The assignment of unique author identifiers provides a major step toward resolving these difficulties. We maintain, however, that in and of themselves, author identifiers are not sufficient to fully address the author uncertainty problem. In this study we build on the author identifier approach by considering commonalities in fielded data between authors containing the same surname and first initial of their first name. We illustrate our approach using three case studies. The approach we advance in this study is based on commonalities among fielded data in search results. We cast a broad initial net - i.e., a Web of Science (WOS) search for a given author's last name, followed by a comma, followed by the first initial of his or her first name (e.g., a search for 'John Doe' would assume the form: 'Doe, J'). Results for this search typically contain all of the scholarship legitimately belonging to this author in the given database (i.e., all of his or her true positives), along with a large amount of noise, or scholarship not belonging to this author (i.e., a large number of false positives). From this corpus we proceed to iteratively weed out false positives and retain true positives. Author identifiers provide a good starting point - e.g., if 'Doe, J' and 'Doe, John' share the same author identifier, this would be sufficient for us to conclude these are one and the same individual. We find email addresses similarly adequate - e.g., if two author names which share the same surname and same first initial have an email address in common, we conclude these authors are the same person. Author identifier and email address data is not always available, however. When this occurs, other fields are used to address the author uncertainty problem. Commonalities among author data other than unique identifiers and email addresses is less conclusive for name consolidation purposes. For example, if 'Doe, John' and 'Doe, J' have an affiliation in common, do we conclude that these names belong the same person? They may or may not; affiliations have employed two or more faculty members sharing the same last and first initial. Similarly, it's conceivable that two individuals with the same last name and first initial publish in the same journal, publish with the same co-authors, and/or cite the same references. Should we then ignore commonalities among these fields and conclude they're too imprecise for name consolidation purposes? It is our position that such commonalities are indeed valuable for addressing the author uncertainty problem, but more so when used in combination. Our approach makes use of automation as well as manual inspection, relying initially on author identifiers, then commonalities among fielded data other than author identifiers, and finally manual verification. To achieve name consolidation independent of author identifier matches, we have developed a procedure that is used with bibliometric software called VantagePoint (see www.thevantagepoint.com) While the application of our technique does not exclusively depend on VantagePoint, it is the software we find most efficient in this study. The script we developed to implement this procedure is designed to implement our name disambiguation procedure in a way that significantly reduces manual effort on the user's part. Those who seek to replicate our procedure independent of VantagePoint can do so by manually following the method we outline, but we note that the manual application of our procedure takes a significant amount of time and effort, especially when working with larger datasets. Our script begins by prompting the user for a surname and a first initial (for any author of interest). It then prompts the user to select a WOS field on which to consolidate author names. After this the user is prompted to point to the name of the authors field, and finally asked to identify a specific author name (referred to by the script as the primary author) within this field whom the user knows to be a true positive (a suggested approach is to point to an author name associated with one of the records that has the author's ORCID iD or email address attached to it). The script proceeds to identify and combine all author names sharing the primary author's surname and first initial of his or her first name who share commonalities in the WOS field on which the user was prompted to consolidate author names. This typically results in significant reduction in the initial dataset size. After the procedure completes the user is usually left with a much smaller (and more manageable) dataset to manually inspect (and/or apply additional name disambiguation techniques to). Match field coverage can be an issue. When field coverage is paltry dataset reduction is not as significant, which results in more manual inspection on the user's part. Our procedure doesn't lend itself to scholars who have had a legal family name change (after marriage, for example). Moreover, the technique we advance is (sometimes, but not always) likely to have a difficult time dealing with scholars who have changed careers or fields dramatically, as well as scholars whose work is highly interdisciplinary. The procedure we advance has the ability to save a significant amount of time and effort for individuals engaged in name disambiguation research, especially when the name under consideration is a more common family name. It is more effective when match field coverage is high and a number of match fields exist. Once again, the procedure we advance has the ability to save a significant amount of time and effort for individuals engaged in name disambiguation research. It combines preexisting with more recent approaches, harnessing the benefits of both. Our study applies the name disambiguation procedure we advance to three case studies. Ideal match fields are not the same for each of our case studies. We find that match field effectiveness is in large part a function of field coverage. Comparing original dataset size, the timeframe analyzed for each case study is not the same, nor are the subject areas in which they publish. Our procedure is more effective when applied to our third case study, both in terms of list reduction and 100% retention of true positives. We attribute this to excellent match field coverage, and especially in more specific match fields, as well as having a more modest/manageable number of publications. While machine learning is considered authoritative by many, we do not see it as practical or replicable. The procedure advanced herein is both practical, replicable and relatively user friendly. It might be categorized into a space between ORCID and machine learning. Machine learning approaches typically look for commonalities among citation data, which is not always available, structured or easy to work with. The procedure we advance is intended to be applied across numerous fields in a dataset of interest (e.g. emails, coauthors, affiliations, etc.), resulting in multiple rounds of reduction. Results indicate that effective match fields include author identifiers, emails, source titles, co-authors and ISSNs. While the script we present is not likely to result in a dataset consisting solely of true positives (at least for more common surnames), it does significantly reduce manual effort on the user's part. Dataset reduction (after our procedure is applied) is in large part a function of (a) field availability and (b) field coverage. Â© 2019 Stephen F. Carley, Alan L. Porter, Jan L. Youtie, published by Sciendo.","The ability to identify the scholarship of individual authors is essential for performance evaluation. A number of factors hinder this endeavor. Common and similarly spelled surnames make it difficult to isolate the scholarship of individual authors indexed on large databases. Variations in name spelling of individual scholars further complicates matters. Common family names in scientific powerhouses like China make it problematic to distinguish between authors possessing ubiquitous and/or anglicized surnames (as well as the same or similar first names). The assignment of unique author identifiers provides a major step toward resolving these difficulties. We maintain, however, that in and of themselves, author identifiers are not sufficient to fully address the author uncertainty problem. In this study we build on the author identifier approach by considering commonalities in fielded data between authors containing the same surname and first initial of their first name. We illustrate our approach using three case studies. The approach we advance in this study is based on commonalities among fielded data in search results. We cast a broad initial net - , a Web of Science (WOS) search for a given author's last name, followed by a comma, followed by the first initial of his or her first name (, a search for 'John Doe' would assume the form: 'Doe, J'). Results for this search typically contain all of the scholarship legitimately belonging to this author in the given database (, all of his or her true positives), along with a large amount of noise, or scholarship not belonging to this author (, a large number of false positives). From this corpus we proceed to iteratively weed out false positives and retain true positives. Author identifiers provide a good starting point - , if 'Doe, J' and 'Doe, John' share the same author identifier, this would be sufficient for us to conclude these are one and the same individual. We find email addresses similarly adequate - , if two author names which share the same surname and same first initial have an email address in common, we conclude these authors are the same person. Author identifier and email address data is not always available, however. When this occurs, other fields are used to address the author uncertainty problem. Commonalities among author data other than unique identifiers and email addresses is less conclusive for name consolidation purposes. For example, if 'Doe, John' and 'Doe, J' have an affiliation in common, do we conclude that these names belong the same person? They may or may not; affiliations have employed two or more faculty members sharing the same last and first initial. Similarly, it's conceivable that two individuals with the same last name and first initial publish in the same journal, publish with the same co-authors, and/or cite the same references. Should we then ignore commonalities among these fields and conclude they're too imprecise for name consolidation purposes? It is our position that such commonalities are indeed valuable for addressing the author uncertainty problem, but more so when used in combination. Our approach makes use of automation as well as manual inspection, relying initially on author identifiers, then commonalities among fielded data other than author identifiers, and finally manual verification. To achieve name consolidation independent of author identifier matches, we have developed a procedure that is used with bibliometric software called VantagePoint (see www.thevantagepoint.com) While the application of our technique does not exclusively depend on VantagePoint, it is the software we find most efficient in this study. The script we developed to implement this procedure is designed to implement our name disambiguation procedure in a way that significantly reduces manual effort on the user's part. Those who seek to replicate our procedure independent of VantagePoint can do so by manually following the method we outline, but we note that the manual application of our procedure takes a significant amount of time and effort, especially when working with larger datasets. Our script begins by prompting the user for a surname and a first initial (for any author of interest). It then prompts the user to select a WOS field on which to consolidate author names. After this the user is prompted to point to the name of the authors field, and finally asked to identify a specific author name (referred to by the script as the primary author) within this field whom the user knows to be a true positive (a suggested approach is to point to an author name associated with one of the records that has the author's ORCID iD or email address attached to it). The script proceeds to identify and combine all author names sharing the primary author's surname and first initial of his or her first name who share commonalities in the WOS field on which the user was prompted to consolidate author names. This typically results in significant reduction in the initial dataset size. After the procedure completes the user is usually left with a much smaller (and more manageable) dataset to manually inspect (and/or apply additional name disambiguation techniques to). Match field coverage can be an issue. When field coverage is paltry dataset reduction is not as significant, which results in more manual inspection on the user's part. Our procedure doesn't lend itself to scholars who have had a legal family name change (after marriage, for example). Moreover, the technique we advance is (sometimes, but not always) likely to have a difficult time dealing with scholars who have changed careers or fields dramatically, as well as scholars whose work is highly interdisciplinary. The procedure we advance has the ability to save a significant amount of time and effort for individuals engaged in name disambiguation research, especially when the name under consideration is a more common family name. It is more effective when match field coverage is high and a number of match fields exist. Once again, the procedure we advance has the ability to save a significant amount of time and effort for individuals engaged in name disambiguation research. It combines preexisting with more recent approaches, harnessing the benefits of both. Our study applies the name disambiguation procedure we advance to three case studies. Ideal match fields are not the same for each of our case studies. We find that match field effectiveness is in large part a function of field coverage. Comparing original dataset size, the timeframe analyzed for each case study is not the same, nor are the subject areas in which they publish. Our procedure is more effective when applied to our third case study, both in terms of list reduction and 100% retention of true positives. We attribute this to excellent match field coverage, and especially in more specific match fields, as well as having a more modest/manageable number of publications. While machine learning is considered authoritative by many, we do not see it as practical or replicable. The procedure advanced herein is both practical, replicable and relatively user friendly. It might be categorized into a space between ORCID and machine learning. Machine learning approaches typically look for commonalities among citation data, which is not always available, structured or easy to work with. The procedure we advance is intended to be applied across numerous fields in a dataset of interest ( emails, coauthors, affiliations, etc.), resulting in multiple rounds of reduction. Results indicate that effective match fields include author identifiers, emails, source titles, co-authors and ISSNs. While the script we present is not likely to result in a dataset consisting solely of true positives (at least for more common surnames), it does significantly reduce manual effort on the user's part. Dataset reduction (after our procedure is applied) is in large part a function of (a) field availability and (b) field coverage."
Identification and prediction of interdisciplinary research topics: A study based on the concept lattice theory,"Formal concept analysis (FCA) and concept lattice theory (CLT) are introduced for constructing a network of IDR topics and for evaluating their effectiveness for knowledge structure exploration. We introduced the theory and applications of FCA and CLT, and then proposed a method for interdisciplinary knowledge discovery based on CLT. As an example of empirical analysis, interdisciplinary research (IDR) topics in Information & Library Science (LIS) and Medical Informatics, and in LIS and Geography-Physical, were utilized as empirical fields. Subsequently, we carried out a comparative analysis with two other IDR topic recognition methods. The CLT approach is suitable for IDR topic identification and predictions. IDR topic recognition based on the CLT is not sensitive to the interdisciplinarity of topic terms, since the data can only reflect whether there is a relationship between the discipline and the topic terms. Moreover, the CLT cannot clearly represent a large amounts of concepts. A deeper understanding of the IDR topics was obtained as the structural and hierarchical relationships between them were identified, which can help to get more precise identification and prediction to IDR topics. IDR topics identification based on CLT have performed well and this theory has several advantages for identifying and predicting IDR topics. First, in a concept lattice, there is a partial order relation between interconnected nodes, and consequently, a complete concept lattice can present hierarchical properties. Second, clustering analysis of IDR topics based on concept lattices can yield clusters that highlight the essential knowledge features and help display the semantic relationship between different IDR topics. Furthermore, the Hasse diagram automatically displays all the IDR topics associated with the different disciplines, thus forming clusters of specific concepts and visually retaining and presenting the associations of IDR topics through multiple inheritance relationships between the concepts. Â© 2019 Haiyun Xu, Chao Wang, Kun Dong, Zenghui Yue, published by Sciendo.","Formal concept analysis (FCA) and concept lattice theory (CLT) are introduced for constructing a network of IDR topics and for evaluating their effectiveness for knowledge structure exploration. We introduced the theory and applications of FCA and CLT, and then proposed a method for interdisciplinary knowledge discovery based on CLT. As an example of empirical analysis, interdisciplinary research (IDR) topics in Information & Library Science (LIS) and Medical Informatics, and in LIS and Geography-Physical, were utilized as empirical fields. Subsequently, we carried out a comparative analysis with two other IDR topic recognition methods. The CLT approach is suitable for IDR topic identification and predictions. IDR topic recognition based on the CLT is not sensitive to the interdisciplinarity of topic terms, since the data can only reflect whether there is a relationship between the discipline and the topic terms. Moreover, the CLT cannot clearly represent a large amounts of concepts. A deeper understanding of the IDR topics was obtained as the structural and hierarchical relationships between them were identified, which can help to get more precise identification and prediction to IDR topics. IDR topics identification based on CLT have performed well and this theory has several advantages for identifying and predicting IDR topics. First, in a concept lattice, there is a partial order relation between interconnected nodes, and consequently, a complete concept lattice can present hierarchical properties. Second, clustering analysis of IDR topics based on concept lattices can yield clusters that highlight the essential knowledge features and help display the semantic relationship between different IDR topics. Furthermore, the Hasse diagram automatically displays all the IDR topics associated with the different disciplines, thus forming clusters of specific concepts and visually retaining and presenting the associations of IDR topics through multiple inheritance relationships between the concepts."
Detection of Malignant and Benign Breast Cancer Using the ANOVA-BOOTSTRAP-SVM,"The aim of this research is to propose a modification of the ANOVA-SVM method that can increase accuracy when detecting benign and malignant breast cancer. We proposed a new method ANOVA-BOOTSTRAP-SVM. It involves applying the analysis of variance (ANOVA) to support vector machines (SVM) but we use the bootstrap instead of cross validation as a train/test splitting procedure. We have tuned the kernel and the C parameter and tested our algorithm on a set of breast cancer datasets. By using the new method proposed, we succeeded in improving accuracy ranging from 4.5 percentage points to 8 percentage points depending on the dataset. The algorithm is sensitive to the type of kernel and value of the optimization parameter C. We believe that the ANOVA-BOOTSTRAP-SVM can be used not only to recognize the type of breast cancer but also for broader research in all types of cancer. Our findings are important as the algorithm can detect various types of cancer with higher accuracy compared to standard versions of the Support Vector Machines.  Â© 2020 Borislava Petrova Vrigazova, published by Sciendo.","The aim of this research is to propose a modification of the ANOVA-SVM method that can increase accuracy when detecting benign and malignant breast cancer. We proposed a new method ANOVA-BOOTSTRAP-SVM. It involves applying the analysis of variance (ANOVA) to support vector machines (SVM) but we use the bootstrap instead of cross validation as a train/test splitting procedure. We have tuned the kernel and the C parameter and tested our algorithm on a set of breast cancer datasets. By using the new method proposed, we succeeded in improving accuracy ranging from 4.5 percentage points to 8 percentage points depending on the dataset. The algorithm is sensitive to the type of kernel and value of the optimization parameter We believe that the ANOVA-BOOTSTRAP-SVM can be used not only to recognize the type of breast cancer but also for broader research in all types of cancer. Our findings are important as the algorithm can detect various types of cancer with higher accuracy compared to standard versions of the Support Vector Machines."
Improving Archival Records and Service of Traditional Korean Performing Arts in a Semantic Web Environment,"This research project aims to organize the archival information of traditional Korean performing arts in a semantic web environment. Key requirements, which the archival records manager should consider for publishing and distribution of gugak performing archival information in a semantic web environment, are presented in the perspective of linked data. This study analyzes the metadata provided by the National Gugak Center's Gugak Archive, the search and browse menus of Gugak Archive's website and K-PAAN, the performing arts portal site. The importance of consistency, continuity, and systematicity-crucial qualities in traditional record management practices-is undiminished in a semantic web environment. However, a semantic web environment also requires new tools such as web identifiers (URIs), data models (RDF), and link information (interlinking). The scope of this study does not include practical implementation strategies for the archival records management system and website services. The suggestions also do not discuss issues related to copyright or policy coordination between related organizations. The findings of this study can assist records managers in converting a traditional performing arts information archive into a semantic web environment-based online archival service and system. This can also be useful for collaboration with record managers who are unfamiliar with relational or triple database system. This study analyzed the metadata of the Gugak Archive and its online services to present practical requirements for managing and disseminating gugak performing arts information in a semantic web environment. In the application of the semantic web services' principles and methods to an Gugak Archive, this study can contribute to the improvement of information organization and services in the field of Korean traditional music. Â© 2020 2020 Ziyoung Park et al., published by Sciendo.","This research project aims to organize the archival information of traditional Korean performing arts in a semantic web environment. Key requirements, which the archival records manager should consider for publishing and distribution of gugak performing archival information in a semantic web environment, are presented in the perspective of linked data. This study analyzes the metadata provided by the National Gugak Center's Gugak Archive, the search and browse menus of Gugak Archive's website and K-PAAN, the performing arts portal site. The importance of consistency, continuity, and systematicity-crucial qualities in traditional record management practices-is undiminished in a semantic web environment. However, a semantic web environment also requires new tools such as web identifiers (URIs), data models (RDF), and link information (interlinking). The scope of this study does not include practical implementation strategies for the archival records management system and website services. The suggestions also do not discuss issues related to copyright or policy coordination between related organizations. The findings of this study can assist records managers in converting a traditional performing arts information archive into a semantic web environment-based online archival service and system. This can also be useful for collaboration with record managers who are unfamiliar with relational or triple database system. This study analyzed the metadata of the Gugak Archive and its online services to present practical requirements for managing and disseminating gugak performing arts information in a semantic web environment. In the application of the semantic web services' principles and methods to an Gugak Archive, this study can contribute to the improvement of information organization and services in the field of Korean traditional music."
Multi-Aspect Incremental Tensor Decomposition Based on Distributed In-Memory Big Data Systems,"We propose InParTen2, a multi-aspect parallel factor analysis three-dimensional tensor decomposition algorithm based on the Apache Spark framework. The proposed method reduces re-decomposition cost and can handle large tensors. Considering that tensor addition increases the size of a given tensor along all axes, the proposed method decomposes incoming tensors using existing decomposition results without generating sub-tensors. Additionally, InParTen2 avoids the calculation of Khari-Rao products and minimizes shuffling by using the Apache Spark platform. The performance of InParTen2 is evaluated by comparing its execution time and accuracy with those of existing distributed tensor decomposition methods on various datasets. The results confirm that InParTen2 can process large tensors and reduce the re-calculation cost of tensor decomposition. Consequently, the proposed method is faster than existing tensor decomposition algorithms and can significantly reduce re-decomposition cost. There are several Hadoop-based distributed tensor decomposition algorithms as well as MATLAB-based decomposition methods. However, the former require longer iteration time, and therefore their execution time cannot be compared with that of Spark-based algorithms, whereas the latter run on a single machine, thus limiting their ability to handle large data. The proposed algorithm can reduce re-decomposition cost when tensors are added to a given tensor by decomposing them based on existing decomposition results without re-decomposing the entire tensor. The proposed method can handle large tensors and is fast within the limited-memory framework of Apache Spark. Moreover, InParTen2 can handle static as well as incremental tensor decomposition.  Â© 2020 2020 Hye-Kyung Yang et al., published by Sciendo.","We propose InParTen2, a multi-aspect parallel factor analysis three-dimensional tensor decomposition algorithm based on the Apache Spark framework. The proposed method reduces re-decomposition cost and can handle large tensors. Considering that tensor addition increases the size of a given tensor along all axes, the proposed method decomposes incoming tensors using existing decomposition results without generating sub-tensors. Additionally, InParTen2 avoids the calculation of Khari-Rao products and minimizes shuffling by using the Apache Spark platform. The performance of InParTen2 is evaluated by comparing its execution time and accuracy with those of existing distributed tensor decomposition methods on various datasets. The results confirm that InParTen2 can process large tensors and reduce the re-calculation cost of tensor decomposition. Consequently, the proposed method is faster than existing tensor decomposition algorithms and can significantly reduce re-decomposition cost. There are several Hadoop-based distributed tensor decomposition algorithms as well as MATLAB-based decomposition methods. However, the former require longer iteration time, and therefore their execution time cannot be compared with that of Spark-based algorithms, whereas the latter run on a single machine, thus limiting their ability to handle large data. The proposed algorithm can reduce re-decomposition cost when tensors are added to a given tensor by decomposing them based on existing decomposition results without re-decomposing the entire tensor. The proposed method can handle large tensors and is fast within the limited-memory framework of Apache Spark. Moreover, InParTen2 can handle static as well as incremental tensor decomposition."
Identification of Sarcasm in Textual Data: A Comparative Study,"Ever increasing penetration of the Internet in our lives has led to an enormous amount of multimedia content generation on the internet. Textual data contributes a major share towards data generated on the world wide web. Understanding people's sentiment is an important aspect of natural language processing, but this opinion can be biased and incorrect, if people use sarcasm while commenting, posting status updates or reviewing any product or a movie. Thus, it is of utmost importance to detect sarcasm correctly and make a correct prediction about the people's intentions. This study tries to evaluate various machine learning models along with standard and hybrid deep learning models across various standardized datasets. We have performed vectorization of text using word embedding techniques. This has been done to convert the textual data into vectors for analytical purposes. We have used three standardized datasets available in public domain and used three word embeddings i.e Word2Vec, GloVe and fastText to validate the hypothesis. The results were analyzed and conclusions are drawn. The key finding is: The hybrid models that include Bidirectional LongTerm Short Memory (Bi-LSTM) and Convolutional Neural Network (CNN) outperform others conventional machine learning as well as deep learning models across all the datasets considered in this study, making our hypothesis valid. Using the data from different sources and customizing the models according to each dataset, slightly decreases the usability of the technique. But, overall this methodology provides effective measures to identify the presence of sarcasm with a minimum average accuracy of 80% or above for one dataset and better than the current baseline results for the other datasets. The results provide solid insights for the system developers to integrate this model into real-time analysis of any review or comment posted in the public domain. This study has various other practical implications for businesses that depend on user ratings and public opinions. This study also provides a launching platform for various researchers to work on the problem of sarcasm identification in textual data. This is a first of its kind study, to provide us the difference between conventional and the hybrid methods of prediction of sarcasm in textual data. The study also provides possible indicators that hybrid models are better when applied to textual data for analysis of sarcasm. Â© 2019 2019 Pulkit Mehndiratta, Devpriya Soni, published by Sciendo.","Ever increasing penetration of the Internet in our lives has led to an enormous amount of multimedia content generation on the internet. Textual data contributes a major share towards data generated on the world wide web. Understanding people's sentiment is an important aspect of natural language processing, but this opinion can be biased and incorrect, if people use sarcasm while commenting, posting status updates or reviewing any product or a movie. Thus, it is of utmost importance to detect sarcasm correctly and make a correct prediction about the people's intentions. This study tries to evaluate various machine learning models along with standard and hybrid deep learning models across various standardized datasets. We have performed vectorization of text using word embedding techniques. This has been done to convert the textual data into vectors for analytical purposes. We have used three standardized datasets available in public domain and used three word embeddings e Word2Vec, GloVe and fastText to validate the hypothesis. The results were analyzed and conclusions are drawn. The key finding is: The hybrid models that include Bidirectional LongTerm Short Memory (Bi-LSTM) and Convolutional Neural Network (CNN) outperform others conventional machine learning as well as deep learning models across all the datasets considered in this study, making our hypothesis valid. Using the data from different sources and customizing the models according to each dataset, slightly decreases the usability of the technique. But, overall this methodology provides effective measures to identify the presence of sarcasm with a minimum average accuracy of 80% or above for one dataset and better than the current baseline results for the other datasets. The results provide solid insights for the system developers to integrate this model into real-time analysis of any review or comment posted in the public domain. This study has various other practical implications for businesses that depend on user ratings and public opinions. This study also provides a launching platform for various researchers to work on the problem of sarcasm identification in textual data. This is a first of its kind study, to provide us the difference between conventional and the hybrid methods of prediction of sarcasm in textual data. The study also provides possible indicators that hybrid models are better when applied to textual data for analysis of sarcasm."
Twitter users' privacy concerns: What do their accounts' first names tell us?,"Purpose: In this paper, we describe how gender recognition on Twitter can be used as an intelligent business tool to determine the privacy concerns among users, and ultimately offer a more personalized service for customers who are more likely to respond positively to targeted advertisements. Design/methodology/approach: We worked with two different data sets to examine whether Twitter users' gender, inferred from the first name of the account and the profile description, correlates with the privacy setting of the account. We also used a set of features including the inferred gender of Twitter users to develop classifiers that predict user privacy settings. Findings: We found that the inferred gender of Twitter users correlates with the account's privacy setting. Specifically, females tend to be more privacy concerned than males. Users whose gender cannot be inferred from their provided first names tend to be more privacy concerned. In addition, our classification performance suggests that inferred gender can be used as an indicator of the user's privacy preference. Research limitations: It is known that not all twitter accounts are real user accounts, and social bots tweet as well. A major limitation of our study is the lack of consideration of social bots in the data. In our study, this implies that at least some percentage of the undefined accounts, that is, accounts that had names non-existent in the name dictionary, are social bots. It will be interesting to explore the privacy setting of social bots in the Twitter space. Practical implications: Companies are investing large amounts of money in business intelligence tools that allow them to know the preferences of their consumers. Due to the large number of consumers around the world, it is very difficult for companies to have direct communication with each customer to anticipate market changes. For this reason, the social network Twitter has gained relevance as one ideal tool for information extraction. On the other hand, users' privacy preference needs to be considered when companies consider leveraging their publicly available data. This paper suggests that gender recognition of Twitter users, based on Twitter users' provided first names and their profile descriptions, can be used to infer the users' privacy preference. Originality/value: This study explored a new way of inferring Twitter user's gender, that is, to recognize the user's gender based on the provided first name and the user's profile description. The potential of this information for predicting the user's privacy preference is explored. Â© 2018 Sciendo. All rights reserved.","In this paper, we describe how gender recognition on Twitter can be used as an intelligent business tool to determine the privacy concerns among users, and ultimately offer a more personalized service for customers who are more likely to respond positively to targeted advertisements. We worked with two different data sets to examine whether Twitter users' gender, inferred from the first name of the account and the profile description, correlates with the privacy setting of the account. We also used a set of features including the inferred gender of Twitter users to develop classifiers that predict user privacy settings. We found that the inferred gender of Twitter users correlates with the account's privacy setting. Specifically, females tend to be more privacy concerned than males. Users whose gender cannot be inferred from their provided first names tend to be more privacy concerned. In addition, our classification performance suggests that inferred gender can be used as an indicator of the user's privacy preference. It is known that not all twitter accounts are real user accounts, and social bots tweet as well. A major limitation of our study is the lack of consideration of social bots in the data. In our study, this implies that at least some percentage of the undefined accounts, that is, accounts that had names non-existent in the name dictionary, are social bots. It will be interesting to explore the privacy setting of social bots in the Twitter space. Companies are investing large amounts of money in business intelligence tools that allow them to know the preferences of their consumers. Due to the large number of consumers around the world, it is very difficult for companies to have direct communication with each customer to anticipate market changes. For this reason, the social network Twitter has gained relevance as one ideal tool for information extraction. On the other hand, users' privacy preference needs to be considered when companies consider leveraging their publicly available data. This paper suggests that gender recognition of Twitter users, based on Twitter users' provided first names and their profile descriptions, can be used to infer the users' privacy preference. This study explored a new way of inferring Twitter user's gender, that is, to recognize the user's gender based on the provided first name and the user's profile description. The potential of this information for predicting the user's privacy preference is explored."
"Sentence, Phrase, and Triple Annotations to Build a Knowledge Graph of Natural Language Processing Contributions - A Trial Dataset","This work aims to normalize the NlpContributions scheme (henceforward, NlpContributionGraph) to structure, directly from article sentences, the contributions information in Natural Language Processing (NLP) scholarly articles via a two-stage annotation methodology: 1) pilot stage - to define the scheme (described in prior work); and 2) adjudication stage - to normalize the graphing model (the focus of this paper). We re-annotate, a second time, the contributions-pertinent information across 50 prior-annotated NLP scholarly articles in terms of a data pipeline comprising: contribution-centered sentences, phrases, and triple statements. To this end, specifically, care was taken in the adjudication annotation stage to reduce annotation noise while formulating the guidelines for our proposed novel NLP contributions structuring and graphing scheme. The application of NlpContributionGraph on the 50 articles resulted finally in a dataset of 900 contribution-focused sentences, 4,702 contribution-information-centered phrases, and 2,980 surface-structured triples. The intra-annotation agreement between the first and second stages, in terms of F1-score, was 67.92% for sentences, 41.82% for phrases, and 22.31% for triple statements indicating that with increased granularity of the information, the annotation decision variance is greater. NlpContributionGraph has limited scope for structuring scholarly contributions compared with STEM (Science, Technology, Engineering, and Medicine) scholarly knowledge at large. Further, the annotation scheme in this work is designed by only an intra-annotator consensus - a single annotator first annotated the data to propose the initial scheme, following which, the same annotator reannotated the data to normalize the annotations in an adjudication stage. However, the expected goal of this work is to achieve a standardized retrospective model of capturing NLP contributions from scholarly articles. This would entail a larger initiative of enlisting multiple annotators to accommodate different worldviews into a ""single""set of structures and relationships as the final scheme. Given that the initial scheme is first proposed and the complexity of the annotation task in the realistic timeframe, our intra-annotation procedure is well-suited. Nevertheless, the model proposed in this work is presently limited since it does not incorporate multiple annotator worldviews. This is planned as future work to produce a robust model. We demonstrate NlpContributionGraph data integrated into the Open Research Knowledge Graph (ORKG), a next-generation KG-based digital library with intelligent computations enabled over structured scholarly knowledge, as a viable aid to assist researchers in their day-to-day tasks. NlpContributionGraph is a novel scheme to annotate research contributions from NLP articles and integrate them in a knowledge graph, which to the best of our knowledge does not exist in the community. Furthermore, our quantitative evaluations over the two-stage annotation tasks offer insights into task difficulty.  Â© 2021 2021 Jennifer D'Souza et al., published by Sciendo.","This work aims to normalize the NlpContributions scheme (henceforward, NlpContributionGraph) to structure, directly from article sentences, the contributions information in Natural Language Processing (NLP) scholarly articles via a two-stage annotation methodology: 1) pilot stage - to define the scheme (described in prior work); and 2) adjudication stage - to normalize the graphing model (the focus of this paper). We re-annotate, a second time, the contributions-pertinent information across 50 prior-annotated NLP scholarly articles in terms of a data pipeline comprising: contribution-centered sentences, phrases, and triple statements. To this end, specifically, care was taken in the adjudication annotation stage to reduce annotation noise while formulating the guidelines for our proposed novel NLP contributions structuring and graphing scheme. The application of NlpContributionGraph on the 50 articles resulted finally in a dataset of 900 contribution-focused sentences, 4,702 contribution-information-centered phrases, and 2,980 surface-structured triples. The intra-annotation agreement between the first and second stages, in terms of F1-score, was 67.92% for sentences, 41.82% for phrases, and 22.31% for triple statements indicating that with increased granularity of the information, the annotation decision variance is greater. NlpContributionGraph has limited scope for structuring scholarly contributions compared with STEM (Science, Technology, Engineering, and Medicine) scholarly knowledge at large. Further, the annotation scheme in this work is designed by only an intra-annotator consensus - a single annotator first annotated the data to propose the initial scheme, following which, the same annotator reannotated the data to normalize the annotations in an adjudication stage. However, the expected goal of this work is to achieve a standardized retrospective model of capturing NLP contributions from scholarly articles. This would entail a larger initiative of enlisting multiple annotators to accommodate different worldviews into a ""single""set of structures and relationships as the final scheme. Given that the initial scheme is first proposed and the complexity of the annotation task in the realistic timeframe, our intra-annotation procedure is well-suited. Nevertheless, the model proposed in this work is presently limited since it does not incorporate multiple annotator worldviews. This is planned as future work to produce a robust model. We demonstrate NlpContributionGraph data integrated into the Open Research Knowledge Graph (ORKG), a next-generation KG-based digital library with intelligent computations enabled over structured scholarly knowledge, as a viable aid to assist researchers in their day-to-day tasks. NlpContributionGraph is a novel scheme to annotate research contributions from NLP articles and integrate them in a knowledge graph, which to the best of our knowledge does not exist in the community. Furthermore, our quantitative evaluations over the two-stage annotation tasks offer insights into task difficulty."
A Novel Method for Resolving and Completing Authors' Country Affiliation Data in Bibliographic Records,"Our work seeks to overcome data quality issues related to incomplete author affiliation data in bibliographic records in order to support accurate and reliable measurement of international research collaboration (IRC). We propose, implement, and evaluate a method that leverages the Web-based knowledge graph Wikidata to resolve publication affiliation data to particular countries. The method is tested with general and domain-specific data sets. Our evaluation covers the magnitude of improvement, accuracy, and consistency. Results suggest the method is beneficial, reliable, and consistent, and thus a viable and improved approach to measuring IRC. Though our evaluation suggests the method works with both general and domain-specific bibliographic data sets, it may perform differently with data sets not tested here. Further limitations stem from the use of the R programming language and R libraries for country identification as well as imbalanced data coverage and quality in Wikidata that may also change over time. The new method helps to increase the accuracy in IRC studies and provides a basis for further development into a general tool that enriches bibliographic data using the Wikidata knowledge graph. This is the first attempt to enrich bibliographic data using a peer-produced, Web-based knowledge graph like Wikidata.  Â© 2020 2020 Ba Xuan Nguyen et al., published by Sciendo.","Our work seeks to overcome data quality issues related to incomplete author affiliation data in bibliographic records in order to support accurate and reliable measurement of international research collaboration (IRC). We propose, implement, and evaluate a method that leverages the Web-based knowledge graph Wikidata to resolve publication affiliation data to particular countries. The method is tested with general and domain-specific data sets. Our evaluation covers the magnitude of improvement, accuracy, and consistency. Results suggest the method is beneficial, reliable, and consistent, and thus a viable and improved approach to measuring IRC. Though our evaluation suggests the method works with both general and domain-specific bibliographic data sets, it may perform differently with data sets not tested here. Further limitations stem from the use of the R programming language and R libraries for country identification as well as imbalanced data coverage and quality in Wikidata that may also change over time. The new method helps to increase the accuracy in IRC studies and provides a basis for further development into a general tool that enriches bibliographic data using the Wikidata knowledge graph. This is the first attempt to enrich bibliographic data using a peer-produced, Web-based knowledge graph like Wikidata."
Classification of paper values based on citation rank and PageRank,"Purpose: The number of citations has been widely used to measure the significance of a paper. However, there is a need in introducing another index to determine superiority or inferiority of papers with the same number of citations. We determine superiority or inferiority of papers by using the ranking based on the number of citations and PageRank. Design/methodology/approach: We show the positive linear correlation between Citation Rank (the ranking of the number of citation) and PageRank. On this basis, we identify highquality, prestige, emerging, and popular papers. Findings: We found that the high-quality papers belong to the subjects of biochemistry and molecular biology, chemistry, and multidisciplinary sciences. The prestige papers correspond to the subjects of computer science, engineering, and information science. The emerging papers are related to biochemistry and molecular biology, as well as those published in the journal ""Cell."" The popular papers belong to the subject of multidisciplinary sciences. Research limitations: We analyze the Science Citation Index Expanded (SCIE) from 1981 to 2015 to calculate Citation Rank and PageRank within a citation network consisting of 34,666,719 papers and 591,321,826 citations. Practical implications: Our method is applicable to forecast emerging fields of research subjects in science and helps policymakers to consider science policy. Originality/value: We calculated PageRank for a giant citation network which is extremely larger than the citation networks investigated by previous researchers. Â© 2020 Sciendo. All rights reserved.","The number of citations has been widely used to measure the significance of a paper. However, there is a need in introducing another index to determine superiority or inferiority of papers with the same number of citations. We determine superiority or inferiority of papers by using the ranking based on the number of citations and PageRank. We show the positive linear correlation between Citation Rank (the ranking of the number of citation) and PageRank. On this basis, we identify highquality, prestige, emerging, and popular papers. We found that the high-quality papers belong to the subjects of biochemistry and molecular biology, chemistry, and multidisciplinary sciences. The prestige papers correspond to the subjects of computer science, engineering, and information science. The emerging papers are related to biochemistry and molecular biology, as well as those published in the journal ""Cell."" The popular papers belong to the subject of multidisciplinary sciences. We analyze the Science Citation Index Expanded (SCIE) from 1981 to 2015 to calculate Citation Rank and PageRank within a citation network consisting of 34,666,719 papers and 591,321,826 citations. Our method is applicable to forecast emerging fields of research subjects in science and helps policymakers to consider science policy. We calculated PageRank for a giant citation network which is extremely larger than the citation networks investigated by previous researchers."
Library and Information Science Papers Discussed on Twitter: A new Network-based Approach for Measuring Public Attention,"In recent years, one can witness a trend in research evaluation to measure the impact on society or attention to research by society (beyond science). We address the following question: Can Twitter be meaningfully used for the mapping of public and scientific discourses? Recently, Haunschild et al. (2019) introduced a new network-oriented approach for using Twitter data in research evaluation. Such a procedure can be used to measure the public discussion around a specific field or topic. In this study, we used all papers published in the Web of Science (WoS, Clarivate Analytics) subject category Information Science & Library Science to explore the publicly discussed topics from the area of library and information science (LIS) in comparison to the topics used by scholars in their publications in this area. The results show that LIS papers are represented rather well on Twitter. Similar topics appear in the networks of author keywords of all LIS papers, not tweeted LIS papers, and tweeted LIS papers. The networks of the author keywords of all LIS papers and not tweeted LIS papers are most similar to each other. Only papers published since 2011 with DOI were analyzed. Although Twitter data do not seem to be useful for quantitative research evaluation, it seems that Twitter data can be used in a more qualitative way for mapping of public and scientific discourses. This study explores a rather new methodology for comparing public and scientific discourses.  Â© 2020 2020 Robin Haunschild et al., published by Sciendo.","In recent years, one can witness a trend in research evaluation to measure the impact on society or attention to research by society (beyond science). We address the following question: Can Twitter be meaningfully used for the mapping of public and scientific discourses? Recently, Haunschild et al. introduced a new network-oriented approach for using Twitter data in research evaluation. Such a procedure can be used to measure the public discussion around a specific field or topic. In this study, we used all papers published in the Web of Science (WoS, Clarivate Analytics) subject category Information Science & Library Science to explore the publicly discussed topics from the area of library and information science (LIS) in comparison to the topics used by scholars in their publications in this area. The results show that LIS papers are represented rather well on Twitter. Similar topics appear in the networks of author keywords of all LIS papers, not tweeted LIS papers, and tweeted LIS papers. The networks of the author keywords of all LIS papers and not tweeted LIS papers are most similar to each other. Only papers published since 2011 with DOI were analyzed. Although Twitter data do not seem to be useful for quantitative research evaluation, it seems that Twitter data can be used in a more qualitative way for mapping of public and scientific discourses. This study explores a rather new methodology for comparing public and scientific discourses."
Knowledge Organization and Representation under the AI Lens,"This paper compares the paradigmatic differences between knowledge organization (KO) in library and information science and knowledge representation (KR) in AI to show the convergence in KO and KR methods and applications. The literature review and comparative analysis of KO and KR paradigms is the primary method used in this paper. A key difference between KO and KR lays in the purpose of KO is to organize knowledge into certain structure for standardizing and/or normalizing the vocabulary of concepts and relations, while KR is problem-solving oriented. Differences between KO and KR are discussed based on the goal, methods, and functions. This is only a preliminary research with a case study as proof of concept. The paper articulates on the opportunities in applying KR and other AI methods and techniques to enhance the functions of KO. Ontologies and linked data as the evidence of the convergence of KO and KR paradigms provide theoretical and methodological support to innovate KO in the AI era. Â© 2020 Jian Qin, published by Sciendo.","This paper compares the paradigmatic differences between knowledge organization (KO) in library and information science and knowledge representation (KR) in AI to show the convergence in KO and KR methods and applications. The literature review and comparative analysis of KO and KR paradigms is the primary method used in this paper. A key difference between KO and KR lays in the purpose of KO is to organize knowledge into certain structure for standardizing and/or normalizing the vocabulary of concepts and relations, while KR is problem-solving oriented. Differences between KO and KR are discussed based on the goal, methods, and functions. This is only a preliminary research with a case study as proof of concept. The paper articulates on the opportunities in applying KR and other AI methods and techniques to enhance the functions of KO. Ontologies and linked data as the evidence of the convergence of KO and KR paradigms provide theoretical and methodological support to innovate KO in the AI era."
Node2vec Representation for Clustering Journals and as A Possible Measure of Diversity,"Purpose: To investigate the effectiveness of using node2vec on journal citation networks to represent journals as vectors for tasks such as clustering, science mapping, and journal diversity measure. Design/methodology/approach: Node2vec is used in a journal citation network to generate journal vector representations. Findings: 1. Journals are clustered based on the node2vec trained vectors to form a science map. 2. The norm of the vector can be seen as an indicator of the diversity of journals. 3. Using node2vec trained journal vectors to determine the Rao-Stirling diversity measure leads to a better measure of diversity than that of direct citation vectors. Research limitations: All analyses use citation data and only focus on the journal level. Practical implications: Node2vec trained journal vectors embed rich information about journals, can be used to form a science map and may generate better values of journal diversity measures. Originality/value: The effectiveness of node2vec in scientometric analysis is tested. Possible indicators for journal diversity measure are presented. Â© 2019 Zhesi Shen, Fuyou Chen, Liying Yang, Jinshan Wu, published by Sciendo.","To investigate the effectiveness of using node2vec on journal citation networks to represent journals as vectors for tasks such as clustering, science mapping, and journal diversity measure. Node2vec is used in a journal citation network to generate journal vector representations. 1. Journals are clustered based on the node2vec trained vectors to form a science map. 2. The norm of the vector can be seen as an indicator of the diversity of journals. 3. Using node2vec trained journal vectors to determine the Rao-Stirling diversity measure leads to a better measure of diversity than that of direct citation vectors. All analyses use citation data and only focus on the journal level. Node2vec trained journal vectors embed rich information about journals, can be used to form a science map and may generate better values of journal diversity measures. The effectiveness of node2vec in scientometric analysis is tested. Possible indicators for journal diversity measure are presented."
Global Collaboration in Artificial Intelligence: Bibliometrics and Network Analysis from 1985 to 2019,"This study aims to explore the trend and status of international collaboration in the field of artificial intelligence (AI) and to understand the hot topics, core groups, and major collaboration patterns in global AI research. We selected 38,224 papers in the field of AI from 1985 to 2019 in the core collection database of Web of Science (WoS) and studied international collaboration from the perspectives of authors, institutions, and countries through bibliometric analysis and social network analysis. The bibliometric results show that in the field of AI, the number of published papers is increasing every year, and 84.8% of them are cooperative papers. Collaboration with more than three authors, collaboration between two countries and collaboration within institutions are the three main levels of collaboration patterns. Through social network analysis, this study found that the US, the UK, France, and Spain led global collaboration research in the field of AI at the country level, while Vietnam, Saudi Arabia, and United Arab Emirates had a high degree of international participation. Collaboration at the institution level reflects obvious regional and economic characteristics. There are the Developing Countries Institution Collaboration Group led by Iran, China, and Vietnam, as well as the Developed Countries Institution Collaboration Group led by the US, Canada, the UK. Also, the Chinese Academy of Sciences (China) plays an important, pivotal role in connecting the these institutional collaboration groups. First, participant contributions in international collaboration may have varied, but in our research they are viewed equally when building collaboration networks. Second, although the edge weight in the collaboration network is considered, it is only used to help reduce the network and does not reflect the strength of collaboration. The findings fill the current shortage of research on international collaboration in AI. They will help inform scientists and policy makers about the future of AI research. This work is the longest to date regarding international collaboration in the field of AI. This research explores the evolution, future trends, and major collaboration patterns of international collaboration in the field of AI over the past 35 years. It also reveals the leading countries, core groups, and characteristics of collaboration in the field of AI.  Â© 2020 2020 Haotian Hu et al., published by Sciendo.","This study aims to explore the trend and status of international collaboration in the field of artificial intelligence (AI) and to understand the hot topics, core groups, and major collaboration patterns in global AI research. We selected 38,224 papers in the field of AI from 1985 to 2019 in the core collection database of Web of Science (WoS) and studied international collaboration from the perspectives of authors, institutions, and countries through bibliometric analysis and social network analysis. The bibliometric results show that in the field of AI, the number of published papers is increasing every year, and 84.8% of them are cooperative papers. Collaboration with more than three authors, collaboration between two countries and collaboration within institutions are the three main levels of collaboration patterns. Through social network analysis, this study found that the US, the UK, France, and Spain led global collaboration research in the field of AI at the country level, while Vietnam, Saudi Arabia, and United Arab Emirates had a high degree of international participation. Collaboration at the institution level reflects obvious regional and economic characteristics. There are the Developing Countries Institution Collaboration Group led by Iran, China, and Vietnam, as well as the Developed Countries Institution Collaboration Group led by the US, Canada, the UK. Also, the Chinese Academy of Sciences (China) plays an important, pivotal role in connecting the these institutional collaboration groups. First, participant contributions in international collaboration may have varied, but in our research they are viewed equally when building collaboration networks. Second, although the edge weight in the collaboration network is considered, it is only used to help reduce the network and does not reflect the strength of collaboration. The findings fill the current shortage of research on international collaboration in AI. They will help inform scientists and policy makers about the future of AI research. This work is the longest to date regarding international collaboration in the field of AI. This research explores the evolution, future trends, and major collaboration patterns of international collaboration in the field of AI over the past 35 years. It also reveals the leading countries, core groups, and characteristics of collaboration in the field of AI."
A rebalancing framework for classification of imbalanced medical appointment no-show data,"Purpose: This paper aims to improve the classification performance when the data is imbalanced by applying different sampling techniques available in Machine Learning. Design/methodology/approach: The medical appointment no-show dataset is imbalanced, and when classification algorithms are applied directly to the dataset, it is biased towards the majority class, ignoring the minority class. To avoid this issue, multiple sampling techniques such as Random Over Sampling (ROS), Random Under Sampling (RUS), Synthetic Minority Oversampling TEchnique (SMOTE), ADAptive SYNthetic Sampling (ADASYN), Edited Nearest Neighbor (ENN), and Condensed Nearest Neighbor (CNN) are applied in order to make the dataset balanced. The performance is assessed by the Decision Tree classifier with the listed sampling techniques and the best performance is identified. Findings: This study focuses on the comparison of the performance metrics of various sampling methods widely used. It is revealed that, compared to other techniques, the Recall is high when ENN is applied CNN and ADASYN have performed equally well on the Imbalanced data. Research limitations: The testing was carried out with limited dataset and needs to be tested with a larger dataset. Practical implications: This framework will be useful whenever the data is imbalanced in real world scenarios, which ultimately improves the performance. Originality/value: This paper uses the rebalancing framework on medical appointment no-show dataset to predict the no-shows and removes the bias towards minority class. Â© 2021 Sciendo. All rights reserved.","This paper aims to improve the classification performance when the data is imbalanced by applying different sampling techniques available in Machine Learning. The medical appointment no-show dataset is imbalanced, and when classification algorithms are applied directly to the dataset, it is biased towards the majority class, ignoring the minority class. To avoid this issue, multiple sampling techniques such as Random Over Sampling (ROS), Random Under Sampling (RUS), Synthetic Minority Oversampling TEchnique (SMOTE), ADAptive SYNthetic Sampling (ADASYN), Edited Nearest Neighbor (ENN), and Condensed Nearest Neighbor (CNN) are applied in order to make the dataset balanced. The performance is assessed by the Decision Tree classifier with the listed sampling techniques and the best performance is identified. This study focuses on the comparison of the performance metrics of various sampling methods widely used. It is revealed that, compared to other techniques, the Recall is high when ENN is applied CNN and ADASYN have performed equally well on the Imbalanced data. The testing was carried out with limited dataset and needs to be tested with a larger dataset. This framework will be useful whenever the data is imbalanced in real world scenarios, which ultimately improves the performance. This paper uses the rebalancing framework on medical appointment no-show dataset to predict the no-shows and removes the bias towards minority class."
A New Citation Recommendation Strategy Based on Term Functions in Related Studies Section,"Researchers frequently encounter the following problems when writing scientific articles: (1) Selecting appropriate citations to support the research idea is challenging. (2) The literature review is not conducted extensively, which leads to working on a research problem that others have well addressed. The study focuses on citation recommendation in the related studies section by applying the term function of a citation context, potentially improving the efficiency of writing a literature review. We present nine term functions with three newly created and six identified from existing literature. Using these term functions as labels, we annotate 531 research papers in three topics to evaluate our proposed recommendation strategy. BM25 and Word2vec with VSM are implemented as the baseline models for the recommendation. Then the term function information is applied to enhance the performance. The experiments show that the term function-based methods outperform the baseline methods regarding the recall, precision, and F1-score measurement, demonstrating that term functions are useful in identifying valuable citations. The dataset is insufficient due to the complexity of annotating citation functions for paragraphs in the related studies section. More recent deep learning models should be performed to future validate the proposed approach. The citation recommendation strategy can be helpful for valuable citation discovery, semantic scientific retrieval, and automatic literature review generation. The proposed citation function-based citation recommendation can generate intuitive explanations of the results for users, improving the transparency, persuasiveness, and effectiveness of recommender systems.  Â© 2021 2021 Haihua Chen, published by Sciendo.","Researchers frequently encounter the following problems when writing scientific articles: Selecting appropriate citations to support the research idea is challenging. The literature review is not conducted extensively, which leads to working on a research problem that others have well addressed. The study focuses on citation recommendation in the related studies section by applying the term function of a citation context, potentially improving the efficiency of writing a literature review. We present nine term functions with three newly created and six identified from existing literature. Using these term functions as labels, we annotate 531 research papers in three topics to evaluate our proposed recommendation strategy. BM25 and Word2vec with VSM are implemented as the baseline models for the recommendation. Then the term function information is applied to enhance the performance. The experiments show that the term function-based methods outperform the baseline methods regarding the recall, precision, and F1-score measurement, demonstrating that term functions are useful in identifying valuable citations. The dataset is insufficient due to the complexity of annotating citation functions for paragraphs in the related studies section. More recent deep learning models should be performed to future validate the proposed approach. The citation recommendation strategy can be helpful for valuable citation discovery, semantic scientific retrieval, and automatic literature review generation. The proposed citation function-based citation recommendation can generate intuitive explanations of the results for users, improving the transparency, persuasiveness, and effectiveness of recommender systems."
The effects of b2c interaction on customer loyalty,"Purpose: This research attempts to examine the relationship between B2C interaction and customer loyalty in Business-to-Customer (B2C) context from a new perspective of the interactive tool. Design/methodology/approach: The scale for B2C interactive tools is of seven dimensions: efficiency, security, fulfillment, mobility, community, cultivation, and customization. A model reflecting the influences of these attributes on customer loyalty is developed and empirically examined based on data collected from 265 B2C customers. Findings: Results reveal that the fulfillment, mobility, community, and customization of B2C interactive tools can enhance customer loyalty directly and significantly. Efficiency and security, serving as the premise for possible purchase behavior, facilitate fulfillment. In addition, cultivation promotes the formation of customization, which directly strengthens customer loyalty. Research limitations: M odels considering individual-level indicators and combined with classic loyalty mechanisms in B2C context may lead to a deeper understanding of the tested effects of interaction on customer loyalty. Practical implications: To strengthen B2C interaction and further cultivate loyal customers, making interactive tools more fundamental, flexible, and personalized is critical for B2C enterprises. Originality/value: T his study proposes a new perspective from interactive tools when measuring the relationship between B2C interaction and customer loyalty, and offers a useful theoretical lens and reasonable explanations for investigating customer loyalty in B2C e-commerce context. Â© 2018 Sciendo. All rights reserved.","This research attempts to examine the relationship between B2C interaction and customer loyalty in Business-to-Customer (B2C) context from a new perspective of the interactive tool. The scale for B2C interactive tools is of seven dimensions: efficiency, security, fulfillment, mobility, community, cultivation, and customization. A model reflecting the influences of these attributes on customer loyalty is developed and empirically examined based on data collected from 265 B2C customers. Results reveal that the fulfillment, mobility, community, and customization of B2C interactive tools can enhance customer loyalty directly and significantly. Efficiency and security, serving as the premise for possible purchase behavior, facilitate fulfillment. In addition, cultivation promotes the formation of customization, which directly strengthens customer loyalty. M odels considering individual-level indicators and combined with classic loyalty mechanisms in B2C context may lead to a deeper understanding of the tested effects of interaction on customer loyalty. To strengthen B2C interaction and further cultivate loyal customers, making interactive tools more fundamental, flexible, and personalized is critical for B2C enterprises. T his study proposes a new perspective from interactive tools when measuring the relationship between B2C interaction and customer loyalty, and offers a useful theoretical lens and reasonable explanations for investigating customer loyalty in B2C e-commerce context."
Erratum: The gender patenting gap: A study on the iberoamerican countries (Journal of Data and Information Science (2020) 5:4 (147-150) DOI: 10.2478/jdis-2020-0025),"Upon verification of the article and discussion with other researchers, we became concerned about the reproducibility of our results as we did not include information on a baseline date for the data collection. In this corrigendum, we seek to amend the article, adding this information and adjusting the data presented in the tables and figures to be accurate to this baseline. Those are minor changes, but they impact on the ability of a researcher to reproduce our results without consulting us. We apologize for any previous inconsistencies and hope this improves the correctness and completeness of this work. Â© 2020 Ãlle Must, published by Sciendo.","Upon verification of the article and discussion with other researchers, we became concerned about the reproducibility of our results as we did not include information on a baseline date for the data collection. In this corrigendum, we seek to amend the article, adding this information and adjusting the data presented in the tables and figures to be accurate to this baseline. Those are minor changes, but they impact on the ability of a researcher to reproduce our results without consulting us. We apologize for any previous inconsistencies and hope this improves the correctness and completeness of this work."
Sentiment analysis of Japanese tourism online reviews,"Online reviews on tourism attractions provide important references for potential tourists to choose tourism spots. The main goal of this study is conducting sentiment analysis to facilitate users comprehending the large scale of the reviews, based on the comments about Chinese attractions from Japanese tourism website 4Travel. Different statistics- A nd rule-based methods are used to analyze the sentiment of the reviews. Three groups of novel statistics-based methods combining feature selection functions and the traditional term frequency-inverse document frequency (TF-IDF) method are proposed. We also make seven groups of different rules-based methods. The macro-average and micro-average values for the best classification results of the methods are calculated respectively and the performance of the methods are shown. We compare the statistics-based and rule-based methods separately and compare the overall performance of the two method. According to the results, it is concluded that the combination of feature selection functions and weightings can strongly improve the overall performance. The emotional vocabulary in the field of tourism (EVT), kaomojis, negative and transitional words can notably improve the performance in all of three categories. The rule-based methods outperform the statistics-based ones with a narrow advantage. Two limitations can be addressed: 1) the empirical studies to verify the validity of the proposed methods are only conducted on Japanese languages; and 2) the deep learning technology is not been incorporated in the methods. The results help to elucidate the intrinsic characteristics of the Japanese language and the influence on sentiment analysis. These findings also provide practical usage guidelines within the field of sentiment analysis of Japanese online tourism reviews. Our research is of practicability. Currently, there are no studies that focus on the sentiment analysis of Japanese reviews about Chinese attractions. Â© 2019 Chuanming Yu, Xingyu Zhu, Bolin Feng, Lin Cai, Lu An, published by Sciendo.","Online reviews on tourism attractions provide important references for potential tourists to choose tourism spots. The main goal of this study is conducting sentiment analysis to facilitate users comprehending the large scale of the reviews, based on the comments about Chinese attractions from Japanese tourism website 4Travel. Different statistics- A nd rule-based methods are used to analyze the sentiment of the reviews. Three groups of novel statistics-based methods combining feature selection functions and the traditional term frequency-inverse document frequency (TF-IDF) method are proposed. We also make seven groups of different rules-based methods. The macro-average and micro-average values for the best classification results of the methods are calculated respectively and the performance of the methods are shown. We compare the statistics-based and rule-based methods separately and compare the overall performance of the two method. According to the results, it is concluded that the combination of feature selection functions and weightings can strongly improve the overall performance. The emotional vocabulary in the field of tourism (EVT), kaomojis, negative and transitional words can notably improve the performance in all of three categories. The rule-based methods outperform the statistics-based ones with a narrow advantage. Two limitations can be addressed: 1) the empirical studies to verify the validity of the proposed methods are only conducted on Japanese languages; and 2) the deep learning technology is not been incorporated in the methods. The results help to elucidate the intrinsic characteristics of the Japanese language and the influence on sentiment analysis. These findings also provide practical usage guidelines within the field of sentiment analysis of Japanese online tourism reviews. Our research is of practicability. Currently, there are no studies that focus on the sentiment analysis of Japanese reviews about Chinese attractions."
Measuring and visualizing research collaboration and productivity,"Purpose: This paper presents findings of a quasi-experimental assessment to gauge the research productivity and degree of interdisciplinarity of research center outputs. Of special interest, we share an enriched visualization of research co-authoring patterns. Design/methodology/approach: We compile publications by 45 researchers in each of 1) the iUTAH project, which we consider here to be analogous to a âresearch center,â 2) CG1- a comparison group of participants in two other Utah environmental research centers, and 3) CG2-a comparison group of Utah university environmental researchers not associated with a research center. We draw bibliometric data from Web of Science and from Google Scholar. We gather publications for a period before iUTAH had been established (2010-2012) and a period after (2014-2016). We compare these research outputs in terms of publications and citations thereto. We also measure interdisciplinarity using Integration scoring and generate science overlay maps to locate the research publications across disciplines. Findings: We find that participation in the iUTAH project appears to increase research outputs (publications in the After period) and increase research citation rates relative to the comparison group researchers (although CG1 research remains most cited, as it was in the Before period). Most notably, participation in iUTAH markedly increases co-authoring among researchers-in general; and for junior, as well as senior, faculty; for men and women: across organizations; and across disciplines. Research limitations: The quasi-experimental design necessarily generates suggestive, not definitively causal, findings because of the imperfect controls. Practical implications: This study demonstrates a viable approach for research assessment of a center or program for which random assignment of control groups is not possible. It illustrates use of bibliometric indicators to inform R&D program management. Originality/value: New visualizations of researcher collaboration provide compelling comparisons of the extent and nature of social networking among target cohorts. Â© 2018 Sciendo. All rights reserved.","This paper presents findings of a quasi-experimental assessment to gauge the research productivity and degree of interdisciplinarity of research center outputs. Of special interest, we share an enriched visualization of research co-authoring patterns. We compile publications by 45 researchers in each of 1) the iUTAH project, which we consider here to be analogous to a research center, 2) CG1- a comparison group of participants in two other Utah environmental research centers, and 3) CG2-a comparison group of Utah university environmental researchers not associated with a research center. We draw bibliometric data from Web of Science and from Google Scholar. We gather publications for a period before iUTAH had been established (2010-2012) and a period after (2014-2016). We compare these research outputs in terms of publications and citations thereto. We also measure interdisciplinarity using Integration scoring and generate science overlay maps to locate the research publications across disciplines. We find that participation in the iUTAH project appears to increase research outputs (publications in the After period) and increase research citation rates relative to the comparison group researchers (although CG1 research remains most cited, as it was in the Before period). Most notably, participation in iUTAH markedly increases co-authoring among researchers-in general; and for junior, as well as senior, faculty; for men and women: across organizations; and across disciplines. The quasi-experimental design necessarily generates suggestive, not definitively causal, findings because of the imperfect controls. This study demonstrates a viable approach for research assessment of a center or program for which random assignment of control groups is not possible. It illustrates use of bibliometric indicators to inform R&D program management. New visualizations of researcher collaboration provide compelling comparisons of the extent and nature of social networking among target cohorts."
A Causal Configuration Analysis of Payment Decision Drivers in Paid Q&A,"This paper examines factors of payment decision as well as the role each factor plays in casual configurations leading to high payment intention under systematic and heuristic information processing routes. Based on heuristic-systematic model (HSM), we propose a configurational analytic framework to investigate complex casual relationships between influencing factors and payment decision. In line with this approach, we use fuzzy-set qualitative comparative analysis (fsQCA) to analyze data crawled from Zhihu.com. The number of previous consultations is a necessary element in all five equivalent configurations which lead to high intention in payment decision. The heuristic processing route plays a core role while the systematic processing route plays a peripheral role in payment decision-making process. Research is limited in that moderating effect of professional fields has not been considered in the framework. Configurations in results can assist managers of knowledge communities and paid Q&A service providers in the management of information elements to motivate more payment decision. This paper is one of the few studies to apply HSM theory and fsQCA method with respect to the payment decision in paid Q&A.  Â© 2021 2021 Wenyu Chen et al., published by Sciendo.","This paper examines factors of payment decision as well as the role each factor plays in casual configurations leading to high payment intention under systematic and heuristic information processing routes. Based on heuristic-systematic model (HSM), we propose a configurational analytic framework to investigate complex casual relationships between influencing factors and payment decision. In line with this approach, we use fuzzy-set qualitative comparative analysis (fsQCA) to analyze data crawled from Zhihu.com. The number of previous consultations is a necessary element in all five equivalent configurations which lead to high intention in payment decision. The heuristic processing route plays a core role while the systematic processing route plays a peripheral role in payment decision-making process. Research is limited in that moderating effect of professional fields has not been considered in the framework. Configurations in results can assist managers of knowledge communities and paid Q&A service providers in the management of information elements to motivate more payment decision. This paper is one of the few studies to apply HSM theory and fsQCA method with respect to the payment decision in paid Q&"
Citationas: A tool of automatic survey generation based on citation content,"Purpose: This study aims to build an automatic survey generation tool, named CitationAS, based on citation content as represented by the set of citing sentences in the original articles. Design/methodology/approach: Firstly, we apply LDA to analyse topic distribution of citation content. Secondly, in CitationAS, we use bisecting K-means, Lingo and STC to cluster retrieved citation content. Then Word2Vec, WordNet and combination of them are applied to generate cluster labels. Next, we employ TF-IDF, MMR, as well as considering sentence location information, to extract important sentences, which are used to generate surveys. Finally, we adopt manual evaluation for the generated surveys. Findings: In experiments, we choose 20 high-frequency phrases as search terms. Results show that Lingo-Word2Vec, STC-WordNet and bisecting K-means-Word2Vec have better clustering effects. In 5 points evaluation system, survey quality scores obtained by designing methods are close to 3, indicating surveys are within acceptable limits. When considering sentence location information, survey quality will be improved. Combination of Lingo, Word2Vec, TF-IDF or MMR can acquire higher survey quality. Research limitations: The manual evaluation method may have a certain subjectivity. We use a simple linear function to combine Word2Vec and WordNet that may not bring out their strengths. The generated surveys may not contain some newly created knowledge of some articles which may concentrate on sentences with no citing. Practical implications: CitationAS tool can automatically generate a comprehensive, detailed and accurate survey according to user's search terms. It can also help researchers learn about research status in a certain field. Originality/value: CitaitonAS tool is of practicability. It merges cluster labels from semantic level to improve clustering results. The tool also considers sentence location information when calculating sentence score by TF-IDF and MMR. Â© 2018 Sciendo. All rights reserved.","This study aims to build an automatic survey generation tool, named CitationAS, based on citation content as represented by the set of citing sentences in the original articles. Firstly, we apply LDA to analyse topic distribution of citation content. Secondly, in CitationAS, we use bisecting K-means, Lingo and STC to cluster retrieved citation content. Then Word2Vec, WordNet and combination of them are applied to generate cluster labels. Next, we employ TF-IDF, MMR, as well as considering sentence location information, to extract important sentences, which are used to generate surveys. Finally, we adopt manual evaluation for the generated surveys. In experiments, we choose 20 high-frequency phrases as search terms. Results show that Lingo-Word2Vec, STC-WordNet and bisecting K-means-Word2Vec have better clustering effects. In 5 points evaluation system, survey quality scores obtained by designing methods are close to 3, indicating surveys are within acceptable limits. When considering sentence location information, survey quality will be improved. Combination of Lingo, Word2Vec, TF-IDF or MMR can acquire higher survey quality. The manual evaluation method may have a certain subjectivity. We use a simple linear function to combine Word2Vec and WordNet that may not bring out their strengths. The generated surveys may not contain some newly created knowledge of some articles which may concentrate on sentences with no citing. CitationAS tool can automatically generate a comprehensive, detailed and accurate survey according to user's search terms. It can also help researchers learn about research status in a certain field. CitaitonAS tool is of practicability. It merges cluster labels from semantic level to improve clustering results. The tool also considers sentence location information when calculating sentence score by TF-IDF and MMR."
Current Status and Enhancement of Collaborative Research in the World: A Case Study of Osaka University,"The purpose of this research is to provide evidence for decision-makers to realize the potentials of collaborations between countries/regions via the scientometric analysis of co-authoring in academic publications. The approach is that Osaka University, which has set a strategy to become a global campus, is positioned to have a leading role to enhance such collaborations. This research measures co-authoring relations between Osaka University and other countries/regions to identify networks for fostering strong research collaborations. Five countries are identified as candidates for the future global campuses of Osaka University based on three factors, co-authoring relations, GDP growth, and population growth. The main limitation of this study is not being able to use the relations by the former positions of authors in Osaka University, because the data retrieved is limited by the query of the organization name at the first step. The significance of this work is to provide evidence for the university strategy to expand abroad based on the quantity and visualization of trends. With wider practical implementations, the approach of this research is useful in making a strategic roadmap for scientific organizations that intend to collaborate internationally.  Â© 2020 Shino Iwami et al., published by Sciendo.","The purpose of this research is to provide evidence for decision-makers to realize the potentials of collaborations between countries/regions via the scientometric analysis of co-authoring in academic publications. The approach is that Osaka University, which has set a strategy to become a global campus, is positioned to have a leading role to enhance such collaborations. This research measures co-authoring relations between Osaka University and other countries/regions to identify networks for fostering strong research collaborations. Five countries are identified as candidates for the future global campuses of Osaka University based on three factors, co-authoring relations, GDP growth, and population growth. The main limitation of this study is not being able to use the relations by the former positions of authors in Osaka University, because the data retrieved is limited by the query of the organization name at the first step. The significance of this work is to provide evidence for the university strategy to expand abroad based on the quantity and visualization of trends. With wider practical implementations, the approach of this research is useful in making a strategic roadmap for scientific organizations that intend to collaborate internationally."
Effective Opinion Spam Detection: A Study on Review Metadata Versus Content,"This paper aims to analyze the effectiveness of two major types of features-metadata-based (behavioral) and content-based (textual)-in opinion spam detection. Based on spam-detection perspectives, our approach works in three settings: Review-centric (spam detection), reviewer-centric (spammer detection) and product-centric (spam-targeted product detection). Besides this, to negate any kind of classifier-bias, we employ four classifiers to get a better and unbiased reflection of the obtained results. In addition, we have proposed a new set of features which are compared against some well-known related works. The experiments performed on two real-world datasets show the effectiveness of different features in opinion spam detection. Our findings indicate that behavioral features are more efficient as well as effective than the textual to detect opinion spam across all three settings. In addition, models trained on hybrid features produce results quite similar to those trained on behavioral features than on the textual, further establishing the superiority of behavioral features as dominating indicators of opinion spam. The features used in this work provide improvement over existing features utilized in other related works. Furthermore, the computation time analysis for feature extraction phase shows the better cost efficiency of behavioral features over the textual. The analyses conducted in this paper are solely limited to two well-known datasets, viz., YelpZip and YelpNYC of Yelp.com. The results obtained in this paper can be used to improve the detection of opinion spam, wherein the researchers may work on improving and developing feature engineering and selection techniques focused more on metadata information. To the best of our knowledge, this study is the first of its kind which considers three perspectives (review, reviewer and product-centric) and four classifiers to analyze the effectiveness of opinion spam detection using two major types of features. This study also introduces some novel features, which help to improve the performance of opinion spam detection methods.  Â© 2020 2020 Ajay Rastogi et al., published by Sciendo.","This paper aims to analyze the effectiveness of two major types of features-metadata-based (behavioral) and content-based (textual)-in opinion spam detection. Based on spam-detection perspectives, our approach works in three settings: Review-centric (spam detection), reviewer-centric (spammer detection) and product-centric (spam-targeted product detection). Besides this, to negate any kind of classifier-bias, we employ four classifiers to get a better and unbiased reflection of the obtained results. In addition, we have proposed a new set of features which are compared against some well-known related works. The experiments performed on two real-world datasets show the effectiveness of different features in opinion spam detection. Our findings indicate that behavioral features are more efficient as well as effective than the textual to detect opinion spam across all three settings. In addition, models trained on hybrid features produce results quite similar to those trained on behavioral features than on the textual, further establishing the superiority of behavioral features as dominating indicators of opinion spam. The features used in this work provide improvement over existing features utilized in other related works. Furthermore, the computation time analysis for feature extraction phase shows the better cost efficiency of behavioral features over the textual. The analyses conducted in this paper are solely limited to two well-known datasets, viz., YelpZip and YelpNYC of Yelp.com. The results obtained in this paper can be used to improve the detection of opinion spam, wherein the researchers may work on improving and developing feature engineering and selection techniques focused more on metadata information. To the best of our knowledge, this study is the first of its kind which considers three perspectives (review, reviewer and product-centric) and four classifiers to analyze the effectiveness of opinion spam detection using two major types of features. This study also introduces some novel features, which help to improve the performance of opinion spam detection methods."
Acknowledgment of Libraries in the Journal Literature: An Exploratory Study,"This study examines acknowledgments to libraries in the journal literature, as well as the efficacy of using Web of Science (WoS) to locate general acknowledgment text. This mixed-methods approach quantifies and characterizes acknowledgments to libraries in the journal literature. Using WoS's Funding Text field, the acknowledgments for six peer universities were identified and then characterized. The efficacy of using WoS to locate library acknowledgments was assessed by comparing the WoS Funding Text search results to the actual acknowledgment text found in the articles. Acknowledgments to libraries were found in articles at all six peer universities, though the absolute and relative numbers were quite low (< 0.5%). Most of the library acknowledgments were for resources (collections, funding, etc.), and many were concentrated in natural history (e.g. zoology). Examination of Texas A&M University zoology articles found that 91.7% of the funding information came from ""acknowledgments""and not specifically a funding acknowledgment section. The WoS Funding Text search found 56% of the library acknowledgments compared to a search of the actual acknowledgment text in the articles. Limiting publications to journals, using a single truncated search term, and including only six research universities in the United States. This study examined library acknowledgments, but the same approach could be applied to searches of other keywords, institutions/organizations, individuals, etc. While not specifically designed to search general acknowledgments, WoS's Funding Text field can be used as an exploratory tool to search acknowledgments beyond funding. There are a few studies that have examined library acknowledgments in the scholarly literature, though to date none of those studies have examined the efficacy of using the WoS Funding Text field to locate those library acknowledgments within the journal literature.  Â© 2020 2020 David E. Hubbard et al., published by Sciendo.","This study examines acknowledgments to libraries in the journal literature, as well as the efficacy of using Web of Science (WoS) to locate general acknowledgment text. This mixed-methods approach quantifies and characterizes acknowledgments to libraries in the journal literature. Using WoS's Funding Text field, the acknowledgments for six peer universities were identified and then characterized. The efficacy of using WoS to locate library acknowledgments was assessed by comparing the WoS Funding Text search results to the actual acknowledgment text found in the articles. Acknowledgments to libraries were found in articles at all six peer universities, though the absolute and relative numbers were quite low (< 0.5%). Most of the library acknowledgments were for resources (collections, funding, etc.), and many were concentrated in natural history ( zoology). Examination of Texas A&M University zoology articles found that 91.7% of the funding information came from ""acknowledgments""and not specifically a funding acknowledgment section. The WoS Funding Text search found 56% of the library acknowledgments compared to a search of the actual acknowledgment text in the articles. Limiting publications to journals, using a single truncated search term, and including only six research universities in the United States. This study examined library acknowledgments, but the same approach could be applied to searches of other keywords, institutions/organizations, individuals, etc. While not specifically designed to search general acknowledgments, WoS's Funding Text field can be used as an exploratory tool to search acknowledgments beyond funding. There are a few studies that have examined library acknowledgments in the scholarly literature, though to date none of those studies have examined the efficacy of using the WoS Funding Text field to locate those library acknowledgments within the journal literature."
Minimum Representative Size in Comparing Research Performance of Universities: The Case of Medicine Faculties in Romania,"The main goal of this study is to provide reliable comparison of performance in higher education. In this respect, we use scientometric measures associated with faculties of medicine in the six health studies universities in Romania. The method to estimate the minimum necessary size, proposed in in Shen et al. (2017), is applied in this article. We collected data from the Scopus data-base for the academics of the departments of medicine within the six health studies universities in Romania during the 2009 to 2014. And two kind of statistic treatments based on that method are implemented, pair-wise comparison and one-to-the-rest comparison. All the results of these comparisons are shown. According to the results: We deem that Cluj and Tg. MureÅ have the superior and inferior performance respectively, since their reasonably small value of the minimum representative size, in either of the kinds of comparison, whichever indexes of citations, h-index, or g-index is used. we can not reliably distinguish differences among the rest of the faculties, since the quite large value of their minimum representative size. There is only six faculties of medicine in health studies universities in Romania are analyzed. Our methods of comparison play an important role in ranking data sets associated with different collective units, such as faculties, universities, institutions, based on some aggregate scores like mean and totality. We applied the minimum representative size to a new emprical context-that of the departments of medicine in the health studies universities in Romania. Â© 2018 Xiaoling Liu, Mihai Paunescu, Viorel Proteasa, Jinshan Wu, published by Sciendo.","The main goal of this study is to provide reliable comparison of performance in higher education. In this respect, we use scientometric measures associated with faculties of medicine in the six health studies universities in Romania. The method to estimate the minimum necessary size, proposed in in Shen et al. , is applied in this article. We collected data from the Scopus data-base for the academics of the departments of medicine within the six health studies universities in Romania during the 2009 to 2014. And two kind of statistic treatments based on that method are implemented, pair-wise comparison and one-to-the-rest comparison. All the results of these comparisons are shown. According to the results: We deem that Cluj and Tg. Mure have the superior and inferior performance respectively, since their reasonably small value of the minimum representative size, in either of the kinds of comparison, whichever indexes of citations, h-index, or g-index is used. we can not reliably distinguish differences among the rest of the faculties, since the quite large value of their minimum representative size. There is only six faculties of medicine in health studies universities in Romania are analyzed. Our methods of comparison play an important role in ranking data sets associated with different collective units, such as faculties, universities, institutions, based on some aggregate scores like mean and totality. We applied the minimum representative size to a new emprical context-that of the departments of medicine in the health studies universities in Romania."
"A Scientometric Study of Digital Literacy, ICT Literacy, Information Literacy, and Media Literacy","Digital literacy and related fields have received interests from scholars and practitioners for more than 20 years; nonetheless, academic communities need to systematically review how the fields have developed. This study aims to investigate the research trends of digital literacy and related concepts since the year of 2000, especially in education. The current study analyzes keywords, co-authorship, and cited publications in digital literacy through the scientometric method. The journal articles have been retrieved from the WoS (Web of Science) using four keywords: ""Digital literacy,""""ICT literacy,""""information literacy,""and ""media literacy.""Further, keywords, publications, and co-authorship are examined and further classified into clusters for more in-depth investigation. Digital literacy is a multidisciplinary field that widely embraces literacy, ICT, the Internet, computer skill proficiency, science, nursing, health, and language education. The participants, or study subjects, in digital literacy research range from primary students to professionals, and the co-authorship clusters are distinctive by countries in America and Europe. This paper analyzes one fixed chunk of a dataset obtained by searching for all four keywords at once. Further studies will retrieve the data from diverse disciplines and will trace the change of the leading research themes by time spans. To shed light on the findings, using customized digital literacy curriculums and technology is critical for learners at different ages to nurture digital literacy according to their learning aims. They need to cultivate their understanding of the social impact of exploiting technology and computational thinking. To increase the originality of digital literacy-related studies, researchers from different countries and cultures may collaborate to investigate a broader range of digital literacy environments. The present study reviews research trends in digital literacy and related areas by performing a scientometric study to analyze multidimensional aspects in the fields, including keywords, journal titles, co-authorship, and cited publications.  Â© 2021 2021 Hyejin Park et al., published by Sciendo.","Digital literacy and related fields have received interests from scholars and practitioners for more than 20 years; nonetheless, academic communities need to systematically review how the fields have developed. This study aims to investigate the research trends of digital literacy and related concepts since the year of 2000, especially in education. The current study analyzes keywords, co-authorship, and cited publications in digital literacy through the scientometric method. The journal articles have been retrieved from the WoS (Web of Science) using four"
A Tailor-made Data Quality Approach for Higher Educational Data,"This paper relates the definition of data quality procedures for knowledge organizations such as Higher Education Institutions. The main purpose is to present the flexible approach developed for monitoring the data quality of the European Tertiary Education Register (ETER) database, illustrating its functioning and highlighting the main challenges that still have to be faced in this domain. The proposed data quality methodology is based on two kinds of checks, one to assess the consistency of cross-sectional data and the other to evaluate the stability of multiannual data. This methodology has an operational and empirical orientation. This means that the proposed checks do not assume any theoretical distribution for the determination of the threshold parameters that identify potential outliers, inconsistencies, and errors in the data. We show that the proposed cross-sectional checks and multiannual checks are helpful to identify outliers, extreme observations and to detect ontological inconsistencies not described in the available meta-data. For this reason, they may be a useful complement to integrate the processing of the available information. The coverage of the study is limited to European Higher Education Institutions. The cross-sectional and multiannual checks are not yet completely integrated. The consideration of the quality of the available data and information is important to enhance data quality-aware empirical investigations, highlighting problems, and areas where to invest for improving the coverage and interoperability of data in future data collection initiatives. The data-driven quality checks proposed in this paper may be useful as a reference for building and monitoring the data quality of new databases or of existing databases available for other countries or systems characterized by high heterogeneity and complexity of the units of analysis without relying on pre-specified theoretical distributions.  Â© 2020 2020 Cinzia Daraio et al., published by Sciendo.","This paper relates the definition of data quality procedures for knowledge organizations such as Higher Education Institutions. The main purpose is to present the flexible approach developed for monitoring the data quality of the European Tertiary Education Register (ETER) database, illustrating its functioning and highlighting the main challenges that still have to be faced in this domain. The proposed data quality methodology is based on two kinds of checks, one to assess the consistency of cross-sectional data and the other to evaluate the stability of multiannual data. This methodology has an operational and empirical orientation. This means that the proposed checks do not assume any theoretical distribution for the determination of the threshold parameters that identify potential outliers, inconsistencies, and errors in the data. We show that the proposed cross-sectional checks and multiannual checks are helpful to identify outliers, extreme observations and to detect ontological inconsistencies not described in the available meta-data. For this reason, they may be a useful complement to integrate the processing of the available information. The coverage of the study is limited to European Higher Education Institutions. The cross-sectional and multiannual checks are not yet completely integrated. The consideration of the quality of the available data and information is important to enhance data quality-aware empirical investigations, highlighting problems, and areas where to invest for improving the coverage and interoperability of data in future data collection initiatives. The data-driven quality checks proposed in this paper may be useful as a reference for building and monitoring the data quality of new databases or of existing databases available for other countries or systems characterized by high heterogeneity and complexity of the units of analysis without relying on pre-specified theoretical distributions."
Topic Evolution and Emerging Topic Analysis Based on Open Source Software,"We present an analytical, open source and flexible natural language processing and text mining method for topic evolution, emerging topic detection and research trend forecasting for all kinds of data-tagged text. We make full use of the functions provided by the open source VOSviewer and Microsoft Office, including a thesaurus for data clean-up and a LOOKUP function for comparative analysis. Through application and verification in the domain of perovskite solar cells research, this method proves to be effective. A certain amount of manual data processing and a specific research domain background are required for better, more illustrative analysis results. Adequate time for analysis is also necessary. We try to set up an easy, useful, and flexible interdisciplinary text analyzing procedure for researchers, especially those without solid computer programming skills or who cannot easily access complex software. This procedure can also serve as a wonderful example for teaching information literacy. This text analysis approach has not been reported before.  Â© 2020 2020 Xiang Shen et al., published by Sciendo.","We present an analytical, open source and flexible natural language processing and text mining method for topic evolution, emerging topic detection and research trend forecasting for all kinds of data-tagged text. We make full use of the functions provided by the open source VOSviewer and Microsoft Office, including a thesaurus for data clean-up and a LOOKUP function for comparative analysis. Through application and verification in the domain of perovskite solar cells research, this method proves to be effective. A certain amount of manual data processing and a specific research domain background are required for better, more illustrative analysis results. Adequate time for analysis is also necessary. We try to set up an easy, useful, and flexible interdisciplinary text analyzing procedure for researchers, especially those without solid computer programming skills or who cannot easily access complex software. This procedure can also serve as a wonderful example for teaching information literacy. This text analysis approach has not been reported before."
Thematic trends in complementary and alternative medicine applied in cancer-related symptoms,"Purpose: The main goal of this study is to discover the scientific evolution of Cancer-Related Symptoms in Complementary and Alternative Medicine research area, analyzing the articles indexed in the Web of Science database from 1980 to 2013. Design/Methodology/Approach: A co-word science mapping analysis is performed under a longitudinal framework (1980 to 2013). The documental corpus is divided into two subperiods, 1980-2008 and 2009-2013. Thus, the performance and impact rates, and conceptual evolution of the research field are shown. Findings: According to the results, the co-word analysis allows us to identify 12 main thematic areas in this emerging research field: anxiety, survivors and palliative care, meditation, treatment, symptoms and cancer types, postmenopause, cancer pain, low back pain, herbal medicine, children, depression and insomnia, inflammation mediators, and lymphedema. The different research lines are identified according to the main thematic areas, centered fundamentally on anxiety and suffering prevention. The scientific community can use this information to identify where the interest is focused and make decisions in different ways. Research limitation: Several limitations can be addressed: 1) some of the Complementary and Alternative Medicine therapies may not have been included; 2) only the documents indexed in Web of Science are analyzed; and 3) the thematic areas detected could change if another dataset was considered. Practical implications: The results obtained in the present study could be considered as an evidence-based framework in which future studies could be built. Originality/value: Currently, there are no studies that show the thematic evolution of this research area. Â© 2018 Sciendo. All rights reserved.","The main goal of this study is to discover the scientific evolution of Cancer-Related Symptoms in Complementary and Alternative Medicine research area, analyzing the articles indexed in the Web of Science database from 1980 to 2013. A co-word science mapping analysis is performed under a longitudinal framework (1980 to 2013). The documental corpus is divided into two subperiods, 1980-2008 and 2009-2013. Thus, the performance and impact rates, and conceptual evolution of the research field are shown. According to the results, the co-word analysis allows us to identify 12 main thematic areas in this emerging research field: anxiety, survivors and palliative care, meditation, treatment, symptoms and cancer types, postmenopause, cancer pain, low back pain, herbal medicine, children, depression and insomnia, inflammation mediators, and lymphedema. The different research lines are identified according to the main thematic areas, centered fundamentally on anxiety and suffering prevention. The scientific community can use this information to identify where the interest is focused and make decisions in different ways. Research limitation: Several limitations can be addressed: 1) some of the Complementary and Alternative Medicine therapies may not have been included; 2) only the documents indexed in Web of Science are analyzed; and 3) the thematic areas detected could change if another dataset was considered. The results obtained in the present study could be considered as an evidence-based framework in which future studies could be built. Currently, there are no studies that show the thematic evolution of this research area."
Bilateral Co-authorship Indicators Based on Fractional Counting,"In this contribution we provide two new co-authorship indicators based on fractional counting. Based on the idea of fractional counting we reflect on what should be an acceptable indicator for co-authorship between two entities. From this reflection we propose an indicator, the co-authorship score, denoted as cs, using the harmonic mean. Dividing this new indicator by the classical co-authorship indicator based on full counting, leads to a co-authorship intensity indicator. We show that the indicators we propose have many necessary or at least highly desirable properties for a proper cs-score. It is pointed out that the two new indicators can be used for countries, but also for institutions and other pairs of entities. A small example shows the feasibility of the co-authorship score and the co-authorship intensity indicator. The indicators are not yet tested in real cases. As the notions of co-authorship and collaboration have many aspects, we think that our contribution may help policy management to take yet another aspect into account as part of a multi-faceted description of research outcomes. The indicators we propose cover yet another aspect of co-authorship.  Â© 2021 2021 Ronald Rousseau et al., published by Sciendo.","In this contribution we provide two new co-authorship indicators based on fractional counting. Based on the idea of fractional counting we reflect on what should be an acceptable indicator for co-authorship between two entities. From this reflection we propose an indicator, the co-authorship score, denoted as cs, using the harmonic mean. Dividing this new indicator by the classical co-authorship indicator based on full counting, leads to a co-authorship intensity indicator. We show that the indicators we propose have many necessary or at least highly desirable properties for a proper cs-score. It is pointed out that the two new indicators can be used for countries, but also for institutions and other pairs of entities. A small example shows the feasibility of the co-authorship score and the co-authorship intensity indicator. The indicators are not yet tested in real cases. As the notions of co-authorship and collaboration have many aspects, we think that our contribution may help policy management to take yet another aspect into account as part of a multi-faceted description of research outcomes. The indicators we propose cover yet another aspect of co-authorship."
"A Micro Perspective of Research Dynamics through ""Citations of Citations"" Topic Analysis","Research dynamics have long been a research interest. It is a macro perspective tool for discovering temporal research trends of a certain discipline or subject. A micro perspective of research dynamics, however, concerning a single researcher or a highly cited paper in terms of their citations and ""citations of citations""(forward chaining) remains unexplored. In this paper, we use a cross-collection topic model to reveal the research dynamics of topic disappearance topic inheritance, and topic innovation in each generation of forward chaining. For highly cited work, scientific influence exists in indirect citations. Topic modeling can reveal how long this influence exists in forward chaining, as well as its influence. This paper measures scientific influence and indirect scientific influence only if the relevant words or phrases are borrowed or used in direct or indirect citations. Paraphrasing or semantically similar concept may be neglected in this research. This paper demonstrates that a scientific influence exists in indirect citations through its analysis of forward chaining. This can serve as an inspiration on how to adequately evaluate research influence. The main contributions of this paper are the following three aspects. First, besides research dynamics of topic inheritance and topic innovation, we model topic disappearance by using a cross-collection topic model. Second, we explore the length and character of the research impact through ""citations of citations""content analysis. Finally, we analyze the research dynamics of artificial intelligence researcher Geoffrey Hinton's publications and the topic dynamics of forward chaining.  Â© 2020 2020 Xiaoli Chen et al., published by Sciendo.","Research dynamics have long been a research interest. It is a macro perspective tool for discovering temporal research trends of a certain discipline or subject. A micro perspective of research dynamics, however, concerning a single researcher or a highly cited paper in terms of their citations and ""citations of citations""(forward chaining) remains unexplored. In this paper, we use a cross-collection topic model to reveal the research dynamics of topic disappearance topic inheritance, and topic innovation in each generation of forward chaining. For highly cited work, scientific influence exists in indirect citations. Topic modeling can reveal how long this influence exists in forward chaining, as well as its influence. This paper measures scientific influence and indirect scientific influence only if the relevant words or phrases are borrowed or used in direct or indirect citations. Paraphrasing or semantically similar concept may be neglected in this research. This paper demonstrates that a scientific influence exists in indirect citations through its analysis of forward chaining. This can serve as an inspiration on how to adequately evaluate research influence. The main contributions of this paper are the following three aspects. First, besides research dynamics of topic inheritance and topic innovation, we model topic disappearance by using a cross-collection topic model. Second, we explore the length and character of the research impact through ""citations of citations""content analysis. Finally, we analyze the research dynamics of artificial intelligence researcher Geoffrey Hinton's publications and the topic dynamics of forward chaining."
Topic Sentiment Analysis in Online Learning Community from College Students,"Opinion mining and sentiment analysis in Online Learning Community can truly reflect the students' learning situation, which provides the necessary theoretical basis for following revision of teaching plans. To improve the accuracy of topic-sentiment analysis, a novel model for topic sentiment analysis is proposed that outperforms other state-of-art models. We aim at highlighting the identification and visualization of topic sentiment based on learning topic mining and sentiment clustering at various granularity-levels. The proposed method comprised data preprocessing, topic detection, sentiment analysis, and visualization. The proposed model can effectively perceive students' sentiment tendencies on different topics, which provides powerful practical reference for improving the quality of information services in teaching practice. The model obtains the topic-terminology hybrid matrix and the document-topic hybrid matrix by selecting the real user's comment information on the basis of LDA topic detection approach, without considering the intensity of students' sentiments and their evolutionary trends. The implication and association rules to visualize the negative sentiment in comments or reviews enable teachers and administrators to access a certain plaint, which can be utilized as a reference for enhancing the accuracy of learning content recommendation, and evaluating the quality of their services. The topic-sentiment analysis model can clarify the hierarchical dependencies between different topics, which lay the foundation for improving the accuracy of teaching content recommendation and optimizing the knowledge coherence of related courses.  Â© 2020 2020 Kai Wang et al., published by Sciendo.","Opinion mining and sentiment analysis in Online Learning Community can truly reflect the students' learning situation, which provides the necessary theoretical basis for following revision of teaching plans. To improve the accuracy of topic-sentiment analysis, a novel model for topic sentiment analysis is proposed that outperforms other state-of-art models. We aim at highlighting the identification and visualization of topic sentiment based on learning topic mining and sentiment clustering at various granularity-levels. The proposed method comprised data preprocessing, topic detection, sentiment analysis, and visualization. The proposed model can effectively perceive students' sentiment tendencies on different topics, which provides powerful practical reference for improving the quality of information services in teaching practice. The model obtains the topic-terminology hybrid matrix and the document-topic hybrid matrix by selecting the real user's comment information on the basis of LDA topic detection approach, without considering the intensity of students' sentiments and their evolutionary trends. The implication and association rules to visualize the negative sentiment in comments or reviews enable teachers and administrators to access a certain plaint, which can be utilized as a reference for enhancing the accuracy of learning content recommendation, and evaluating the quality of their services. The topic-sentiment analysis model can clarify the hierarchical dependencies between different topics, which lay the foundation for improving the accuracy of teaching content recommendation and optimizing the knowledge coherence of related courses."
"Identifying Scientific and Technical ""unicorns""","Using the metaphor of ""unicorn,""we identify the scientific papers and technical patents characterized by the informetric feature of very high citations in the first ten years after publishing, which may provide a new pattern to understand very high impact works in science and technology. When we set CT as the total citations of papers or patents in the first ten years after publication, with CTâ¥ 5,000 for scientific ""unicorn""and CTâ¥ 500 for technical ""unicorn,""we have an absolute standard for identifying scientific and technical ""unicorn""publications. We identify 165 scientific ""unicorns""in 14,301,875 WoS papers and 224 technical ""unicorns""in 13,728,950 DII patents during 2001-2012. About 50% of ""unicorns""belong to biomedicine, in which selected cases are individually discussed. The rare ""unicorns""increase following linear model, the fitting data show 95% confidence with the RMSE of scientific ""unicorn""is 0.2127 while the RMSE of technical ""unicorn""is 0.0923. A ""unicorn""is a pure quantitative consideration without concerning its quality, and ""potential unicorns""as CTâ¤5,000 for papers and CTâ¤500 for patents are left in future studies. Scientific and technical ""unicorns""provide a new pattern to understand high-impact works in science and technology. The ""unicorn""pattern supplies a concise approach to identify very high-impact scientific papers and technical patents. The ""unicorn""pattern supplies a concise approach to identify very high impact scientific papers and technical patents.  Â© 2021 2021 Lucy L. Xu et al., published by Sciendo.","Using the metaphor of ""unicorn,""we identify the scientific papers and technical patents characterized by the informetric feature of very high citations in the first ten years after publishing, which may provide a new pattern to understand very high impact works in science and technology. When we set CT as the total citations of papers or patents in the first ten years after publication, with CT 5,000 for scientific ""unicorn""and CT 500 for technical ""unicorn,""we have an absolute standard for identifying scientific and technical ""unicorn""publications. We identify 165 scientific ""unicorns""in 14,301,875 WoS papers and 224 technical ""unicorns""in 13,728,950 DII patents during 2001-2012. About 50% of ""unicorns""belong to biomedicine, in which selected cases are individually discussed. The rare ""unicorns""increase following linear model, the fitting data show 95% confidence with the RMSE of scientific ""unicorn""is 0.2127 while the RMSE of technical ""unicorn""is 0.0923. A ""unicorn""is a pure quantitative consideration without concerning its quality, and ""potential unicorns""as CT5,000 for papers and CT500 for patents are left in future studies. Scientific and technical ""unicorns""provide a new pattern to understand high-impact works in science and technology. The ""unicorn""pattern supplies a concise approach to identify very high-impact scientific papers and technical patents. The ""unicorn""pattern supplies a concise approach to identify very high impact scientific papers and technical patents."
Measuring Societal Impact Is as Complex as ABC,"Purpose: This paper describes an alternative way of assessing journals considering a broader perspective of its impact. The Area-based connectedness (ABC) to society of journals applied here contributes to the assessment of the dissemination task of journals but with more data it may also contribute to the assessment of other missions. The ABC approach assesses the performance of research actors, in this case journals, considering the characteristics of the research areas in which they are active. Each paper in a journal inherits the characteristics of its area. These areas are defined by a publication-based classification. The characteristics of areas relate to 5 dimensions of connectedness to society (news, policy, industrial R&D, technology and local interest) and are calculated by bibliometric indicators and social media metrics. In the paper, I illustrate the approach by showing the results for a few journals. They illustrate the diverse profiles that journals may have. We are able to provide a profile for each journal in the Web of Science database. The profiles we present show an appropriate view on the journals' societal connectedness. The classification I apply to perform the analyses is a CWTS in house classification based on Web of Science data. As such the application depends on the (updates of) that system. The classification is available at www.leidenranking.com The dimensions of connectedness discussed in this paper relate to the dissemination task of journals but further development of this method may provide more options to monitor the tasks/mission of journals. The ABC approach is a unique way to assess performance or impact of research actors considering the characteristics of the areas in which output is published and as such less prone to manipulation or gaming. Â© 2019Ed Noyons, published by Sciendo.","This paper describes an alternative way of assessing journals considering a broader perspective of its impact. The Area-based connectedness (ABC) to society of journals applied here contributes to the assessment of the dissemination task of journals but with more data it may also contribute to the assessment of other missions. The ABC approach assesses the performance of research actors, in this case journals, considering the characteristics of the research areas in which they are active. Each paper in a journal inherits the characteristics of its area. These areas are defined by a publication-based classification. The characteristics of areas relate to 5 dimensions of connectedness to society (news, policy, industrial R&D, technology and local interest) and are calculated by bibliometric indicators and social media metrics. In the paper, I illustrate the approach by showing the results for a few journals. They illustrate the diverse profiles that journals may have. We are able to provide a profile for each journal in the Web of Science database. The profiles we present show an appropriate view on the journals' societal connectedness. The classification I apply to perform the analyses is a CWTS in house classification based on Web of Science data. As such the application depends on the (updates of) that system. The classification is available at www.leidenranking.com The dimensions of connectedness discussed in this paper relate to the dissemination task of journals but further development of this method may provide more options to monitor the tasks/mission of journals. The ABC approach is a unique way to assess performance or impact of research actors considering the characteristics of the areas in which output is published and as such less prone to manipulation or gaming."
Automatic Classification of Swedish Metadata Using Dewey Decimal Classification: A Comparison of Approaches,"With more and more digital collections of various information resources becoming available, also increasing is the challenge of assigning subject index terms and classes from quality knowledge organization systems. While the ultimate purpose is to understand the value of automatically produced Dewey Decimal Classification (DDC) classes for Swedish digital collections, the paper aims to evaluate the performance of six machine learning algorithms as well as a string-matching algorithm based on characteristics of DDC. State-of-the-art machine learning algorithms require at least 1,000 training examples per class. The complete data set at the time of research involved 143,838 records which had to be reduced to top three hierarchical levels of DDC in order to provide sufficient training data (totaling 802 classes in the training and testing sample, out of 14,413 classes at all levels). Evaluation shows that Support Vector Machine with linear kernel outperforms other machine learning algorithms as well as the string-matching algorithm on average; the string-matching algorithm outperforms machine learning for specific classes when characteristics of DDC are most suitable for the task. Word embeddings combined with different types of neural networks (simple linear network, standard neural network, 1D convolutional neural network, and recurrent neural network) produced worse results than Support Vector Machine, but reach close results, with the benefit of a smaller representation size. Impact of features in machine learning shows that using keywords or combining titles and keywords gives better results than using only titles as input. Stemming only marginally improves the results. Removed stop-words reduced accuracy in most cases, while removing less frequent words increased it marginally. The greatest impact is produced by the number of training examples: 81.90% accuracy on the training set is achieved when at least 1,000 records per class are available in the training set, and 66.13% when too few records (often less than 100 per class) on which to train are available-and these hold only for top 3 hierarchical levels (803 instead of 14,413 classes). Having to reduce the number of hierarchical levels to top three levels of DDC because of the lack of training data for all classes, skews the results so that they work in experimental conditions but barely for end users in operational retrieval systems. In conclusion, for operative information retrieval systems applying purely automatic DDC does not work, either using machine learning (because of the lack of training data for the large number of DDC classes) or using string-matching algorithm (because DDC characteristics perform well for automatic classification only in a small number of classes). Over time, more training examples may become available, and DDC may be enriched with synonyms in order to enhance accuracy of automatic classification which may also benefit information retrieval performance based on DDC. In order for quality information services to reach the objective of highest possible precision and recall, automatic classification should never be implemented on its own; instead, machine-aided indexing that combines the efficiency of automatic suggestions with quality of human decisions at the final stage should be the way for the future. The study explored machine learning on a large classification system of over 14,000 classes which is used in operational information retrieval systems. Due to lack of sufficient training data across the entire set of classes, an approach complementing machine learning, that of string matching, was applied. This combination should be explored further since it provides the potential for real-life applications with large target classification systems. Â© 2020 Koraljka Golub et al., published by Sciendo.","With more and more digital collections of various information resources becoming available, also increasing is the challenge of assigning subject index terms and classes from quality knowledge organization systems. While the ultimate purpose is to understand the value of automatically produced Dewey Decimal Classification classes for Swedish digital collections, the paper aims to evaluate the performance of six machine learning algorithms as well as a string-matching algorithm based on characteristics of State-of-the-art machine learning algorithms require at least 1,000 training examples per class. The complete data set at the time of research involved 143,838 records which had to be reduced to top three hierarchical levels of DDC in order to provide sufficient training data (totaling 802 classes in the training and testing sample, out of 14,413 classes at all levels). Evaluation shows that Support Vector Machine with linear kernel outperforms other machine learning algorithms as well as the string-matching algorithm on average; the string-matching algorithm outperforms machine learning for specific classes when characteristics of DDC are most suitable for the task. Word embeddings combined with different types of neural networks (simple linear network, standard neural network, 1D convolutional neural network, and recurrent neural network) produced worse results than Support Vector Machine, but reach close results, with the benefit of a smaller representation size. Impact of features in machine learning shows that using keywords or combining titles and keywords gives better results than using only titles as input. Stemming only marginally improves the results. Removed stop-words reduced accuracy in most cases, while removing less frequent words increased it marginally. The greatest impact is produced by the number of training examples: 81.90% accuracy on the training set is achieved when at least 1,000 records per class are available in the training set, and 66.13% when too few records (often less than 100 per class) on which to train are available-and these hold only for top 3 hierarchical levels (803 instead of 14,413 classes). Having to reduce the number of hierarchical levels to top three levels of DDC because of the lack of training data for all classes, skews the results so that they work in experimental conditions but barely for end users in operational retrieval systems. In conclusion, for operative information retrieval systems applying purely automatic DDC does not work, either using machine learning (because of the lack of training data for the large number of DDC classes) or using string-matching algorithm (because DDC characteristics perform well for automatic classification only in a small number of classes). Over time, more training examples may become available, and DDC may be enriched with synonyms in order to enhance accuracy of automatic classification which may also benefit information retrieval performance based on In order for quality information services to reach the objective of highest possible precision and recall, automatic classification should never be implemented on its own; instead, machine-aided indexing that combines the efficiency of automatic suggestions with quality of human decisions at the final stage should be the way for the future. The study explored machine learning on a large classification system of over 14,000 classes which is used in operational information retrieval systems. Due to lack of sufficient training data across the entire set of classes, an approach complementing machine learning, that of string matching, was applied. This combination should be explored further since it provides the potential for real-life applications with large target classification systems."
"""sEMANTIC"" in a Digital Curation Model","This study attempts to propose an abstract model by gathering concepts that can focus on resource representation and description in a digital curation model and suggest a conceptual model that emphasizes semantic enrichment in a digital curation model. This study conducts a literature review to analyze the preceding curation models, DCC CLM, DCC&U, UC3, and DCN. The concept of semantic enrichment is expressed in a single word, SEMANTIC in this study. The Semantic Enrichment Model, SEMANTIC has elements, subject, extraction, multi-language, authority, network, thing, identity, and connect. This study does not reflect the actual information environment because it focuses on the concepts of the representation of digital objects. This study presents the main considerations for creating and reinforcing the description and representation of digital objects when building and developing digital curation models in specific institutions. This study summarizes the elements that should be emphasized in the representation of digital objects in terms of information organization. Â© 2020 2020 Hyewon Lee et al., published by Sciendo.","This study attempts to propose an abstract model by gathering concepts that can focus on resource representation and description in a digital curation model and suggest a conceptual model that emphasizes semantic enrichment in a digital curation model. This study conducts a literature review to analyze the preceding curation models, DCC CLM, DCC&U, UC3, and DCN. The concept of semantic enrichment is expressed in a single word, SEMANTIC in this study. The Semantic Enrichment Model, SEMANTIC has elements, subject, extraction, multi-language, authority, network, thing, identity, and connect. This study does not reflect the actual information environment because it focuses on the concepts of the representation of digital objects. This study presents the main considerations for creating and reinforcing the description and representation of digital objects when building and developing digital curation models in specific institutions. This study summarizes the elements that should be emphasized in the representation of digital objects in terms of information organization."
Embedding-based Detection and Extraction of Research Topics from Academic Documents Using Deep Clustering,"Detection of research fields or topics and understanding the dynamics help the scientific community in their decisions regarding the establishment of scientific fields. This also helps in having a better collaboration with governments and businesses. This study aims to investigate the development of research fields over time, translating it into a topic detection problem. To achieve the objectives, we propose a modified deep clustering method to detect research trends from the abstracts and titles of academic documents. Document embedding approaches are utilized to transform documents into vector-based representations. The proposed method is evaluated by comparing it with a combination of different embedding and clustering approaches and the classical topic modeling algorithms (i.e. LDA) against a benchmark dataset. A case study is also conducted exploring the evolution of Artificial Intelligence (AI) detecting the research topics or sub-fields in related AI publications. Evaluating the performance of the proposed method using clustering performance indicators reflects that our proposed method outperforms similar approaches against the benchmark dataset. Using the proposed method, we also show how the topics have evolved in the period of the recent 30 years, taking advantage of a keyword extraction method for cluster tagging and labeling, demonstrating the context of the topics. We noticed that it is not possible to generalize one solution for all downstream tasks. Hence, it is required to fine-tune or optimize the solutions for each task and even datasets. In addition, interpretation of cluster labels can be subjective and vary based on the readers' opinions. It is also very difficult to evaluate the labeling techniques, rendering the explanation of the clusters further limited. As demonstrated in the case study, we show that in a real-world example, how the proposed method would enable the researchers and reviewers of the academic research to detect, summarize, analyze, and visualize research topics from decades of academic documents. This helps the scientific community and all related organizations in fast and effective analysis of the fields, by establishing and explaining the topics. In this study, we introduce a modified and tuned deep embedding clustering coupled with Doc2Vec representations for topic extraction. We also use a concept extraction method as a labeling approach in this study. The effectiveness of the method has been evaluated in a case study of AI publications, where we analyze the AI topics during the past three decades. Â© 2021 2021 Sahand Vahidnia et al., published by Sciendo.","Detection of research fields or topics and understanding the dynamics help the scientific community in their decisions regarding the establishment of scientific fields. This also helps in having a better collaboration with governments and businesses. This study aims to investigate the development of research fields over time, translating it into a topic detection problem. To achieve the objectives, we propose a modified deep clustering method to detect research trends from the abstracts and titles of academic documents. Document embedding approaches are utilized to transform documents into vector-based representations. The proposed method is evaluated by comparing it with a combination of different embedding and clustering approaches and the classical topic modeling algorithms ( LDA) against a benchmark dataset. A case study is also conducted exploring the evolution of Artificial Intelligence (AI) detecting the research topics or sub-fields in related AI publications. Evaluating the performance of the proposed method using clustering performance indicators reflects that our proposed method outperforms similar approaches against the benchmark dataset. Using the proposed method, we also show how the topics have evolved in the period of the recent 30 years, taking advantage of a keyword extraction method for cluster tagging and labeling, demonstrating the context of the topics. We noticed that it is not possible to generalize one solution for all downstream tasks. Hence, it is required to fine-tune or optimize the solutions for each task and even datasets. In addition, interpretation of cluster labels can be subjective and vary based on the readers' opinions. It is also very difficult to evaluate the labeling techniques, rendering the explanation of the clusters further limited. As demonstrated in the case study, we show that in a real-world example, how the proposed method would enable the researchers and reviewers of the academic research to detect, summarize, analyze, and visualize research topics from decades of academic documents. This helps the scientific community and all related organizations in fast and effective analysis of the fields, by establishing and explaining the topics. In this study, we introduce a modified and tuned deep embedding clustering coupled with Doc2Vec representations for topic extraction. We also use a concept extraction method as a labeling approach in this study. The effectiveness of the method has been evaluated in a case study of AI publications, where we analyze the AI topics during the past three decades."
A study of methods to identify industry-university-research institution cooperation partners based on innovation Chain theory,"Purpose: This study aims at identifying potential industry-university-research collaboration (IURC) partners effectively and analyzes the conditions and dynamics in the IURC process based on innovation chain theory. Design/methodology/approach: The method utilizes multisource data, combining bibliometric and econometrics analyses to capture the core network of the existing collaboration networks and institution competitiveness in the innovation chain. Furthermore, a new identification method is constructed that takes into account the law of scientific research cooperation and economic factors. Findings: Empirical analysis of the genetic engineering vaccine field shows that through the distribution characteristics of creative technologies from different institutions, the analysis based on the innovation chain can identify the more complementary capacities among organizations. Research limitations: In this study, the overall approach is shaped by the theoretical concept of an innovation chain, a linear innovation model with specific types or stages of innovation activities in each phase of the chain, and may, thus, overlook important feedback mechanisms in the innovation process. Practical implications: Industry-university-research institution collaborations are extremely important in promoting the dissemination of innovative knowledge, enhancing the quality of innovation products, and facilitating the transformation of scientific achievements. Originality/value: Compared to previous studies, this study emulates the real conditions of IURC. Thus, the rule of technological innovation can be better revealed, the potential partners of IURC can be identified more readily, and the conclusion has more value. Â© 2018 Sciendo. All rights reserved.","This study aims at identifying potential industry-university-research collaboration (IURC) partners effectively and analyzes the conditions and dynamics in the IURC process based on innovation chain theory. The method utilizes multisource data, combining bibliometric and econometrics analyses to capture the core network of the existing collaboration networks and institution competitiveness in the innovation chain. Furthermore, a new identification method is constructed that takes into account the law of scientific research cooperation and economic factors. Empirical analysis of the genetic engineering vaccine field shows that through the distribution characteristics of creative technologies from different institutions, the analysis based on the innovation chain can identify the more complementary capacities among organizations. In this study, the overall approach is shaped by the theoretical concept of an innovation chain, a linear innovation model with specific types or stages of innovation activities in each phase of the chain, and may, thus, overlook important feedback mechanisms in the innovation process. Industry-university-research institution collaborations are extremely important in promoting the dissemination of innovative knowledge, enhancing the quality of innovation products, and facilitating the transformation of scientific achievements. Compared to previous studies, this study emulates the real conditions of IURC. Thus, the rule of technological innovation can be better revealed, the potential partners of IURC can be identified more readily, and the conclusion has more value."
"Male, Female, and Nonbinary Differences in UK Twitter Self-descriptions: A Fine-grained Systematic Exploration","Although gender identities influence how people present themselves on social media, previous studies have tested pre-specified dimensions of difference, potentially overlooking other differences and ignoring nonbinary users. Word association thematic analysis was used to systematically check for fine-grained statistically significant gender differences in Twitter profile descriptions between 409,487 UK-based female, male, and nonbinary users in 2020. A series of statistical tests systematically identified 1,474 differences at the individual word level, and a follow up thematic analysis grouped these words into themes. The results reflect offline variations in interests and in jobs. They also show differences in personal disclosures, as reflected by words, with females mentioning qualifications, relationships, pets, and illnesses much more, nonbinaries discussing sexuality more, and males declaring political and sports affiliations more. Other themes were internally imbalanced, including personal appearance (e.g. male: beardy; female: redhead), self-evaluations (e.g. male: legend; nonbinary: witch; female: feisty), and gender identity (e.g. male: dude; nonbinary: enby; female: queen). The methods are affected by linguistic styles and probably under-report nonbinary differences. The gender differences found may inform gender theory, and aid social web communicators and marketers. The results show a much wider range of gender expression differences than previously acknowledged for any social media site.  Â© 2021 2021 Mike Thelwall et al., published by Sciendo.","Although gender identities influence how people present themselves on social media, previous studies have tested pre-specified dimensions of difference, potentially overlooking other differences and ignoring nonbinary users. Word association thematic analysis was used to systematically check for fine-grained statistically significant gender differences in Twitter profile descriptions between 409,487 UK-based female, male, and nonbinary users in 2020. A series of statistical tests systematically identified 1,474 differences at the individual word level, and a follow up thematic analysis grouped these words into themes. The results reflect offline variations in interests and in jobs. They also show differences in personal disclosures, as reflected by words, with females mentioning qualifications, relationships, pets, and illnesses much more, nonbinaries discussing sexuality more, and males declaring political and sports affiliations more. Other themes were internally imbalanced, including personal appearance ( male: beardy; female: redhead), self-evaluations ( male: legend; nonbinary: witch; female: feisty), and gender identity ( male: dude; nonbinary: enby; female: queen). The methods are affected by linguistic styles and probably under-report nonbinary differences. The gender differences found may inform gender theory, and aid social web communicators and marketers. The results show a much wider range of gender expression differences than previously acknowledged for any social media site."
An Automatic Approach to Extending the Consumer Health Vocabulary,"Given the ubiquitous presence of the internet in our lives, many individuals turn to the web for medical information. A challenge here is that many laypersons (as ""consumers"") do not use professional terms found in the medical nomenclature when describing their conditions and searching the internet. The Consumer Health Vocabulary (CHV) ontology, initially developed in 2007, aimed to bridge this gap, although updates have been limited over the last decade. The purpose of this research is to implement a means of automatically creating a hierarchical consumer health vocabulary. This overall purpose is improving consumers' ability to search for medical conditions and symptoms with an enhanced CHV and improving the search capabilities of our searching and indexing tool HIVE (Helping Interdisciplinary Vocabulary Engineering). The research design uses ontological fusion, an approach for automatically extracting and integrating the Medical Subject Headings (MeSH) ontology into CHV, and further convert CHV from a flat mapping to a hierarchical ontology. The additional relationships and parent terms from MeSH allow us to uncover relationships between existing terms in the CHV ontology as well. The research design also included improving the search capabilities of HIVE identifying alternate relationships and consolidating them to a single entry. The key findings are an improved CHV with a hierarchical structure that enables consumers to search through the ontology and uncover more relationships. There are some cases where the improved search results in HIVE return terms that are related but not completely synonymous. We present an example and discuss the implications of this result. This research makes available an updated and richer CHV ontology using the HIVE tool. Consumers may use this tool to search consumer terminology for medical conditions and symptoms. The HIVE tool will return results about the medical term linked with the consumer term as well as the hierarchy of other medical terms connected to the term. This is a first attempt in over a decade to improve and enhance the CHV ontology with current terminology and the first research effort to convert CHV's original flat ontology structure to a hierarchical structure. This research also enhances the HIVE infrastructure and provides consumers with a simple, efficient mechanism for searching the CHV ontology and providing meaningful data to consumers.  Â© 2021 2021 Michal Monselise et al., published by Sciendo.","Given the ubiquitous presence of the internet in our lives, many individuals turn to the web for medical information. A challenge here is that many laypersons (as ""consumers"") do not use professional terms found in the medical nomenclature when describing their conditions and searching the internet. The Consumer Health Vocabulary (CHV) ontology, initially developed in 2007, aimed to bridge this gap, although updates have been limited over the last decade. The purpose of this research is to implement a means of automatically creating a hierarchical consumer health vocabulary. This overall purpose is improving consumers' ability to search for medical conditions and symptoms with an enhanced CHV and improving the search capabilities of our searching and indexing tool HIVE (Helping Interdisciplinary Vocabulary Engineering). The research design uses ontological fusion, an approach for automatically extracting and integrating the Medical Subject Headings (MeSH) ontology into CHV, and further convert CHV from a flat mapping to a hierarchical ontology. The additional relationships and parent terms from MeSH allow us to uncover relationships between existing terms in the CHV ontology as well. The research design also included improving the search capabilities of HIVE identifying alternate relationships and consolidating them to a single entry. The key findings are an improved CHV with a hierarchical structure that enables consumers to search through the ontology and uncover more relationships. There are some cases where the improved search results in HIVE return terms that are related but not completely synonymous. We present an example and discuss the implications of this result. This research makes available an updated and richer CHV ontology using the HIVE tool. Consumers may use this tool to search consumer terminology for medical conditions and symptoms. The HIVE tool will return results about the medical term linked with the consumer term as well as the hierarchy of other medical terms connected to the term. This is a first attempt in over a decade to improve and enhance the CHV ontology with current terminology and the first research effort to convert CHV's original flat ontology structure to a hierarchical structure. This research also enhances the HIVE infrastructure and provides consumers with a simple, efficient mechanism for searching the CHV ontology and providing meaningful data to consumers."
A Scientometric Approach to Analyze Scientific Development on Renewable Energy Sources,"This paper aims to point out the scientific development and research density of renewable energy sources such as photovoltaic, wind, and biomass, using a mix of computational tools. Based on this, it was possible to verify the existence of new research trends and opportunities in a macro view regarding management, performance evaluation, and decision-making in renewable energy generation systems and installations. A scientometric approach was used based on a research protocol to retrieve papers from the Scopus database, and through four scientometric questions, to analyze each area. Software such as the Science Mapping Analysis Software Tool (SciMAT) and Sci2 Tool were used to map the science development and density. The scientific development of renewable energy areas is highlighted, pointing out research opportunities regarding management, studies on costs and investments, systemic diagnosis, and performance evaluation for decision-making in businesses in these areas. This paper was limited to the articles indexed in the Scopus database and by the questions used to analyze the scientific development of renewable energy areas. The results show the need for a managerial perspective in businesses related to renewable energy sources at the managerial, technical, and operational levels, including performance evaluation, assertive decision making, and adequate use of technical and financial resources. This paper shows that there is a research field to be explored, with gaps to fill and further research to be carried out in this area. Besides, this paper can serve as a basis for other studies and research in other areas and domains.  Â© 2021 2021 Jones LuÃ­s Schaefer et al., published by Sciendo.","This paper aims to point out the scientific development and research density of renewable energy sources such as photovoltaic, wind, and biomass, using a mix of computational tools. Based on this, it was possible to verify the existence of new research trends and opportunities in a macro view regarding management, performance evaluation, and decision-making in renewable energy generation systems and installations. A scientometric approach was used based on a research protocol to retrieve papers from the Scopus database, and through four scientometric questions, to analyze each area. Software such as the Science Mapping Analysis Software Tool (SciMAT) and Sci2 Tool were used to map the science development and density. The scientific development of renewable energy areas is highlighted, pointing out research opportunities regarding management, studies on costs and investments, systemic diagnosis, and performance evaluation for decision-making in businesses in these areas. This paper was limited to the articles indexed in the Scopus database and by the questions used to analyze the scientific development of renewable energy areas. The results show the need for a managerial perspective in businesses related to renewable energy sources at the managerial, technical, and operational levels, including performance evaluation, assertive decision making, and adequate use of technical and financial resources. This paper shows that there is a research field to be explored, with gaps to fill and further research to be carried out in this area. Besides, this paper can serve as a basis for other studies and research in other areas and domains."
The ARQUIGRAFIA project: A Web Collaborative Environment for Architecture and Urban Heritage Image,"This paper presents the ARQUIGRAFIA project, an open, public and nonprofit, continuous growth web collaborative environment dedicated to Brazilian architectural photographic images. The ARQUIGRAFIA project promotes the active and collaborative participation among its institutional users (GLAMs, NGOs, laboratories and research groups) and private users (students, professionals, professors, researchers), both can create an account and share their digitized iconographic collections in the same Web environment by uploading their files, indexing, georeferencing and assigning a Creative Commons license. The development of users interactions by means of semantic differentials impressions recording on visible plastic-spatial aspects of the architectures in synthetic infographics, as well as by the retrieval of images through an advanced system search based on those impressions parameters. By gamification means, the system often invites users to review images' in order to improve images' data accuracy. The pilot project named Open Air Museum that allows users to add audio descriptions to images in situ. An interface for users' digital curatorship will be soon available. The ARQUIGRAFIA's multidisciplinary team gathering professors-researchers, graduate and undergraduate students from the Architecture and Urbanism, Design, Information Science, Computer Science faculties of the University of SÃ£o Paulo, demands continuous financial resources for grants, for contracting third party services, for the participation in scientific events in Brazil and abroad, and for equipment. Since 2016, significant budget cuts in the University of SÃ£o Paulo own research funds and in Brazilian federal scientific agencies can compromise the continuity of this project. The open source template called +GRAFIA that can freely help other areas of knowledge to build their own visual Web collaborative environments. The collaborative nature of the ARQUIGRAFIA distinguishes it from institutional image databases on the internet, precisely because it involves a heterogeneous network of collaborators. Â© 2020 VÃ¢nia Mara Alves Lima et al., published by Sciendo.","This paper presents the ARQUIGRAFIA project, an open, public and nonprofit, continuous growth web collaborative environment dedicated to Brazilian architectural photographic images. The ARQUIGRAFIA project promotes the active and collaborative participation among its institutional users (GLAMs, NGOs, laboratories and research groups) and private users (students, professionals, professors, researchers), both can create an account and share their digitized iconographic collections in the same Web environment by uploading their files, indexing, georeferencing and assigning a Creative Commons license. The development of users interactions by means of semantic differentials impressions recording on visible plastic-spatial aspects of the architectures in synthetic infographics, as well as by the retrieval of images through an advanced system search based on those impressions parameters. By gamification means, the system often invites users to review images' in order to improve images' data accuracy. The pilot project named Open Air Museum that allows users to add audio descriptions to images in situ. An interface for users' digital curatorship will be soon available. The ARQUIGRAFIA's multidisciplinary team gathering professors-researchers, graduate and undergraduate students from the Architecture and Urbanism, Design, Information Science, Computer Science faculties of the University of So Paulo, demands continuous financial resources for grants, for contracting third party services, for the participation in scientific events in Brazil and abroad, and for equipment. Since 2016, significant budget cuts in the University of So Paulo own research funds and in Brazilian federal scientific agencies can compromise the continuity of this project. The open source template called +GRAFIA that can freely help other areas of knowledge to build their own visual Web collaborative environments. The collaborative nature of the ARQUIGRAFIA distinguishes it from institutional image databases on the internet, precisely because it involves a heterogeneous network of collaborators."
The Second Edition of the Integrative Levels Classification: Evolution of a KOS,"This paper informs about the publication of the second edition of the Integrative Levels Classification (ILC2), a freely-faceted knowledge organization system (KOS), and reviews the main changes that have been introduced as compared to its first edition (ILC1). The most relevant changes are illustrated, with special reference to those of interest to general classification theory, by means of examples of notation for individual classes and combinations of them. Changes introduced in ILC2 include: the names and order of some main classes; the development of subclasses for various phenomena, especially quantities and algebraic structures; the order of facet categories and the new category of Disorder; notation for special facets; distinction of the semantical function of facets (attributes) from their syntactic function. The system can be freely accessed online through a PHP browser as well as in SKOS format. Only a selection of changed classes is discussed for space reasons. ILC1 has been previously applied to the BARTOC directory of KOSs. Update of BARTOC data to ILC2 and application of ILC2 to further information systems are envisaged. Possible methods for reclassifying BARTOC with ILC2 are discussed. ILC is a newly developed classification system, based on phenomena instead of traditional disciplines and featuring various innovative devices. This paper is an original account of its most recent evolution. Â© 2020 Ziyoung Park et al., published by Sciendo.","This paper informs about the publication of the second edition of the Integrative Levels Classification (ILC2), a freely-faceted knowledge organization system (KOS), and reviews the main changes that have been introduced as compared to its first edition (ILC1). The most relevant changes are illustrated, with special reference to those of interest to general classification theory, by means of examples of notation for individual classes and combinations of them. Changes introduced in ILC2 include: the names and order of some main classes; the development of subclasses for various phenomena, especially quantities and algebraic structures; the order of facet categories and the new category of Disorder; notation for special facets; distinction of the semantical function of facets (attributes) from their syntactic function. The system can be freely accessed online through a PHP browser as well as in SKOS format. Only a selection of changed classes is discussed for space reasons. ILC1 has been previously applied to the BARTOC directory of KOSs. Update of BARTOC data to ILC2 and application of ILC2 to further information systems are envisaged. Possible methods for reclassifying BARTOC with ILC2 are discussed. ILC is a newly developed classification system, based on phenomena instead of traditional disciplines and featuring various innovative devices. This paper is an original account of its most recent evolution."
Measuring scientific productivity in China using malmquist productivity index,"This paper aims to investigate the scientific productivity of China's science system. This paper employs the Malmquist productivity index (MPI) based on Data Envelopment Analysis (DEA). The results reveal that the overall efficiency of Chinese universities increased significantly from 2009 to 2016, which is mainly driven by technological progress. From the perspective of the functions of higher education, research and transfer activities perform better than the teaching activities. As an implication, the indicator selection mechanism, investigation period and the MPI model can be further extended in the future research. The results indicate that Chinese education administrative departments should take actions to guide and promote the teaching activities and formulate reasonable resource allocation regulations to reach the balanced development in Chinese universities. This paper selects 58 Chinese universities and conducts a quantified measurement during the period 2009-2016. Three main functional activities of universities (i.e. teaching, researching, and application) are innovatively categorized into different schemes, and we calculate their performance, respectively. Â© 2019 Yaoyao Song, Torben Schubert, Huihui Liu, Guoliang Yang, published by Sciendo.","This paper aims to investigate the scientific productivity of China's science system. This paper employs the Malmquist productivity index (MPI) based on Data Envelopment Analysis (DEA). The results reveal that the overall efficiency of Chinese universities increased significantly from 2009 to 2016, which is mainly driven by technological progress. From the perspective of the functions of higher education, research and transfer activities perform better than the teaching activities. As an implication, the indicator selection mechanism, investigation period and the MPI model can be further extended in the future research. The results indicate that Chinese education administrative departments should take actions to guide and promote the teaching activities and formulate reasonable resource allocation regulations to reach the balanced development in Chinese universities. This paper selects 58 Chinese universities and conducts a quantified measurement during the period 2009-2016. Three main functional activities of universities ( teaching, researching, and application) are innovatively categorized into different schemes, and we calculate their performance, respectively."
Overview of Trends in Global Single Cell Research Based on Bibliometric Analysis and LDA Model (2009-2019),"This article aims to describe the global research profile and the development trends of single cell research from the perspective of bibliometric analysis and semantic mining. The literatures on single cell research were extracted from Clarivate Analytic's Web of Science Core Collection between 2009 and 2019. Firstly, bibliometric analyses were performed with Thomson Data Analyzer (TDA). Secondly, topic identification and evolution trends of single cell research was conducted through the LDA topic model. Thirdly, taking the post-discretized method which is used for topic evolution analysis for reference, the topics were also be dispersed to countries to detect the spatial distribution. The publication of single cell research shows significantly increasing tendency in the last decade. The topics of single cell research field can be divided into three categories, which respectively refers to single cell research methods, mechanism of biological process, and clinical application of single cell technologies. The different trends of these categories indicate that technological innovation drives the development of applied research. The continuous and rapid growth of the topic strength in the field of cancer diagnosis and treatment indicates that this research topic has received extensive attention in recent years. The topic distributions of some countries are relatively balanced, while for the other countries, several topics show significant superiority. The analyzed data of this study only contain those were included in the Web of Science Core Collection. This study provides insights into the research progress regarding single cell field and identifies the most concerned topics which reflect potential opportunities and challenges. The national topic distribution analysis based on the post-discretized analysis method extends topic analysis from time dimension to space dimension. This paper combines bibliometric analysis and LDA model to analyze the evolution trends of single cell research field. The method of extending post-discretized analysis from time dimension to space dimension is distinctive and insightful.  Â© 2021 2021 Tian Jiang et al., published by Sciendo.","This article aims to describe the global research profile and the development trends of single cell research from the perspective of bibliometric analysis and semantic mining. The literatures on single cell research were extracted from Clarivate Analytic's Web of Science Core Collection between 2009 and 2019. Firstly, bibliometric analyses were performed with Thomson Data Analyzer (TDA). Secondly, topic identification and evolution trends of single cell research was conducted through the LDA topic model. Thirdly, taking the post-discretized method which is used for topic evolution analysis for reference, the topics were also be dispersed to countries to detect the spatial distribution. The publication of single cell research shows significantly increasing tendency in the last decade. The topics of single cell research field can be divided into three categories, which respectively refers to single cell research methods, mechanism of biological process, and clinical application of single cell technologies. The different trends of these categories indicate that technological innovation drives the development of applied research. The continuous and rapid growth of the topic strength in the field of cancer diagnosis and treatment indicates that this research topic has received extensive attention in recent years. The topic distributions of some countries are relatively balanced, while for the other countries, several topics show significant superiority. The analyzed data of this study only contain those were included in the Web of Science Core Collection. This study provides insights into the research progress regarding single cell field and identifies the most concerned topics which reflect potential opportunities and challenges. The national topic distribution analysis based on the post-discretized analysis method extends topic analysis from time dimension to space dimension. This paper combines bibliometric analysis and LDA model to analyze the evolution trends of single cell research field. The method of extending post-discretized analysis from time dimension to space dimension is distinctive and insightful."
New Indicators of the Technological Impact of Scientific Production,"Purpose: Building upon pioneering work by Francis Narin and others, a new methodological approach to assessing the technological impact of scientific research is presented. Design/methodology/approach: It is based on the analysis of citations made in patent families included in the PATSTAT database that is to scientific papers indexed in Scopus. Findings: An advanced citation matching procedure is applied to the data in order to construct two indicators of technological impact: on the citing (patent) side, the country/region in which protection is sought and a patent family's propensity to cite scientific papers are taken into account, and on the cited (paper) side, a relative citation rate is defined for patent citations to papers that is similar to the scientific paper-to-paper citation rate in classical bibliometrics. Research limitations: The results are limited by the available data, in our case Scopus and PATSTAT, and especially by the lack of standardization of references in patents. This required a matching procedure that is neither trivial nor exact. Practical implications: Results at the country/region, document type, and publication age levels are presented. The country/region-level results in particular reveal features that have remained hidden in analyses of straight counts. Especially notable is that the rankings of some Asian countries/regions move upwards when the proposed normalized indicator of technological impact is applied as against the case with straight counts of patent citations to those countries/regions' published papers. Originality/value: In our opinion, the level of sophistication of the indicators proposed in the current paper is unparalleled in the scientific literature, and provides a solid basis for the assessment of the technological impact of scientific research in countries/regions and institutions.  Â© 2021 Vicente P. Guerrero-Bote et al., published by Sciendo.","Building upon pioneering work by Francis Narin and others, a new methodological approach to assessing the technological impact of scientific research is presented. It is based on the analysis of citations made in patent families included in the PATSTAT database that is to scientific papers indexed in Scopus. An advanced citation matching procedure is applied to the data in order to construct two indicators of technological impact: on the citing (patent) side, the country/region in which protection is sought and a patent family's propensity to cite scientific papers are taken into account, and on the cited (paper) side, a relative citation rate is defined for patent citations to papers that is similar to the scientific paper-to-paper citation rate in classical bibliometrics. The results are limited by the available data, in our case Scopus and PATSTAT, and especially by the lack of standardization of references in patents. This required a matching procedure that is neither trivial nor exact. Results at the country/region, document type, and publication age levels are presented. The country/region-level results in particular reveal features that have remained hidden in analyses of straight counts. Especially notable is that the rankings of some Asian countries/regions move upwards when the proposed normalized indicator of technological impact is applied as against the case with straight counts of patent citations to those countries/regions' published papers. In our opinion, the level of sophistication of the indicators proposed in the current paper is unparalleled in the scientific literature, and provides a solid basis for the assessment of the technological impact of scientific research in countries/regions and institutions."
Scientometric Analysis of Research Output from Brazil in Response to the Zika Crisis Using e-Lattes,"This paper aims to test the use of e-Lattes to map the Brazilian scientific output in a recent research health subject: Zika Virus. From a set of Lattes CVs of Zika researchers registered on the Lattes Platform, we used the e-Lattes to map the Brazilian scientific response to the Zika crisis. Brazilian science articulated quickly during the public health emergency of international concern (PHEIC) due to the creation of mechanisms to streamline funding of scientific research. We did not assess any dimension of research quality, including the scientific impact and societal value. e-Lattes can provide useful guidelines for different stakeholders in research groups from Lattes CVs of members. The information included in Lattes CVs permits us to assess science from a broader perspective taking into account not only scientific research production but also the training of human resources and scientific collaboration.  Â© 2020 2020 Ricardo Barros Sampaio et al., published by Sciendo.","This paper aims to test the use of e-Lattes to map the Brazilian scientific output in a recent research health subject: Zika Virus. From a set of Lattes CVs of Zika researchers registered on the Lattes Platform, we used the e-Lattes to map the Brazilian scientific response to the Zika crisis. Brazilian science articulated quickly during the public health emergency of international concern (PHEIC) due to the creation of mechanisms to streamline funding of scientific research. We did not assess any dimension of research quality, including the scientific impact and societal value. e-Lattes can provide useful guidelines for different stakeholders in research groups from Lattes CVs of members. The information included in Lattes CVs permits us to assess science from a broader perspective taking into account not only scientific research production but also the training of human resources and scientific collaboration."
Government data openness and coverage. How do they affect trust in European countries?,"Purpose: This paper aims to assess if the extent of openness and the coverage of data sets released by European governments have a significant impact on citizen trust in public institutions. Design/methodology/approach: Data for openness and coverage have been collected from the Open Data Inventory 2018 (ODIN), by Open Data Watch; institutional trust is built up as a formative construct based on the European Social Survey (ESS), Round 9. The relations between the open government data features and trust have been tested on the basis of structural equation modelling (SEM). Findings: The paper reveals that as European governments improve data openness, disaggregation, and time coverage, people tend to trust them more. However, the size of the effect is still small and, comparatively, data coverage effect on citizensâ confidence is more than twice than the impact of openness. Research limitations: This paper analyzes the causal effect of Open Government Data (OGD) features captured in a certain moment of time. In upcoming years, as OGD is implemented and a more consistent effect on people is expected, time series analysis will provide with a deeper insight. Practical implications: Public officers should continue working in the development of a technological framework that contributes to make OGD truly open. They should improve the added value of the increasing amount of open data currently available in order to boost internal and external innovations valuable both for public agencies and citizens. Originality/value: In a field of knowledge with little quantitative empirical evidence, this paper provides updated support for the positive effect of OGD strategies and it also points out areas of improvement in terms of the value that citizens can get from OGD coverage and openness. Â© 2021 Sciendo. All rights reserved.","This paper aims to assess if the extent of openness and the coverage of data sets released by European governments have a significant impact on citizen trust in public institutions. Data for openness and coverage have been collected from the Open Data Inventory 2018 (ODIN), by Open Data Watch; institutional trust is built up as a formative construct based on the European Social Survey (ESS), Round 9. The relations between the open government data features and trust have been tested on the basis of structural equation modelling (SEM). The paper reveals that as European governments improve data openness, disaggregation, and time coverage, people tend to trust them more. However, the size of the effect is still small and, comparatively, data coverage effect on citizens confidence is more than twice than the impact of openness. This paper analyzes the causal effect of Open Government Data (OGD) features captured in a certain moment of time. In upcoming years, as OGD is implemented and a more consistent effect on people is expected, time series analysis will provide with a deeper insight. Public officers should continue working in the development of a technological framework that contributes to make OGD truly open. They should improve the added value of the increasing amount of open data currently available in order to boost internal and external innovations valuable both for public agencies and citizens. In a field of knowledge with little quantitative empirical evidence, this paper provides updated support for the positive effect of OGD strategies and it also points out areas of improvement in terms of the value that citizens can get from OGD coverage and openness."
Co-author Weighting in Bibliometric Methodology and Subfields of a Scientific Discipline,"To give a theoretical framework to measure the relative impact of bibliometric methodology on the subfields of a scientific discipline, and how that impact depends on the method of evaluation used to credit individual scientists with citations and publications. The authors include a study of the discipline of physics to illustrate the method. Indicators are introduced to measure the proportion of a credit space awarded to a subfield or a set of authors. The theoretical methodology introduces the notion of credit spaces for a discipline. These quantify the total citation or publication credit accumulated by the scientists in the discipline. One can then examine how the credit is divided among the subfields. The design of the physics study uses the American Physical Society print journals to assign subdiscipline classifications to articles and gather citation, publication, and author information. Credit spaces for the collection of Physical Review Journal articles are computed as a proxy for physics. There is a substantial difference in the value or impact of a specific subfield depending on the credit system employed to credit individual authors. Subfield classification information is difficult to obtain. In the illustrative physics study, subfields are treated in groups designated by the Physical Review journals. While this collection of articles represents a broad part of the physics literature, it is not all the literature nor a random sample. The method of crediting individual scientists has consequences beyond the individual and affects the perceived impact of whole subfields and institutions. The article reveals the consequences of bibliometric methodology on subfields of a disciple by introducing a systematic theoretical framework for measuring the consequences.  Â© 2020 2020 Lawrence Smolinsky et al., published by Sciendo.","To give a theoretical framework to measure the relative impact of bibliometric methodology on the subfields of a scientific discipline, and how that impact depends on the method of evaluation used to credit individual scientists with citations and publications. The authors include a study of the discipline of physics to illustrate the method. Indicators are introduced to measure the proportion of a credit space awarded to a subfield or a set of authors. The theoretical methodology introduces the notion of credit spaces for a discipline. These quantify the total citation or publication credit accumulated by the scientists in the discipline. One can then examine how the credit is divided among the subfields. The design of the physics study uses the American Physical Society print journals to assign subdiscipline classifications to articles and gather citation, publication, and author information. Credit spaces for the collection of Physical Review Journal articles are computed as a proxy for physics. There is a substantial difference in the value or impact of a specific subfield depending on the credit system employed to credit individual authors. Subfield classification information is difficult to obtain. In the illustrative physics study, subfields are treated in groups designated by the Physical Review journals. While this collection of articles represents a broad part of the physics literature, it is not all the literature nor a random sample. The method of crediting individual scientists has consequences beyond the individual and affects the perceived impact of whole subfields and institutions. The article reveals the consequences of bibliometric methodology on subfields of a disciple by introducing a systematic theoretical framework for measuring the consequences."
"Historical Bibliometrics Using Google Scholar: The Case of Roman Law, 1727-2016","The purpose of this study is to investigate the historical and linguistic coverage of Google Scholar, using publications in the field of Roman law as an example. To create a dataset of Roman law publications, we retrieved a total of 21,300 records of publications, published between years 1500 and 2016, with title including words denoting ""Roman law""in English, French, German, Italian, and Spanish. We were able to find publications dating back to 1727. The largest number of publications and authors date to the late 19th century, and this peak might be explained by the role of Roman law in French legal education at the time. Furthermore, we found exceptionally skewed concentration of publications to authors, as well as of citations to publications. We speculate that this could be explained by the long time-frame of the study, and the importance of classic works. Major limitations, and potential future work, relate to data quality, and cleaning, disambiguation of publications and authors, as well as comparing coverage with other data sources. We find Google Scholar to be a promising data source for historical bibliometrics. This approach may help bridge the gap between bibliometrics and the ""digital humanities"". Earlier studies have focused mainly on Google Scholar's coverage of publications and citations in general, or in specific fields. The historical coverage has, however, received less attention.  Â© 2020 2020 Janne PÃ¶lÃ¶nen et al., published by Sciendo.","The purpose of this study is to investigate the historical and linguistic coverage of Google Scholar, using publications in the field of Roman law as an example. To create a dataset of Roman law publications, we retrieved a total of 21,300 records of publications, published between years 1500 and 2016, with title including words denoting ""Roman law""in English, French, German, Italian, and Spanish. We were able to find publications dating back to 1727. The largest number of publications and authors date to the late 19th century, and this peak might be explained by the role of Roman law in French legal education at the time. Furthermore, we found exceptionally skewed concentration of publications to authors, as well as of citations to publications. We speculate that this could be explained by the long time-frame of the study, and the importance of classic works. Major limitations, and potential future work, relate to data quality, and cleaning, disambiguation of publications and authors, as well as comparing coverage with other data sources. We find Google Scholar to be a promising data source for historical bibliometrics. This approach may help bridge the gap between bibliometrics and the ""digital humanities"". Earlier studies have focused mainly on Google Scholar's coverage of publications and citations in general, or in specific fields. The historical coverage has, however, received less attention."
Masked Sentence Model Based on BERT for Move Recognition in Medical Scientific Abstracts,"Move recognition in scientific abstracts is an NLP task of classifying sentences of the abstracts into different types of language units. To improve the performance of move recognition in scientific abstracts, a novel model of move recognition is proposed that outperforms the BERT-based method. Prevalent models based on BERT for sentence classification often classify sentences without considering the context of the sentences. In this paper, inspired by the BERT masked language model (MLM), we propose a novel model called the masked sentence model that integrates the content and contextual information of the sentences in move recognition. Experiments are conducted on the benchmark dataset PubMed 20K RCT in three steps. Then, we compare our model with HSLN-RNN, BERT-based and SciBERT using the same dataset. Compared with the BERT-based and SciBERT models, the F1 score of our model outperforms them by 4.96% and 4.34%, respectively, which shows the feasibility and effectiveness of the novel model and the result of our model comes closest to the state-of-the-art results of HSLN-RNN at present. The sequential features of move labels are not considered, which might be one of the reasons why HSLN-RNN has better performance. Our model is restricted to dealing with biomedical English literature because we use a dataset from PubMed, which is a typical biomedical database, to fine-tune our model. The proposed model is better and simpler in identifying move structures in scientific abstracts and is worthy of text classification experiments for capturing contextual features of sentences. T he study proposes a masked sentence model based on BERT that considers the contextual features of the sentences in abstracts in a new way. The performance of this classification model is significantly improved by rebuilding the input layer without changing the structure of neural networks. Â© 2019 2019 Gaihong Yu, Zhixiong Zhang, Huan Liu, Liangping Ding, published by Sciendo.","Move recognition in scientific abstracts is an NLP task of classifying sentences of the abstracts into different types of language units. To improve the performance of move recognition in scientific abstracts, a novel model of move recognition is proposed that outperforms the BERT-based method. Prevalent models based on BERT for sentence classification often classify sentences without considering the context of the sentences. In this paper, inspired by the BERT masked language model , we propose a novel model called the masked sentence model that integrates the content and contextual information of the sentences in move recognition. Experiments are conducted on the benchmark dataset PubMed 20K RCT in three steps. Then, we compare our model with HSLN-RNN, BERT-based and SciBERT using the same dataset. Compared with the BERT-based and SciBERT models, the F1 score of our model outperforms them by 4.96% and 4.34%, respectively, which shows the feasibility and effectiveness of the novel model and the result of our model comes closest to the state-of-the-art results of HSLN-RNN at present. The sequential features of move labels are not considered, which might be one of the reasons why HSLN-RNN has better performance. Our model is restricted to dealing with biomedical English literature because we use a dataset from PubMed, which is a typical biomedical database, to fine-tune our model. The proposed model is better and simpler in identifying move structures in scientific abstracts and is worthy of text classification experiments for capturing contextual features of sentences. T he study proposes a masked sentence model based on BERT that considers the contextual features of the sentences in abstracts in a new way. The performance of this classification model is significantly improved by rebuilding the input layer without changing the structure of neural networks."
Are University Rankings Statistically Significant? A Comparison among Chinese Universities and with the USA,"Building on Leydesdorff, Bornmann, and Mingers (2019), we elaborate the differences between Tsinghua and Zhejiang University as an empirical example. We address the question of whether differences are statistically significant in the rankings of Chinese universities. We propose methods for measuring statistical significance among different universities within or among countries. Based on z-testing and overlapping confidence intervals, and using data about 205 Chinese universities included in the Leiden Rankings 2020, we argue that three main groups of Chinese research universities can be distinguished (low, middle, and high). When the sample of 205 Chinese universities is merged with the 197 US universities included in Leiden Rankings 2020, the results similarly indicate three main groups: low, middle, and high. Using this data (Leiden Rankings and Web of Science), the z-scores of the Chinese universities are significantly below those of the US universities albeit with some overlap. We show empirically that differences in ranking may be due to changes in the data, the models, or the modeling effects on the data. The scientometric groupings are not always stable when we use different methods. Differences among universities can be tested for their statistical significance. The statistics relativize the values of decimals in the rankings. One can operate with a scheme of low/middle/high in policy debates and leave the more fine-grained rankings of individual universities to operational management and local settings. In the discussion about the rankings of universities, the question of whether differences are statistically significant, has, in our opinion, insufficiently been addressed in research evaluations.  Â© 2021 2021 Loet Leydesdorff et al., published by Sciendo.","Building on Leydesdorff, Bornmann, and Mingers , we elaborate the differences between Tsinghua and Zhejiang University as an empirical example. We address the question of whether differences are statistically significant in the rankings of Chinese universities. We propose methods for measuring statistical significance among different universities within or among countries. Based on z-testing and overlapping confidence intervals, and using data about 205 Chinese universities included in the Leiden Rankings 2020, we argue that three main groups of Chinese research universities can be distinguished (low, middle, and high). When the sample of 205 Chinese universities is merged with the 197 US universities included in Leiden Rankings 2020, the results similarly indicate three main groups: low, middle, and high. Using this data (Leiden Rankings and Web of Science), the z-scores of the Chinese universities are significantly below those of the US universities albeit with some overlap. We show empirically that differences in ranking may be due to changes in the data, the models, or the modeling effects on the data. The scientometric groupings are not always stable when we use different methods. Differences among universities can be tested for their statistical significance. The statistics relativize the values of decimals in the rankings. One can operate with a scheme of low/middle/high in policy debates and leave the more fine-grained rankings of individual universities to operational management and local settings. In the discussion about the rankings of universities, the question of whether differences are statistically significant, has, in our opinion, insufficiently been addressed in research evaluations."
The gender patenting gap: A study on the Iberoamerican countries,"Purpose: This work presents a study on the female involvement in patent applications in all 23 Ibero-American countries that are WIPO members, in order to measure gender inequalities in institutional collaborations and technological fields, across time. Design/methodology/approach: The data used in this paper come from EPO Worldwide Patent Statistical Database (PATSTAT). PATSTAT contains bibliographical data relating to more than 100 million patent documents from leading industrialized and developing countries, as well as legal event data from more than 40 patent authorities contained in the EPO worldwide legal event data (INPADOC). The extracted subset is composed of 150,863 patent applications with priority years between 2007 and 2016. Findings: Our observations indicate that even in more dynamic economies such as Portugal and Spain, the participation of women per patent applications does not exceed 30%. Additionally, the distribution of female participation among institutional sectors and technological fields is consistent with previous studies in other regions and indicate a socio-cultural divide. Research limitations: Unisex names were not considered and were counted as gender unknown, and patent applications for which no inventor information was available were discarded, but further effort of data analysis may provide more information about gender inequalities. Practical implications: While patents are imperfect variables of inventive step and therefore should be considered as a variable proxy of innovation, our findings may help to guide the implementation of policies for balancing gender participation in innovative activities, as well as instigating research into the issues causing divisive participation along gender lines. Originality/value: While there is a widespread effort into evaluating and improving the participation of groups recognized as minorities within state-of-the-art activities, research about women participation in the innovation sector is fragmented due to differing regional characteristics: industrial and academic segmentation, socio-economic disparities, and cultural factors. Thus, localized studies present an opportunity of filling the gaps of knowledge on societal participation in innovation activities. Â© 2020 Sciendo. All rights reserved.","This work presents a study on the female involvement in patent applications in all 23 Ibero-American countries that are WIPO members, in order to measure gender inequalities in institutional collaborations and technological fields, across time. The data used in this paper come from EPO Worldwide Patent Statistical Database (PATSTAT). PATSTAT contains bibliographical data relating to more than 100 million patent documents from leading industrialized and developing countries, as well as legal event data from more than 40 patent authorities contained in the EPO worldwide legal event data (INPADOC). The extracted subset is composed of 150,863 patent applications with priority years between 2007 and 2016. Our observations indicate that even in more dynamic economies such as Portugal and Spain, the participation of women per patent applications does not exceed 30%. Additionally, the distribution of female participation among institutional sectors and technological fields is consistent with previous studies in other regions and indicate a socio-cultural divide. Unisex names were not considered and were counted as gender unknown, and patent applications for which no inventor information was available were discarded, but further effort of data analysis may provide more information about gender inequalities. While patents are imperfect variables of inventive step and therefore should be considered as a variable proxy of innovation, our findings may help to guide the implementation of policies for balancing gender participation in innovative activities, as well as instigating research into the issues causing divisive participation along gender lines. While there is a widespread effort into evaluating and improving the participation of groups recognized as minorities within state-of-the-art activities, research about women participation in the innovation sector is fragmented due to differing regional characteristics: industrial and academic segmentation, socio-economic disparities, and cultural factors. Thus, localized studies present an opportunity of filling the gaps of knowledge on societal participation in innovation activities."
Is participating in MOOC forums important for students? A Data-driven Study from the Perspective of the Supernetwork,"Purpose: Compared with traditional course materials used in the classroom, the massive open online course (MOOC) forum that delivers unlimited learning content to students has various advantages. Yet MOOC has also received criticism recently, notably the problem of extremely low participation rates in its discussion forums. This study aims to explore the correlation between forum activity and student course grade in MOOC, and identify more accurately the forum activity levels of participants and the quality of threads in MOOC. Design/Methodology/Approach: We crawled students' tests, final exams, exercises, discussions performance data and total scores from a course in Chinese College MOOC from May 2014 to August 2014. And we use the data to analyze the correlation between Forum Participation and Course Performance based on nonparametric tests as well as multiple linear regressions with the software of R. The study provides definitions and algorithms of super degrees based on the supernetwork model to help find high-quality threads and active participants. Findings: A positive correlation between forum activity and course grade is found in this study. Students who participate in the forum have better performance than those who do not. Using the definitions and algorithms of super degrees in the supernetwork, forum activity levels of participants as well as the quality of threads they employ are identified. Research limitation: Only limited representative forum participants and threads are used to analyze the activity level and significance of the MOOC forum. Also, the study only investigates one Chinese course on information retrieval. More data and more data sources could be helpful in better understanding the MOOC forum phenomenon. Practical implications: As super degrees can reveal more latent information and recognize high-quality threads as well as active participants, these parameters can be used to assess needs to improve forum settings and alleviate the problem of low forum participation. The proposed super degrees can be applied in social network domains for further research. Originality/Value: Definitions and algorithms of super degrees are provided and used for forum analysis. Super degrees can be applied to find high-quality threads and active participants, which is beneficial to guide students to participate in these high-quality threads and have a better understanding of knowledge MOOC provides. Â© 2018 Sciendo. All rights reserved.","Compared with traditional course materials used in the classroom, the massive open online course (MOOC) forum that delivers unlimited learning content to students has various advantages. Yet MOOC has also received criticism recently, notably the problem of extremely low participation rates in its discussion forums. This study aims to explore the correlation between forum activity and student course grade in MOOC, and identify more accurately the forum activity levels of participants and the quality of threads in MOOC. We crawled students' tests, final exams, exercises, discussions performance data and total scores from a course in Chinese College MOOC from May 2014 to August 2014. And we use the data to analyze the correlation between Forum Participation and Course Performance based on nonparametric tests as well as multiple linear regressions with the software of The study provides definitions and algorithms of super degrees based on the supernetwork model to help find high-quality threads and active participants. A positive correlation between forum activity and course grade is found in this study. Students who participate in the forum have better performance than those who do not. Using the definitions and algorithms of super degrees in the supernetwork, forum activity levels of participants as well as the quality of threads they employ are identified. Research limitation: Only limited representative forum participants and threads are used to analyze the activity level and significance of the MOOC forum. Also, the study only investigates one Chinese course on information retrieval. More data and more data sources could be helpful in better understanding the MOOC forum phenomenon. As super degrees can reveal more latent information and recognize high-quality threads as well as active participants, these parameters can be used to assess needs to improve forum settings and alleviate the problem of low forum participation. The proposed super degrees can be applied in social network domains for further research. Definitions and algorithms of super degrees are provided and used for forum analysis. Super degrees can be applied to find high-quality threads and active participants, which is beneficial to guide students to participate in these high-quality threads and have a better understanding of knowledge MOOC provides."
CiteOpinion: Evidence-based Evaluation Tool for Academic Contributions of Research Papers Based on Citing Sentences,"To uncover the evaluation information on the academic contribution of research papers cited by peers based on the content cited by citing papers, and to provide an evidence-based tool for evaluating the academic value of cited papers. CiteOpinion uses a deep learning model to automatically extract citing sentences from representative citing papers; it starts with an analysis on the citing sentences, then it identifies major academic contribution points of the cited paper, positive/negative evaluations from citing authors and the changes in the subjects of subsequent citing authors by means of Recognizing Categories of Moves (problems, methods, conclusions, etc.), and sentiment analysis and topic clustering. Citing sentences in a citing paper contain substantial evidences useful for academic evaluation. They can also be used to objectively and authentically reveal the nature and degree of contribution of the cited paper reflected by citation, beyond simple citation statistics. The evidence-based evaluation tool CiteOpinion can provide an objective and in-depth academic value evaluation basis for the representative papers of scientific researchers, research teams, and institutions. No other similar practical tool is found in papers retrieved. There are difficulties in acquiring full text of citing papers. There is a need to refine the calculation based on the sentiment scores of citing sentences. Currently, the tool is only used for academic contribution evaluation, while its value in policy studies, technical application, and promotion of science is not yet tested. Â© 2019 2019 Xiaoqiu Le, Jingdan Chu, Siyi Deng, Qihang Jiao, Jingjing Pei, Liya Zhu, Junliang Yao, published by Sciendo.","To uncover the evaluation information on the academic contribution of research papers cited by peers based on the content cited by citing papers, and to provide an evidence-based tool for evaluating the academic value of cited papers. CiteOpinion uses a deep learning model to automatically extract citing sentences from representative citing papers; it starts with an analysis on the citing sentences, then it identifies major academic contribution points of the cited paper, positive/negative evaluations from citing authors and the changes in the subjects of subsequent citing authors by means of Recognizing Categories of Moves (problems, methods, conclusions, etc.), and sentiment analysis and topic clustering. Citing sentences in a citing paper contain substantial evidences useful for academic evaluation. They can also be used to objectively and authentically reveal the nature and degree of contribution of the cited paper reflected by citation, beyond simple citation statistics. The evidence-based evaluation tool CiteOpinion can provide an objective and in-depth academic value evaluation basis for the representative papers of scientific researchers, research teams, and institutions. No other similar practical tool is found in papers retrieved. There are difficulties in acquiring full text of citing papers. There is a need to refine the calculation based on the sentiment scores of citing sentences. Currently, the tool is only used for academic contribution evaluation, while its value in policy studies, technical application, and promotion of science is not yet tested."
Automatic Keyphrase Extraction from Scientific Chinese Medical Abstracts Based on Character-Level Sequence Labeling,"Automatic keyphrase extraction (AKE) is an important task for grasping the main points of the text. In this paper, we aim to combine the benefits of sequence labeling formulation and pretrained language model to propose an automatic keyphrase extraction model for Chinese scientific research. We regard AKE from Chinese text as a character-level sequence labeling task to avoid segmentation errors of Chinese tokenizer and initialize our model with pretrained language model BERT, which was released by Google in 2018. We collect data from Chinese Science Citation Database and construct a large-scale dataset from medical domain, which contains 100,000 abstracts as training set, 6,000 abstracts as development set and 3,094 abstracts as test set. We use unsupervised keyphrase extraction methods including term frequency (TF), TF-IDF, TextRank and supervised machine learning methods including Conditional Random Field (CRF), Bidirectional Long Short Term Memory Network (BiLSTM), and BiLSTM-CRF as baselines. Experiments are designed to compare word-level and character-level sequence labeling approaches on supervised machine learning models and BERT-based models. Compared with character-level BiLSTM-CRF, the best baseline model with F1 score of 50.16%, our character-level sequence labeling model based on BERT obtains F1 score of 59.80%, getting 9.64% absolute improvement. We just consider automatic keyphrase extraction task rather than keyphrase generation task, so only keyphrases that are occurred in the given text can be extracted. In addition, our proposed dataset is not suitable for dealing with nested keyphrases. We make our character-level IOB format dataset of Chinese Automatic Keyphrase Extraction from scientific Chinese medical abstracts (CAKE) publicly available for the benefits of research community, which is available at: https://github.com/possible1402/Dataset-For-Chinese-Medical-Keyphrase-Extraction. By designing comparative experiments, our study demonstrates that character-level formulation is more suitable for Chinese automatic keyphrase extraction task under the general trend of pretrained language models. And our proposed dataset provides a unified method for model evaluation and can promote the development of Chinese automatic keyphrase extraction to some extent. Â© 2021 Liangping Ding et al., published by Sciendo.","Automatic keyphrase extraction (AKE) is an important task for grasping the main points of the text. In this paper, we aim to combine the benefits of sequence labeling formulation and pretrained language model to propose an automatic keyphrase extraction model for Chinese scientific research. We regard AKE from Chinese text as a character-level sequence labeling task to avoid segmentation errors of Chinese tokenizer and initialize our model with pretrained language model BERT, which was released by Google in 2018. We collect data from Chinese Science Citation Database and construct a large-scale dataset from medical domain, which contains 100,000 abstracts as training set, 6,000 abstracts as development set and 3,094 abstracts as test set. We use unsupervised keyphrase extraction methods including term frequency (TF), TF-IDF, TextRank and supervised machine learning methods including Conditional Random Field (CRF), Bidirectional Long Short Term Memory Network (BiLSTM), and BiLSTM-CRF as baselines. Experiments are designed to compare word-level and character-level sequence labeling approaches on supervised machine learning models and BERT-based models. Compared with character-level BiLSTM-CRF, the best baseline model with F1 score of 50.16%, our character-level sequence labeling model based on BERT obtains F1 score of 59.80%, getting 9.64% absolute improvement. We just consider automatic keyphrase extraction task rather than keyphrase generation task, so only keyphrases that are occurred in the given text can be extracted. In addition, our proposed dataset is not suitable for dealing with nested keyphrases. We make our character-level IOB format dataset of Chinese Automatic Keyphrase Extraction from scientific Chinese medical abstracts (CAKE) publicly available for the benefits of research community, which is available at: https://github.com/possible1402/Dataset-For-Chinese-Medical-Keyphrase-Extraction. By designing comparative experiments, our study demonstrates that character-level formulation is more suitable for Chinese automatic keyphrase extraction task under the general trend of pretrained language models. And our proposed dataset provides a unified method for model evaluation and can promote the development of Chinese automatic keyphrase extraction to some extent."
A Two-Level Approach based on Integration of Bagging and Voting for Outlier Detection,"The main aim of this study is to build a robust novel approach that is able to detect outliers in the datasets accurately. To serve this purpose, a novel approach is introduced to determine the likelihood of an object to be extremely different from the general behavior of the entire dataset. This paper proposes a novel two-level approach based on the integration of bagging and voting techniques for anomaly detection problems. The proposed approach, named Bagged and Voted Local Outlier Detection (BV-LOF), benefits from the Local Outlier Factor (LOF) as the base algorithm and improves its detection rate by using ensemble methods. Several experiments have been performed on ten benchmark outlier detection datasets to demonstrate the effectiveness of the BV-LOF method. According to the results, the BV-LOF approach significantly outperformed LOF on 9 datasets of 10 ones on average. In the BV-LOF approach, the base algorithm is applied to each subset data multiple times with different neighborhood sizes (k) in each case and with different ensemble sizes (T). In our study, we have chosen k and T value ranges as [1-100]; however, these ranges can be changed according to the dataset handled and to the problem addressed. The proposed method can be applied to the datasets from different domains (i.e. health, finance, manufacturing, etc.) without requiring any prior information. Since the BV-LOF method includes two-level ensemble operations, it may lead to more computational time than single-level ensemble methods; however, this drawback can be overcome by parallelization and by using a proper data structure such as Râ-tree or KD-tree. The proposed approach (BV-LOF) investigates multiple neighborhood sizes (k), which provides findings of instances with different local densities, and in this way, it provides more likelihood of outlier detection that LOF may neglect. It also brings many benefits such as easy implementation, improved capability, higher applicability, and interpretability. Â© 2020 2020 Alican Dogan et al., published by Sciendo.","The main aim of this study is to build a robust novel approach that is able to detect outliers in the datasets accurately. To serve this purpose, a novel approach is introduced to determine the likelihood of an object to be extremely different from the general behavior of the entire dataset. This paper proposes a novel two-level approach based on the integration of bagging and voting techniques for anomaly detection problems. The proposed approach, named Bagged and Voted Local Outlier Detection (BV-LOF), benefits from the Local Outlier Factor (LOF) as the base algorithm and improves its detection rate by using ensemble methods. Several experiments have been performed on ten benchmark outlier detection datasets to demonstrate the effectiveness of the BV-LOF method. According to the results, the BV-LOF approach significantly outperformed LOF on 9 datasets of 10 ones on average. In the BV-LOF approach, the base algorithm is applied to each subset data multiple times with different neighborhood sizes (k) in each case and with different ensemble sizes (T). In our study, we have chosen k and T value ranges as [1-100]; however, these ranges can be changed according to the dataset handled and to the problem addressed. The proposed method can be applied to the datasets from different domains ( health, finance, manufacturing, etc.) without requiring any prior information. Since the BV-LOF method includes two-level ensemble operations, it may lead to more computational time than single-level ensemble methods; however, this drawback can be overcome by parallelization and by using a proper data structure such as R-tree or KD-tree. The proposed approach (BV-LOF) investigates multiple neighborhood sizes (k), which provides findings of instances with different local densities, and in this way, it provides more likelihood of outlier detection that LOF may neglect. It also brings many benefits such as easy implementation, improved capability, higher applicability, and interpretability."
RDFAdaptor: Efficient ETL Plugins for RDF Data Process,"The interdisciplinary nature and rapid development of the Semantic Web led to the mass publication of RDF data in a large number of widely accepted serialization formats, thus developing out the necessity for RDF data processing with specific purposes. The paper reports on an assessment of chief RDF data endpoint challenges and introduces the RDF Adaptor, a set of plugins for RDF data processing which covers the whole life-cycle with high efficiency. The RDFAdaptor is designed based on the prominent ETL tool - Pentaho Data Integration - which provides a user-friendly and intuitive interface and allows connect to various data sources and formats, and reuses the Java framework RDF4J as middleware that realizes access to data repositories, SPARQL endpoints and all leading RDF database solutions with SPARQL 1.1 support. It can support effortless services with various configuration templates in multi-scenario applications, and help extend data process tasks in other services or tools to complement missing functions. The proposed comprehensive RDF ETL solution - RDFAdaptor - provides an easy-to-use and intuitive interface, supports data integration and federation over multi-source heterogeneous repositories or endpoints, as well as manage linked data in hybrid storage mode. The plugin set can support several application scenarios of RDF data process, but error detection/check and interaction with other graph repositories remain to be improved. The plugin set can provide user interface and configuration templates which enable its usability in various applications of RDF data generation, multi-format data conversion, remote RDF data migration, and RDF graph update in semantic query process. This is the first attempt to develop components instead of systems that can include extract, consolidate, and store RDF data on the basis of an ecologically mature data warehousing environment.  Â© 2021 2021 Jiao Li et al., published by Sciendo.","The interdisciplinary nature and rapid development of the Semantic Web led to the mass publication of RDF data in a large number of widely accepted serialization formats, thus developing out the necessity for RDF data processing with specific purposes. The paper reports on an assessment of chief RDF data endpoint challenges and introduces the RDF Adaptor, a set of plugins for RDF data processing which covers the whole life-cycle with high efficiency. The RDFAdaptor is designed based on the prominent ETL tool - Pentaho Data Integration - which provides a user-friendly and intuitive interface and allows connect to various data sources and formats, and reuses the Java framework RDF4J as middleware that realizes access to data repositories, SPARQL endpoints and all leading RDF database solutions with SPARQL 1.1 support. It can support effortless services with various configuration templates in multi-scenario applications, and help extend data process tasks in other services or tools to complement missing functions. The proposed comprehensive RDF ETL solution - RDFAdaptor - provides an easy-to-use and intuitive interface, supports data integration and federation over multi-source heterogeneous repositories or endpoints, as well as manage linked data in hybrid storage mode. The plugin set can support several application scenarios of RDF data process, but error detection/check and interaction with other graph repositories remain to be improved. The plugin set can provide user interface and configuration templates which enable its usability in various applications of RDF data generation, multi-format data conversion, remote RDF data migration, and RDF graph update in semantic query process. This is the first attempt to develop components instead of systems that can include extract, consolidate, and store RDF data on the basis of an ecologically mature data warehousing environment."
"Co-occurrence of Cell Lines, Basal Media and Supplementation in the Biomedical Research Literature","The use of in vitro cell culture and experimentation is a cornerstone of biomedical research, however, more attention has recently been given to the potential consequences of using such artificial basal medias and undefined supplements. As a first step towards better understanding and measuring the impact these systems have on experimental results, we use text mining to capture typical research practices and trends around cell culture. To measure the scale of in vitro cell culture use, we have analyzed a corpus of 94,695 research articles that appear in biomedical research journals published in ScienceDirect from 2000-2018. Central to our investigation is the observation that studies using cell culture describe conditions using the typical sentence structure of cell line, basal media, and supplemented compounds. Here we tag our corpus with a curated list of basal medias and the Cellosaurus ontology using the Aho-Corasick algorithm. We also processed the corpus with Stanford CoreNLP to find nouns that follow the basal media, in an attempt to identify supplements used. Interestingly, we find that researchers frequently use DMEM even if a cell line's vendor recommends less concentrated media. We see long-tailed distributions for the usage of media and cell lines, with DMEM and RPMI dominating the media, and HEK293, HEK293T, and HeLa dominating cell lines used. Our analysis was restricted to documents in ScienceDirect, and our text mining method achieved high recall but low precision and mandated manual inspection of many tokens. Our findings document current cell culture practices in the biomedical research community, which can be used as a resource for future experimental design. No other work has taken a text mining approach to surveying cell culture practices in biomedical research.  Â© 2020 2020 Jessica Cox et al., published by Sciendo.","The use of in vitro cell culture and experimentation is a cornerstone of biomedical research, however, more attention has recently been given to the potential consequences of using such artificial basal medias and undefined supplements. As a first step towards better understanding and measuring the impact these systems have on experimental results, we use text mining to capture typical research practices and trends around cell culture. To measure the scale of in vitro cell culture use, we have analyzed a corpus of 94,695 research articles that appear in biomedical research journals published in ScienceDirect from 2000-2018. Central to our investigation is the observation that studies using cell culture describe conditions using the typical sentence structure of cell line, basal media, and supplemented compounds. Here we tag our corpus with a curated list of basal medias and the Cellosaurus ontology using the Aho-Corasick algorithm. We also processed the corpus with Stanford CoreNLP to find nouns that follow the basal media, in an attempt to identify supplements used. Interestingly, we find that researchers frequently use DMEM even if a cell line's vendor recommends less concentrated media. We see long-tailed distributions for the usage of media and cell lines, with DMEM and RPMI dominating the media, and HEK293, HEK293T, and HeLa dominating cell lines used. Our analysis was restricted to documents in ScienceDirect, and our text mining method achieved high recall but low precision and mandated manual inspection of many tokens. Our findings document current cell culture practices in the biomedical research community, which can be used as a resource for future experimental design. No other work has taken a text mining approach to surveying cell culture practices in biomedical research."
Infrastructure of Scientometrics: The Big and Network Picture,"A network is a set of nodes connected via edges, with possibly directions and weights on the edges. Sometimes, in a multi-layer network, the nodes can also be heterogeneous. In this perspective, based on previous studies, we argue that networks can be regarded as the infrastructure of scientometrics in the sense that networks can be used to represent scientometric data. Then the task of answering various scientometric questions related to this data becomes an algorithmic problem in the corresponding network. Â© 2019 Jinshan Wu, published by Sciendo 2019.","A network is a set of nodes connected via edges, with possibly directions and weights on the edges. Sometimes, in a multi-layer network, the nodes can also be heterogeneous. In this perspective, based on previous studies, we argue that networks can be regarded as the infrastructure of scientometrics in the sense that networks can be used to represent scientometric data. Then the task of answering various scientometric questions related to this data becomes an algorithmic problem in the corresponding network."
Can Crossref Citations Replace Web of Science for Research Evaluation? The Share of Open Citations,"We study the proportion of Web of Science (WoS) citation links that are represented in the Crossref Open Citation Index (COCI), with the possible aim of using COCI in research evaluation instead of the WoS, if the level of coverage was sufficient. We calculate the proportion on citation links where both publications have a WoS accession number and a DOI simultaneously, and where the cited publications have had at least one author from our institution, the Czech Technical University in Prague. We attempt to look up each such citation link in COCI. We find that 53.7% of WoS citation links are present in the COCI. The proportion varies largely by discipline. The total figures differ significantly from 40% in the large-scale study by Van Eck, Waltman, LariviÃ¨re, and Sugimoto (blog 2018, https://www.cwts.nl/blog?article=n-r2s234). The sample does not cover all science areas uniformly; it is heavily focused on Engineering and Technology, and only some disciplines of Natural Sciences are present. However, this reflects the real scientific orientation and publication profile of our institution. The current level of coverage is not sufficient for the WoS to be replaced by COCI for research evaluation. The present study illustrates a COCI vs WoS comparison on the scale of a larger technical university in Central Europe.  Â© 2020 2020 TomÃ¡Å¡ ChudlarskÃ½ et al., published by Sciendo.","We study the proportion of Web of Science (WoS) citation links that are represented in the Crossref Open Citation Index (COCI), with the possible aim of using COCI in research evaluation instead of the WoS, if the level of coverage was sufficient. We calculate the proportion on citation links where both publications have a WoS accession number and a DOI simultaneously, and where the cited publications have had at least one author from our institution, the Czech Technical University in Prague. We attempt to look up each such citation link in COCI. We find that 53.7% of WoS citation links are present in the COCI. The proportion varies largely by discipline. The total figures differ significantly from 40% in the large-scale study by Van Eck, Waltman, Larivire, and Sugimoto (blog 2018, https://www.cwts.nl/blog?article=n-r2s234). The sample does not cover all science areas uniformly; it is heavily focused on Engineering and Technology, and only some disciplines of Natural Sciences are present. However, this reflects the real scientific orientation and publication profile of our institution. The current level of coverage is not sufficient for the WoS to be replaced by COCI for research evaluation. The present study illustrates a COCI vs WoS comparison on the scale of a larger technical university in Central Europe."
Trends analysis of graphene research and development,"Purpose: This study aims to reveal the landscape and trends of graphene research in the world by using data from Chemical Abstracts Service (CAS). Design/methodology/approach: Index data from CAS have been retrieved on 78,756 papers and 23,057 patents on graphene from 1985 to March 2016, and scientometric methods were used to analyze the growth and distribution of R&D output, topic distribution and evolution, and distribution and evolution of substance properties and roles. Findings: In recent years R&D in graphene keeps in rapid growth, while China, South Korea and United States are the largest producers in research but China is relatively weak in patent applications in other countries. Research topics in graphene are continuously expanding from mechanical, material, and electrical properties to a diverse range of application areas such as batteries, capacitors, semiconductors, and sensors devices. The roles of emerging substances are increasing in Preparation and Biological Study. More techniques have been included to improve the preparation processes and applications of graphene in various fields. Research limitations: Only data from CAS is used and some R&D activities solely reported through other channels may be missed. Also more detailed analysis need to be done to reveal the impact of research on development or vice verse, development dynamics among the players, and impact of emerging terms or substance roles on research and technology development. Practical implications: This will provide a valuable reference for scientists and developers, R&D managers, R&D policy makers, industrial and business investers to understand the landscape and trends of graphene research. Its methodologies can be applied to other fields or with data from other similar sources. Originality/value: The integrative use of indexing data on papers and patents of CAS and the systematic exploration of the distribution trends in output, topics, substance roles are distinctive and insightful. Â© 2018 Sciendo. All rights reserved.","This study aims to reveal the landscape and trends of graphene research in the world by using data from Chemical Abstracts Service (CAS). Index data from CAS have been retrieved on 78,756 papers and 23,057 patents on graphene from 1985 to March 2016, and scientometric methods were used to analyze the growth and distribution of R&D output, topic distribution and evolution, and distribution and evolution of substance properties and roles. In recent years R&D in graphene keeps in rapid growth, while China, South Korea and United States are the largest producers in research but China is relatively weak in patent applications in other countries. Research topics in graphene are continuously expanding from mechanical, material, and electrical properties to a diverse range of application areas such as batteries, capacitors, semiconductors, and sensors devices. The roles of emerging substances are increasing in Preparation and Biological Study. More techniques have been included to improve the preparation processes and applications of graphene in various fields. Only data from CAS is used and some R&D activities solely reported through other channels may be missed. Also more detailed analysis need to be done to reveal the impact of research on development or vice verse, development dynamics among the players, and impact of emerging terms or substance roles on research and technology development. This will provide a valuable reference for scientists and developers, R&D managers, R&D policy makers, industrial and business investers to understand the landscape and trends of graphene research. Its methodologies can be applied to other fields or with data from other similar sources. The integrative use of indexing data on papers and patents of CAS and the systematic exploration of the distribution trends in output, topics, substance roles are distinctive and insightful."
Content Characteristics of Knowledge Integration in the eHealth Field: An Analysis Based on Citation Contexts,"This study attempts to disclose the characteristics of knowledge integration in an interdisciplinary field by looking into the content aspect of knowledge. The eHealth field was chosen in the case study. Associated knowledge phrases (AKPs) that are shared between citing papers and their references were extracted from the citation contexts of the eHealth papers by applying a stem-matching method. A classification schema that considers the functions of knowledge in the domain was proposed to categorize the identified AKPs. The source disciplines of each knowledge type were analyzed. Quantitative indicators and a co-occurrence analysis were applied to disclose the integration patterns of different knowledge types. The annotated AKPs evidence the major disciplines supplying each type of knowledge. Different knowledge types have remarkably different integration patterns in terms of knowledge amount, the breadth of source disciplines, and the integration time lag. We also find several frequent co-occurrence patterns of different knowledge types. The collected articles of the field are limited to the two leading open access journals. The stem-matching method to extract AKPs could not identify those phrases with the same meaning but expressed in words with different stems. The type of Research Subject dominates the recognized AKPs, which calls on an improvement of the classification schema for better knowledge integration analysis on knowledge units. The methodology proposed in this paper sheds new light on knowledge integration characteristics of an interdisciplinary field from the content perspective. The findings have practical implications on the future development of research strategies in eHealth and the policies about interdisciplinary research. This study proposed a new methodology to explore the content characteristics of knowledge integration in an interdisciplinary field. Â© 2021 2021 Shiyun Wang et al., published by Sciendo.","This study attempts to disclose the characteristics of knowledge integration in an interdisciplinary field by looking into the content aspect of knowledge. The eHealth field was chosen in the case study. Associated knowledge phrases (AKPs) that are shared between citing papers and their references were extracted from the citation contexts of the eHealth papers by applying a stem-matching method. A classification schema that considers the functions of knowledge in the domain was proposed to categorize the identified AKPs. The source disciplines of each knowledge type were analyzed. Quantitative indicators and a co-occurrence analysis were applied to disclose the integration patterns of different knowledge types. The annotated AKPs evidence the major disciplines supplying each type of knowledge. Different knowledge types have remarkably different integration patterns in terms of knowledge amount, the breadth of source disciplines, and the integration time lag. We also find several frequent co-occurrence patterns of different knowledge types. The collected articles of the field are limited to the two leading open access journals. The stem-matching method to extract AKPs could not identify those phrases with the same meaning but expressed in words with different stems. The type of Research Subject dominates the recognized AKPs, which calls on an improvement of the classification schema for better knowledge integration analysis on knowledge units. The methodology proposed in this paper sheds new light on knowledge integration characteristics of an interdisciplinary field from the content perspective. The findings have practical implications on the future development of research strategies in eHealth and the policies about interdisciplinary research. This study proposed a new methodology to explore the content characteristics of knowledge integration in an interdisciplinary field."
FAIR + FIT: Guiding Principles and Functional Metrics for Linked Open Data (LOD) KOS Products,"To develop a set of metrics and identify criteria for assessing the functionality of LOD KOS products while providing common guiding principles that can be used by LOD KOS producers and users to maximize the functions and usages of LOD KOS products. Data collection and analysis were conducted at three time periods in 2015-16, 2017 and 2019. The sample data used in the comprehensive data analysis comprises all datasets tagged as types of KOS in the Datahub and extracted through their respective SPARQL endpoints. A comparative study of the LOD KOS collected from terminology services Linked Open Vocabularies (LOV) and BioPortal was also performed. The study proposes a set of Functional, Impactful and Transformable (FIT) metrics for LOD KOS as value vocabularies. The FAIR principles, with additional recommendations, are presented for LOD KOS as open data. The metrics need to be further tested and aligned with the best practices and international standards of both open data and various types of KOS. Assessment performed with FAIR and FIT metrics support the creation and delivery of user-friendly, discoverable and interoperable LOD KOS datasets which can be used for innovative applications, act as a knowledge base, become a foundation of semantic analysis and entity extractions and enhance research in science and the humanities. Our research provides best practice guidelines for LOD KOS as value vocabularies. Â© 2020 Marcia Lei Zeng et al., published by Sciendo.","To develop a set of metrics and identify criteria for assessing the functionality of LOD KOS products while providing common guiding principles that can be used by LOD KOS producers and users to maximize the functions and usages of LOD KOS products. Data collection and analysis were conducted at three time periods in 2015-16, 2017 and 2019. The sample data used in the comprehensive data analysis comprises all datasets tagged as types of KOS in the Datahub and extracted through their respective SPARQL endpoints. A comparative study of the LOD KOS collected from terminology services Linked Open Vocabularies (LOV) and BioPortal was also performed. The study proposes a set of Functional, Impactful and Transformable (FIT) metrics for LOD KOS as value vocabularies. The FAIR principles, with additional recommendations, are presented for LOD KOS as open data. The metrics need to be further tested and aligned with the best practices and international standards of both open data and various types of KOS. Assessment performed with FAIR and FIT metrics support the creation and delivery of user-friendly, discoverable and interoperable LOD KOS datasets which can be used for innovative applications, act as a knowledge base, become a foundation of semantic analysis and entity extractions and enhance research in science and the humanities. Our research provides best practice guidelines for LOD KOS as value vocabularies."
Equalities between h-type Indices and Definitions of Rational h-type Indicators,"To show for which publication-citation arrays h-type indices are equal and to reconsider rational h-type indices. Results for these research questions fill some gaps in existing basic knowledge about h-type indices. The results and introduction of new indicators are based on well-known definitions. The research purpose has been reached: Answers to the first questions are obtained and new indicators are defined. h-type indices do not meet the Bouyssou-Marchant independence requirement. On the one hand, more insight has been obtained for well-known indices such as the h- A nd the g-index and on the other hand, simple extensions of existing indicators have been added to the bibliometric toolbox. Relative rational h-type indices are more useful for individuals than the existing absolute ones. Answers to basic questions such as ""when are the values of two h-type indices equal"" are provided. A new rational h-index is introduced. Â© 2019 Leo Egghe, Yves Fassin, Ronald Rousseau, published by Sciendo.","To show for which publication-citation arrays h-type indices are equal and to reconsider rational h-type indices. Results for these research questions fill some gaps in existing basic knowledge about h-type indices. The results and introduction of new indicators are based on well-known definitions. The research purpose has been reached: Answers to the first questions are obtained and new indicators are defined. h-type indices do not meet the Bouyssou-Marchant independence requirement. On the one hand, more insight has been obtained for well-known indices such as the h- A nd the g-index and on the other hand, simple extensions of existing indicators have been added to the bibliometric toolbox. Relative rational h-type indices are more useful for individuals than the existing absolute ones. Answers to basic questions such as ""when are the values of two h-type indices equal"" are provided. A new rational h-index is introduced."
Describing Citations as a Function of Time,"Providing an overview of types of citation curves. The terms citation curves or citation graphs are made explicit. A framework for the study of diachronous (and synchronous) citation curves is proposed. No new practical applications are given. This short note about citation curves will help readers to make the optimal choice for their applications. A new scheme for the meaning of the term ""citation curve""is designed.  Â© 2020 Xiaojun Hu et al., published by Sciendo.","Providing an overview of types of citation curves. The terms citation curves or citation graphs are made explicit. A framework for the study of diachronous (and synchronous) citation curves is proposed. No new practical applications are given. This short note about citation curves will help readers to make the optimal choice for their applications. A new scheme for the meaning of the term ""citation curve""is designed."
Priorities for Social and Humanities Projects Based on Text Analysis,"Changes in the world show that the role, importance, and coherence of SSH (social sciences and the humanities) will increase significantly in the coming years. This paper aims to monitor and analyze the evolution (or overlapping) of the SSH thematic pattern through three funding instruments since 2007. The goal of the paper is to check to what extent the EU Framework Program (FP) affects/does not affect research on national level, and to highlight hot topics from a given period with the help of text analysis. Funded project titles and abstracts derived from the EU FP, Slovenian, and Estonian RIS were used. The final analysis and comparisons between different datasets were made based on the 200 most frequent words. After removing punctuation marks, numeric values, articles, prepositions, conjunctions, and auxiliary verbs, 4,854 unique words in ETIS, 4,421 unique words in the Slovenian Research Information System (SICRIS), and 3,950 unique words in FP were identified. Across all funding instruments, about a quarter of the top words constitute half of the word occurrences. The text analysis results show that in the majority of cases words do not overlap between FP and nationally funded projects. In some cases, it may be due to using different vocabulary. There is more overlapping between words in the case of Slovenia (SL) and Estonia (EE) and less in the case of Estonia and EU Framework Programmes (FP). At the same time, overlapping words indicate a wider reach (culture, education, social, history, human, innovation, etc.). In nationally funded projects (bottom-up), it was relatively difficult to observe the change in thematic trends over time. More specific results emerged from the comparison of the different programs throughout FP (top-down). Only projects with English titles and abstracts were analyzed. The specifics of SSH have to take into account - the one-to-one meaning of terms/words is not as important as, for example, in the exact sciences. Thus, even in co-word analysis, the final content may go unnoticed. This was the first attempt to monitor the trends of SSH projects using text analysis. The text analysis of the SSH projects of the two new EU Member States used in the study showed that SSH's thematic coverage is not much affected by the EU Framework Program. Whether this result is field-specific or country-specific should be shown in the following study, which targets SSH projects in the so-called old Member States.  Â© 2020 2020 Ãlle Must, published by Sciendo.","Changes in the world show that the role, importance, and coherence of SSH (social sciences and the humanities) will increase significantly in the coming years. This paper aims to monitor and analyze the evolution (or overlapping) of the SSH thematic pattern through three funding instruments since 2007. The goal of the paper is to check to what extent the EU Framework Program (FP) affects/does not affect research on national level, and to highlight hot topics from a given period with the help of text analysis. Funded project titles and abstracts derived from the EU FP, Slovenian, and Estonian RIS were used. The final analysis and comparisons between different datasets were made based on the 200 most frequent words. After removing punctuation marks, numeric values, articles, prepositions, conjunctions, and auxiliary verbs, 4,854 unique words in ETIS, 4,421 unique words in the Slovenian Research Information System (SICRIS), and 3,950 unique words in FP were identified. Across all funding instruments, about a quarter of the top words constitute half of the word occurrences. The text analysis results show that in the majority of cases words do not overlap between FP and nationally funded projects. In some cases, it may be due to using different vocabulary. There is more overlapping between words in the case of Slovenia (SL) and Estonia (EE) and less in the case of Estonia and EU Framework Programmes (FP). At the same time, overlapping words indicate a wider reach (culture, education, social, history, human, innovation, etc.). In nationally funded projects (bottom-up), it was relatively difficult to observe the change in thematic trends over time. More specific results emerged from the comparison of the different programs throughout FP (top-down). Only projects with English titles and abstracts were analyzed. The specifics of SSH have to take into account - the one-to-one meaning of terms/words is not as important as, for example, in the exact sciences. Thus, even in co-word analysis, the final content may go unnoticed. This was the first attempt to monitor the trends of SSH projects using text analysis. The text analysis of the SSH projects of the two new EU Member States used in the study showed that SSH's thematic coverage is not much affected by the EU Framework Program. Whether this result is field-specific or country-specific should be shown in the following study, which targets SSH projects in the so-called old Member States."
A criteria-based assessment of the coverage of scopus and web of science,"The purpose of this study is to assess the coverage of the scientific literature in Scopus and Web of Science from the perspective of research evaluation. The academic communities of Norway have agreed on certain criteria for what should be included as original research publications in research evaluation and funding contexts. These criteria have been applied since 2004 in a comprehensive bibliographic database called the Norwegian Science Index (NSI). The relative coverages of Scopus and Web of Science are compared with regard to publication type, field of research and language. Our results show that Scopus covers 72 percent of the total Norwegian scientific and scholarly publication output in 2015 and 2016, while the corresponding figure for Web of Science Core Collection is 69 percent. The coverages are most comprehensive in medicine and health (89 and 87 percent) and in the natural sciences and technology (85 and 84 percent). The social sciences (48 percent in Scopus and 40 percent in Web of Science Core Collection) and particularly the humanities (27 and 23 percent) are much less covered in the two international data sources. Comparing with data from only one country is a limitation of the study, but the criteria used to define a country's scientific output as well as the identification of patterns of field-dependent partial representations in Scopus and Web of Science should be recognizable and useful also for other countries. The novelty of this study is the criteria-based approach to studying coverage problems in the two data sources. Â© 2019 Dag W. Aksnes, Gunnar Sivertsen, published by Sciendo.","The purpose of this study is to assess the coverage of the scientific literature in Scopus and Web of Science from the perspective of research evaluation. The academic communities of Norway have agreed on certain criteria for what should be included as original research publications in research evaluation and funding contexts. These criteria have been applied since 2004 in a comprehensive bibliographic database called the Norwegian Science Index (NSI). The relative coverages of Scopus and Web of Science are compared with regard to publication type, field of research and language. Our results show that Scopus covers 72 percent of the total Norwegian scientific and scholarly publication output in 2015 and 2016, while the corresponding figure for Web of Science Core Collection is 69 percent. The coverages are most comprehensive in medicine and health (89 and 87 percent) and in the natural sciences and technology (85 and 84 percent). The social sciences (48 percent in Scopus and 40 percent in Web of Science Core Collection) and particularly the humanities (27 and 23 percent) are much less covered in the two international data sources. Comparing with data from only one country is a limitation of the study, but the criteria used to define a country's scientific output as well as the identification of patterns of field-dependent partial representations in Scopus and Web of Science should be recognizable and useful also for other countries. The novelty of this study is the criteria-based approach to studying coverage problems in the two data sources."
Evidence-based Nomenclature and Taxonomy of Research Impact Indicators,"This study aims to classify research impact indicators based on their characteristics and scope. A concept of evidence-based nomenclature of research impact (RI) indicator has been introduced for generalization and transformation of scope. Literature was collected related to the research impact assessment. It was categorized in conceptual and applied case studies. One hundred and nineteen indicators were selected to prepare classification and nomenclature. The nomenclature was developed based on the principle-""every indicator is a contextual-function to explain the impact"". Every indicator was disintegrated into three parts, i.e. Function, Domain, and Target Areas. The main functions of research impact indicators express improvement (63%), recognition (23%), and creation/development (14%). The focus of research impact indicators in literature is more towards the academic domain (59%) whereas the environment/sustainability domain is least considered (4%). As a result, research impact related to the research aspects is felt the most (29%). Other target areas include system and services, methods and procedures, networking, planning, policy development, economic aspects and commercialisation, etc. This research applied to 119 research impact indicators. However, the inclusion of additional indicators may change the result. The plausible effect of nomenclature is a better organization of indicators with appropriate tags of functions, domains, and target areas. This approach also provides a framework of indicator generalization and transformation. Therefore, similar indicators can be applied in other fields and target areas with modifications. The development of nomenclature for research impact indicators is a novel approach in scientometrics. It is developed on the same line as presented in other scientific disciplines, where fundamental objects need to classify on common standards such as biology and chemistry.  Â© 2020 2020 Mudassar Arsalan et al., published by Sciendo.","This study aims to classify research impact indicators based on their characteristics and scope. A concept of evidence-based nomenclature of research impact (RI) indicator has been introduced for generalization and transformation of scope. Literature was collected related to the research impact assessment. It was categorized in conceptual and applied case studies. One hundred and nineteen indicators were selected to prepare classification and nomenclature. The nomenclature was developed based on the principle-""every indicator is a contextual-function to explain the impact"". Every indicator was disintegrated into three parts, Function, Domain, and Target Areas. The main functions of research impact indicators express improvement (63%), recognition (23%), and creation/development (14%). The focus of research impact indicators in literature is more towards the academic domain (59%) whereas the environment/sustainability domain is least considered (4%). As a result, research impact related to the research aspects is felt the most (29%). Other target areas include system and services, methods and procedures, networking, planning, policy development, economic aspects and commercialisation, etc. This research applied to 119 research impact indicators. However, the inclusion of additional indicators may change the result. The plausible effect of nomenclature is a better organization of indicators with appropriate tags of functions, domains, and target areas. This approach also provides a framework of indicator generalization and transformation. Therefore, similar indicators can be applied in other fields and target areas with modifications. The development of nomenclature for research impact indicators is a novel approach in scientometrics. It is developed on the same line as presented in other scientific disciplines, where fundamental objects need to classify on common standards such as biology and chemistry."
An Automatic Method to Identify Citations to Journals in News Stories: A Case Study of UK Newspapers Citing Web of Science Journals,"Communicating scientific results to the public is essential to inspire future researchers and ensure that discoveries are exploited. News stories about research are a key communication pathway for this and have been manually monitored to assess the extent of press coverage of scholarship. To make larger scale studies practical, this paper introduces an automatic method to extract citations from newspaper stories to large sets of academic journals. Curated ProQuest queries were used to search for citations to 9,639 Science and 3,412 Social Science Web of Science (WoS) journals from eight UK daily newspapers during 2006-2015. False matches were automatically filtered out by a new program, with 94% of the remaining stories meaningfully citing research. Most Science (95%) and Social Science (94%) journals were never cited by these newspapers. Half of the cited Science journals covered medical or health-related topics, whereas 43% of the Social Sciences journals were related to psychiatry or psychology. From the citing news stories, 60% described research extensively and 53% used multiple sources, but few commented on research quality. The method has only been tested in English and from the ProQuest Newspapers database. Others can use the new method to systematically harvest press coverage of research. An automatic method was introduced and tested to extract citations from newspaper stories to large sets of academic journals. Â© 2019Kayvan Kousha, Mike Thelwall, published by Sciendo.","Communicating scientific results to the public is essential to inspire future researchers and ensure that discoveries are exploited. News stories about research are a key communication pathway for this and have been manually monitored to assess the extent of press coverage of scholarship. To make larger scale studies practical, this paper introduces an automatic method to extract citations from newspaper stories to large sets of academic journals. Curated ProQuest queries were used to search for citations to 9,639 Science and 3,412 Social Science Web of Science (WoS) journals from eight UK daily newspapers during 2006-2015. False matches were automatically filtered out by a new program, with 94% of the remaining stories meaningfully citing research. Most Science (95%) and Social Science (94%) journals were never cited by these newspapers. Half of the cited Science journals covered medical or health-related topics, whereas 43% of the Social Sciences journals were related to psychiatry or psychology. From the citing news stories, 60% described research extensively and 53% used multiple sources, but few commented on research quality. The method has only been tested in English and from the ProQuest Newspapers database. Others can use the new method to systematically harvest press coverage of research. An automatic method was introduced and tested to extract citations from newspaper stories to large sets of academic journals."
Disclosing and Evaluating Artistic Research,"This study expands on the results of a stakeholder-driven research project on quality indicators and output assessment of art and design research in Flanders-the Northern, Dutch-speaking region of Belgium. Herein, it emphasizes the value of arts & design output registration as a modality to articulate the disciplinary demarcations of art and design research. The particularity of art and design research in Flanders is first analyzed and compared to international examples. Hereafter, the results of the stakeholder-driven project on the creation of indicators for arts & design research output assessment are discussed. The findings accentuate the importance of allowing an assessment culture to emerge from practitioners themselves, instead of imposing ill-suited methods borrowed from established scientific evaluation models (Biggs & Karlsson, 2011)-notwithstanding the practical difficulties it generates. They point to the potential of stakeholder-driven approaches for artistic research, which benefits from constructing a shared metadiscourse among its practitioners regarding the continuities and discontinuities between ""artistic"" and ""traditional"" research, and the communal goals and values that guide its knowledge production (Biggs & Karlsson, 2011; HellstrÃ¶m, 2010; Ysebaert & Martens, 2018). The central limitation of the study is that it focuses exclusively on the ""Architecture & Design"" panel of the project, and does not account for intra-disciplinary complexities in output assessment. The goal of the research project is to create a robust assessment system for arts & design research in Flanders, which may later guide similar international projects. This study is currently the only one to consider the productive potential of (collaborative) PRFSs for artistic research. Â© 2019Florian Vanlee, Walter Ysebaert, published by Sciendo.","This study expands on the results of a stakeholder-driven research project on quality indicators and output assessment of art and design research in Flanders-the Northern, Dutch-speaking region of Belgium. Herein, it emphasizes the value of arts & design output registration as a modality to articulate the disciplinary demarcations of art and design research. The particularity of art and design research in Flanders is first analyzed and compared to international examples. Hereafter, the results of the stakeholder-driven project on the creation of indicators for arts & design research output assessment are discussed. The findings accentuate the importance of allowing an assessment culture to emerge from practitioners themselves, instead of imposing ill-suited methods borrowed from established scientific evaluation models (Biggs & Karlsson, 2011)-notwithstanding the practical difficulties it generates. They point to the potential of stakeholder-driven approaches for artistic research, which benefits from constructing a shared metadiscourse among its practitioners regarding the continuities and discontinuities between ""artistic"" and ""traditional"" research, and the communal goals and values that guide its knowledge production (Biggs & Karlsson, 2011; Hellstrm, 2010; Ysebaert & Martens, 2018). The central limitation of the study is that it focuses exclusively on the ""Architecture & Design"" panel of the project, and does not account for intra-disciplinary complexities in output assessment. The goal of the research project is to create a robust assessment system for arts & design research in Flanders, which may later guide similar international projects. This study is currently the only one to consider the productive potential of (collaborative) PRFSs for artistic research."
Methods and Practices for Institutional Benchmarking based on Research Impact and Competitiveness: A Case Study of ShanghaiTech University,"To develop and test a mission-oriented and multi-dimensional benchmarking method for a small scale university aiming for internationally first-class basic research. An individualized evidence-based assessment scheme was employed to benchmark ShanghaiTech University against selected top research institutions, focusing on research impact and competitiveness at the institutional and disciplinary levels. Topic maps opposing ShanghaiTech and corresponding top institutions were produced for the main research disciplines of ShanghaiTech. This provides opportunities for further exploration of strengths and weakness. This study establishes a preliminary framework for assessing the mission of the university. It further provides assessment principles, assessment questions, and indicators. Analytical methods and data sources were tested and proved to be applicable and efficient. To better fit the selective research focuses of this university, its schema of research disciplines needs to be re-organized and benchmarking targets should include disciplinary top institutions and not necessarily those universities leading overall rankings. Current reliance on research articles and certain databases may neglect important research output types. This study provides a working framework and practical methods for mission-oriented, individual, and multi-dimensional benchmarking that ShanghaiTech decided to use for periodical assessments. It also offers a working reference for other institutions to adapt. Further needs are identified so that ShanghaiTech can tackle them for future benchmarking. This is an effort to develop a mission-oriented, individually designed, systematically structured, and multi-dimensional assessment methodology which differs from often used composite indices. Â© 2019Jiang Chang, Jianhua Liu, published by Sciendo.","To develop and test a mission-oriented and multi-dimensional benchmarking method for a small scale university aiming for internationally first-class basic research. An individualized evidence-based assessment scheme was employed to benchmark ShanghaiTech University against selected top research institutions, focusing on research impact and competitiveness at the institutional and disciplinary levels. Topic maps opposing ShanghaiTech and corresponding top institutions were produced for the main research disciplines of ShanghaiTech. This provides opportunities for further exploration of strengths and weakness. This study establishes a preliminary framework for assessing the mission of the university. It further provides assessment principles, assessment questions, and indicators. Analytical methods and data sources were tested and proved to be applicable and efficient. To better fit the selective research focuses of this university, its schema of research disciplines needs to be re-organized and benchmarking targets should include disciplinary top institutions and not necessarily those universities leading overall rankings. Current reliance on research articles and certain databases may neglect important research output types. This study provides a working framework and practical methods for mission-oriented, individual, and multi-dimensional benchmarking that ShanghaiTech decided to use for periodical assessments. It also offers a working reference for other institutions to adapt. Further needs are identified so that ShanghaiTech can tackle them for future benchmarking. This is an effort to develop a mission-oriented, individually designed, systematically structured, and multi-dimensional assessment methodology which differs from often used composite indices."
Normalizing Book Citations in Google Scholar: A Hybrid Cited-side Citing-side Method,"To design and test a method for normalizing book citations in Google Scholar. A hybrid citing-side, cited-side normalization method was developed and this was tested on a sample of 285 research monographs. The results were analyzed and conclusions drawn. The method was technically feasible but required extensive manual intervention because of the poor quality of the Google Scholar data. The sample of books was limited and also all were from one discipline - business and management. Also, the method has only been tested on Google Scholar, it would be useful to test it on Web of Science or Scopus. Google Scholar is a poor source of data although it does cover a much wider range citation sources that other databases. This is the first method that has been developed specifically for normalizing books which have so far not been able to be normalized. Â© 2019 John Mingers,Eren Kaymaz, published by Sciendo.","To design and test a method for normalizing book citations in Google Scholar. A hybrid citing-side, cited-side normalization method was developed and this was tested on a sample of 285 research monographs. The results were analyzed and conclusions drawn. The method was technically feasible but required extensive manual intervention because of the poor quality of the Google Scholar data. The sample of books was limited and also all were from one discipline - business and management. Also, the method has only been tested on Google Scholar, it would be useful to test it on Web of Science or Scopus. Google Scholar is a poor source of data although it does cover a much wider range citation sources that other databases. This is the first method that has been developed specifically for normalizing books which have so far not been able to be normalized."
Are Contributions from Chinese Physicists Undercited?,"In this work, we want to examine whether or not there are some scientific fields to which contributions from Chinese scholars have been under or over cited. We do so by comparing the number of received citations and the IOF of publications in each scientific field from each country. The IOF is calculated from applying the modified closed system input-output analysis (MCSIOA) to the citation network. MCSIOA is a PageRank-like algorithm which means here that citations from the more influential subfields are weighted more towards the IOF. About 40% of subfields in physics in China are undercited, meaning that their net influence ranks are higher (better) than the direct rank, while about 75% of subfields in the USA and German are undercited. Only APS data is analyzed in this work. The expected citation influence is assumed to be represented by the IOF, and this can be wrong. MCSIOA provides a measure of net influences and according to that measure. Overall, Chinese physicists' publications are more likely overcited rather than being undercited. The issue of under or over cited has been analyzed in this work using MCSIOA. Â© 2019 2019 Jinzhong Guo, Xiaoling Liu, Liying Yang, Jinshan Wu, published by Sciendo.","In this work, we want to examine whether or not there are some scientific fields to which contributions from Chinese scholars have been under or over cited. We do so by comparing the number of received citations and the IOF of publications in each scientific field from each country. The IOF is calculated from applying the modified closed system input-output analysis (MCSIOA) to the citation network. MCSIOA is a PageRank-like algorithm which means here that citations from the more influential subfields are weighted more towards the IOF. About 40% of subfields in physics in China are undercited, meaning that their net influence ranks are higher (better) than the direct rank, while about 75% of subfields in the USA and German are undercited. Only APS data is analyzed in this work. The expected citation influence is assumed to be represented by the IOF, and this can be wrong. MCSIOA provides a measure of net influences and according to that measure. Overall, Chinese physicists' publications are more likely overcited rather than being undercited. The issue of under or over cited has been analyzed in this work using MCSIOA."
Exploring the Potentialities of Automatic Extraction of University Webometric Information,"The main objective of this work is to show the potentialities of recently developed approaches for automatic knowledge extraction directly from the universities' websites. The information automatically extracted can be potentially updated with a frequency higher than once per year, and be safe from manipulations or misinterpretations. Moreover, this approach allows us flexibility in collecting indicators about the efficiency of universities' websites and their effectiveness in disseminating key contents. These new indicators can complement traditional indicators of scientific research (e.g. number of articles and number of citations) and teaching (e.g. number of students and graduates) by introducing further dimensions to allow new insights for ""profiling""the analyzed universities. Webometrics relies on web mining methods and techniques to perform quantitative analyses of the web. This study implements an advanced application of the webometric approach, exploiting all the three categories of web mining: web content mining; web structure mining; web usage mining. The information to compute our indicators has been extracted from the universities' websites by using web scraping and text mining techniques. The scraped information has been stored in a NoSQL DB according to a semi-structured form to allow for retrieving information efficiently by text mining techniques. This provides increased flexibility in the design of new indicators, opening the door to new types of analyses. Some data have also been collected by means of batch interrogations of search engines (Bing, www.bing.com) or from a leading provider of Web analytics (SimilarWeb, http://www.similarweb.com). The information extracted from the Web has been combined with the University structural information taken from the European Tertiary Education Register (https://eter.joanneum.at/#/home), a database collecting information on Higher Education Institutions (HEIs) at European level. All the above was used to perform a clusterization of 79 Italian universities based on structural and digital indicators. The main findings of this study concern the evaluation of the potential in digitalization of universities, in particular by presenting techniques for the automatic extraction of information from the web to build indicators of quality and impact of universities' websites. These indicators can complement traditional indicators and can be used to identify groups of universities with common features using clustering techniques working with the above indicators. The results reported in this study refers to Italian universities only, but the approach could be extended to other university systems abroad. The approach proposed in this study and its illustration on Italian universities show the usefulness of recently introduced automatic data extraction and web scraping approaches and its practical relevance for characterizing and profiling the activities of universities on the basis of their websites. The approach could be applied to other university systems. This work applies for the first time to university websites some recently introduced techniques for automatic knowledge extraction based on web scraping, optical character recognition and nontrivial text mining operations (Bruni & Bianchi, 2020).  Â© 2020 2020 Gianpiero Bianchi et al., published by Sciendo.","The main objective of this work is to show the potentialities of recently developed approaches for automatic knowledge extraction directly from the universities' websites. The information automatically extracted can be potentially updated with a frequency higher than once per year, and be safe from manipulations or misinterpretations. Moreover, this approach allows us flexibility in collecting indicators about the efficiency of universities' websites and their effectiveness in disseminating key contents. These new indicators can complement traditional indicators of scientific research ( number of articles and number of citations) and teaching ( number of students and graduates) by introducing further dimensions to allow new insights for ""profiling""the analyzed universities. Webometrics relies on web mining methods and techniques to perform quantitative analyses of the web. This study implements an advanced application of the webometric approach, exploiting all the three categories of web mining: web content mining; web structure mining; web usage mining. The information to compute our indicators has been extracted from the universities' websites by using web scraping and text mining techniques. The scraped information has been stored in a NoSQL DB according to a semi-structured form to allow for retrieving information efficiently by text mining techniques. This provides increased flexibility in the design of new indicators, opening the door to new types of analyses. Some data have also been collected by means of batch interrogations of search engines (Bing, www.bing.com) or from a leading provider of Web analytics (SimilarWeb, http://www.similarweb.com). The information extracted from the Web has been combined with the University structural information taken from the European Tertiary Education Register (https://eter.joanneum.at/#/home), a database collecting information on Higher Education Institutions (HEIs) at European level. All the above was used to perform a clusterization of 79 Italian universities based on structural and digital indicators. The main findings of this study concern the evaluation of the potential in digitalization of universities, in particular by presenting techniques for the automatic extraction of information from the web to build indicators of quality and impact of universities' websites. These indicators can complement traditional indicators and can be used to identify groups of universities with common features using clustering techniques working with the above indicators. The results reported in this study refers to Italian universities only, but the approach could be extended to other university systems abroad. The approach proposed in this study and its illustration on Italian universities show the usefulness of recently introduced automatic data extraction and web scraping approaches and its practical relevance for characterizing and profiling the activities of universities on the basis of their websites. The approach could be applied to other university systems. This work applies for the first time to university websites some recently introduced techniques for automatic knowledge extraction based on web scraping, optical character recognition and nontrivial text mining operations (Bruni & Bianchi, 2020)."
The Association between Researchers' Conceptions of Research and Their Strategic Research Agendas,"In studies of the research process, the association between how researchers conceptualize research and their strategic research agendas has been largely overlooked. This study aims to address this gap. This study analyzes this relationship using a dataset of more than 8,500 researchers across all scientific fields and the globe. It studies the associations between the dimensions of two inventories: the Conceptions of Research Inventory (CoRI) and the Multi-Dimensional Research Agenda Inventory - Revised (MDRAI-R). The findings show a relatively strong association between researchers' conceptions of research and their research agendas. While all conceptions of research are positively related to scientific ambition, the findings are mixed regarding how the dimensions of the two inventories relate to one another, which is significant for those seeking to understand the knowledge production process better. The study relies on self-reported data, which always carries a risk of response bias. The findings provide a greater understanding of the inner workings of knowledge processes and indicate that the two inventories, whether used individually or in combination, may provide complementary analytical perspectives to research performance indicators. They may thus offer important insights for managers of research environments regarding how to assess the research culture, beliefs, and conceptualizations of individual researchers and research teams when designing strategies to promote specific institutional research focuses and strategies. To the best of the authors' knowledge, this is the first study to associate research agendas and conceptions of research. It is based on a large sample of researchers working worldwide and in all fields of knowledge, which ensures that the findings have a reasonable degree of generalizability to the global population of researchers.  Â© 2020 2020 JoÃ£o M. Santos et al., published by Sciendo.","In studies of the research process, the association between how researchers conceptualize research and their strategic research agendas has been largely overlooked. This study aims to address this gap. This study analyzes this relationship using a dataset of more than 8,500 researchers across all scientific fields and the globe. It studies the associations between the dimensions of two inventories: the Conceptions of Research Inventory (CoRI) and the Multi-Dimensional Research Agenda Inventory - Revised (MDRAI-R). The findings show a relatively strong association between researchers' conceptions of research and their research agendas. While all conceptions of research are positively related to scientific ambition, the findings are mixed regarding how the dimensions of the two inventories relate to one another, which is significant for those seeking to understand the knowledge production process better. The study relies on self-reported data, which always carries a risk of response bias. The findings provide a greater understanding of the inner workings of knowledge processes and indicate that the two inventories, whether used individually or in combination, may provide complementary analytical perspectives to research performance indicators. They may thus offer important insights for managers of research environments regarding how to assess the research culture, beliefs, and conceptualizations of individual researchers and research teams when designing strategies to promote specific institutional research focuses and strategies. To the best of the authors' knowledge, this is the first study to associate research agendas and conceptions of research. It is based on a large sample of researchers working worldwide and in all fields of knowledge, which ensures that the findings have a reasonable degree of generalizability to the global population of researchers."
Practice and Challenge of International Peer Review: A Case Study of Research Evaluation of CAS Centers for Excellence,"Purpose: The main goal of this paper is to show that international peer review can work in China's context with satisfactory outcomes. Moreover, this paper also provides a reference for the practice of science and technology management. Design/methodology/approach:This paper starts with a discussion of two critical questions about the significance and design of international peer review. A case study of international peer review of CAS Centers for Excellence is further analyzed. International peer review may provide a solution to address the problem of quantitative oriented research evaluation in China. The case study of research evaluation of CAS Centers for Excellence shows that it is possible and feasible to conduct an international peer review in China's context. When applying this approach to other scenarios, there are still many issues to consider including individualized design of international peer review combined with practical demands, and further improvement of theories and methods of international peer review. 1) Only the case of international peer review of CAS Centers for Excellence is analyzed; 2) A relatively small number of respondents were surveyed in the questionnaire. The work presented in this study can be used as a reference for future studies. Currently, there are no similarly detailed studies exploring the significance and methodology of international peer review in China. Â© 2019Fang Xu, Xiaoxuan Li, published by Sciendo.","The main goal of this paper is to show that international peer review can work in China's context with satisfactory outcomes. Moreover, this paper also provides a reference for the practice of science and technology management. This paper starts with a discussion of two critical questions about the significance and design of international peer review. A case study of international peer review of CAS Centers for Excellence is further analyzed. International peer review may provide a solution to address the problem of quantitative oriented research evaluation in China. The case study of research evaluation of CAS Centers for Excellence shows that it is possible and feasible to conduct an international peer review in China's context. When applying this approach to other scenarios, there are still many issues to consider including individualized design of international peer review combined with practical demands, and further improvement of theories and methods of international peer review. 1) Only the case of international peer review of CAS Centers for Excellence is analyzed; 2) A relatively small number of respondents were surveyed in the questionnaire. The work presented in this study can be used as a reference for future studies. Currently, there are no similarly detailed studies exploring the significance and methodology of international peer review in China."
Using Network Embedding to Obtain a Richer and More Stable Network Layout for a Large Scale Bibliometric Network,"The goal of this study is to explore whether deep learning based embedded models can provide a better visualization solution for large citation networks. Our team compared the visualization approach borrowed from the deep learning community with the well-known bibliometric network visualization for large scale data. 47,294 highly cited papers were visualized by using three network embedding models plus the t-SNE dimensionality reduction technique. Besides, three base maps were created with the same dataset for evaluation purposes. All base maps used the classic OpenOrd method with different edge cutting strategies and parameters. The network embedded maps with t-SNE preserve a very similar global structure to the full edges classic force-directed map, while the maps vary in local structure. Among them, the Node2Vec model has the best overall visualization performance, the local structure has been significantly improved and the maps' layout has very high stability. The computational and time costs of training are very high for network embedded models to obtain high dimensional latent vector. Only one dimensionality reduction technique was tested. This paper demonstrates that the network embedding models are able to accurately reconstruct the large bibliometric network in the vector space. In the future, apart from network visualization, many classical vector-based machine learning algorithms can be applied to network representations for solving bibliometric analysis tasks. This paper provides the first systematic comparison of classical science mapping visualization with network embedding based visualization on a large scale dataset. We showed deep learning based network embedding model with t-SNE can provide a richer, more stable science map. We also designed a practical evaluation method to investigate and compare maps.  Â© 2021 2021 Ting Chen et al., published by Sciendo.","The goal of this study is to explore whether deep learning based embedded models can provide a better visualization solution for large citation networks. Our team compared the visualization approach borrowed from the deep learning community with the well-known bibliometric network visualization for large scale data. 47,294 highly cited papers were visualized by using three network embedding models plus the t-SNE dimensionality reduction technique. Besides, three base maps were created with the same dataset for evaluation purposes. All base maps used the classic OpenOrd method with different edge cutting strategies and parameters. The network embedded maps with t-SNE preserve a very similar global structure to the full edges classic force-directed map, while the maps vary in local structure. Among them, the Node2Vec model has the best overall visualization performance, the local structure has been significantly improved and the maps' layout has very high stability. The computational and time costs of training are very high for network embedded models to obtain high dimensional latent vector. Only one dimensionality reduction technique was tested. This paper demonstrates that the network embedding models are able to accurately reconstruct the large bibliometric network in the vector space. In the future, apart from network visualization, many classical vector-based machine learning algorithms can be applied to network representations for solving bibliometric analysis tasks. This paper provides the first systematic comparison of classical science mapping visualization with network embedding based visualization on a large scale dataset. We showed deep learning based network embedding model with t-SNE can provide a richer, more stable science map. We also designed a practical evaluation method to investigate and compare maps."
"""my ADHD Hellbrain"": A Twitter Data Science Perspective on a Behavioural Disorder","Attention deficit hyperactivity disorder (ADHD) is a common behavioural condition. This article introduces a new data science method, word association thematic analysis, to investigate whether ADHD tweets can give insights into patient concerns and online communication needs. Tweets matching ""my ADHD""(n=58,893) and 99 other conditions (n=1,341,442) were gathered and two thematic analyses conducted. Analysis 1: A standard thematic analysis of ADHD-related tweets. Analysis 2: A word association thematic analysis of themes unique to ADHD. The themes that emerged from the two analyses included people ascribing their brains agency to explain and justify their symptoms and using the concept of neurodivergence for a positive self-image. This is a single case study and the results may differ for other topics. Health professionals should be sensitive to patients' needs to understand their behaviour, find ways to justify and explain it to others and to be positive about their condition. Word association thematic analysis can give new insights into the (self-reported) patient perspective.  Â© 2021 2021 Mike Thelwall et al., published by Sciendo.","Attention deficit hyperactivity disorder (ADHD) is a common behavioural condition. This article introduces a new data science method, word association thematic analysis, to investigate whether ADHD tweets can give insights into patient concerns and online communication needs. Tweets matching ""my ADHD""(n=58,893) and 99 other conditions (n=1,341,442) were gathered and two thematic analyses conducted. Analysis 1: A standard thematic analysis of ADHD-related tweets. Analysis 2: A word association thematic analysis of themes unique to ADHD. The themes that emerged from the two analyses included people ascribing their brains agency to explain and justify their symptoms and using the concept of neurodivergence for a positive self-image. This is a single case study and the results may differ for other topics. Health professionals should be sensitive to patients' needs to understand their behaviour, find ways to justify and explain it to others and to be positive about their condition. Word association thematic analysis can give new insights into the (self-reported) patient perspective."
Does a Country/Region's Economic Status Affect Its Universities' Presence in International Rankings?,"Purpose: Study how economic parameters affect positions in the Academic Ranking of World Universities' top 500 published by the Shanghai Jiao Tong University Graduate School of Education in countries/regions with listed higher education institutions. Design/methodology/approach: The methodology used capitalises on the multi-variate characteristics of the data analysed. The multi-colinearity problem posed is solved by running principal components prior to regression analysis, using both classical (OLS) and robust (Huber and Tukey) methods. Findings: Our results revealed that countries/regions with long ranking traditions are highly competitive. Findings also showed that some countries/regions such as Germany, United Kingdom, Canada, and Italy, had a larger number of universities in the top positions than predicted by the regression model. In contrast, for Japan, a country where social and economic performance is high, the number of ARWU universities projected by the model was much larger than the actual figure. In much the same vein, countries/regions that invest heavily in education, such as Japan and Denmark, had lower than expected results. Research limitations: Using data from only one ranking is a limitation of this study, but the methodology used could be useful to other global rankings. Practical implications: The results provide good insights for policy makers. They indicate the existence of a relationship between research output and the number of universities per million inhabitants. Countries/regions, which have historically prioritised higher education, exhibited highest values for indicators that compose the rankings methodology; furthermore, minimum increase in welfare indicators could exhibited significant rises in the presence of their universities on the rankings. Originality/value: This study is well defined and the result answers important questions about characteristics of countries/regions and their higher education system. Â© 2019 Esteban FernÃ¡ndez Tuesta, Carlos Garcia-Zorita, Rosario Romera Ayllon, ElÃ­as Sanz-Casado, publsihed by Sceindo.","Study how economic parameters affect positions in the Academic Ranking of World Universities' top 500 published by the Shanghai Jiao Tong University Graduate School of Education in countries/regions with listed higher education institutions. The methodology used capitalises on the multi-variate characteristics of the data analysed. The multi-colinearity problem posed is solved by running principal components prior to regression analysis, using both classical (OLS) and robust (Huber and Tukey) methods. Our results revealed that countries/regions with long ranking traditions are highly competitive. Findings also showed that some countries/regions such as Germany, United Kingdom, Canada, and Italy, had a larger number of universities in the top positions than predicted by the regression model. In contrast, for Japan, a country where social and economic performance is high, the number of ARWU universities projected by the model was much larger than the actual figure. In much the same vein, countries/regions that invest heavily in education, such as Japan and Denmark, had lower than expected results. Using data from only one ranking is a limitation of this study, but the methodology used could be useful to other global rankings. The results provide good insights for policy makers. They indicate the existence of a relationship between research output and the number of universities per million inhabitants. Countries/regions, which have historically prioritised higher education, exhibited highest values for indicators that compose the rankings methodology; furthermore, minimum increase in welfare indicators could exhibited significant rises in the presence of their universities on the rankings. This study is well defined and the result answers important questions about characteristics of countries/regions and their higher education system."
Functions of Uni- and Multi-citations: Implications for Weighted Citation Analysis,"Purpose: (1) To test basic assumptions underlying frequency-weighted citation analysis: (a) Uni-citations correspond to citations that are nonessential to the citing papers; (b) The influence of a cited paper on the citing paper increases with the frequency with which it is cited in the citing paper. (2) To explore the degree to which citation location may be used to help identify nonessential citations. Design/methodology/approach: Each of the in-text citations in all research articles published in Issue 1 of the Journal of the Association for Information Science and Technology (JASIST) 2016 was manually classified into one of these five categories: Applied, Contrastive, Supportive, Reviewed, and Perfunctory. The distributions of citations at different in-text frequencies and in different locations in the text by these functions were analyzed. Findings: Filtering out nonessential citations before assigning weight is important for frequency-weighted citation analysis. For this purpose, removing citations by location is more effective than re-citation analysis that simply removes uni-citations. Removing all citation occurrences in the Background and Literature Review sections and uni-citations in the Introduction section appears to provide a good balance between filtration and error rates. Research limitations: This case study suffers from the limitation of scalability and generalizability. We took careful measures to reduce the impact of other limitations of the data collection approach used. Relying on the researcher's judgment to attribute citation functions, this approach is unobtrusive but speculative, and can suffer from a low degree of confidence, thus creating reliability concerns. Practical implications: Weighted citation analysis promises to improve citation analysis for research evaluation, knowledge network analysis, knowledge representation, and information retrieval. The present study showed the importance of filtering out nonessential citations before assigning weight in a weighted citation analysis, which may be a significant step forward to realizing these promises. Originality/value: Weighted citation analysis has long been proposed as a theoretical solution to the problem of citation analysis that treats all citations equally, and has attracted increasing research interest in recent years. The present study showed, for the first time, the importance of filtering out nonessential citations in weighted citation analysis, pointing research in this area in a new direction.  Â© 2017 Dangzhi Zhao, Alicia Cappello & Lucinda Johnston.","To test basic assumptions underlying frequency-weighted citation analysis: (a) Uni-citations correspond to citations that are nonessential to the citing papers; (b) The influence of a cited paper on the citing paper increases with the frequency with which it is cited in the citing paper. To explore the degree to which citation location may be used to help identify nonessential citations. Each of the in-text citations in all research articles published in Issue 1 of the Journal of the Association for Information Science and Technology (JASIST) 2016 was manually classified into one of these five categories: Applied, Contrastive, Supportive, Reviewed, and Perfunctory. The distributions of citations at different in-text frequencies and in different locations in the text by these functions were analyzed. Filtering out nonessential citations before assigning weight is important for frequency-weighted citation analysis. For this purpose, removing citations by location is more effective than re-citation analysis that simply removes uni-citations. Removing all citation occurrences in the Background and Literature Review sections and uni-citations in the Introduction section appears to provide a good balance between filtration and error rates. This case study suffers from the limitation of scalability and generalizability. We took careful measures to reduce the impact of other limitations of the data collection approach used. Relying on the researcher's judgment to attribute citation functions, this approach is unobtrusive but speculative, and can suffer from a low degree of confidence, thus creating reliability concerns. Weighted citation analysis promises to improve citation analysis for research evaluation, knowledge network analysis, knowledge representation, and information retrieval. The present study showed the importance of filtering out nonessential citations before assigning weight in a weighted citation analysis, which may be a significant step forward to realizing these promises. Weighted citation analysis has long been proposed as a theoretical solution to the problem of citation analysis that treats all citations equally, and has attracted increasing research interest in recent years. The present study showed, for the first time, the importance of filtering out nonessential citations in weighted citation analysis, pointing research in this area in a new direction."
Identifying scientific project-generated data citation from full-text articles: An investigation of TCGA data citation,"Purpose: In the open science era, it is typical to share project-generated scientific data by depositing it in an open and accessible database. Moreover, scientific publications are preserved in a digital library archive. It is challenging to identify the data usage that is mentioned in literature and associate it with its source. Here, we investigated the data usage of a government-funded cancer genomics project, The Cancer Genome Atlas (TCGA), via a full-text literature analysis. Design/methodology/approach: We focused on identifying articles using the TCGA dataset and constructing linkages between the articles and the specific TCGA dataset. First, we collected 5,372 TCGA-related articles from PubMed Central (PMC). Second, we constructed a benchmark set with 25 full-text articles that truly used the TCGA data in their studies, and we summarized the key features of the benchmark set. Third, the key features were applied to the remaining PMC full-text articles that were collected from PMC. Findings: The amount of publications that use TCGA data has increased significantly since 2011, although the TCGA project was launched in 2005. Additionally, we found that the critical areas of focus in the studies that use the TCGA data were glioblastoma multiforme, lung cancer, and breast cancer; meanwhile, data from the RNA-sequencing (RNA-seq) platform is the most preferable for use. Research limitations: The current workflow to identify articles that truly used TCGA data is labor-intensive. An automatic method is expected to improve the performance. Practical implications: This study will help cancer genomics researchers determine the latest advancements in cancer molecular therapy, and it will promote data sharing and data-intensive scientific discovery. Originality/value: Few studies have been conducted to investigate data usage by government-funded projects/programs since their launch. In this preliminary study, we extracted articles that use TCGA data from PMC, and we created a link between the full-text articles and the source data. Â© Journal of Data and Information Science. All rights reserved.","In the open science era, it is typical to share project-generated scientific data by depositing it in an open and accessible database. Moreover, scientific publications are preserved in a digital library archive. It is challenging to identify the data usage that is mentioned in literature and associate it with its source. Here, we investigated the data usage of a government-funded cancer genomics project, The Cancer Genome Atlas (TCGA), via a full-text literature analysis. We focused on identifying articles using the TCGA dataset and constructing linkages between the articles and the specific TCGA dataset. First, we collected 5,372 TCGA-related articles from PubMed Central (PMC). Second, we constructed a benchmark set with 25 full-text articles that truly used the TCGA data in their studies, and we summarized the key features of the benchmark set. Third, the key features were applied to the remaining PMC full-text articles that were collected from PMC. The amount of publications that use TCGA data has increased significantly since 2011, although the TCGA project was launched in 2005. Additionally, we found that the critical areas of focus in the studies that use the TCGA data were glioblastoma multiforme, lung cancer, and breast cancer; meanwhile, data from the RNA-sequencing (RNA-seq) platform is the most preferable for use. The current workflow to identify articles that truly used TCGA data is labor-intensive. An automatic method is expected to improve the performance. This study will help cancer genomics researchers determine the latest advancements in cancer molecular therapy, and it will promote data sharing and data-intensive scientific discovery. Few studies have been conducted to investigate data usage by government-funded projects/programs since their launch. In this preliminary study, we extracted articles that use TCGA data from PMC, and we created a link between the full-text articles and the source data."
"Big Metadata, Smart Metadata, and Metadata Capital: Toward Greater Synergy between Data Science and Metadata","Purpose: The purpose of the paper is to provide a framework for addressing the disconnect between metadata and data science. Data science cannot progress without metadata research. This paper takes steps toward advancing the synergy between metadata and data science, and identifies pathways for developing a more cohesive metadata research agenda in data science. Design/methodology/approach: This paper identifies factors that challenge metadata research in the digital ecosystem, defines metadata and data science, and presents the concepts big metadata, smart metadata, and metadata capital as part of a metadata lingua franca connecting to data science. Findings: The ""utilitarian nature""and ""historical and traditional views""of metadata are identified as two intersecting factors that have inhibited metadata research. Big metadata, smart metadata, and metadata capital are presented as part of a metadata lingua franca to help frame research in the data science research space. Research limitations: There are additional, intersecting factors to consider that likely inhibit metadata research, and other significant metadata concepts to explore. Practical implications: The immediate contribution of this work is that it may elicit response, critique, revision, or, more significantly, motivate research. The work presented can encourage more researchers to consider the significance of metadata as a research worthy topic within data science and the larger digital ecosystem. Originality/value: Although metadata research has not kept pace with other data science topics, there is little attention directed to this problem. This is surprising, given that metadata is essential for data science endeavors. This examination synthesizes original and prior scholarship to provide new grounding for metadata research in data science.  Â© 2017 Jane Greenberg.","The purpose of the paper is to provide a framework for addressing the disconnect between metadata and data science. Data science cannot progress without metadata research. This paper takes steps toward advancing the synergy between metadata and data science, and identifies pathways for developing a more cohesive metadata research agenda in data science. This paper identifies factors that challenge metadata research in the digital ecosystem, defines metadata and data science, and presents the concepts big metadata, smart metadata, and metadata capital as part of a metadata lingua franca connecting to data science. The ""utilitarian nature""and ""historical and traditional views""of metadata are identified as two intersecting factors that have inhibited metadata research. Big metadata, smart metadata, and metadata capital are presented as part of a metadata lingua franca to help frame research in the data science research space. There are additional, intersecting factors to consider that likely inhibit metadata research, and other significant metadata concepts to explore. The immediate contribution of this work is that it may elicit response, critique, revision, or, more significantly, motivate research. The work presented can encourage more researchers to consider the significance of metadata as a research worthy topic within data science and the larger digital ecosystem. Although metadata research has not kept pace with other data science topics, there is little attention directed to this problem. This is surprising, given that metadata is essential for data science endeavors. This examination synthesizes original and prior scholarship to provide new grounding for metadata research in data science."
Open peer review in scientific publishing: A web mining study of peerj authors and reviewers,"Purpose: To understand how authors and reviewers are accepting and embracing Open Peer Review (OPR), one of the newest innovations in the Open Science movement. Design/methodology/approach: This research collected and analyzed data from the Open Access journal PeerJ over its first three years (2013-2016). Web data were scraped, cleaned, and structured using several Web tools and programs. The structured data were imported into a relational database. Data analyses were conducted using analytical tools as well as programs developed by the researchers. Findings: PeerJ, which supports optional OPR, has a broad international representation of authors and referees. Approximately 73.89% of articles provide full review histories. Of the articles with published review histories, 17.61% had identities of all reviewers and 52.57% had at least one signed reviewer. In total, 43.23% of all reviews were signed. The observed proportions of signed reviews have been relatively stable over the period since the Journal's inception. Research limitations: This research is constrained by the availability of the peer review history data. Some peer reviews were not available when the authors opted out of publishing their review histories. The anonymity of reviewers made it impossible to give an accurate count of reviewers who contributed to the review process. Practical implications: These findings shed light on the current characteristics of OPR. Given the policy that authors are encouraged to make their articles' review history public and referees are encouraged to sign their review reports, the three years of PeerJ review data demonstrate that there is still some reluctance by authors to make their reviews public and by reviewers to identify themselves. Originality/value: This is the first study to closely examine PeerJ as an example of an OPR model journal. As Open Science moves further towards open research, OPR is a final and critical component. Research in this area must identify the best policies and paths towards a transparent and open peer review process for scientific communication. Â© Journal of Data and Information Science. All rights reserved.","To understand how authors and reviewers are accepting and embracing Open Peer Review (OPR), one of the newest innovations in the Open Science movement. This research collected and analyzed data from the Open Access journal PeerJ over its first three years (2013-2016). Web data were scraped, cleaned, and structured using several Web tools and programs. The structured data were imported into a relational database. Data analyses were conducted using analytical tools as well as programs developed by the researchers. PeerJ, which supports optional OPR, has a broad international representation of authors and referees. Approximately 73.89% of articles provide full review histories. Of the articles with published review histories, 17.61% had identities of all reviewers and 52.57% had at least one signed reviewer. In total, 43.23% of all reviews were signed. The observed proportions of signed reviews have been relatively stable over the period since the Journal's inception. This research is constrained by the availability of the peer review history data. Some peer reviews were not available when the authors opted out of publishing their review histories. The anonymity of reviewers made it impossible to give an accurate count of reviewers who contributed to the review process. These findings shed light on the current characteristics of OPR. Given the policy that authors are encouraged to make their articles' review history public and referees are encouraged to sign their review reports, the three years of PeerJ review data demonstrate that there is still some reluctance by authors to make their reviews public and by reviewers to identify themselves. This is the first study to closely examine PeerJ as an example of an OPR model journal. As Open Science moves further towards open research, OPR is a final and critical component. Research in this area must identify the best policies and paths towards a transparent and open peer review process for scientific communication."
Does monetary support increase the number of scientific papers? An interrupted time series analysis,"Purpose: One of the main indicators of scientific production is the number of papers published in scholarly journals. Turkey ranks 18th place in the world based on the number of scholarly publications. The objective of this paper is to find out if the monetary support program initiated in 1993 by the Turkish Scientific and Technological Research Council (TÃBÄ°TAK) to incentivize researchers and increase the number, impact, and quality of international publications has been effective in doing so. Design/methodology/approach: We analyzed some 390,000 publications with Turkish affiliations listed in the Web of Science (WoS) database between 1976 and 2015 along with about 157,000 supported ones between 1997 and 2015. We used the interrupted time series (ITS) analysis technique (also known as âquasi-experimental time series analysisâ or âintervention analysisâ) to test if TÃBÄ°TAK's support program helped increase the number of publications. We defined ARIMA (1,1,0) model for ITS data and observed the impact of TÃBÄ°TAK's support program in 1994, 1997, and 2003 (after one, four and 10 years of its start, respectively). The majority of publications (93%) were full papers (articles), which were used as the experimental group while other types of contributions functioned as the control group. We also carried out a multiple regression analysis. Findings: TÃBÄ°TAK's support program has had negligible effect on the increase of the number of papers with Turkish affiliations. Yet, the number of other types of contributions continued to increase even though they were not well supported, suggesting that TÃBÄ°TAK's support program is probably not the main factor causing the increase in the number of papers with Turkish affiliations. Research limitations: Interrupted time series analysis shows if the âinterventionâ has had any significant effect on the dependent variable but it does not explain what caused the increase in the number of papers if it was not the intervention. Moreover, except the âinterventionâ, other âevent(s)â that might affect the time series data (e.g., increase in the number of research personnel over the years) should not occur during the period of analysis, a prerequisite that is beyond the control of the researcher. Practical implications: TÃBÄ°TAK's âcash-for-publicationâ program did not seem to have direct impact on the increase of the number of papers published by Turkish authors, suggesting that small amounts of payments are not much of an incentive for authors to publish more. It might perhaps be a better strategy to concentrate limited resources on a few high impact projects rather than to disperse them to thousands of authors as âmicropayments.â Originality/value: Based on 25 years' worth of payments data, this is perhaps one of the first large-scale studies showing that âcash-for-publicationâ policies or âpiece ratesâ paid to researchers tend to have little or no effect on the increase of researchers' productivity. The main finding of this paper has some implications for countries wherein publication subsidies are used as an incentive to increase the number and quality of papers published in international journals. They should be prepared to consider reviewing their existing support programs (based usually on bibliometric measures such as journal impact factors) and revising their reward policies. Â© 2018 Sciendo. All rights reserved.","One of the main indicators of scientific production is the number of papers published in scholarly journals. Turkey ranks 18th place in the world based on the number of scholarly publications. The objective of this paper is to find out if the monetary support program initiated in 1993 by the Turkish Scientific and Technological Research Council (TBTAK) to incentivize researchers and increase the number, impact, and quality of international publications has been effective in doing so. We analyzed some 390,000 publications with Turkish affiliations listed in the Web of Science (WoS) database between 1976 and 2015 along with about 157,000 supported ones between 1997 and 2015. We used the interrupted time series (ITS) analysis technique (also known as quasi-experimental time series analysis or intervention analysis) to test if TBTAK's support program helped increase the number of publications. We defined ARIMA (1,1,0) model for ITS data and observed the impact of TBTAK's support program in 1994, 1997, and 2003 (after one, four and 10 years of its start, respectively). The majority of publications (93%) were full papers (articles), which were used as the experimental group while other types of contributions functioned as the control group. We also carried out a multiple regression analysis. TBTAK's support program has had negligible effect on the increase of the number of papers with Turkish affiliations. Yet, the number of other types of contributions continued to increase even though they were not well supported, suggesting that TBTAK's support program is probably not the main factor causing the increase in the number of papers with Turkish affiliations. Interrupted time series analysis shows if the intervention has had any significant effect on the dependent variable but it does not explain what caused the increase in the number of papers if it was not the intervention. Moreover, except the intervention, other event(s) that might affect the time series data (, increase in the number of research personnel over the years) should not occur during the period of analysis, a prerequisite that is beyond the control of the researcher. TBTAK's cash-for-publication program did not seem to have direct impact on the increase of the number of papers published by Turkish authors, suggesting that small amounts of payments are not much of an incentive for authors to publish more. It might perhaps be a better strategy to concentrate limited resources on a few high impact projects rather than to disperse them to thousands of authors as micropayments. Based on 25 years' worth of payments data, this is perhaps one of the first large-scale studies showing that cash-for-publication policies or piece rates paid to researchers tend to have little or no effect on the increase of researchers' productivity. The main finding of this paper has some implications for countries wherein publication subsidies are used as an incentive to increase the number and quality of papers published in international journals. They should be prepared to consider reviewing their existing support programs (based usually on bibliometric measures such as journal impact factors) and revising their reward policies."
Factors Influencing Cities' Publishing Efficiency,"Recently, a vast number of scientific publications have been produced in cities in emerging countries. It has long been observed that the publication output of Beijing has exceeded that of any other city in the world, including such leading centres of science as Boston, New York, London, Paris, and Tokyo. Researchers have suggested that, instead of focusing on cities' total publication output, the quality of the output in terms of the number of highly cited papers should be examined. However, in the period from 2014 to 2016, Beijing produced as many highly cited papers as Boston, London, or New York. In this paper, another method is proposed to measure cities' publishing performance by focusing on cities' publishing efficiency (i.e., the ratio of highly cited articles to all articles produced in that city). First, 554 cities are ranked based on their publishing efficiency, then some general factors influencing cities' publishing efficiency are revealed. The general factors examined in this paper are as follows: the linguistic environment of cities, cities' economic development level, the location of excellent organisations, cities' international collaboration patterns, and their scientific field profile. Furthermore, the paper examines the fundamental differences between the general factors influencing the publishing efficiency of the top 100 most efficient cities and the bottom 100 least efficient cities. Based on the research results, the conclusion can be drawn that a city's publishing efficiency will be high if meets the following general conditions: it is in a country in the Anglosphere-Core; it is in a high-income country; it is home to top-ranked universities and/or world-renowned research institutions; researchers affiliated with that city most intensely collaborate with researchers affiliated with cities in the United States, Germany, England, France, Canada, Australia, and Italy; and the most productive scientific disciplines of highly cited articles are published in high-impact multidisciplinary journals, disciplines in health sciences (especially general internal medicine and oncology), and disciplines in natural sciences (especially physics, astronomy, and astrophysics). It is always problematic to demarcate the boundaries of cities (e.g., New York City vs. Greater New York), and regarding this issue there is no consensus among researchers. The Web of Science presents the name of cities in the addresses reported by the authors of publications. In this paper cities correspond to the spatial units between the country/state level and the institution level as indicated in the Web of Science. Furthermore, it is necessary to highlight that the Web of Science is biased towards English-language journals and journals published in the field of biomedicine. These facts may influence the outcome of the research. Publishing efficiency, as an indicator, shows how successful a city is at the production of science. Naturally, cities have limited opportunities to compete for components of the science establishment (e.g., universities, hospitals). However, cities can compete to attract innovation-oriented companies, high tech firms, and R&D facilities of multinational companies by for example establishing science parks. The positive effect of this process on the city's performance in science can be observed in the example of Beijing, which publishing efficiency has been increased rapidly. Previous scientometric studies have examined cities' publication output in terms of the number of papers, or the number of highly cited papers, which are largely size dependent indicators; however this paper attempts to present a more quality-based approach. Â© 2018 CsomÃ³s GyÃ¶rgy, published by Sciendo.","Recently, a vast number of scientific publications have been produced in cities in emerging countries. It has long been observed that the publication output of Beijing has exceeded that of any other city in the world, including such leading centres of science as Boston, New York, London, Paris, and Tokyo. Researchers have suggested that, instead of focusing on cities' total publication output, the quality of the output in terms of the number of highly cited papers should be examined. However, in the period from 2014 to 2016, Beijing produced as many highly cited papers as Boston, London, or New York. In this paper, another method is proposed to measure cities' publishing performance by focusing on cities' publishing efficiency (, the ratio of highly cited articles to all articles produced in that city). First, 554 cities are ranked based on their publishing efficiency, then some general factors influencing cities' publishing efficiency are revealed. The general factors examined in this paper are as follows: the linguistic environment of cities, cities' economic development level, the location of excellent organisations, cities' international collaboration patterns, and their scientific field profile. Furthermore, the paper examines the fundamental differences between the general factors influencing the publishing efficiency of the top 100 most efficient cities and the bottom 100 least efficient cities. Based on the research results, the conclusion can be drawn that a city's publishing efficiency will be high if meets the following general conditions: it is in a country in the Anglosphere-Core; it is in a high-income country; it is home to top-ranked universities and/or world-renowned research institutions; researchers affiliated with that city most intensely collaborate with researchers affiliated with cities in the United States, Germany, England, France, Canada, Australia, and Italy; and the most productive scientific disciplines of highly cited articles are published in high-impact multidisciplinary journals, disciplines in health sciences (especially general internal medicine and oncology), and disciplines in natural sciences (especially physics, astronomy, and astrophysics). It is always problematic to demarcate the boundaries of cities (, New York City vs. Greater New York), and regarding this issue there is no consensus among researchers. The Web of Science presents the name of cities in the addresses reported by the authors of publications. In this paper cities correspond to the spatial units between the country/state level and the institution level as indicated in the Web of Science. Furthermore, it is necessary to highlight that the Web of Science is biased towards English-language journals and journals published in the field of biomedicine. These facts may influence the outcome of the research. Publishing efficiency, as an indicator, shows how successful a city is at the production of science. Naturally, cities have limited opportunities to compete for components of the science establishment (, universities, hospitals). However, cities can compete to attract innovation-oriented companies, high tech firms, and R&D facilities of multinational companies by for example establishing science parks. The positive effect of this process on the city's performance in science can be observed in the example of Beijing, which publishing efficiency has been increased rapidly. Previous scientometric studies have examined cities' publication output in terms of the number of papers, or the number of highly cited papers, which are largely size dependent indicators; however this paper attempts to present a more quality-based approach."
Usage Count: A New Indicator to Detect Research Fronts,"Purpose: Research fronts build on recent work, but using times cited as a traditional indicator to detect research fronts will inevitably result in a certain time lag. This study attempts to explore the effects of usage count as a new indicator to detect research fronts in shortening the time lag of classic indicators in research fronts detection. Design/methodology/approach: An exploratory study was conducted where the new indicator ""usage count""was compared to the traditional citation count, ""times cited,""in detecting research fronts of the regenerative medicine domain. An initial topic search of the term ""regenerative medicine""returned 10,553 records published between 2000 and 2015 in the Web of Science (WoS). We first ranked these records with usage count and times cited, respectively, and selected the top 2,000 records for each. We then performed a co-citation analysis in order to obtain the citing papers of the co-citation clusters as the research fronts. Finally, we compared the average publication year of the citing papers as well as the mean cited year of the co-citation clusters. Findings: The citing articles detected by usage count tend to be published more recently compared with times cited within the same research front. Moreover, research fronts detected by usage count tend to be within the last two years, which presents a higher immediacy and real-time feature compared to times cited. There is approximately a three-year time span among the mean cited years (known as ""intellectual base"") of all clusters generated by usage count and this figure is about four years in the network of times cited. In comparison to times cited, usage count is a dynamic and instant indicator. Research limitations: We are trying to find the cutting-edge research fronts, but those generated based on co-citations may refer to the hot research fronts. The usage count of older highly cited papers was not taken into consideration, because the usage count indicator released by WoS only reflects usage logs after February 2013. Practical implications: The article provides a new perspective on using usage count as a new indicator to detect research fronts. Originality/value: Usage count can greatly shorten the time lag in research fronts detection, which would be a promising complementary indicator in detection of the latest research fronts.  Â© 2017 Guoqiang Liang, Haiyan Hou, Zhigang Hu, Fu Huang, Yajie Wang & Shanshan Zhang.","Research fronts build on recent work, but using times cited as a traditional indicator to detect research fronts will inevitably result in a certain time lag. This study attempts to explore the effects of usage count as a new indicator to detect research fronts in shortening the time lag of classic indicators in research fronts detection. An exploratory study was conducted where the new indicator ""usage count""was compared to the traditional citation count, ""times cited,""in detecting research fronts of the regenerative medicine domain. An initial topic search of the term ""regenerative medicine""returned 10,553 records published between 2000 and 2015 in the Web of Science (WoS). We first ranked these records with usage count and times cited, respectively, and selected the top 2,000 records for each. We then performed a co-citation analysis in order to obtain the citing papers of the co-citation clusters as the research fronts. Finally, we compared the average publication year of the citing papers as well as the mean cited year of the co-citation clusters. The citing articles detected by usage count tend to be published more recently compared with times cited within the same research front. Moreover, research fronts detected by usage count tend to be within the last two years, which presents a higher immediacy and real-time feature compared to times cited. There is approximately a three-year time span among the mean cited years (known as ""intellectual base"") of all clusters generated by usage count and this figure is about four years in the network of times cited. In comparison to times cited, usage count is a dynamic and instant indicator. We are trying to find the cutting-edge research fronts, but those generated based on co-citations may refer to the hot research fronts. The usage count of older highly cited papers was not taken into consideration, because the usage count indicator released by WoS only reflects usage logs after February 2013. The article provides a new perspective on using usage count as a new indicator to detect research fronts. Usage count can greatly shorten the time lag in research fronts detection, which would be a promising complementary indicator in detection of the latest research fronts."
"Document type profiles in nature, science, and PNAs: Journal and country level","Purpose: In this contribution, we want to detect the document type profiles of the three prestigious journals Nature, Science, and Proceedings of the National Academy of Sciences of the United States (PNAS) with regard to two levels: journal and country. Design/methodology/approach: Using relative values based on fractional counting, we investigate the distribution of publications across document types at both the journal and country level, and we use (cosine) document type profile similarity values to compare pairs of publication years within countries. Findings: Nature and Science mainly publish Editorial Material, Article, News Item and Letter, whereas the publications of PNAS are heavily concentrated on Article. The shares of Article for Nature and Science are decreasing slightly from 1999 to 2014, while the corresponding shares of Editorial Material are increasing. Most studied countries focus on Article and Letter in Nature, but on Letter in Science and PNAS. The document type profiles of some of the studied countries change to a relatively large extent over publication years. Research limitations: The main limitation of this research concerns the Web of Science classification of publications into document types. Since the analysis of the paper is based on document types of Web of Science, the classification in question is not free from errors, and the accuracy of the analysis might be affected. Practical implications: Results show that Nature and Science are quite diversified with regard to document types. In bibliometric assessments, where publications in Nature and Science play a role, other document types than Article and Review might therefore be taken into account. Originality/value: Results highlight the importance of other document types than Article and Review in Nature and Science. Large differences are also found when comparing the country document type profiles of the three journals with the corresponding profiles in all Web of Science journals. Â© Journal of Data and Information Science. All rights reserved.","In this contribution, we want to detect the document type profiles of the three prestigious journals Nature, Science, and Proceedings of the National Academy of Sciences of the United States (PNAS) with regard to two levels: journal and country. Using relative values based on fractional counting, we investigate the distribution of publications across document types at both the journal and country level, and we use (cosine) document type profile similarity values to compare pairs of publication years within countries. Nature and Science mainly publish Editorial Material, Article, News Item and Letter, whereas the publications of PNAS are heavily concentrated on Article. The shares of Article for Nature and Science are decreasing slightly from 1999 to 2014, while the corresponding shares of Editorial Material are increasing. Most studied countries focus on Article and Letter in Nature, but on Letter in Science and PNAS. The document type profiles of some of the studied countries change to a relatively large extent over publication years. The main limitation of this research concerns the Web of Science classification of publications into document types. Since the analysis of the paper is based on document types of Web of Science, the classification in question is not free from errors, and the accuracy of the analysis might be affected. Results show that Nature and Science are quite diversified with regard to document types. In bibliometric assessments, where publications in Nature and Science play a role, other document types than Article and Review might therefore be taken into account. Results highlight the importance of other document types than Article and Review in Nature and Science. Large differences are also found when comparing the country document type profiles of the three journals with the corresponding profiles in all Web of Science journals."
Using Machine Reading to Understand Alzheimer's and Related Diseases from the Literature,"Purpose: This paper aims to better understand a large number of papers in the medical domain of Alzheimer's disease (AD) and related diseases using the machine reading approach. Design/methodology/approach: The study uses the topic modeling method to obtain an overview of the field, and employs open information extraction to further comprehend the field at a specific fact level. Findings: Several topics within the AD research field are identified, such as the Human Immunodeficiency Virus (HIV)/Acquired Immune Deficiency Syndrome (AIDS), which can help answer the question of how AIDS/HIV and AD are very different yet related diseases. Research limitations: Some manual data cleaning could improve the study, such as removing incorrect facts found by open information extraction. Practical implications: This study uses the literature to answer specific questions on a scientific domain, which can help domain experts find interesting and meaningful relations among entities in a similar manner, such as to discover relations between AD and AIDS/HIV. Originality/value: Both the overview and specific information from the literature are obtained using two distinct methods in a complementary manner. This combination is novel because previous work has only focused on one of them, and thus provides a better way to understand an important scientific field using data-driven methods.  Â© 2017 Walter de Gruyter GmbH, Berlin/Boston.","This paper aims to better understand a large number of papers in the medical domain of Alzheimer's disease (AD) and related diseases using the machine reading approach. The study uses the topic modeling method to obtain an overview of the field, and employs open information extraction to further comprehend the field at a specific fact level. Several topics within the AD research field are identified, such as the Human Immunodeficiency Virus (HIV)/Acquired Immune Deficiency Syndrome (AIDS), which can help answer the question of how AIDS/HIV and AD are very different yet related diseases. Some manual data cleaning could improve the study, such as removing incorrect facts found by open information extraction. This study uses the literature to answer specific questions on a scientific domain, which can help domain experts find interesting and meaningful relations among entities in a similar manner, such as to discover relations between AD and AIDS/HIV. Both the overview and specific information from the literature are obtained using two distinct methods in a complementary manner. This combination is novel because previous work has only focused on one of them, and thus provides a better way to understand an important scientific field using data-driven methods."
Smart Data for Digital Humanities,"The emergence of ""Big Data""has been a dramatic development in recent years. Alongside it, a lesser-known but equally important set of concepts and practices has also come into being - ""Smart Data.""This paper shares the author's understanding of what, why, how, who, where, and which data in relation to Smart Data and digital humanities. It concludes that, challenges and opportunities co-exist, but it is certain that Smart Data, the ability to achieve big insights from trusted, contextualized, relevant, cognitive, predictive, and consumable data at any scale, will continue to have extraordinary value in digital humanities. The emergence of ""Big Data""has been a dramatic development in recent years. Alongside it, a lesser-known but equally important set of concepts and practices has also come into being - ""Smart Data."" Â© 2017 Marcia Lei Zeng.","The emergence of ""Big Data""has been a dramatic development in recent years. Alongside it, a lesser-known but equally important set of concepts and practices has also come into being - ""Smart Data.""This paper shares the author's understanding of what, why, how, who, where, and which data in relation to Smart Data and digital humanities. It concludes that, challenges and opportunities co-exist, but it is certain that Smart Data, the ability to achieve big insights from trusted, contextualized, relevant, cognitive, predictive, and consumable data at any scale, will continue to have extraordinary value in digital humanities. The emergence of ""Big Data""has been a dramatic development in recent years. Alongside it, a lesser-known but equally important set of concepts and practices has also come into being - ""Smart Data."""
Performance-based Research Funding in Denmark: The Adoption and Translation of the Norwegian Model,"The main goal of this study is to outline and analyze the Danish adoption and translation of the Norwegian Publication Indicator. The study takes the form of a policy analysis mainly drawing on document analysis of policy papers, previously published studies and grey literature. The study highlights a number of crucial factors that relate both to the Danish process and to the final Danish result underscoring that the Danish BFI model is indeed a quite different system than its Norwegian counterpart. One consequence of these process-and design differences is the fact that the broader legitimacy of the Danish BFI today appears to be quite poor. Reasons for this include: unclear and shifting objectives throughout the process; limited willingness to take ownership of the model among stakeholders; lack of communication throughout the implementation process and an apparent underestimation of the challenges associated with the use of bibliometric indicators. The conclusions of the study are based on the authors' interpretation of a long drawn and complex process with many different stakeholders involved. The format of this article does not allow for a detailed documentation of all elements, but further details can be provided upon request. The analysis may feed into current policy discussions on the future of the Danish BFI. Some elements of the present analysis have previously been published in Danish outlets, but this article represents the first publication on this issue targeting a broader international audience. Â© 2018 2018 Kaare Aagaard, published by Sciendo.","The main goal of this study is to outline and analyze the Danish adoption and translation of the Norwegian Publication Indicator. The study takes the form of a policy analysis mainly drawing on document analysis of policy papers, previously published studies and grey literature. The study highlights a number of crucial factors that relate both to the Danish process and to the final Danish result underscoring that the Danish BFI model is indeed a quite different system than its Norwegian counterpart. One consequence of these process-and design differences is the fact that the broader legitimacy of the Danish BFI today appears to be quite poor. Reasons for this include: unclear and shifting objectives throughout the process; limited willingness to take ownership of the model among stakeholders; lack of communication throughout the implementation process and an apparent underestimation of the challenges associated with the use of bibliometric indicators. The conclusions of the study are based on the authors' interpretation of a long drawn and complex process with many different stakeholders involved. The format of this article does not allow for a detailed documentation of all elements, but further details can be provided upon request. The analysis may feed into current policy discussions on the future of the Danish BFI. Some elements of the present analysis have previously been published in Danish outlets, but this article represents the first publication on this issue targeting a broader international audience."
The F-measure for Research Priority,"Purpose: In this contribution we continue our investigations related to the activity index (AI) and its formal analogs. We try to replace the AI by an indicator which is better suited for policy applications. Design/methodology/approach: We point out that fluctuations in the value of the AI for a given country and domain are never the result of that country's policy with respect to that domain alone because there are exogenous factors at play. For this reason we introduce the F-measure. This F-measure is nothing but the harmonic mean of the country's share in the world's publication output in the given domain and the given domain's share in the country's publication output. Findings: The F-measure does not suffer from the problems the AI does. Research limitations: The indicator is not yet fully tested in real cases. R&D policy management: In policy considerations, the AI should better be replaced by the F-measure as this measure can better show the results of science policy measures (which the AI cannot as it depends on exogenous factors). Originality/value: We provide an original solution for a problem that is not fully realized by policy makers. Â© 2018 Sciendo. All rights reserved.","In this contribution we continue our investigations related to the activity index (AI) and its formal analogs. We try to replace the AI by an indicator which is better suited for policy applications. We point out that fluctuations in the value of the AI for a given country and domain are never the result of that country's policy with respect to that domain alone because there are exogenous factors at play. For this reason we introduce the F-measure. This F-measure is nothing but the harmonic mean of the country's share in the world's publication output in the given domain and the given domain's share in the country's publication output. The F-measure does not suffer from the problems the AI does. The indicator is not yet fully tested in real cases. R&D policy management: In policy considerations, the AI should better be replaced by the F-measure as this measure can better show the results of science policy measures (which the AI cannot as it depends on exogenous factors). We provide an original solution for a problem that is not fully realized by policy makers."
Seeking Health Information Online: The Moderating Effects of Problematic Situations on User Intention,"Purpose: This study investigates how online user intention in searching health information is affected by problematic situations. Design/methodology/approach: Based on the Theory of Reasoned Action, the Technology Acceptance Model, and Sense-making theory, we propose two dimensions of problematic situations: urgency and severity of health issues being searched online. Data were collected through a questionnaire survey among 214 Wuhan University students and analyzed using hierarchical regression analysis. Findings: Perceived usefulness, perceived ease of use, and subjective norm can influence user intention to seek health information online. The urgency of problematic situations has a negative moderating effect on the relationship between perceived ease of use and user intention and the relationship between subjective norm and user intention. The severity of problematic situations has a negative moderating effect on the relationship between subjective norm and user intention. Research limitations: The respondents of the survey are limited to students in one Chinese university, so whether this study's results can be applied to another population or not remains to be verified. In addition, only two dimensions of problematic situations are considered in this study. Practical implications: The paper puts forward the moderating effect of problematic situations and verifies it, which is the compensation for online health information-seeking behavior research. Besides, our analyses have implications for professional design of health care systems and related consumer information searches, and improve their performance. Originality/value: Previous work has reported the effects of problematic situation on user intention to seek health information online, ignoring its influence on other factors. This empirical study extends that work to identify the influence of problematic situation when seeking intention-behavior data in two dimensions, urgency and severity.  Â© 2017 Lidan Xia, Shengli Deng & Yirong Liu.","This study investigates how online user intention in searching health information is affected by problematic situations. Based on the Theory of Reasoned Action, the Technology Acceptance Model, and Sense-making theory, we propose two dimensions of problematic situations: urgency and severity of health issues being searched online. Data were collected through a questionnaire survey among 214 Wuhan University students and analyzed using hierarchical regression analysis. Perceived usefulness, perceived ease of use, and subjective norm can influence user intention to seek health information online. The urgency of problematic situations has a negative moderating effect on the relationship between perceived ease of use and user intention and the relationship between subjective norm and user intention. The severity of problematic situations has a negative moderating effect on the relationship between subjective norm and user intention. The respondents of the survey are limited to students in one Chinese university, so whether this study's results can be applied to another population or not remains to be verified. In addition, only two dimensions of problematic situations are considered in this study. The paper puts forward the moderating effect of problematic situations and verifies it, which is the compensation for online health information-seeking behavior research. Besides, our analyses have implications for professional design of health care systems and related consumer information searches, and improve their performance. Previous work has reported the effects of problematic situation on user intention to seek health information online, ignoring its influence on other factors. This empirical study extends that work to identify the influence of problematic situation when seeking intention-behavior data in two dimensions, urgency and severity."
Knowledge representation in patient safety reporting: An ontological approach,"Purpose: The current development of patient safety reporting systems is criticized for loss of information and low data quality due to the lack of a uniformed domain knowledge base and text processing functionality. To improve patient safety reporting, the present paper suggests an ontological representation of patient safety knowledge. Design/methodology/approach: We propose a framework for constructing an ontological knowledge base of patient safety. The present paper describes our design, implementation, and evaluation of the ontology at its initial stage. Findings: We describe the design and initial outcomes of the ontology implementation. The evaluation results demonstrate the clinical validity of the ontology by a self-developed survey measurement. Research limitations: The proposed ontology was developed and evaluated using a small number of information sources. Presently, US data are used, but they are not essential for the ultimate structure of the ontology. Practical implications: The goal of improving patient safety can be aided through investigating patient safety reports and providing actionable knowledge to clinical practitioners. As such, constructing a domain specific ontology for patient safety reports serves as a cornerstone in information collection and text mining methods. Originality/value: The use of ontologies provides abstracted representation of semantic information and enables a wealth of applications in a reporting system. Therefore, constructing such a knowledge base is recognized as a high priority in health care. Â© Journal of Data and Information Science. All rights reserved.","The current development of patient safety reporting systems is criticized for loss of information and low data quality due to the lack of a uniformed domain knowledge base and text processing functionality. To improve patient safety reporting, the present paper suggests an ontological representation of patient safety knowledge. We propose a framework for constructing an ontological knowledge base of patient safety. The present paper describes our design, implementation, and evaluation of the ontology at its initial stage. We describe the design and initial outcomes of the ontology implementation. The evaluation results demonstrate the clinical validity of the ontology by a self-developed survey measurement. The proposed ontology was developed and evaluated using a small number of information sources. Presently, US data are used, but they are not essential for the ultimate structure of the ontology. The goal of improving patient safety can be aided through investigating patient safety reports and providing actionable knowledge to clinical practitioners. As such, constructing a domain specific ontology for patient safety reports serves as a cornerstone in information collection and text mining methods. The use of ontologies provides abstracted representation of semantic information and enables a wealth of applications in a reporting system. Therefore, constructing such a knowledge base is recognized as a high priority in health care."
Data-driven Discovery: A new era of exploiting the literature and data,"In the current data-intensive era, the traditional hands-on method of conducting scientific research by exploring related publications to generate a testable hypothesis is well on its way of becoming obsolete within just a year or two. Analyzing the literature and data to automatically generate a hypothesis might become the de facto approach to inform the core research efforts of those trying to master the exponentially rapid expansion of publications and datasets. Here, viewpoints are provided and discussed to help the understanding of challenges of data-driven discovery. Â© Journal of Data and Information Science. All rights reserved.","In the current data-intensive era, the traditional hands-on method of conducting scientific research by exploring related publications to generate a testable hypothesis is well on its way of becoming obsolete within just a year or two. Analyzing the literature and data to automatically generate a hypothesis might become the de facto approach to inform the core research efforts of those trying to master the exponentially rapid expansion of publications and datasets. Here, viewpoints are provided and discussed to help the understanding of challenges of data-driven discovery."
The Norwegian Model in Norway,"The ""Norwegian Model"" attempts to comprehensively cover all the peer-reviewed scholarly literatures in all areas of research in one single weighted indicator. Thereby, scientific production is made comparable across departments and faculties within and between research institutions, and the indicator may serve institutional evaluation and funding. This article describes the motivation for creating the model in Norway, how it was designed, organized and implemented, as well as the effects and experiences with the model. The article ends with an overview of a new type of bibliometric studies that are based on the type of comprehensive national publication data that the Norwegian Model provides. Â© 2018 Gunnar Sivertsen, published by Sciendo 2018.","The ""Norwegian Model"" attempts to comprehensively cover all the peer-reviewed scholarly literatures in all areas of research in one single weighted indicator. Thereby, scientific production is made comparable across departments and faculties within and between research institutions, and the indicator may serve institutional evaluation and funding. This article describes the motivation for creating the model in Norway, how it was designed, organized and implemented, as well as the effects and experiences with the model. The article ends with an overview of a new type of bibliometric studies that are based on the type of comprehensive national publication data that the Norwegian Model provides."
Detecting Dynamics of Hot Topics with Alluvial Diagrams: A Timeline Visualization,"Purpose: In this paper, we combined the method of co-word analysis and alluvial diagram to detect hot topics and illustrate their dynamics. Design/methodology/approach: Articles in the field of scientometrics were chosen as research cases in this study. A time-sliced co-word network was generated and then clustered. Afterwards, we generated an alluvial diagram to show dynamic changes of hot topics, including their merges and splits over time. Findings: After analyzing the dynamic changes in the field of scientometrics from 2011 to 2015, we found that two clusters being merged did not mean that the old topics had disappeared and a totally new one had emerged. The topics were possibly still active the following year, but the newer topics had drawn more attention. The changes of hot topics reflected the shift in researchers' interests. Research topics in scientometrics were constantly subdivided and re-merged. For example, a cluster involving ""industry""was divided into several topics as research progressed. Research limitations: When examining longer time periods, we encounter the problem of dealing with bigger data sets. Analyzing data year by year would be tedious, but if we combine, e.g. two years into one time slice, important details would be missed. Practical implications: This method can be applied to any research field to illustrate the dynamics of hot topics. It can indicate the promising directions for researchers and provide guidance to decision makers. Originality/value: The use of alluvial diagrams is a distinctive and meaningful approach to detecting hot topics and especially to illustrating their dynamics.  Â© 2017 Wenjing Ruan, Haiyan Hou & Zhigang Hu.","In this paper, we combined the method of co-word analysis and alluvial diagram to detect hot topics and illustrate their dynamics. Articles in the field of scientometrics were chosen as research cases in this study. A time-sliced co-word network was generated and then clustered. Afterwards, we generated an alluvial diagram to show dynamic changes of hot topics, including their merges and splits over time. After analyzing the dynamic changes in the field of scientometrics from 2011 to 2015, we found that two clusters being merged did not mean that the old topics had disappeared and a totally new one had emerged. The topics were possibly still active the following year, but the newer topics had drawn more attention. The changes of hot topics reflected the shift in researchers' interests. Research topics in scientometrics were constantly subdivided and re-merged. For example, a cluster involving ""industry""was divided into several topics as research progressed. When examining longer time periods, we encounter the problem of dealing with bigger data sets. Analyzing data year by year would be tedious, but if we combine, two years into one time slice, important details would be missed. This method can be applied to any research field to illustrate the dynamics of hot topics. It can indicate the promising directions for researchers and provide guidance to decision makers. The use of alluvial diagrams is a distinctive and meaningful approach to detecting hot topics and especially to illustrating their dynamics."
Understanding the correlations between social attention and topic trends of scientific publications,"Purpose: We propose and apply a simplified nowcasting model to understand the correlations between social attention and topic trends of scientific publications. Design/methodology/approach: First, topics are generated from the obesity corpus by using the latent Dirichlet allocation (LDA) algorithm and time series of keyword search trends in Google Trends are obtained. We then establish the structural time series model using data from January 2004 to December 2012, and evaluate the model using data from January 2013. We employ a state-space model to separate different non-regression components in an observational time series (i.e. the tendency and the seasonality) and apply the âspike and slab priorâ and stepwise regression to analyze the correlations between the regression component and the social media attention. The two parts are combined using Markov-chain Monte Carlo sampling techniques to obtain our results. Findings: The results of our study show that (1) the number of publications on child obesity increases at a lower rate than that of diabetes publications; (2) the number of publication on a given topic may exhibit a relationship with the season or time of year; and (3) there exists a correlation between the number of publications on a given topic and its social media attention, i.e. the search frequency related to that topic as identified by Google Trends. We found that our model is also able to predict the number of publications related to a given topic. Research limitations: First, we study a correlation rather than causality between topics' trends and social media. As a result, the relationships might not be robust, so we cannot predict the future in the long run. Second, we cannot identify the reasons or conditions that are driving obesity topics to present such tendencies and seasonal patterns, so we might need to do âfieldâ study in the future. Third, we need to improve the efficiency of our model by finding more efficient variable selection models, because the stepwise regression method is time consuming, especially for a large number of variables. Practical implications: This paper analyzes publication topic trends from three perspectives: tendency, seasonality, and correlation with social media attention, providing a new perspective for identifying and understanding topical themes in academic publications. Originality/value: To the best of our knowledge, we are the first to apply the state-space model to examine the relationships between healthcare-related publications and social media to investigate the relationships between a topic's evolvement and people's search behavior in social media. This paper thus provides a new viewpoint in the correlation analysis area, and demonstrates the value of considering social media attention in the analysis of publication topic trends. Â© Journal of Data and Information Science. All rights reserved.","We propose and apply a simplified nowcasting model to understand the correlations between social attention and topic trends of scientific publications. First, topics are generated from the obesity corpus by using the latent Dirichlet allocation (LDA) algorithm and time series of keyword search trends in Google Trends are obtained. We then establish the structural time series model using data from January 2004 to December 2012, and evaluate the model using data from January 2013. We employ a state-space model to separate different non-regression components in an observational time series ( the tendency and the seasonality) and apply the spike and slab prior and stepwise regression to analyze the correlations between the regression component and the social media attention. The two parts are combined using Markov-chain Monte Carlo sampling techniques to obtain our results. The results of our study show that the number of publications on child obesity increases at a lower rate than that of diabetes publications; the number of publication on a given topic may exhibit a relationship with the season or time of year; and there exists a correlation between the number of publications on a given topic and its social media attention, the search frequency related to that topic as identified by Google Trends. We found that our model is also able to predict the number of publications related to a given topic. First, we study a correlation rather than causality between topics' trends and social media. As a result, the relationships might not be robust, so we cannot predict the future in the long run. Second, we cannot identify the reasons or conditions that are driving obesity topics to present such tendencies and seasonal patterns, so we might need to do field study in the future. Third, we need to improve the efficiency of our model by finding more efficient variable selection models, because the stepwise regression method is time consuming, especially for a large number of variables. This paper analyzes publication topic trends from three perspectives: tendency, seasonality, and correlation with social media attention, providing a new perspective for identifying and understanding topical themes in academic publications. To the best of our knowledge, we are the first to apply the state-space model to examine the relationships between healthcare-related publications and social media to investigate the relationships between a topic's evolvement and people's search behavior in social media. This paper thus provides a new viewpoint in the correlation analysis area, and demonstrates the value of considering social media attention in the analysis of publication topic trends."
Mining related articles for automatic journal cataloging,"Purpose: This paper is an investigation of the effectiveness of the method of clustering biomedical journals through mining the content similarity of journal articles. Design/methodology/approach: 3,265 journals in PubMed are analyzed based on article content similarity and Web usage, respectively. Comparisons of the two analysis approaches and a citation-based approach are given. Findings: Our results suggest that article content similarity is useful for clustering biomedical journals, and the content-similarity-based journal clustering method is more robust and less subject to human factors compared with the usage-based approach and the citation-based approach. Research limitations: Our paper currently focuses on clustering journals in the biomedical domain because there are a large volume of freely available resources such as PubMed and MeSH in this field. Further investigation is needed to improve this approach to fit journals in other domains. Practical implications: Our results show that it is feasible to catalog biomedical journals by mining the article content similarity. This work is also significant in serving practical needs in research portfolio analysis. Originality/value: To the best of our knowledge, we are among the first to report on clustering journals in the biomedical field through mining the article content similarity. This method can be integrated with existing approaches to create a new paradigm for future studies of journal clustering. Â© Journal of Data and Information Science. All rights reserved.","This paper is an investigation of the effectiveness of the method of clustering biomedical journals through mining the content similarity of journal articles. 3,265 journals in PubMed are analyzed based on article content similarity and Web usage, respectively. Comparisons of the two analysis approaches and a citation-based approach are given. Our results suggest that article content similarity is useful for clustering biomedical journals, and the content-similarity-based journal clustering method is more robust and less subject to human factors compared with the usage-based approach and the citation-based approach. Our paper currently focuses on clustering journals in the biomedical domain because there are a large volume of freely available resources such as PubMed and MeSH in this field. Further investigation is needed to improve this approach to fit journals in other domains. Our results show that it is feasible to catalog biomedical journals by mining the article content similarity. This work is also significant in serving practical needs in research portfolio analysis. To the best of our knowledge, we are among the first to report on clustering journals in the biomedical field through mining the article content similarity. This method can be integrated with existing approaches to create a new paradigm for future studies of journal clustering."
"Rediscovering Don Swanson:The Past, Present and Future of Literature-based Discovery","Purpose: The late Don R. Swanson was well appreciated during his lifetime as Dean of the Graduate Library School at University of Chicago, as winner of the American Society for Information Science Award of Merit for 2000, and as author of many seminal articles. In this informal essay, I will give my personal perspective on Don's contributions to science, and outline some current and future directions in literature-based discovery that are rooted in concepts that he developed. Design/methodology/approach: Personal recollections and literature review. Findings: The Swanson A-B-C model of literature-based discovery has been successfully used by laboratory investigators analyzing their findings and hypotheses. It continues to be a fertile area of research in a wide range of application areas including text mining, drug repurposing, studies of scientific innovation, knowledge discovery in databases, and bioinformatics. Recently, additional modes of discovery that do not follow the A-B-C model have also been proposed and explored (e.g. so-called storytelling, gaps, analogies, link prediction, negative consensus, outliers, and revival of neglected or discarded research questions). Research limitations: This paper reflects the opinions of the author and is not a comprehensive nor technically based review of literature-based discovery. Practical implications: The general scientific public is still not aware of the availability of tools for literature-based discovery. Our Arrowsmith project site maintains a suite of discovery tools that are free and open to the public (http://arrowsmith.psych.uic.edu), as does BITOLA which is maintained by Dmitar Hristovski (http://http://ibmi.mf.uni-lj.si/bitola), and Epiphanet which is maintained by Trevor Cohen (http://epiphanet.uth.tmc.edu/). Bringing user-friendly tools to the public should be a high priority, since even more than advancing basic research in informatics, it is vital that we ensure that scientists actually use discovery tools and that these are actually able to help them make experimental discoveries in the lab and in the clinic. Originality/value: This paper discusses problems and issues which were inherent in Don's thoughts during his life, including those which have not yet been fully taken up and studied systematically.  Â© 2017 Walter de Gruyter GmbH, Berlin/Boston.","The late Don Swanson was well appreciated during his lifetime as Dean of the Graduate Library School at University of Chicago, as winner of the American Society for Information Science Award of Merit for 2000, and as author of many seminal articles. In this informal essay, I will give my personal perspective on Don's contributions to science, and outline some current and future directions in literature-based discovery that are rooted in concepts that he developed. Personal recollections and literature review. The Swanson A-B-C model of literature-based discovery has been successfully used by laboratory investigators analyzing their findings and hypotheses. It continues to be a fertile area of research in a wide range of application areas including text mining, drug repurposing, studies of scientific innovation, knowledge discovery in databases, and bioinformatics. Recently, additional modes of discovery that do not follow the A-B-C model have also been proposed and explored ( so-called storytelling, gaps, analogies, link prediction, negative consensus, outliers, and revival of neglected or discarded research questions). This paper reflects the opinions of the author and is not a comprehensive nor technically based review of literature-based discovery. The general scientific public is still not aware of the availability of tools for literature-based discovery. Our Arrowsmith project site maintains a suite of discovery tools that are free and open to the public (http://arrowsmith.psych.uic.edu), as does BITOLA which is maintained by Dmitar Hristovski (http://http://ibmi.mf.uni-lj.si/bitola), and Epiphanet which is maintained by Trevor Cohen (http://epiphanet.uth.tmc.edu/). Bringing user-friendly tools to the public should be a high priority, since even more than advancing basic research in informatics, it is vital that we ensure that scientists actually use discovery tools and that these are actually able to help them make experimental discoveries in the lab and in the clinic. This paper discusses problems and issues which were inherent in Don's thoughts during his life, including those which have not yet been fully taken up and studied systematically."
Topic detection based on weak tie analysis: A case study of LIS research,"Purpose: Based on the weak tie theory, this paper proposes a series of connection indicators of weak tie subnets and weak tie nodes to detect research topics, recognize their connections, and understand their evolution. Design/methodology/approach: First, keywords are extracted from article titles and preprocessed. Second, high-frequency keywords are selected to generate weak tie co-occurrence networks. By removing the internal lines of clustered sub-topic networks, we focus on the analysis of weak tie subnets' composition and functions and the weak tie nodes' roles. Findings: The research topics' clusters and themes changed yearly; the subnets clustered with technique-related and methodology-related topics have been the core, important subnets for years; while close subnets are highly independent, research topics are generally concentrated and most topics are application-related; the roles and functions of nodes and weak ties are diversified. Research limitations: The parameter values are somewhat inconsistent; the weak tie subnets and nodes are classified based on empirical observations, and the conclusions are not verified or compared to other methods. Practical implications: The research is valuable for detecting important research topics as well as their roles, interrelations, and evolution trends. Originality/value: To contribute to the strength of weak tie theory, the research translates weak and strong ties concepts to co-occurrence strength, and analyzes weak ties' functions. Also, the research proposes a quantitative method to classify and measure the topics' clusters and nodes. Â© Journal of Data and Information Science. All rights reserved.","Based on the weak tie theory, this paper proposes a series of connection indicators of weak tie subnets and weak tie nodes to detect research topics, recognize their connections, and understand their evolution. First, keywords are extracted from article titles and preprocessed. Second, high-frequency keywords are selected to generate weak tie co-occurrence networks. By removing the internal lines of clustered sub-topic networks, we focus on the analysis of weak tie subnets' composition and functions and the weak tie nodes' roles. The research topics' clusters and themes changed yearly; the subnets clustered with technique-related and methodology-related topics have been the core, important subnets for years; while close subnets are highly independent, research topics are generally concentrated and most topics are application-related; the roles and functions of nodes and weak ties are diversified. The parameter values are somewhat inconsistent; the weak tie subnets and nodes are classified based on empirical observations, and the conclusions are not verified or compared to other methods. The research is valuable for detecting important research topics as well as their roles, interrelations, and evolution trends. To contribute to the strength of weak tie theory, the research translates weak and strong ties concepts to co-occurrence strength, and analyzes weak ties' functions. Also, the research proposes a quantitative method to classify and measure the topics' clusters and nodes."
Analyzing the activities of visitors of the Leiden Ranking website,"To get a better understanding of the way in which university rankings are used. Detailed analysis of the activities of visitors of the website of the CWTS Leiden Ranking. Visitors of the Leiden Ranking website originate disproportionally from specific countries. They are more interested in impact indicators than in collaboration indicators, while they are about equally interested in size-dependent indicators and size-independent indicators. Many visitors do not seem to realize that they should decide themselves which criterion they consider most appropriate for ranking universities. The analysis is restricted to the website of a single university ranking. Moreover, the analysis does not provide any detailed insights into the motivations of visitors of university ranking websites. The Leiden Ranking website may need to be improved in order to make more clear to visitors that they should decide themselves which criterion they want to use for ranking universities. This is the first analysis of the activities of visitors of a university ranking website. Â© 2018 Nees Jan van Eck, Ludo Waltman, published by Sciendo.","To get a better understanding of the way in which university rankings are used. Detailed analysis of the activities of visitors of the website of the CWTS Leiden Ranking. Visitors of the Leiden Ranking website originate disproportionally from specific countries. They are more interested in impact indicators than in collaboration indicators, while they are about equally interested in size-dependent indicators and size-independent indicators. Many visitors do not seem to realize that they should decide themselves which criterion they consider most appropriate for ranking universities. The analysis is restricted to the website of a single university ranking. Moreover, the analysis does not provide any detailed insights into the motivations of visitors of university ranking websites. The Leiden Ranking website may need to be improved in order to make more clear to visitors that they should decide themselves which criterion they want to use for ranking universities. This is the first analysis of the activities of visitors of a university ranking website."
Erratum: Open Peer Review in Scientific Publishing: A Web Mining Study of PeerJ Authors and Reviewers (Journal of Data and Information Science (2016) 1:4 (60-80) DOI: 10.20309/jdis.201625),"In this paper, the third authorâs last name is Rath and first Name is Manasa. Her name has been reversed in order. CopyrightÂ© 2017 Journal of Data and Information Science. All rights reserved.","In this paper, the third authors last name is Rath and first Name is Manasa. Her name has been reversed in order. Copyright"
Provenance Description of Metadata Vocabularies for the Long-term Maintenance of Metadata,"Purpose: The purpose of this paper is to discuss provenance description of metadata terms and metadata vocabularies as a set of metadata terms. Provenance is crucial information to keep track of changes of metadata terms and metadata vocabularies for their consistent maintenance. Design/methodology/approach: The W3C PROV standard for general provenance description and Resource Description Framework (RDF) are adopted as the base models to formally define provenance description for metadata vocabularies. Findings: This paper defines a few primitive change types of metadata terms, and a provenance description model of the metadata terms based on the primitive change types. We also provide examples of provenance description in RDF graphs to show the proposed model. Research limitations: The model proposed in this paper is defined based on a few primitive relationships (e.g. addition, deletion, and replacement) between pre-version and post-version of a metadata term. The model is simplified and the practical changes of metadata terms can be more complicated than the primitive relationships discussed in the model. Practical implications: Formal provenance description of metadata vocabularies can improve maintainability of metadata vocabularies over time. Conventional maintenance of metadata terms is the maintenance of documents of terms. The proposed model enables effective and automated tracking of change history of metadata vocabularies using simple formal description scheme defined based on widely-used standards. Originality/value: Changes in metadata vocabularies may cause inconsistencies in the longterm use of metadata. This paper proposes a simple and formal scheme of provenance description of metadata vocabularies. The proposed model works as the basis of automated maintenance of metadata terms and their vocabularies and is applicable to various types of changes.  Â© 2017 Chunqiu Li & Shigeo Sugimoto.","The purpose of this paper is to discuss provenance description of metadata terms and metadata vocabularies as a set of metadata terms. Provenance is crucial information to keep track of changes of metadata terms and metadata vocabularies for their consistent maintenance. The W3C PROV standard for general provenance description and Resource Description Framework (RDF) are adopted as the base models to formally define provenance description for metadata vocabularies. This paper defines a few primitive change types of metadata terms, and a provenance description model of the metadata terms based on the primitive change types. We also provide examples of provenance description in RDF graphs to show the proposed model. The model proposed in this paper is defined based on a few primitive relationships ( addition, deletion, and replacement) between pre-version and post-version of a metadata term. The model is simplified and the practical changes of metadata terms can be more complicated than the primitive relationships discussed in the model. Formal provenance description of metadata vocabularies can improve maintainability of metadata vocabularies over time. Conventional maintenance of metadata terms is the maintenance of documents of terms. The proposed model enables effective and automated tracking of change history of metadata vocabularies using simple formal description scheme defined based on widely-used standards. Changes in metadata vocabularies may cause inconsistencies in the longterm use of metadata. This paper proposes a simple and formal scheme of provenance description of metadata vocabularies. The proposed model works as the basis of automated maintenance of metadata terms and their vocabularies and is applicable to various types of changes."
A Local Adaptation in an Output-Based Research Support Scheme (OBRSS) at University College Dublin,"University College Dublin (UCD) has implemented the Output-Based Research Support Scheme (OBRSS) since 2016. Adapted from the Norwegian model, the OBRSS awards individual academic staff using a points system based on the number of publications and doctoral students. This article describes the design and implementation processes of the OBRSS, including the creation of the ranked publication list and points system and infrastructure requirements. Some results of the OBRSS will be presented, focusing on the coverage of publications reported in the OBRSS ranked publication list and Scopus, as well as information about spending patterns. Challenges such as the evaluation of the OBRSS in terms of fairness, transparency, and effectiveness will also be discussed. Â© 2018 Liam Cleere, Lai Ma, published by Sciendo 2018.","University College Dublin (UCD) has implemented the Output-Based Research Support Scheme (OBRSS) since 2016. Adapted from the Norwegian model, the OBRSS awards individual academic staff using a points system based on the number of publications and doctoral students. This article describes the design and implementation processes of the OBRSS, including the creation of the ranked publication list and points system and infrastructure requirements. Some results of the OBRSS will be presented, focusing on the coverage of publications reported in the OBRSS ranked publication list and Scopus, as well as information about spending patterns. Challenges such as the evaluation of the OBRSS in terms of fairness, transparency, and effectiveness will also be discussed."
"Predictive Characteristics of Co-authorship Networks: Comparing the unweighted, weighted, and bipartite cases","Purpose: This study aims to answer the question to what extent different types of networks can be used to predict future co-authorship among authors. Design/methodology/approach: We compare three types of networks: unweighted networks, in which a link represents a past collaboration; weighted networks, in which links are weighted by the number of joint publications; and bipartite author-publication networks. The analysis investigates their relation to positive stability, as well as their potential in predicting links in future versions of the co-authorship network. Several hypotheses are tested. Findings: Among other results, we find that weighted networks do not automatically lead to better predictions. Bipartite networks, however, outperform unweighted networks in almost all cases. Research limitations: Only two relatively small case studies are considered. Practical implications: The study suggests that future link prediction studies on co-occurrence networks should consider using the bipartite network as a training network. Originality/value: This is the first systematic comparison of unweighted, weighted, and bipartite training networks in link prediction. Â© Journal of Data and Information Science. All rights reserved.","This study aims to answer the question to what extent different types of networks can be used to predict future co-authorship among authors. We compare three types of networks: unweighted networks, in which a link represents a past collaboration; weighted networks, in which links are weighted by the number of joint publications; and bipartite author-publication networks. The analysis investigates their relation to positive stability, as well as their potential in predicting links in future versions of the co-authorship network. Several hypotheses are tested. Among other results, we find that weighted networks do not automatically lead to better predictions. Bipartite networks, however, outperform unweighted networks in almost all cases. Only two relatively small case studies are considered. The study suggests that future link prediction studies on co-occurrence networks should consider using the bipartite network as a training network. This is the first systematic comparison of unweighted, weighted, and bipartite training networks in link prediction."
An Accurate and Impartial Expert Assignment Method for Scientific Project Review,"Purpose: This paper proposes an expert assignment method for scientific project review that considers both accuracy and impartiality. As impartial and accurate peer review is extremely important to ensure the quality and feasibility of scientific projects, enhanced methods for managing the process are needed. Design/methodology/approach: To ensure both accuracy and impartiality, we design four criteria, the reviewers' fitness degree, research intensity, academic association, and potential conflict of interest, to express the characteristics of an appropriate peer review expert. We first formalize the expert assignment problem as an optimization problem based on the designed criteria, and then propose a randomized algorithm to solve the expert assignment problem of identifying reviewer adequacy. Findings: Simulation results show that the proposed method is quite accurate and impartial during expert assignment. Research limitations: Although the criteria used in this paper can properly show the characteristics of a good and appropriate peer review expert, more criteria/conditions can be included in the proposed scheme to further enhance accuracy and impartiality of the expert assignment. Practical implications: The proposed method can help project funding agencies (e.g. the National Natural Science Foundation of China) find better experts for project peer review. Originality/value: To the authors' knowledge, this is the first publication that proposes an algorithm that applies an impartial approach to the project review expert assignment process. The simulation results show the effectiveness of the proposed method.  Â© 2017 Walter de Gruyter GmbH, Berlin/Boston.","This paper proposes an expert assignment method for scientific project review that considers both accuracy and impartiality. As impartial and accurate peer review is extremely important to ensure the quality and feasibility of scientific projects, enhanced methods for managing the process are needed. To ensure both accuracy and impartiality, we design four criteria, the reviewers' fitness degree, research intensity, academic association, and potential conflict of interest, to express the characteristics of an appropriate peer review expert. We first formalize the expert assignment problem as an optimization problem based on the designed criteria, and then propose a randomized algorithm to solve the expert assignment problem of identifying reviewer adequacy. Simulation results show that the proposed method is quite accurate and impartial during expert assignment. Although the criteria used in this paper can properly show the characteristics of a good and appropriate peer review expert, more criteria/conditions can be included in the proposed scheme to further enhance accuracy and impartiality of the expert assignment. The proposed method can help project funding agencies ( the National Natural Science Foundation of China) find better experts for project peer review. To the authors' knowledge, this is the first publication that proposes an algorithm that applies an impartial approach to the project review expert assignment process. The simulation results show the effectiveness of the proposed method."
Patent Citations Analysis and Its Value in Research Evaluation: A Review and a New Approach to Map Technology-relevant Research,"Purpose: First, to review the state-of-the-art in patent citation analysis, particularly characteristics of patent citations to scientific literature (scientific non-patent references, SNPRs). Second, to present a novel mapping approach to identify technology-relevant research based on the papers cited by and referring to the SNPRs. Design/methodology/approach: In the review part we discuss the context of SNPRs such as the time lags between scientific achievements and inventions. Also patent-to-patent citation is addressed particularly because this type of patent citation analysis is a major element in the assessment of the economic value of patents. We also review the research on the role of universities and researchers in technological development, with important issues such as universities as sources of technological knowledge and inventor-author relations. We conclude the review part of this paper with an overview of recent research on mapping and network analysis of the science and technology interface and of technological progress in interaction with science. In the second part we apply new techniques for the direct visualization of the cited and citing relations of SNPRs, the mapping of the landscape around SNPRs by bibliographic coupling and co-citation analysis, and the mapping of the conceptual environment of SNPRs by keyword co-occurrence analysis. Findings: We discuss several properties of SNPRs. Only a small minority of publications covered by the Web of Science or Scopus are cited by patents, about 3%-4%. However, for publications based on university-industry collaboration the number of SNPRs is considerably higher, around 15%. The proposed mapping methodology based on a ""second order SNPR approach""enables a better assessment of the technological relevance of research. Research limitations: The main limitation is that a more advanced merging of patent and publication data, in particular unification of author and inventor names, in still a necessity. Practical implications: The proposed mapping methodology enables the creation of a database of technology-relevant papers (TRPs). In a bibliometric assessment the publications of research groups, research programs or institutes can be matched with the TRPs and thus the extent to which the work of groups, programs or institutes are relevant for technological development can be measured. Originality/value: The review part examines a wide range of findings in the research of patent citation analysis. The mapping approach to identify a broad range of technology-relevant papers is novel and offers new opportunities in research evaluation practices.  Â© 2017 Anthony F.J. van Raan.","First, to review the state-of-the-art in patent citation analysis, particularly characteristics of patent citations to scientific literature (scientific non-patent references, SNPRs). Second, to present a novel mapping approach to identify technology-relevant research based on the papers cited by and referring to the SNPRs. In the review part we discuss the context of SNPRs such as the time lags between scientific achievements and inventions. Also patent-to-patent citation is addressed particularly because this type of patent citation analysis is a major element in the assessment of the economic value of patents. We also review the research on the role of universities and researchers in technological development, with important issues such as universities as sources of technological knowledge and inventor-author relations. We conclude the review part of this paper with an overview of recent research on mapping and network analysis of the science and technology interface and of technological progress in interaction with science. In the second part we apply new techniques for the direct visualization of the cited and citing relations of SNPRs, the mapping of the landscape around SNPRs by bibliographic coupling and co-citation analysis, and the mapping of the conceptual environment of SNPRs by keyword co-occurrence analysis. We discuss several properties of SNPRs. Only a small minority of publications covered by the Web of Science or Scopus are cited by patents, about 3%-4%. However, for publications based on university-industry collaboration the number of SNPRs is considerably higher, around 15%. The proposed mapping methodology based on a ""second order SNPR approach""enables a better assessment of the technological relevance of research. The main limitation is that a more advanced merging of patent and publication data, in particular unification of author and inventor names, in still a necessity. The proposed mapping methodology enables the creation of a database of technology-relevant papers (TRPs). In a bibliometric assessment the publications of research groups, research programs or institutes can be matched with the TRPs and thus the extent to which the work of groups, programs or institutes are relevant for technological development can be measured. The review part examines a wide range of findings in the research of patent citation analysis. The mapping approach to identify a broad range of technology-relevant papers is novel and offers new opportunities in research evaluation practices."
A comparison of citation disciplinary structure in science between the G7 countries and the BRICS countries,"This study aims to compare the characteristics of citation disciplinary structure between the G7 countries and the BRICS countries. In this contribution, which uses about 1 million Web of Science publications and two publications years (1993 and 2013), we compare the G7 countries and the BRICS countries with regard to this type of structure. For the publication year 2013, cosine similarity values regarding the citation disciplinary structures of these countries (and of nine other countries) were used as input to cluster analysis. We also obtained cosine similarity values for a given country and its citation disciplinary structures across the two publication years. Moreover, for the publication year 2013, the within-country Jeffreys-Matusita distance between publication and citation disciplinary structure was measured. First, the citation disciplinary structures of countries depend on multiple and complex factors. It is therefore difficult to completely explain the formation and change of the citation disciplinary structure of a country. This study suggests some possible causes, whereas detailed explanations might be given by future research. Second, the length of the citation window used in this study is three years. However, scientific disciplines differ in their citation practices. Comparison between citations across disciplines using the same citation window length may affect the citation discipline structure results for some countries. First, the results of this study are based on the WoS database. However, in this database some fields are covered to a greater extent than others, which may affect the results for the citation discipline structure for some studied countries. In future research, we might repeat this study using another database (like Scopus) and, in that case, we would like to make comparisons between the two outcomes. Second, the use of a constant journal set yielded that a large share of the journals covered by WoS year 2013 is ignored in the study. Thus, disciplinary structure is studied based on a quite restricted set of publications. The three mentioned limitations should be kept in mind when the results of this study are interpreted. Disciplinary structure on country level is a highlighted topic for the S&T policy makers, especially for those come from developing countries. This study observes the disciplinary structure in the view of academic impact, and the result will provide some evidence to make decision for the discipline strategy and funding allocation. Besides, Jeffreys-Matusita distance is introduced to measure the similarity of citation disciplinary structure and publication disciplinary structure. By applying this measure, some new observations were drawn, for example, ""Based on the comparison of publication disciplinary structure and citation disciplinary structure, the paper finds most BRICS counties have less impact with more publications"". The outcome of the cluster analysis indicates that the G7 countries and BRICS countries are quite heterogeneous regarding their citation disciplinary structure. For a majority of the G7 countries, the citation disciplinary structure tend to be more stable compared to BRICS countries with regard to the years 1993 and 2013. Most G7 countries, with United States as an exception, turned out to have lower values on the Jeffreys-Matusita distance than BRICS countries, indicating a higher degree of heterogeneity between the publication and the citation disciplinary structure for the latter countries. In other words, BRICS countries still receive much less citations in most disciplines than their publication output would suggest. G7 countries can still expect more citations than is to be expected based on their publication output, thereby generating relatively more impact than BRICS countries. Â© 2018 Ting Yue, Liying Yang, Per Ahlgren, Jielan Ding, Shuangqing Shi, Rainer Frietsch, published by Sciendo.","This study aims to compare the characteristics of citation disciplinary structure between the G7 countries and the BRICS countries. In this contribution, which uses about 1 million Web of Science publications and two publications years (1993 and 2013), we compare the G7 countries and the BRICS countries with regard to this type of structure. For the publication year 2013, cosine similarity values regarding the citation disciplinary structures of these countries (and of nine other countries) were used as input to cluster analysis. We also obtained cosine similarity values for a given country and its citation disciplinary structures across the two publication years. Moreover, for the publication year 2013, the within-country Jeffreys-Matusita distance between publication and citation disciplinary structure was measured. First, the citation disciplinary structures of countries depend on multiple and complex factors. It is therefore difficult to completely explain the formation and change of the citation disciplinary structure of a country. This study suggests some possible causes, whereas detailed explanations might be given by future research. Second, the length of the citation window used in this study is three years. However, scientific disciplines differ in their citation practices. Comparison between citations across disciplines using the same citation window length may affect the citation discipline structure results for some countries. First, the results of this study are based on the WoS database. However, in this database some fields are covered to a greater extent than others, which may affect the results for the citation discipline structure for some studied countries. In future research, we might repeat this study using another database (like Scopus) and, in that case, we would like to make comparisons between the two outcomes. Second, the use of a constant journal set yielded that a large share of the journals covered by WoS year 2013 is ignored in the study. Thus, disciplinary structure is studied based on a quite restricted set of publications. The three mentioned limitations should be kept in mind when the results of this study are interpreted. Disciplinary structure on country level is a highlighted topic for the S&T policy makers, especially for those come from developing countries. This study observes the disciplinary structure in the view of academic impact, and the result will provide some evidence to make decision for the discipline strategy and funding allocation. Besides, Jeffreys-Matusita distance is introduced to measure the similarity of citation disciplinary structure and publication disciplinary structure. By applying this measure, some new observations were drawn, for example, ""Based on the comparison of publication disciplinary structure and citation disciplinary structure, the paper finds most BRICS counties have less impact with more publications"". The outcome of the cluster analysis indicates that the G7 countries and BRICS countries are quite heterogeneous regarding their citation disciplinary structure. For a majority of the G7 countries, the citation disciplinary structure tend to be more stable compared to BRICS countries with regard to the years 1993 and 2013. Most G7 countries, with United States as an exception, turned out to have lower values on the Jeffreys-Matusita distance than BRICS countries, indicating a higher degree of heterogeneity between the publication and the citation disciplinary structure for the latter countries. In other words, BRICS countries still receive much less citations in most disciplines than their publication output would suggest. G7 countries can still expect more citations than is to be expected based on their publication output, thereby generating relatively more impact than BRICS countries."
Visualization of Disciplinary Profiles: Enhanced Science Overlay Maps,"Purpose: The purpose of this study is to modernize previous work on science overlay maps by updating the underlying citation matrix, generating new clusters of scientific disciplines, enhancing visualizations, and providing more accessible means for analysts to generate their own maps. Design/methodology/approach: We use the combined set of 2015 Journal Citation Reports for the Science Citation Index (n of journals = 8,778) and the Social Sciences Citation Index (n = 3,212) for a total of 11,365 journals. The set of Web of Science Categories in the Science Citation Index and the Social Sciences Citation Index increased from 224 in 2010 to 227 in 2015. Using dedicated software, a matrix of 227 Ã 227 cells is generated on the basis of whole-number citation counting. We normalize this matrix using the cosine function. We first develop the citing-side, cosine-normalized map using 2015 data and VOSviewer visualization with default parameter values. A routine for making overlays on the basis of the map (""wc15.exe"") is available at http://www.leydesdorff.net/wc15/index.htm. Findings: Findings appear in the form of visuals throughout the manuscript. In Figures 1-9 we provide basemaps of science and science overlay maps for a number of companies, universities, and technologies. Research limitations: As Web of Science Categories change and/or are updated so is the need to update the routine we provide. Also, to apply the routine we provide users need access to the Web of Science. Practical implications: Visualization of science overlay maps is now more accurate and true to the 2015 Journal Citation Reports than was the case with the previous version of the routine advanced in our paper. Originality/value: The routine we advance allows users to visualize science overlay maps in VOSviewer using data from more recent Journal Citation Reports.  Â© 2017 Stephen Carley, Alan L. Porter, Ismael Rafols & Loet Leydesdorff.","The purpose of this study is to modernize previous work on science overlay maps by updating the underlying citation matrix, generating new clusters of scientific disciplines, enhancing visualizations, and providing more accessible means for analysts to generate their own maps. We use the combined set of 2015 Journal Citation Reports for the Science Citation Index (n of journals = 8,778) and the Social Sciences Citation Index (n = 3,212) for a total of 11,365 journals. The set of Web of Science Categories in the Science Citation Index and the Social Sciences Citation Index increased from 224 in 2010 to 227 in 2015. Using dedicated software, a matrix of 227 227 cells is generated on the basis of whole-number citation counting. We normalize this matrix using the cosine function. We first develop the citing-side, cosine-normalized map using 2015 data and VOSviewer visualization with default parameter values. A routine for making overlays on the basis of the map (""wc15.exe"") is available at http://www.leydesdorff.net/wc15/index.htm. Findings appear in the form of visuals throughout the manuscript. In Figures 1-9 we provide basemaps of science and science overlay maps for a number of companies, universities, and technologies. As Web of Science Categories change and/or are updated so is the need to update the routine we provide. Also, to apply the routine we provide users need access to the Web of Science. Visualization of science overlay maps is now more accurate and true to the 2015 Journal Citation Reports than was the case with the previous version of the routine advanced in our paper. The routine we advance allows users to visualize science overlay maps in VOSviewer using data from more recent Journal Citation Reports."
Big Data and Data Science: Opportunities and Challenges of iSchools,"Due to the recent explosion of big data, our society has been rapidly going through digital transformation and entering a new world with numerous eye-opening developments. These new trends impact the society and future jobs, and thus student careers. At the heart of this digital transformation is data science, the discipline that makes sense of big data. With many rapidly emerging digital challenges ahead of us, this article discusses perspectives on iSchools' opportunities and suggestions in data science education. We argue that iSchools should empower their students with ""information computing""disciplines, which we define as the ability to solve problems and create values, information, and knowledge using tools in application domains. As specific approaches to enforcing information computing disciplines in data science education, we suggest the three foci of user-based, tool-based, and application-based. These three foci will serve to differentiate the data science education of iSchools from that of computer science or business schools. We present a layered Data Science Education Framework (DSEF) with building blocks that include the three pillars of data science (people, technology, and data), computational thinking, data-driven paradigms, and data science lifecycles. Data science courses built on the top of this framework should thus be executed with user-based, tool-based, and application-based approaches. This framework will help our students think about data science problems from the big picture perspective and foster appropriate problem-solving skills in conjunction with broad perspectives of data science lifecycles. We hope the DSEF discussed in this article will help fellow iSchools in their design of new data science curricula.  Â© 2017 Il-Yeol Song & Yongjun Zhu.","Due to the recent explosion of big data, our society has been rapidly going through digital transformation and entering a new world with numerous eye-opening developments. These new trends impact the society and future jobs, and thus student careers. At the heart of this digital transformation is data science, the discipline that makes sense of big data. With many rapidly emerging digital challenges ahead of us, this article discusses perspectives on iSchools' opportunities and suggestions in data science education. We argue that iSchools should empower their students with ""information computing""disciplines, which we define as the ability to solve problems and create values, information, and knowledge using tools in application domains. As specific approaches to enforcing information computing disciplines in data science education, we suggest the three foci of user-based, tool-based, and application-based. These three foci will serve to differentiate the data science education of iSchools from that of computer science or business schools. We present a layered Data Science Education Framework (DSEF) with building blocks that include the three pillars of data science (people, technology, and data), computational thinking, data-driven paradigms, and data science lifecycles. Data science courses built on the top of this framework should thus be executed with user-based, tool-based, and application-based approaches. This framework will help our students think about data science problems from the big picture perspective and foster appropriate problem-solving skills in conjunction with broad perspectives of data science lifecycles. We hope the DSEF discussed in this article will help fellow iSchools in their design of new data science curricula."
Digitizing Dunhuang Cultural Heritage: A User Evaluation of Mogao Cave Panorama Digital Library,"Purpose: This study is a user evaluation on the usability of the Mogao Cave Panorama Digital Library (DL), aiming to measure its effectiveness from the users' perspective and to propose suggestions for improvement. Design/methodology/approach: Usability tests were conducted based on a framework of evaluation criteria and a set of information seeking tasks designed for the Dunhuang cultural heritage, and interviews were conducted for soliciting in-depth opinions from participants. Findings: The results of the usability tests indicate that the DL was more efficient in supporting simple information seeking tasks than those of higher-complexity levels. Statistical tests reveal that there were correlations among dimensions of usability criteria and user effectiveness measures. Moreover, interview discourses exposed specific usability issues of the DL. Research limitations: This research is based on a relatively small sample size, resulting in a limited representativeness of user diversity. A larger sample size is needed for a systematic cross group comparison. Practical implications: This study evaluated the usability of the Mogao Cave Panorama DL and proposed suggestions for its improvement for better experience. The results also provide a reference to other cultural heritage DLs with panorama functions. Originality/value: This study is one of the first evaluating cultural heritage DLs from the perspective of user experience. It provides methodological references for relevant studies: the evaluation framework, the designed information seeking tasks, and the interview questions can be adopted or adapted in evaluating other visually centric DLs of cultural heritage.  Â© 2017 Xiao Hu, Eric M. Y. Ho & Chen Qiao.","This study is a user evaluation on the usability of the Mogao Cave Panorama Digital Library , aiming to measure its effectiveness from the users' perspective and to propose suggestions for improvement. Usability tests were conducted based on a framework of evaluation criteria and a set of information seeking tasks designed for the Dunhuang cultural heritage, and interviews were conducted for soliciting in-depth opinions from participants. The results of the usability tests indicate that the DL was more efficient in supporting simple information seeking tasks than those of higher-complexity levels. Statistical tests reveal that there were correlations among dimensions of usability criteria and user effectiveness measures. Moreover, interview discourses exposed specific usability issues of the This research is based on a relatively small sample size, resulting in a limited representativeness of user diversity. A larger sample size is needed for a systematic cross group comparison. This study evaluated the usability of the Mogao Cave Panorama DL and proposed suggestions for its improvement for better experience. The results also provide a reference to other cultural heritage DLs with panorama functions. This study is one of the first evaluating cultural heritage DLs from the perspective of user experience. It provides methodological references for relevant studies: the evaluation framework, the designed information seeking tasks, and the interview questions can be adopted or adapted in evaluating other visually centric DLs of cultural heritage."
Identification and Analysis of Multi-tasking Product Information Search Sessions with Query Logs,"Purpose: This research aims to identify product search tasks in online shopping and analyze the characteristics of consumer multi-tasking search sessions. Design/methodology/approach: The experimental dataset contains 8,949 queries of 582 users from 3,483 search sessions. A sequential comparison of the Jaccard similarity coefficient between two adjacent search queries and hierarchical clustering of queries is used to identify search tasks. Findings: (1) Users issued a similar number of queries (1.43 to 1.47) with similar lengths (7.3-7.6 characters) per task in mono-tasking and multi-tasking sessions, and (2) Users spent more time on average in sessions with more tasks, but spent less time for each task when the number of tasks increased in a session. Research limitations: The task identification method that relies only on query terms does not completely reflect the complex nature of consumer shopping behavior. Practical implications: These results provide an exploratory understanding of the relationships among multiple shopping tasks, and can be useful for product recommendation and shopping task prediction. Originality/value: The originality of this research is its use of query clustering with online shopping task identification and analysis, and the analysis of product search session characteristics. Â© Journal of Data and Information Science. All rights reserved.","This research aims to identify product search tasks in online shopping and analyze the characteristics of consumer multi-tasking search sessions. The experimental dataset contains 8,949 queries of 582 users from 3,483 search sessions. A sequential comparison of the Jaccard similarity coefficient between two adjacent search queries and hierarchical clustering of queries is used to identify search tasks. Users issued a similar number of queries (1.43 to 1.47) with similar lengths (7.3-7.6 characters) per task in mono-tasking and multi-tasking sessions, and Users spent more time on average in sessions with more tasks, but spent less time for each task when the number of tasks increased in a session. The task identification method that relies only on query terms does not completely reflect the complex nature of consumer shopping behavior. These results provide an exploratory understanding of the relationships among multiple shopping tasks, and can be useful for product recommendation and shopping task prediction. The originality of this research is its use of query clustering with online shopping task identification and analysis, and the analysis of product search session characteristics."
A bibliometric framework for identifying âPrincesâ who wake up the âsleeping beautyâ in challenge-type scientific discoveries,"Purpose: This paper develops and validates a bibliometric framework for identifying the âprincesâ (PR) who wake up the âsleeping beautyâ (SB) in challenge-type scientific discoveries, so as to figure out the awakening mechanisms, and promote potentially valuable but not readily accepted innovative research. (A PR is a research study.) Design/methodology/approach: We propose that PR candidates must meet the following four criteria: (1) be published near the time when the SB began to attract a lot of citations; (2) be highly cited papers themselves; (3) receive a substantial number of co-citations with the SB; and (4) within the challenge-type discoveries which contradict established theories, the âpulling effectâ of the PR on the SB must be strong. We test the usefulness of the bibliometric framework through a case study of a key publication by the 2014 chemistry Nobel laureate Stefan W. Hell, who negated Ernst Abbe's diffraction limit theory, one of the most prominent paradigms in the natural sciences. Findings: The first-ranked candidate PR article identified by the bibliometric framework is in line with historical facts. An SB may need one or more PRs and even âretinuesâ to be âawakened.â Documents with potential awakening functionality tend to be published in prestigious multidisciplinary journals with higher impact and wider scope than the journals publishing SBs. Research limitations: The above framework is only applicable to transformative innovations, and the conclusions are drawn from the analysis of one typical SB and her awakening process. Therefore the generality of our work might be limited. Practical implications: Publications belonging to so-called transformative research, even when less frequently cited, should be given special attention as early as possible, because they may suddenly attract many citations after a period of sleep, as reflected in our case study. Originality/value: The definition of PR(s) as the first paper(s) that cited the SB article (self-citing excluded) has its limitations. Instead, the SB-PR co-citations should be given priority in current environment of scholarly communication. Since the âprematureâ or âtransformativeâ breakthroughs in the challenge-type SB documents are either beyond the current knowledge domain, or violate established paradigms, people's psychological distance from the SB is larger than that from the PR, which explains why the annual citations of the PR are usually higher than those of the SB, especially prior to or during the SB's citation boom period. Â© Journal of Data and Information Science. All rights reserved.","This paper develops and validates a bibliometric framework for identifying the princes (PR) who wake up the sleeping beauty (SB) in challenge-type scientific discoveries, so as to figure out the awakening mechanisms, and promote potentially valuable but not readily accepted innovative research. (A PR is a research study.) We propose that PR candidates must meet the following four criteria: be published near the time when the SB began to attract a lot of citations; be highly cited papers themselves; receive a substantial number of co-citations with the SB; and within the challenge-type discoveries which contradict established theories, the pulling effect of the PR on the SB must be strong. We test the usefulness of the bibliometric framework through a case study of a key publication by the 2014 chemistry Nobel laureate Stefan Hell, who negated Ernst Abbe's diffraction limit theory, one of the most prominent paradigms in the natural sciences. The first-ranked candidate PR article identified by the bibliometric framework is in line with historical facts. An SB may need one or more PRs and even retinues to be awakened. Documents with potential awakening functionality tend to be published in prestigious multidisciplinary journals with higher impact and wider scope than the journals publishing SBs. The above framework is only applicable to transformative innovations, and the conclusions are drawn from the analysis of one typical SB and her awakening process. Therefore the generality of our work might be limited. Publications belonging to so-called transformative research, even when less frequently cited, should be given special attention as early as possible, because they may suddenly attract many citations after a period of sleep, as reflected in our case study. The definition of PR(s) as the first paper(s) that cited the SB article (self-citing excluded) has its limitations. Instead, the SB-PR co-citations should be given priority in current environment of scholarly communication. Since the premature or transformative breakthroughs in the challenge-type SB documents are either beyond the current knowledge domain, or violate established paradigms, people's psychological distance from the SB is larger than that from the PR, which explains why the annual citations of the PR are usually higher than those of the SB, especially prior to or during the SB's citation boom period."
Redesigning the Model of Book Evaluation in the Polish Performance-based Research Funding System,"This study aims to present the key systemic changes in the Polish book evaluation model to focus on the publisher list, as inspired by the Norwegian Model. In this study we reconstruct the framework of the 2010 and 2018 models of book evaluation in Poland within the performance-based research funding system. For almost 20 years the book evaluation system in Poland has been based on the verification of various technical criteria (e.g. length of the book). The new 2018 model is based on the principle of prestige inheritance (a book is worth as much as its publisher is) and is inspired by the publisher list used in the Norwegian Model. In this paper, we argue that this solution may be a more balanced policy instrument than the previous 2010 model in which neither the quality of the publisher nor the quality of the book played any role in the evaluation. We work from the framework of the 2018 model of book evaluation specified in the law on higher education and science from 20 July 2018, as implementation acts are not available yet. This study may provide a valuable point of reference on how structural reforms in the research evaluation model were implemented on a country level. The results of this study may be interesting to policy makers, stakeholders and researchers focused on science policy. This is the very first study that presents the new framework of the Polish research evaluation model and policy instruments for scholarly book evaluation. We describe what motivated policy makers to change the book evaluation model, and what arguments were explicitly raised to argue for the new solution. Â© 2018 2018 Emanuel Kulczycki, PrzemysÅaw Korytkowski, published by Sciendo.","This study aims to present the key systemic changes in the Polish book evaluation model to focus on the publisher list, as inspired by the Norwegian Model. In this study we reconstruct the framework of the 2010 and 2018 models of book evaluation in Poland within the performance-based research funding system. For almost 20 years the book evaluation system in Poland has been based on the verification of various technical criteria ( length of the book). The new 2018 model is based on the principle of prestige inheritance (a book is worth as much as its publisher is) and is inspired by the publisher list used in the Norwegian Model. In this paper, we argue that this solution may be a more balanced policy instrument than the previous 2010 model in which neither the quality of the publisher nor the quality of the book played any role in the evaluation. We work from the framework of the 2018 model of book evaluation specified in the law on higher education and science from 20 July 2018, as implementation acts are not available yet. This study may provide a valuable point of reference on how structural reforms in the research evaluation model were implemented on a country level. The results of this study may be interesting to policy makers, stakeholders and researchers focused on science policy. This is the very first study that presents the new framework of the Polish research evaluation model and policy instruments for scholarly book evaluation. We describe what motivated policy makers to change the book evaluation model, and what arguments were explicitly raised to argue for the new solution."
Enhancing Navigability: An Algorithm for Constructing Tag Trees,"Purpose: This study introduces an algorithm to construct tag trees that can be used as a user-friendly navigation tool for knowledge sharing and retrieval by solving two issues of previous studies, i.e. semantic drift and structural skew. Design/methodology/approach: Inspired by the generality based methods, this study builds tag trees from a co-occurrence tag network and uses the h-degree as a node generality metric. The proposed algorithm is characterized by the following four features: (1) the ancestors should be more representative than the descendants, (2) the semantic meaning along the ancestor-descendant paths needs to be coherent, (3) the children of one parent are collectively exhaustive and mutually exclusive in describing their parent, and (4) tags are roughly evenly distributed to their upper-level parents to avoid structural skew. Findings: The proposed algorithm has been compared with a well-established solution Heymann Tag Tree (HTT). The experimental results using a social tag dataset showed that the proposed algorithm with its default condition outperformed HTT in precision based on Open Directory Project (ODP) classification. It has been verified that h-degree can be applied as a better node generality metric compared with degree centrality. Research limitations: A thorough investigation into the evaluation methodology is needed, including user studies and a set of metrics for evaluating semantic coherence and navigation performance. Practical implications: The algorithm will benefit the use of digital resources by generating a flexible domain knowledge structure that is easy to navigate. It could be used to manage multiple resource collections even without social annotations since tags can be keywords created by authors or experts, as well as automatically extracted from text. Originality/value: Few previous studies paid attention to the issue of whether the tagging systems are easy to navigate for users. The contributions of this study are twofold: (1) an algorithm was developed to construct tag trees with consideration given to both semantic coherence and structural balance and (2) the effectiveness of a node generality metric, h-degree, was investigated in a tag co-occurrence network.  Â© 2017 Chong Chen & Pengcheng Luo.","This study introduces an algorithm to construct tag trees that can be used as a user-friendly navigation tool for knowledge sharing and retrieval by solving two issues of previous studies, semantic drift and structural skew. Inspired by the generality based methods, this study builds tag trees from a co-occurrence tag network and uses the h-degree as a node generality metric. The proposed algorithm is characterized by the following four features: the ancestors should be more representative than the descendants, the semantic meaning along the ancestor-descendant paths needs to be coherent, the children of one parent are collectively exhaustive and mutually exclusive in describing their parent, and tags are roughly evenly distributed to their upper-level parents to avoid structural skew. The proposed algorithm has been compared with a well-established solution Heymann Tag Tree (HTT). The experimental results using a social tag dataset showed that the proposed algorithm with its default condition outperformed HTT in precision based on Open Directory Project (ODP) classification. It has been verified that h-degree can be applied as a better node generality metric compared with degree centrality. A thorough investigation into the evaluation methodology is needed, including user studies and a set of metrics for evaluating semantic coherence and navigation performance. The algorithm will benefit the use of digital resources by generating a flexible domain knowledge structure that is easy to navigate. It could be used to manage multiple resource collections even without social annotations since tags can be keywords created by authors or experts, as well as automatically extracted from text. Few previous studies paid attention to the issue of whether the tagging systems are easy to navigate for users. The contributions of this study are twofold: an algorithm was developed to construct tag trees with consideration given to both semantic coherence and structural balance and the effectiveness of a node generality metric, h-degree, was investigated in a tag co-occurrence network."
Can automatic classification help to increase accuracy in data collection?,"Purpose: The authors aim at testing the performance of a set of machine learning algorithms that could improve the process of data cleaning when building datasets. Design/methodology/approach: The paper is centered on cleaning datasets gathered from publishers and online resources by the use of specific keywords. In this case, we analyzed data from the Web of Science. The accuracy of various forms of automatic classification was tested here in comparison with manual coding in order to determine their usefulness for data collection and cleaning. We assessed the performance of seven supervised classification algorithms (Support Vector Machine (SVM), Scaled Linear Discriminant Analysis, Lasso and elastic-net regularized generalized linear models, Maximum Entropy, Regression Tree, Boosting, and Random Forest) and analyzed two properties: accuracy and recall. We assessed not only each algorithm individually, but also their combinations through a voting scheme. We also tested the performance of these algorithms with different sizes of training data. When assessing the performance of different combinations, we used an indicator of coverage to account for the agreement and disagreement on classification between algorithms. Findings: We found that the performance of the algorithms used vary with the size of the sample for training. However, for the classification exercise in this paper the best performing algorithms were SVM and Boosting. The combination of these two algorithms achieved a high agreement on coverage and was highly accurate. This combination performs well with a small training dataset (10%), which may reduce the manual work needed for classification tasks. Research limitations: The dataset gathered has significantly more records related to the topic of interest compared to unrelated topics. This may affect the performance of some algorithms, especially in their identification of unrelated papers. Practical implications: Although the classification achieved by this means is not completely accurate, the amount of manual coding needed can be greatly reduced by using classification algorithms. This can be of great help when the dataset is big. With the help of accuracy, recall, and coverage measures, it is possible to have an estimation of the error involved in this classification, which could open the possibility of incorporating the use of these algorithms in software specifically designed for data cleaning and classification. Originality/value: We analyzed the performance of seven algorithms and whether combinations of these algorithms improve accuracy in data collection. Use of these algorithms could reduce time needed for manual data cleaning. Â© Journal of Data and Information Science. All rights reserved.","The authors aim at testing the performance of a set of machine learning algorithms that could improve the process of data cleaning when building datasets. The paper is centered on cleaning datasets gathered from publishers and online resources by the use of specific keywords. In this case, we analyzed data from the Web of Science. The accuracy of various forms of automatic classification was tested here in comparison with manual coding in order to determine their usefulness for data collection and cleaning. We assessed the performance of seven supervised classification algorithms (Support Vector Machine (SVM), Scaled Linear Discriminant Analysis, Lasso and elastic-net regularized generalized linear models, Maximum Entropy, Regression Tree, Boosting, and Random Forest) and analyzed two properties: accuracy and recall. We assessed not only each algorithm individually, but also their combinations through a voting scheme. We also tested the performance of these algorithms with different sizes of training data. When assessing the performance of different combinations, we used an indicator of coverage to account for the agreement and disagreement on classification between algorithms. We found that the performance of the algorithms used vary with the size of the sample for training. However, for the classification exercise in this paper the best performing algorithms were SVM and Boosting. The combination of these two algorithms achieved a high agreement on coverage and was highly accurate. This combination performs well with a small training dataset (10%), which may reduce the manual work needed for classification tasks. The dataset gathered has significantly more records related to the topic of interest compared to unrelated topics. This may affect the performance of some algorithms, especially in their identification of unrelated papers. Although the classification achieved by this means is not completely accurate, the amount of manual coding needed can be greatly reduced by using classification algorithms. This can be of great help when the dataset is big. With the help of accuracy, recall, and coverage measures, it is possible to have an estimation of the error involved in this classification, which could open the possibility of incorporating the use of these algorithms in software specifically designed for data cleaning and classification. We analyzed the performance of seven algorithms and whether combinations of these algorithms improve accuracy in data collection. Use of these algorithms could reduce time needed for manual data cleaning."
DPaper: An authoring tool for extractable digital papers,"Purpose: To develop a structured, rich media digital paper authoring tool with an object-based model that enables interactive, playable, and convertible functions. Design/methodology/approach: We propose Dpaper to organize the content (text, data, rich media, etc.) of dissertation papers as XML and HTML5 files by means of digital objects and digital templates. Findings: Dpaper provides a structured-paper editorial platform for the authors of PhDs to organize research materials and to generate various digital paper objects that are playable and reusable. The PhD papers are represented as Web pages and structured XML files, which are marked with semantic tags. Research limitations: The proposed tool only provides access to a limited number of digital objects. For instance, the tool cannot create equations and graphs, and typesetting is not yet flexible compared to MS Word. Practical implications: The Dpaper tool is designed to break through the patterns of unstructured content organization of traditional papers, and makes the paper accessible for not only reading but for exploitation as data, where the document can be extractable and reusable. As a result, Dpaper can make the digital publishing of dissertation texts more flexible and efficient, and their data more assessable. Originality/value: The Dpaper tool solves the challenge of making a paper structured and object-based in the stage of authoring, and has practical values for semantic publishing. Â© Journal of Data and Information Science. All rights reserved.","To develop a structured, rich media digital paper authoring tool with an object-based model that enables interactive, playable, and convertible functions. We propose Dpaper to organize the content (text, data, rich media, etc.) of dissertation papers as XML and HTML5 files by means of digital objects and digital templates. Dpaper provides a structured-paper editorial platform for the authors of PhDs to organize research materials and to generate various digital paper objects that are playable and reusable. The PhD papers are represented as Web pages and structured XML files, which are marked with semantic tags. The proposed tool only provides access to a limited number of digital objects. For instance, the tool cannot create equations and graphs, and typesetting is not yet flexible compared to MS Word. The Dpaper tool is designed to break through the patterns of unstructured content organization of traditional papers, and makes the paper accessible for not only reading but for exploitation as data, where the document can be extractable and reusable. As a result, Dpaper can make the digital publishing of dissertation texts more flexible and efficient, and their data more assessable. The Dpaper tool solves the challenge of making a paper structured and object-based in the stage of authoring, and has practical values for semantic publishing."
Insight into the Disciplinary Structure of Nanoscience & Nanotechnology,"Purpose: This paper aims to gain an insight into the disciplinary structure of nanoscience & nanotechnology (N&N): What is the disciplinary network of N&N like? Which disciplines are being integrated into N&N over time? For a specific discipline, how many other disciplines have direct or indirect connections with it? What are the distinct subgroups of N&N at different evolutionary stages? Such critical issues are to be addressed in this paper. Design/methodology/approach: We map the disciplinary network structure of N&N by employing the social network analysis tool, Netdraw, identifying which Web of Science Categories (WCs) mediate nbetweenness centrality in different stages of nano development. Cliques analysis embedded in the Ucinet program is applied to do the disciplinary cluster analysis in the study according to the path of ""Network-Subgroup-Cliques,""and a tree diagram is selected as the visualizing type. Findings: The disciplinary network structure reveals the relationships among different disciplines in the N&N developing process clearly, and it is easy for us to identify which disciplines are connected with the core ""N&N""directly or indirectly. The tree diagram showing N&N related disciplines provides an interesting perspective on nano research and development (R&D) structure. Research limitations: The matrices used to draw the N&N disciplinary network are the original ones, and normalized matrix could be tried in future similar studies. Practical implications: Results in this paper can help us better understand the disciplinary structure of N&N, and the dynamic evolution of N&N related disciplines over time. The findings could benefit R&D decision making. It can support policy makers from government agencies engaging in science and technology (S&T) management or S&T strategy planners to formulate efficient decisions according to a perspective of converging sciences and technologies. Originality/value: The novelty of this study lies in mapping the disciplinary network structure of N&N clearly, identifying which WCs have a mediating effect in different developmental stages (especially analyzing clusters among disciplines related to N&N, revealing close or distant relationships among distinct areas pertinent to N&N).  Â© 2017 Chunjuan Luan et al.","This paper aims to gain an insight into the disciplinary structure of nanoscience & nanotechnology (N&N): What is the disciplinary network of N&N like? Which disciplines are being integrated into N&N over time? For a specific discipline, how many other disciplines have direct or indirect connections with it? What are the distinct subgroups of N&N at different evolutionary stages? Such critical issues are to be addressed in this paper. We map the disciplinary network structure of N&N by employing the social network analysis tool, Netdraw, identifying which Web of Science Categories (WCs) mediate nbetweenness centrality in different stages of nano development. Cliques analysis embedded in the Ucinet program is applied to do the disciplinary cluster analysis in the study according to the path of ""Network-Subgroup-Cliques,""and a tree diagram is selected as the visualizing type. The disciplinary network structure reveals the relationships among different disciplines in the N&N developing process clearly, and it is easy for us to identify which disciplines are connected with the core ""N&N""directly or indirectly. The tree diagram showing N&N related disciplines provides an interesting perspective on nano research and development (R&D) structure. The matrices used to draw the N&N disciplinary network are the original ones, and normalized matrix could be tried in future similar studies. Results in this paper can help us better understand the disciplinary structure of N&N, and the dynamic evolution of N&N related disciplines over time. The findings could benefit R&D decision making. It can support policy makers from government agencies engaging in science and technology (S&T) management or S&T strategy planners to formulate efficient decisions according to a perspective of converging sciences and technologies. The novelty of this study lies in mapping the disciplinary network structure of N&N clearly, identifying which WCs have a mediating effect in different developmental stages (especially analyzing clusters among disciplines related to N&N, revealing close or distant relationships among distinct areas pertinent to N&N)."
A Framework for the Assessment of Research and Its Impacts,"This paper proposes a holistic framework for the development of models for the assessment of research activities and their impacts. It distinguishes three dimensions, including in an original way, data as a main dimension, together with theory and methodology. Each dimension of the framework is further characterized by three main building blocks: education, research, and innovation (theory); efficiency, effectiveness, and impact (methodology); and availability, interoperability, and ""unit-free""property (data). The different dimensions and their nine constituent building blocks are attributes of an overarching concept, denoted as ""quality.""Three additional quality attributes are identified as implementation factors (tailorability, transparency, and openness) and three ""enabling""conditions (convergence, mixed methods, and knowledge infrastructures) complete the framework. A framework is required to develop models of metrics. Models of metrics are necessary to assess the meaning, validity, and robustness of metrics. The proposed framework can be a useful reference for the development of the ethics of research evaluation. It can act as a common denominator for different analytical levels and relevant aspects and is able to embrace many different and heterogeneous streams of literature. Directions for future research are provided.  Â© 2017 Walter de Gruyter GmbH, Berlin/Boston.","This paper proposes a holistic framework for the development of models for the assessment of research activities and their impacts. It distinguishes three dimensions, including in an original way, data as a main dimension, together with theory and methodology. Each dimension of the framework is further characterized by three main building blocks: education, research, and innovation (theory); efficiency, effectiveness, and impact (methodology); and availability, interoperability, and ""unit-free""property (data). The different dimensions and their nine constituent building blocks are attributes of an overarching concept, denoted as ""quality.""Three additional quality attributes are identified as implementation factors (tailorability, transparency, and openness) and three ""enabling""conditions (convergence, mixed methods, and knowledge infrastructures) complete the framework. A framework is required to develop models of metrics. Models of metrics are necessary to assess the meaning, validity, and robustness of metrics. The proposed framework can be a useful reference for the development of the ethics of research evaluation. It can act as a common denominator for different analytical levels and relevant aspects and is able to embrace many different and heterogeneous streams of literature. Directions for future research are provided."
Understanding Big Data for Industrial Innovation and Design: The Missing Information Systems Perspective,"This paper identifies a need to complement the current rich technical and mathematical research agenda on big data with a more information systems and information science strand, which focuses on the business value of big data. An agenda of research for information systems would explore motives for using big data in real organizational contexts, and consider proposed benefits, such as increased effectiveness and efficiency, production of high-quality products/services, creation of added business value, and stimulation of innovation and design. Impacts of such research on the academic community, the industrial and business world, and policy-makers are discussed.  Â© 2017 Walter de Gruyter GmbH, Berlin/Boston.","This paper identifies a need to complement the current rich technical and mathematical research agenda on big data with a more information systems and information science strand, which focuses on the business value of big data. An agenda of research for information systems would explore motives for using big data in real organizational contexts, and consider proposed benefits, such as increased effectiveness and efficiency, production of high-quality products/services, creation of added business value, and stimulation of innovation and design. Impacts of such research on the academic community, the industrial and business world, and policy-makers are discussed."
Under-reporting of Adverse Events in the Biomedical Literature,"Purpose: To address the under-reporting of research results, with emphasis on the under-reporting/distorted reporting of adverse events in the biomedical research literature. Design/methodology/approach: A four-step approach is used: (1) To identify the characteristics of literature that make it adequate to support policy; (2) to show how each of these characteristics becomes degraded to make inadequate literature; (3) to identify incentives to prevent inadequate literature; and (4) to show policy implications of inadequate literature. Findings: This review has provided reasons for, and examples of, adverse health effects of myriad substances (1) being under-reported in the premiere biomedical literature, or (2) entering this literature in distorted form. Since there is no way to gauge the extent of this under/distorted-reporting, the quality and credibility of the 'premiere' biomedical literature is unknown. Therefore, any types of meta-analyses or scientometric analyses of this literature will have unknown quality and credibility. The most sophisticated scientometric analysis cannot compensate for a highly flawed database. Research limitations: The main limitation is in identifying examples of under-reporting. There are many incentives for under-reporting and few dis-incentives. Practical implications: Almost all research publications, addressing causes of disease, treatments for disease, diagnoses for disease, scientometrics of disease and health issues, and other aspects of healthcare, build upon previous healthcare-related research published. Many researchers will not have laboratories or other capabilities to replicate or validate the published research, and depend almost completely on the integrity of this literature. If the literature is distorted, then future research can be misguided, and health policy recommendations can be ineffective or worse. Originality/value: This review has examined a much wider range of technical and nontechnical causes for under-reporting of adverse events in the biomedical literature than previous studies. Â© Journal of Data and Information Science. All rights reserved.","To address the under-reporting of research results, with emphasis on the under-reporting/distorted reporting of adverse events in the biomedical research literature. A four-step approach is used: To identify the characteristics of literature that make it adequate to support policy; to show how each of these characteristics becomes degraded to make inadequate literature; to identify incentives to prevent inadequate literature; and to show policy implications of inadequate literature. This review has provided reasons for, and examples of, adverse health effects of myriad substances being under-reported in the premiere biomedical literature, or entering this literature in distorted form. Since there is no way to gauge the extent of this under/distorted-reporting, the quality and credibility of the 'premiere' biomedical literature is unknown. Therefore, any types of meta-analyses or scientometric analyses of this literature will have unknown quality and credibility. The most sophisticated scientometric analysis cannot compensate for a highly flawed database. The main limitation is in identifying examples of under-reporting. There are many incentives for under-reporting and few dis-incentives. Almost all research publications, addressing causes of disease, treatments for disease, diagnoses for disease, scientometrics of disease and health issues, and other aspects of healthcare, build upon previous healthcare-related research published. Many researchers will not have laboratories or other capabilities to replicate or validate the published research, and depend almost completely on the integrity of this literature. If the literature is distorted, then future research can be misguided, and health policy recommendations can be ineffective or worse. This review has examined a much wider range of technical and nontechnical causes for under-reporting of adverse events in the biomedical literature than previous studies."
Comparative study of trace metrics between bibliometrics and patentometrics,"Purpose: To comprehensively evaluate the overall performance of a group or an individual in both bibliometrics and patentometrics. Design/methodology/approach: Trace metrics were applied to the top 30 universities in the 2014 Academic Ranking of World Universities (ARWU) - computer sciences, the top 30 ESI highly cited papers in the computer sciences field in 2014, as well as the top 30 assignees and the top 30 most cited patents in the National Bureau of Economic Research (NBER) computer hardware and software category. Findings: We found that, by applying trace metrics, the research or marketing impact efficiency, at both group and individual levels, was clearly observed. Furthermore, trace metrics were more sensitive to the different publication-citation distributions than the average citation and h-index were. Research limitations: Trace metrics considered publications with zero citations as negative contributions. One should clarify how he/she evaluates a zero-citation paper or patent before applying trace metrics. Practical implications: Decision makers could regularly examine the performance of their university/company by applying trace metrics and adjust their policies accordingly. Originality/value: Trace metrics could be applied both in bibliometrics and patentometrics and provide a comprehensive view. Moreover, the high sensitivity and unique impact efficiency view provided by trace metrics can facilitate decision makers in examining and adjusting their policies. Â© Journal of Data and Information Science. All rights reserved.","To comprehensively evaluate the overall performance of a group or an individual in both bibliometrics and patentometrics. Trace metrics were applied to the top 30 universities in the 2014 Academic Ranking of World Universities (ARWU) - computer sciences, the top 30 ESI highly cited papers in the computer sciences field in 2014, as well as the top 30 assignees and the top 30 most cited patents in the National Bureau of Economic Research (NBER) computer hardware and software category. We found that, by applying trace metrics, the research or marketing impact efficiency, at both group and individual levels, was clearly observed. Furthermore, trace metrics were more sensitive to the different publication-citation distributions than the average citation and h-index were. Trace metrics considered publications with zero citations as negative contributions. One should clarify how he/she evaluates a zero-citation paper or patent before applying trace metrics. Decision makers could regularly examine the performance of their university/company by applying trace metrics and adjust their policies accordingly. Trace metrics could be applied both in bibliometrics and patentometrics and provide a comprehensive view. Moreover, the high sensitivity and unique impact efficiency view provided by trace metrics can facilitate decision makers in examining and adjusting their policies."
Gauging a firm's innovative performance using an integrated structural index for patents,"Purpose: In this contribution we try to find new indicators to measure characteristics of a firm's patents and their influence on a company's profits. Design/methodology/approach: We realize that patent evaluation and influence on a company's profits is a complicated issue requiring different perspectives. For this reason we design two types of structural h-indices, derived from the International Patent Classification (IPC). In a case study we apply not only basic statistics but also a nested case-control methodology. Findings: The resulting indicator values based on a large dataset (19,080 patents in total) from the pharmaceutical industry show that the new structural indices are significantly correlated with a firm's profits. Research limitations: The new structural index and the synthetic structural index have just been applied in one case study in the pharmaceutical industry. Practical implications: Our study suggests useful implications for patentometric studies and leads to suggestions for different sized firms to include a healthy research and development (R&D) policy management. The structural h-index can be used to gauge the profits resulting from the innovative performance of a firm's patent portfolio. Originality/value: Traditionally, the breadth and depth of patents of a firm and their citations are considered separately. This approach, however, does not provide an integrated insight in the major characteristics of a firm's patents. The Sh(Y) index, proposed in our investigation, can reflect a firm's innovation activities, its technological breadth, and its influence in an integrated way. Â© Journal of Data and Information Science. All rights reserved.","In this contribution we try to find new indicators to measure characteristics of a firm's patents and their influence on a company's profits. We realize that patent evaluation and influence on a company's profits is a complicated issue requiring different perspectives. For this reason we design two types of structural h-indices, derived from the International Patent Classification (IPC). In a case study we apply not only basic statistics but also a nested case-control methodology. The resulting indicator values based on a large dataset (19,080 patents in total) from the pharmaceutical industry show that the new structural indices are significantly correlated with a firm's profits. The new structural index and the synthetic structural index have just been applied in one case study in the pharmaceutical industry. Our study suggests useful implications for patentometric studies and leads to suggestions for different sized firms to include a healthy research and development (R&D) policy management. The structural h-index can be used to gauge the profits resulting from the innovative performance of a firm's patent portfolio. Traditionally, the breadth and depth of patents of a firm and their citations are considered separately. This approach, however, does not provide an integrated insight in the major characteristics of a firm's patents. The Sh(Y) index, proposed in our investigation, can reflect a firm's innovation activities, its technological breadth, and its influence in an integrated way."
The Power-weakness Ratios (PWR) as a Journal Indicator: Testing the âtournamentsâ metaphor in citation impact studies,"Purpose: Ramanujacharyulu developed the Power-weakness Ratio (PWR) for scoring tournaments. The PWR algorithm has been advocated (and used) for measuring the impact of journals. We show how such a newly proposed indicator can empirically be tested. Design/methodology/approach: PWR values can be found by recursively multiplying the citation matrix by itself until convergence is reached in both the cited and citing dimensions; the quotient of these two values is defined as PWR. We study the effectiveness of PWR using journal ecosystems drawn from the Library and Information Science (LIS) set of the Web of Science (83 journals) as an example. Pajek is used to compute PWRs for the full set, and Excel for the computation in the case of the two smaller sub-graphs: (1) JASIST+ the seven journals that cite JASIST more than 100 times in 2012; and (2) MIS Quart+ the nine journals citing this journal to the same extent. Findings: A test using the set of 83 journals converged, but did not provide interpretable results. Further decomposition of this set into homogeneous sub-graphs shows that-like most other journal indicators-PWR can perhaps be used within homogeneous sets, but not across citation communities. We conclude that PWR does not work as a journal impact indicator; journal impact, for example, is not a tournament. Research limitations: Journals that are not represented on the âcitingâ dimension of the matrix-for example, because they no longer appear, but are still registered as âcitedâ (e.g. ARIST)-distort the PWR ranking because of zeros or very low values in the denominator. Practical implications: The association of âcitedâ with âpowerâ and âcitingâ with âweaknessâ can be considered as a metaphor. In our opinion, referencing is an actor category and can be studied in terms of behavior, whereas âcitednessâ is a property of a document with an expected dynamics very different from that of âciting.â From this perspective, the PWR model is not valid as a journal indicator. Originality/value: Arguments for using PWR are: (1) its symmetrical handling of the rows and columns in the asymmetrical citation matrix, (2) its recursive algorithm, and (3) its mathematical elegance. In this study, PWR is discussed and critically assessed. Â© Journal of Data and Information Science. All rights reserved.","Ramanujacharyulu developed the Power-weakness Ratio (PWR) for scoring tournaments. The PWR algorithm has been advocated (and used) for measuring the impact of journals. We show how such a newly proposed indicator can empirically be tested. PWR values can be found by recursively multiplying the citation matrix by itself until convergence is reached in both the cited and citing dimensions; the quotient of these two values is defined as PWR. We study the effectiveness of PWR using journal ecosystems drawn from the Library and Information Science (LIS) set of the Web of Science (83 journals) as an example. Pajek is used to compute PWRs for the full set, and Excel for the computation in the case of the two smaller sub-graphs: JASIST+ the seven journals that cite JASIST more than 100 times in 2012; and MIS Quart+ the nine journals citing this journal to the same extent. A test using the set of 83 journals converged, but did not provide interpretable results. Further decomposition of this set into homogeneous sub-graphs shows that-like most other journal indicators-PWR can perhaps be used within homogeneous sets, but not across citation communities. We conclude that PWR does not work as a journal impact indicator; journal impact, for example, is not a tournament. Journals that are not represented on the citing dimension of the matrix-for example, because they no longer appear, but are still registered as cited ( ARIST)-distort the PWR ranking because of zeros or very low values in the denominator. The association of cited with power and citing with weakness can be considered as a metaphor. In our opinion, referencing is an actor category and can be studied in terms of behavior, whereas citedness is a property of a document with an expected dynamics very different from that of citing. From this perspective, the PWR model is not valid as a journal indicator. Arguments for using PWR are: its symmetrical handling of the rows and columns in the asymmetrical citation matrix, its recursive algorithm, and its mathematical elegance. In this study, PWR is discussed and critically assessed."
Taking Comfort in Points: The Appeal of the Norwegian Model in Sweden,"The ""Norwegian model"" has become widely used for assessment and resource allocation purposes. This paper investigates why this model has becomes so widespread and influential. A theoretical background is outlined in which the reduction of ""uncertainty"" is highlighted as a key feature of performance measurement systems. These theories are then drawn upon when revisiting previous studies of the Norwegian model, its use, and reactions to it, in Sweden. The empirical examples, which concern more formal use on the level of universities as well as responses from individual researchers, shows how particular parts-especially the ""publication indicator""-are employed in Swedish academia. The discussion posits that the attractiveness of the Norwegian model largely can be explained by its ability to reduce complexity and uncertainty, even in fields where traditional bibliometric measurement is less applicable. The findings presented should be regarded as examples that can be used for discussion, but one should be careful to interpret these as representative for broader sentiments and trends. The sheer popularity of the Norwegian model, leading to its application in contexts for which it was not designed, can be seen as a major challenge for the future. This paper offers a novel perspective on the Norwegian model by focusing on its general ""appeal"", rather than on its design, use or (mis)-use. Â© 2018 2018 BjÃ¶rn Hammarfelt, published by Sciendo.","The ""Norwegian model"" has become widely used for assessment and resource allocation purposes. This paper investigates why this model has becomes so widespread and influential. A theoretical background is outlined in which the reduction of ""uncertainty"" is highlighted as a key feature of performance measurement systems. These theories are then drawn upon when revisiting previous studies of the Norwegian model, its use, and reactions to it, in Sweden. The empirical examples, which concern more formal use on the level of universities as well as responses from individual researchers, shows how particular parts-especially the ""publication indicator""-are employed in Swedish academia. The discussion posits that the attractiveness of the Norwegian model largely can be explained by its ability to reduce complexity and uncertainty, even in fields where traditional bibliometric measurement is less applicable. The findings presented should be regarded as examples that can be used for discussion, but one should be careful to interpret these as representative for broader sentiments and trends. The sheer popularity of the Norwegian model, leading to its application in contexts for which it was not designed, can be seen as a major challenge for the future. This paper offers a novel perspective on the Norwegian model by focusing on its general ""appeal"", rather than on its design, use or (mis)-use."
"Applications of, and Experiences with, the Norwegian Model in Finland","The purpose of this article is to describe the development, components and properties of a publication indicator that the Ministry of Education and Culture in Finland uses for allocating direct core funding annually to universities. Since 2013, 13% of the core funding has been allocated on basis of publication indicator that, like the Norwegian model, is based on comprehensive national level publication data that is currently provided by the VIRTA publication information service. In 2015, the publication indicator was complemented with other components of the Norwegian model, namely, quality-weighted publication counts based on national Publication Forum authority list of the publication channels with ratings established by experts in the field. The funding model allocates around 1.6 billion euros annually to universities with the publication indicator annually distributing over 200 million euros. Besides the funding model, the indicator provides comparable data for monitoring the research performance of Finnish universities, fields and subunits. The indicator may also be used in the universities' local funding models and research management systems, sometimes even at individual level evaluation. Positive and negative effects of the indicator have been extensively discussed and speculated. Since 2011, the Finnish universities' productivity appears to have increased in terms of both quantity and quality of publications. Â© 2018 Janne PÃ¶lÃ¶nen, published by Sciendo 2018.","The purpose of this article is to describe the development, components and properties of a publication indicator that the Ministry of Education and Culture in Finland uses for allocating direct core funding annually to universities. Since 2013, 13% of the core funding has been allocated on basis of publication indicator that, like the Norwegian model, is based on comprehensive national level publication data that is currently provided by the VIRTA publication information service. In 2015, the publication indicator was complemented with other components of the Norwegian model, namely, quality-weighted publication counts based on national Publication Forum authority list of the publication channels with ratings established by experts in the field. The funding model allocates around 1.6 billion euros annually to universities with the publication indicator annually distributing over 200 million euros. Besides the funding model, the indicator provides comparable data for monitoring the research performance of Finnish universities, fields and subunits. The indicator may also be used in the universities' local funding models and research management systems, sometimes even at individual level evaluation. Positive and negative effects of the indicator have been extensively discussed and speculated. Since 2011, the Finnish universities' productivity appears to have increased in terms of both quantity and quality of publications."
The Flemish Performance-based Research Funding System: A Unique Variant of the Norwegian Model,"The BOF-key is the performance-based research funding system that is used in Flanders, Belgium. In this paper we describe the historical background of the system, its current design and organization, as well as its effects on the Flemish higher education landscape. The BOF-key in its current form relies on three bibliometric parameters: publications in Web of Science, citations in Web of Science, and publications in a comprehensive regional database for SSH publications. Taken together, the BOF-key forms a unique variant of the Norwegian model: while the system to a large extent relies on a commercial database, it avoids the problem of inadequate coverage of the SSH. Because the bibliometric parameters of the BOF-key are reused in other funding allocation schemes, their overall importance to the Flemish universities is substantial. Â© 2018 Tim C. E. Engels, Raf Guns, published by Sciendo 2018.","The BOF-key is the performance-based research funding system that is used in Flanders, Belgium. In this paper we describe the historical background of the system, its current design and organization, as well as its effects on the Flemish higher education landscape. The BOF-key in its current form relies on three bibliometric parameters: publications in Web of Science, citations in Web of Science, and publications in a comprehensive regional database for SSH publications. Taken together, the BOF-key forms a unique variant of the Norwegian model: while the system to a large extent relies on a commercial database, it avoids the problem of inadequate coverage of the SSH. Because the bibliometric parameters of the BOF-key are reused in other funding allocation schemes, their overall importance to the Flemish universities is substantial."
How users search the mobile web: A model for understanding the impact of motivation and context on search behaviors,"Purpose: This study explores how search motivation and context influence mobile Web search behaviors. Design/methodology/approach: We studied 30 experienced mobile Web users via questionnaires, semi-structured interviews, and an online diary tool that participants used to record their daily search activities. SQLite Developer was used to extract data from the users' phone logs for correlation analysis in Statistical Product and Service Solutions (SPSS). Findings: One quarter of mobile search sessions were driven by two or more search motivations. It was especially difficult to distinguish curiosity from time killing in particular user reporting. Multi-dimensional contexts and motivations influenced mobile search behaviors, and among the context dimensions, gender, place, activities they engaged in while searching, task importance, portal, and interpersonal relations (whether accompanied or alone when searching) correlated with each other. Research limitations: The sample was comprised entirely of college students, so our findings may not generalize to other populations. More participants and longer experimental duration will improve the accuracy and objectivity of the research. Practical implications: Motivation analysis and search context recognition can help mobile service providers design applications and services for particular mobile contexts and usages. Originality/value: Most current research focuses on specific contexts, such as studies on place, or other contextual influences on mobile search, and lacks a systematic analysis of mobile search context. Based on analysis of the impact of mobile search motivations and search context on search behaviors, we built a multi-dimensional model of mobile search behaviors. Â© Journal of Data and Information Science. All rights reserved.","This study explores how search motivation and context influence mobile Web search behaviors. We studied 30 experienced mobile Web users via questionnaires, semi-structured interviews, and an online diary tool that participants used to record their daily search activities. SQLite Developer was used to extract data from the users' phone logs for correlation analysis in Statistical Product and Service Solutions (SPSS). One quarter of mobile search sessions were driven by two or more search motivations. It was especially difficult to distinguish curiosity from time killing in particular user reporting. Multi-dimensional contexts and motivations influenced mobile search behaviors, and among the context dimensions, gender, place, activities they engaged in while searching, task importance, portal, and interpersonal relations (whether accompanied or alone when searching) correlated with each other. The sample was comprised entirely of college students, so our findings may not generalize to other populations. More participants and longer experimental duration will improve the accuracy and objectivity of the research. Motivation analysis and search context recognition can help mobile service providers design applications and services for particular mobile contexts and usages. Most current research focuses on specific contexts, such as studies on place, or other contextual influences on mobile search, and lacks a systematic analysis of mobile search context. Based on analysis of the impact of mobile search motivations and search context on search behaviors, we built a multi-dimensional model of mobile search behaviors."
A Bootstrapping-based Method to Automatically Identify Data-usage Statements in Publications,"Purpose: Our study proposes a bootstrapping-based method to automatically extract data-usage statements from academic texts. Design/methodology/approach: The method for data-usage statements extraction starts with seed entities and iteratively learns patterns and data-usage statements from unlabeled text. In each iteration, new patterns are constructed and added to the pattern list based on their calculated score. Three seed-selection strategies are also proposed in this paper. Findings: The performance of the method is verified by means of experiments on real data collected from computer science journals. The results show that the method can achieve satisfactory performance regarding precision of extraction and extensibility of obtained patterns. Research limitations: While the triple representation of sentences is effective and efficient for extracting data-usage statements, it is unable to handle complex sentences. Additional features that can address complex sentences should thus be explored in the future. Practical implications: Data-usage statements extraction is beneficial for data-repository construction and facilitates research on data-usage tracking, dataset-based scholar search, and dataset evaluation. Originality/value: To the best of our knowledge, this paper is among the first to address the important task of automatically extracting data-usage statements from real data. Â© Journal of Data and Information Science. All rights reserved.","Our study proposes a bootstrapping-based method to automatically extract data-usage statements from academic texts. The method for data-usage statements extraction starts with seed entities and iteratively learns patterns and data-usage statements from unlabeled text. In each iteration, new patterns are constructed and added to the pattern list based on their calculated score. Three seed-selection strategies are also proposed in this paper. The performance of the method is verified by means of experiments on real data collected from computer science journals. The results show that the method can achieve satisfactory performance regarding precision of extraction and extensibility of obtained patterns. While the triple representation of sentences is effective and efficient for extracting data-usage statements, it is unable to handle complex sentences. Additional features that can address complex sentences should thus be explored in the future. Data-usage statements extraction is beneficial for data-repository construction and facilitates research on data-usage tracking, dataset-based scholar search, and dataset evaluation. To the best of our knowledge, this paper is among the first to address the important task of automatically extracting data-usage statements from real data."
Science Mapping: A Systematic Review of the Literature,"Purpose: We present a systematic review of the literature concerning major aspects of science mapping to serve two primary purposes: First, to demonstrate the use of a science mapping approach to perform the review so that researchers may apply the procedure to the review of a scientific domain of their own interest, and second, to identify major areas of research activities concerning science mapping, intellectual milestones in the development of key specialties, evolutionary stages of major specialties involved, and the dynamics of transitions from one specialty to another. Design/methodology/approach: We first introduce a theoretical framework of the evolution of a scientific specialty. Then we demonstrate a generic search strategy that can be used to construct a representative dataset of bibliographic records of a domain of research. Next, progressively synthesized co-citation networks are constructed and visualized to aid visual analytic studies of the domain's structural and dynamic patterns and trends. Finally, trajectories of citations made by particular types of authors and articles are presented to illustrate the predictive potential of the analytic approach. Findings: The evolution of the science mapping research involves the development of a number of interrelated specialties. Four major specialties are discussed in detail in terms of four evolutionary stages: conceptualization, tool construction, application, and codification. Underlying connections between major specialties are also explored. The predictive analysis demonstrates citations trajectories of potentially transformative contributions. Research limitations: The systematic review is primarily guided by citation patterns in the dataset retrieved from the literature. The scope of the data is limited by the source of the retrieval, i.e. the Web of Science, and the composite query used. An iterative query refinement is possible if one would like to improve the data quality, although the current approach serves our purpose adequately. More in-depth analyses of each specialty would be more revealing by incorporating additional methods such as citation context analysis and studies of other aspects of scholarly publications. Practical implications: The underlying analytic process of science mapping serves many practical needs, notably bibliometric mapping, knowledge domain visualization, and visualization of scientific literature. In order to master such a complex process of science mapping, researchers often need to develop a diverse set of skills and knowledge that may span multiple disciplines. The approach demonstrated in this article provides a generic method for conducting a systematic review. Originality/value: Incorporating the evolutionary stages of a specialty into the visual analytic study of a research domain is innovative. It provides a systematic methodology for researchers to achieve a good understanding of how scientific fields evolve, to recognize potentially insightful patterns from visually encoded signs, and to synthesize various information so as to capture the state of the art of the domain.  Â© 2017 Chaomei Chen.","We present a systematic review of the literature concerning major aspects of science mapping to serve two primary purposes: First, to demonstrate the use of a science mapping approach to perform the review so that researchers may apply the procedure to the review of a scientific domain of their own interest, and second, to identify major areas of research activities concerning science mapping, intellectual milestones in the development of key specialties, evolutionary stages of major specialties involved, and the dynamics of transitions from one specialty to another. We first introduce a theoretical framework of the evolution of a scientific specialty. Then we demonstrate a generic search strategy that can be used to construct a representative dataset of bibliographic records of a domain of research. Next, progressively synthesized co-citation networks are constructed and visualized to aid visual analytic studies of the domain's structural and dynamic patterns and trends. Finally, trajectories of citations made by particular types of authors and articles are presented to illustrate the predictive potential of the analytic approach. The evolution of the science mapping research involves the development of a number of interrelated specialties. Four major specialties are discussed in detail in terms of four evolutionary stages: conceptualization, tool construction, application, and codification. Underlying connections between major specialties are also explored. The predictive analysis demonstrates citations trajectories of potentially transformative contributions. The systematic review is primarily guided by citation patterns in the dataset retrieved from the literature. The scope of the data is limited by the source of the retrieval, the Web of Science, and the composite query used. An iterative query refinement is possible if one would like to improve the data quality, although the current approach serves our purpose adequately. More in-depth analyses of each specialty would be more revealing by incorporating additional methods such as citation context analysis and studies of other aspects of scholarly publications. The underlying analytic process of science mapping serves many practical needs, notably bibliometric mapping, knowledge domain visualization, and visualization of scientific literature. In order to master such a complex process of science mapping, researchers often need to develop a diverse set of skills and knowledge that may span multiple disciplines. The approach demonstrated in this article provides a generic method for conducting a systematic review. Incorporating the evolutionary stages of a specialty into the visual analytic study of a research domain is innovative. It provides a systematic methodology for researchers to achieve a good understanding of how scientific fields evolve, to recognize potentially insightful patterns from visually encoded signs, and to synthesize various information so as to capture the state of the art of the domain."
Critical factors for personal cloud storage adoption in China,"Purpose: In order to explain and predict the adoption of personal cloud storage, this study explores the critical factors involved in the adoption of personal cloud storage and empirically validates their relationships to a user's intentions. Design/methodology/approach: Based on technology acceptance model (TAM), network externality, trust, and an interview survey, this study proposes a personal cloud storage adoption model. We conducted an empirical analysis by structural equation modeling based on survey data obtained with a questionnaire. Findings: Among the adoption factors we identified, network externality has the salient influence on a user's adoption intention, followed by perceived usefulness, individual innovation, perceived trust, perceived ease of use, and subjective norms. Cloud storage characteristics are the most important indirect factors, followed by awareness to personal cloud storage and perceived risk. However, although perceived risk is regarded as an important factor by other cloud computing researchers, we found that it has no significant influence. Also, subjective norms have no significant influence on perceived usefulness. This indicates that users are rational when they choose whether to adopt personal cloud storage. Research limitations: This study ignores time and cost factors that might affect a user's intention to adopt personal cloud storage. Practical implications: Our findings might be helpful in designing and developing personal cloud storage products, and helpful to regulators crafting policies. Originality/value: This study is one of the first research efforts that discuss Chinese users' personal cloud storage adoption, which should help to further the understanding of personal cloud adoption behavior among Chinese users. Â© Journal of Data and Information Science. All rights reserved.","In order to explain and predict the adoption of personal cloud storage, this study explores the critical factors involved in the adoption of personal cloud storage and empirically validates their relationships to a user's intentions. Based on technology acceptance model (TAM), network externality, trust, and an interview survey, this study proposes a personal cloud storage adoption model. We conducted an empirical analysis by structural equation modeling based on survey data obtained with a questionnaire. Among the adoption factors we identified, network externality has the salient influence on a user's adoption intention, followed by perceived usefulness, individual innovation, perceived trust, perceived ease of use, and subjective norms. Cloud storage characteristics are the most important indirect factors, followed by awareness to personal cloud storage and perceived risk. However, although perceived risk is regarded as an important factor by other cloud computing researchers, we found that it has no significant influence. Also, subjective norms have no significant influence on perceived usefulness. This indicates that users are rational when they choose whether to adopt personal cloud storage. This study ignores time and cost factors that might affect a user's intention to adopt personal cloud storage. Our findings might be helpful in designing and developing personal cloud storage products, and helpful to regulators crafting policies. This study is one of the first research efforts that discuss Chinese users' personal cloud storage adoption, which should help to further the understanding of personal cloud adoption behavior among Chinese users."
Mapping diversity of publication patterns in the social sciences and humanities: An approach making use of fuzzy cluster analysis,"Purpose: To present a method for systematically mapping diversity of publication patterns at the author level in the social sciences and humanities in terms of publication type, publication language and co-authorship. Design/methodology/approach: In a follow-up to the hard partitioning clustering by Verleysen and Weeren in 2016, we now propose the complementary use of fuzzy cluster analysis, making use of a membership coefficient to study gradual differences between publication styles among authors within a scholarly discipline. The analysis of the probability density function of the membership coefficient allows to assess the distribution of publication styles within and between disciplines. Findings: As an illustration we analyze 1,828 productive authors affiliated in Flanders, Belgium. Whereas a hard partitioning previously identified two broad publication styles, an international one vs. a domestic one, fuzzy analysis now shows gradual differences among authors. Internal diversity also varies across disciplines and can be explained by researchers' specialization and dissemination strategies. Research limitations: The dataset used is limited to one country for the years 2000-2011; a cognitive classification of authors may yield a different result from the affiliation-based classification used here. Practical implications: Our method is applicable to other bibliometric and research evaluation contexts, especially for the social sciences and humanities in non-Anglophone countries. Originality/value: The method proposed is a novel application of cluster analysis to the field of bibliometrics. Applied to publication patterns at the author level in the social sciences and humanities, for the first time it systematically documents intra-disciplinary diversity. Â© Journal of Data and Information Science. All rights reserved.","To present a method for systematically mapping diversity of publication patterns at the author level in the social sciences and humanities in terms of publication type, publication language and co-authorship. In a follow-up to the hard partitioning clustering by Verleysen and Weeren in 2016, we now propose the complementary use of fuzzy cluster analysis, making use of a membership coefficient to study gradual differences between publication styles among authors within a scholarly discipline. The analysis of the probability density function of the membership coefficient allows to assess the distribution of publication styles within and between disciplines. As an illustration we analyze 1,828 productive authors affiliated in Flanders, Belgium. Whereas a hard partitioning previously identified two broad publication styles, an international one vs. a domestic one, fuzzy analysis now shows gradual differences among authors. Internal diversity also varies across disciplines and can be explained by researchers' specialization and dissemination strategies. The dataset used is limited to one country for the years 2000-2011; a cognitive classification of authors may yield a different result from the affiliation-based classification used here. Our method is applicable to other bibliometric and research evaluation contexts, especially for the social sciences and humanities in non-Anglophone countries. The method proposed is a novel application of cluster analysis to the field of bibliometrics. Applied to publication patterns at the author level in the social sciences and humanities, for the first time it systematically documents intra-disciplinary diversity."
A Tailored Approach: A model for literature searching in complex systematic reviews,"Our previous work identified that nine leading guidance documents for seven different types of systematic review advocated the same process of literature searching. We defined and illustrated this process and we named it âthe Conventional Approachâ. The Conventional Approach appears to meet the needs of researchers undertaking literature searches for systematic reviews of clinical interventions. In this article, we report a new and alternate process model of literature searching called âA Tailored Approachâ. A Tailored Approach is indicated as a search process for complex reviews which do not focus on the evaluation of clinical interventions. The aims of this article are to (1) explain the rationale for, and the theories behind, the design of A Tailored Approach; (2) report the current conceptual illustration of A Tailored Approach and to describe a userâs interaction with the process model; and (3) situate the elements novel to A Tailored Approach (when compared with the Conventional Approach) in the relevant literature. A Tailored Approach suggests investing time at the start of a review, to develop the information needs from the research objectives, and to tailor the search approach to studies or data. Tailored Approaches should be led by the information specialist (librarian) but developed by the research team. The aim is not necessarily to focus on comprehensive retrieval. Further research is indicated to evaluate the use of supplementary search methods, methods of team-working to define search approaches, and to evaluate the use of conceptual models of information retrieval for testing and evaluation. Â© The Author(s) 2022.","Our previous work identified that nine leading guidance documents for seven different types of systematic review advocated the same process of literature searching. We defined and illustrated this process and we named it the Conventional Approach. The Conventional Approach appears to meet the needs of researchers undertaking literature searches for systematic reviews of clinical interventions. In this article, we report a new and alternate process model of literature searching called A Tailored Approach. A Tailored Approach is indicated as a search process for complex reviews which do not focus on the evaluation of clinical interventions. The aims of this article are to explain the rationale for, and the theories behind, the design of A Tailored Approach; report the current conceptual illustration of A Tailored Approach and to describe a users interaction with the process model; and situate the elements novel to A Tailored Approach (when compared with the Conventional Approach) in the relevant literature. A Tailored Approach suggests investing time at the start of a review, to develop the information needs from the research objectives, and to tailor the search approach to studies or data. Tailored Approaches should be led by the information specialist (librarian) but developed by the research team. The aim is not necessarily to focus on comprehensive retrieval. Further research is indicated to evaluate the use of supplementary search methods, methods of team-working to define search approaches, and to evaluate the use of conceptual models of information retrieval for testing and evaluation."
WisRule: First cognitive algorithm of wise association rule mining,"This article proposes a new algorithm for a newly emerging domain wisdom mining that claims to extract wisdom from data. Association rule mining is one of the dominant data mining techniques based on which a new algorithm called WisRule is proposed that generates both positive and negative association rules. These rules can be used for decision-making with less influence from a specialist. The existing algorithms of association rule extraction are based on the frequency of an itemset, which was introduced into the Apriori algorithm for the first time. In these algorithms, those itemsets are converted to the rules of the form Antecedent â Consequent that qualify the threshold of support, confidence and similar other measures. WisRule is proposed as an extension to the CBPNARM algorithm. WisRule produces both positive and negative association rules based on their frequency evaluated in a certain context (C), utility (U), time (T) and location (L). Rules that are valid in a given context, have high utility and are valid across multiple time intervals and locations become part of the final ruleset. The evaluation of a rule in these four dimensions is claimed as mining wisdom from the given data that is currently used as a hypothetical basis for a domain expertâs decision. WisRule is compared with the Apriori, PNARM and CBPNARM algorithms in terms of precision, recall, number of rules, average confidence, F-measure and execution time. Â© The Author(s) 2022.","This article proposes a new algorithm for a newly emerging domain wisdom mining that claims to extract wisdom from data. Association rule mining is one of the dominant data mining techniques based on which a new algorithm called WisRule is proposed that generates both positive and negative association rules. These rules can be used for decision-making with less influence from a specialist. The existing algorithms of association rule extraction are based on the frequency of an itemset, which was introduced into the Apriori algorithm for the first time. In these algorithms, those itemsets are converted to the rules of the form Antecedent Consequent that qualify the threshold of support, confidence and similar other measures. WisRule is proposed as an extension to the CBPNARM algorithm. WisRule produces both positive and negative association rules based on their frequency evaluated in a certain context , utility (U), time (T) and location . Rules that are valid in a given context, have high utility and are valid across multiple time intervals and locations become part of the final ruleset. The evaluation of a rule in these four dimensions is claimed as mining wisdom from the given data that is currently used as a hypothetical basis for a domain experts decision. WisRule is compared with the Apriori, PNARM and CBPNARM algorithms in terms of precision, recall, number of rules, average confidence, F-measure and execution time."
Investigating the relationships between dialog patterns and user satisfaction in customer service chat systems based on chat log analysis,"While previous studies of customer service chat systems (CSCS) understood user satisfaction as individualsâ subjective perceptions and depended heavily on self-report methods for satisfaction measurement, this article presents an obtrusive chat log analysis that followed the established approaches of search log analysis and examined the relationships between dialog patterns and user satisfaction. An 81-day chat log was obtained from a real-world CSCS that involves both a chatbot and human representatives. A total of 75,918 chat sessions/147,972 sub-sessions containing 251,556 user messages and 349,416 system messages were extracted after data processing and analysed in terms of topic, length and path. As found in this study, the users were more likely to get satisfied on low-difficulty topics. The dialog between the CSCS and users was shallow in general. While human representativesâ elaboration contributed to user satisfaction, the chatbot was responsible for damaging user satisfaction. The significance of this study consists not only in providing objective evidence about user satisfaction in online chat but also in generating practical implications for the CSCS to improve user satisfaction. Â© The Author(s) 2022.","While previous studies of customer service chat systems (CSCS) understood user satisfaction as individuals subjective perceptions and depended heavily on self-report methods for satisfaction measurement, this article presents an obtrusive chat log analysis that followed the established approaches of search log analysis and examined the relationships between dialog patterns and user satisfaction. An 81-day chat log was obtained from a real-world CSCS that involves both a chatbot and human representatives. A total of 75,918 chat sessions/147,972 sub-sessions containing 251,556 user messages and 349,416 system messages were extracted after data processing and analysed in terms of topic, length and path. As found in this study, the users were more likely to get satisfied on low-difficulty topics. The dialog between the CSCS and users was shallow in general. While human representatives elaboration contributed to user satisfaction, the chatbot was responsible for damaging user satisfaction. The significance of this study consists not only in providing objective evidence about user satisfaction in online chat but also in generating practical implications for the CSCS to improve user satisfaction."
Anatomising the impact of ResearchGate followers and followings on influence identification,"Influence analysis, derived from Social Network Analysis (SNA), is extremely useful in academic literature analytic. Different Academic Social Network Sites (ASNS) have been widely examined for influence analysis in terms of co-authorship and co-citation networks. The impact of other network-based features, such as followers and followings, provided by ASNS such as ResearchGate (RG) and Academia is yet to be anatomised. As proven in ingrained social theories, the followers and followings have significant impact in influence prorogation. This research aims at examining the same in one of the widely adopted ASNS, RG. The rendering process is developed to render real-time RG information, which is modelled into graph. Standard centrality measures are implemented to identify influential users from the constructed RG graph. Each centrality measure gives a list of top-k influential RG users. The results are compared with RGScore and Total Research Interest (TRI) to discover the most effective centrality measure. Betweenness and closeness centrality measures have shown the outperforming results compared with others. A procedure is established to discover influential RG users that are commonly present in all top-k centrality results to identify dominant skills, affiliations, departments and locations from the rendered data. Â© The Author(s) 2022.","Influence analysis, derived from Social Network Analysis (SNA), is extremely useful in academic literature analytic. Different Academic Social Network Sites (ASNS) have been widely examined for influence analysis in terms of co-authorship and co-citation networks. The impact of other network-based features, such as followers and followings, provided by ASNS such as ResearchGate (RG) and Academia is yet to be anatomised. As proven in ingrained social theories, the followers and followings have significant impact in influence prorogation. This research aims at examining the same in one of the widely adopted ASNS, RG. The rendering process is developed to render real-time RG information, which is modelled into graph. Standard centrality measures are implemented to identify influential users from the constructed RG graph. Each centrality measure gives a list of top-k influential RG users. The results are compared with RGScore and Total Research Interest (TRI) to discover the most effective centrality measure. Betweenness and closeness centrality measures have shown the outperforming results compared with others. A procedure is established to discover influential RG users that are commonly present in all top-k centrality results to identify dominant skills, affiliations, departments and locations from the rendered data."
Research on information dissemination of blockchain network community under the action of negative incentive mechanism,"Traditional online communities suffer from false, repetitive or low-level content, with blockchain technology able to solve these problems. Specifically, the incentive mechanism is the blockchainâs core value, including positive and negative incentive mechanisms. The former strengthens peopleâs behaviour positively, while the latter, on the contrary, adopts mandatory methods such as punishment to eliminate the occurrence of certain types of behaviour. The negative incentive mechanism is the key factor to solve the problems presented above that traditional online communities face. Specifically, this article develops a solution that utilises the negative incentive mechanism, based on the classic infectious disease model (SIR model), introduces smart nodes, puts forward the SSIR model of information dissemination in the blockchain network community, and establishes a set of differential equations reflecting the information dissemination rules. Based on the parameter assumption and solving the equations with MATLAB, this article compares and reveals the changes of different user types on the SIR and SSIR models. Furthermore, we utilise the data collected from the Steemit blockchain community and Sina Weibo platform and apply the Social Network Analysis method to compare and analyse the information dissemination between the blockchain and the traditional network community. The research results highlight that the negative incentive mechanism in the blockchain network community affords a more rational behaviour of user information dissemination, a simpler interaction between users, and reducing to a certain extent the dissemination of âdistortedâ or âuncertainâ information. Â© The Author(s) 2022.","Traditional online communities suffer from false, repetitive or low-level content, with blockchain technology able to solve these problems. Specifically, the incentive mechanism is the blockchains core value, including positive and negative incentive mechanisms. The former strengthens peoples behaviour positively, while the latter, on the contrary, adopts mandatory methods such as punishment to eliminate the occurrence of certain types of behaviour. The negative incentive mechanism is the key factor to solve the problems presented above that traditional online communities face. Specifically, this article develops a solution that utilises the negative incentive mechanism, based on the classic infectious disease model (SIR model), introduces smart nodes, puts forward the SSIR model of information dissemination in the blockchain network community, and establishes a set of differential equations reflecting the information dissemination rules. Based on the parameter assumption and solving the equations with MATLAB, this article compares and reveals the changes of different user types on the SIR and SSIR models. Furthermore, we utilise the data collected from the Steemit blockchain community and Sina Weibo platform and apply the Social Network Analysis method to compare and analyse the information dissemination between the blockchain and the traditional network community. The research results highlight that the negative incentive mechanism in the blockchain network community affords a more rational behaviour of user information dissemination, a simpler interaction between users, and reducing to a certain extent the dissemination of distorted or uncertain information."
"Representation, detection and usage of the content semantics of comments in a social platform","The analysis of peopleâs comments in social platforms is a widely investigated topic because comments are the place where people show their spontaneity most clearly. In this article, we present a network-based data structure and a related approach to represent and manage the underlying semantics of a set of comments. Our approach is based on the extraction of text patterns that take into account not only the frequency, but also the utility of the analysed comments. Our data structure and approach are âmultidimensionalâ and âholisticâ, in the sense that they can simultaneously handle content semantics from multiple perspectives. They are also easily extensible, because additional content semantics perspectives can be easily added to them. Furthermore, our approach is able to evaluate the semantic similarity of two sets of comments. In this article, we also illustrate the results of several tests we conducted on Reddit comments, even if our approach can be applied to any social platform. Finally, we provide an overview of some possible applications of this research. Â© The Author(s) 2022.","The analysis of peoples comments in social platforms is a widely investigated topic because comments are the place where people show their spontaneity most clearly. In this article, we present a network-based data structure and a related approach to represent and manage the underlying semantics of a set of comments. Our approach is based on the extraction of text patterns that take into account not only the frequency, but also the utility of the analysed comments. Our data structure and approach are multidimensional and holistic, in the sense that they can simultaneously handle content semantics from multiple perspectives. They are also easily extensible, because additional content semantics perspectives can be easily added to them. Furthermore, our approach is able to evaluate the semantic similarity of two sets of comments. In this article, we also illustrate the results of several tests we conducted on Reddit comments, even if our approach can be applied to any social platform. Finally, we provide an overview of some possible applications of this research."
Publishing in library and information science journals: The success of less experienced researchers,"This study explores the publishing success of less experienced researchers including early career researchers in a selection of library and information science journals. The study includes all authors of articles and reviews published in 10 library and information science journals during a 20-year period (2001â2020). The prior publication of each author is determined at the time of publication in one of the ten journals. The analysis includes 14,612 publications and publication histories of 36,417 authors. The results show that there are considerable differences between journals, and that the share of publications by less experienced researchers has generally decreased over time. Library automation journals publish considerably more publications by early career researchers than information science journals do. Publications in these 10 library and information science journals are being published by authors with an increasing publishing experience and fewer papers are being published by author teams with little experience. Â© The Author(s) 2022.","This study explores the publishing success of less experienced researchers including early career researchers in a selection of library and information science journals. The study includes all authors of articles and reviews published in 10 library and information science journals during a 20-year period (20012020). The prior publication of each author is determined at the time of publication in one of the ten journals. The analysis includes 14,612 publications and publication histories of 36,417 authors. The results show that there are considerable differences between journals, and that the share of publications by less experienced researchers has generally decreased over time. Library automation journals publish considerably more publications by early career researchers than information science journals do. Publications in these 10 library and information science journals are being published by authors with an increasing publishing experience and fewer papers are being published by author teams with little experience."
Investigation of university websites from technology acceptance model and information architecture perspective: A case study,"Factors, such as whether a website is designed to be user-oriented beyond its mere visual design, its effectiveness and efficiency, its usability, and the organisation of the information it offers, have come to the fore once again after the Covid-19 pandemic. It has been evident that the link structure in a website, better known as the websiteâs information architecture, helps the practitioners with identifying factors that affect the usability of a website. In this sense, practitioners must ensure that the information architecture supports the usage intentions of a websitesâ visitors to better serve and motivate them. However, in many cases, different types of users navigate websites that contain immense amounts of information, so understanding their needs is also important for practitioners. In parallel, this article addresses the problem that different visitors of a large-scale website will need to navigate through dense information to find the information they are looking for, and the information architecture of the website must support different user tasks for the website to be widely adopted. Thus, unlike previous studies, this article combines the principles of information architecture and the technology acceptance model to investigate the effect of information architecture on visitorâs usage intentions. The work also guides practitioners in developing architectural strategies to better enable visitors to fulfil their objectives in the least amount of time. Â© The Author(s) 2022.","Factors, such as whether a website is designed to be user-oriented beyond its mere visual design, its effectiveness and efficiency, its usability, and the organisation of the information it offers, have come to the fore once again after the Covid-19 pandemic. It has been evident that the link structure in a website, better known as the websites information architecture, helps the practitioners with identifying factors that affect the usability of a website. In this sense, practitioners must ensure that the information architecture supports the usage intentions of a websites visitors to better serve and motivate them. However, in many cases, different types of users navigate websites that contain immense amounts of information, so understanding their needs is also important for practitioners. In parallel, this article addresses the problem that different visitors of a large-scale website will need to navigate through dense information to find the information they are looking for, and the information architecture of the website must support different user tasks for the website to be widely adopted. Thus, unlike previous studies, this article combines the principles of information architecture and the technology acceptance model to investigate the effect of information architecture on visitors usage intentions. The work also guides practitioners in developing architectural strategies to better enable visitors to fulfil their objectives in the least amount of time."
SceneFND: Multimodal fake news detection by modelling scene context information,"Fake news is a threat for the society and can create a lot of confusion to people regarding what is true and what not. Fake news usually contain manipulated content, such as text or images that attract the interest of the readers with the aim to convince them on their truthfulness. In this article, we propose SceneFND (Scene Fake News Detection), a system that combines textual, contextual scene and visual representation to address the problem of multimodal fake news detection. The textual representation is based on word embeddings that are passed into a bidirectional long short-term memory network. Both the contextual scene and the visual representations are based on the images contained in the news post. The place, weather and season scenes are extracted from the image. Our statistical analysis on the scenes showed that there are statistically significant differences regarding their frequency in fake and real news. In addition, our experimental results on two real world datasets show that the integration of the contextual scenes is effective for fake news detection. In particular, SceneFND improved the performance of the textual baseline by 3.48% in PolitiFact and by 3.32% in GossipCop datasets. Finally, we show the suitability of the scene information for the task and present some examples to explain its effectiveness in capturing the relevance between images and text. Â© The Author(s) 2022.","Fake news is a threat for the society and can create a lot of confusion to people regarding what is true and what not. Fake news usually contain manipulated content, such as text or images that attract the interest of the readers with the aim to convince them on their truthfulness. In this article, we propose SceneFND (Scene Fake News Detection), a system that combines textual, contextual scene and visual representation to address the problem of multimodal fake news detection. The textual representation is based on word embeddings that are passed into a bidirectional long short-term memory network. Both the contextual scene and the visual representations are based on the images contained in the news post. The place, weather and season scenes are extracted from the image. Our statistical analysis on the scenes showed that there are statistically significant differences regarding their frequency in fake and real news. In addition, our experimental results on two real world datasets show that the integration of the contextual scenes is effective for fake news detection. In particular, SceneFND improved the performance of the textual baseline by 3.48% in PolitiFact and by 3.32% in GossipCop datasets. Finally, we show the suitability of the scene information for the task and present some examples to explain its effectiveness in capturing the relevance between images and text."
Scaling up search engine audits: Practical insights for algorithm auditing,"Algorithm audits have increased in recent years due to a growing need to independently assess the performance of automatically curated services that process, filter and rank the large and dynamic amount of information available on the Internet. Among several methodologies to perform such audits, virtual agents stand out because they offer the ability to perform systematic experiments, simulating human behaviour without the associated costs of recruiting participants. Motivated by the importance of research transparency and replicability of results, this article focuses on the challenges of such an approach. It provides methodological details, recommendations, lessons learned and limitations based on our experience of setting up experiments for eight search engines (including main, news, image and video sections) with hundreds of virtual agents placed in different regions. We demonstrate the successful performance of our research infrastructure across multiple data collections, with diverse experimental designs, and point to different changes and strategies that improve the quality of the method. We conclude that virtual agents are a promising venue for monitoring the performance of algorithms across long periods of time, and we hope that this article can serve as a basis for further research in this area. Â© The Author(s) 2022.","Algorithm audits have increased in recent years due to a growing need to independently assess the performance of automatically curated services that process, filter and rank the large and dynamic amount of information available on the Internet. Among several methodologies to perform such audits, virtual agents stand out because they offer the ability to perform systematic experiments, simulating human behaviour without the associated costs of recruiting participants. Motivated by the importance of research transparency and replicability of results, this article focuses on the challenges of such an approach. It provides methodological details, recommendations, lessons learned and limitations based on our experience of setting up experiments for eight search engines (including main, news, image and video sections) with hundreds of virtual agents placed in different regions. We demonstrate the successful performance of our research infrastructure across multiple data collections, with diverse experimental designs, and point to different changes and strategies that improve the quality of the method. We conclude that virtual agents are a promising venue for monitoring the performance of algorithms across long periods of time, and we hope that this article can serve as a basis for further research in this area."
"The role of psychological, skill level and demographic variables in information-seeking behaviours in mental health professionals","The aim of this study was to identify the variables that can potentially affect information-seeking behaviour in mental health service providers using a quasi-experimental research design. The sample included 30 mental health professionals (with minimum 2 years of experience) to each of whom a scenario was presented in which signs and symptoms of three patients were presented, simulating an actual diagnostic interview. Stress response evaluation (SRE), questionnaires, behavioural observation by the Morae software, and semi-structured interviews were used as means of data collection. Our findings showed that variables such as demographic (e.g. field of study, level of education, work experience and age), psychological (e.g. state and trait anxiety, and therapistâs self-assessment) and skill level (e.g. information literacy and expert knowledge) had significant effects on information-seeking behaviour. These results can hopefully provide insights to designers and librarians who seek to create novel or optimise the existing physician-assisted systems. Â© The Author(s) 2022.","The aim of this study was to identify the variables that can potentially affect information-seeking behaviour in mental health service providers using a quasi-experimental research design. The sample included 30 mental health professionals (with minimum 2 years of experience) to each of whom a scenario was presented in which signs and symptoms of three patients were presented, simulating an actual diagnostic interview. Stress response evaluation (SRE), questionnaires, behavioural observation by the Morae software, and semi-structured interviews were used as means of data collection. Our findings showed that variables such as demographic ( field of study, level of education, work experience and age), psychological ( state and trait anxiety, and therapists self-assessment) and skill level ( information literacy and expert knowledge) had significant effects on information-seeking behaviour. These results can hopefully provide insights to designers and librarians who seek to create novel or optimise the existing physician-assisted systems."
Assessing childrenâs information and knowledge organisation competency in elementary schools of Hong Kong,"This article evaluates the information and knowledge organisation competency of third- to fifth-grade primary school students in Hong Kong directly or indirectly. The majority of the students are aged 8â11 years. The types of information and knowledge organisation schemes to be identified or organised include shallow taxonomies (e.g. a list of entities, a list of features of an entity, a list of events) and simple descriptive ontologies (e.g. a sequence of events, reasons of events, relation between entities or events). A total of 86 students participated in the study. Each student was asked to read an English book and a Chinese book, and to answer assessment questions about the content within the books. The questions ask children to identify members of a flat taxonomy and organise simple descriptive ontologies. The childrenâs overall information and knowledge organisation competency is found to be weak, but childrenâs information and knowledge organisation capabilities are not equally weak. The children identify features of an entity significantly better than a list of events, and identify reasons significantly better than flat taxonomies and relations. The findings have theoretical and practical implications for book writers, book cover designers, teachers, librarians and designers of information systems for children. Â© The Author(s) 2022.","This article evaluates the information and knowledge organisation competency of third- to fifth-grade primary school students in Hong Kong directly or indirectly. The majority of the students are aged 811 years. The types of information and knowledge organisation schemes to be identified or organised include shallow taxonomies ( a list of entities, a list of features of an entity, a list of events) and simple descriptive ontologies ( a sequence of events, reasons of events, relation between entities or events). A total of 86 students participated in the study. Each student was asked to read an English book and a Chinese book, and to answer assessment questions about the content within the books. The questions ask children to identify members of a flat taxonomy and organise simple descriptive ontologies. The childrens overall information and knowledge organisation competency is found to be weak, but childrens information and knowledge organisation capabilities are not equally weak. The children identify features of an entity significantly better than a list of events, and identify reasons significantly better than flat taxonomies and relations. The findings have theoretical and practical implications for book writers, book cover designers, teachers, librarians and designers of information systems for children."
Research on the co-evolution of temporal networks structure and public opinion propagation,"Under the new media environment, social platforms, as the carrier of information propagation, have shown a drastic change in their form and structure, endowing public opinion with unique propagation characteristics. In view of this, considering the dynamic changes of online social network (OSN) structure, this article intends to analyse the spreading mechanism of public opinion in temporal networks and improve the applicability of public opinion governance strategies. Combing the changes of OSN topology with the classical susceptibleâinfectedârecovered (SIR) dynamics model, we constructed a co-evolution model of temporal networks structure and public opinion propagation, and the propagation threshold of public opinion was derived with the help of Markov process. Then, the propagation characteristics of public opinion in temporal networks and their co-evolution process under different factors were discussed through simulation experiments. The results show that the propagation of public opinion in temporal networks has faster speed and wider scope compared with that in static networks; netizensâ social activity has a phased impact on the evolution process of public opinion and with its significant heterogeneity, the propagation of public opinion is gradually suppressed; compared with (Formula presented.), the evolution process of public opinion in temporal networks is more sensitive to the state change of public opinion (Formula presented.). Our research can further enrich the theoretical research system of network science and information science and also provide a certain decision-making reference for the regulators to reasonably govern the propagation of public opinion in social platforms. Â© The Author(s) 2022.","Under the new media environment, social platforms, as the carrier of information propagation, have shown a drastic change in their form and structure, endowing public opinion with unique propagation characteristics. In view of this, considering the dynamic changes of online social network (OSN) structure, this article intends to analyse the spreading mechanism of public opinion in temporal networks and improve the applicability of public opinion governance strategies. Combing the changes of OSN topology with the classical susceptibleinfectedrecovered (SIR) dynamics model, we constructed a co-evolution model of temporal networks structure and public opinion propagation, and the propagation threshold of public opinion was derived with the help of Markov process. Then, the propagation characteristics of public opinion in temporal networks and their co-evolution process under different factors were discussed through simulation experiments. The results show that the propagation of public opinion in temporal networks has faster speed and wider scope compared with that in static networks; netizens social activity has a phased impact on the evolution process of public opinion and with its significant heterogeneity, the propagation of public opinion is gradually suppressed; compared with (Formula presented.), the evolution process of public opinion in temporal networks is more sensitive to the state change of public opinion (Formula presented.). Our research can further enrich the theoretical research system of network science and information science and also provide a certain decision-making reference for the regulators to reasonably govern the propagation of public opinion in social platforms."
The use of subject headings varied in Embase and MEDLINE: An analysis of indexing across six subject areas,"Many bibliographic databases describe the content of a publication using a thesaurus. The vocabularies vary and the extent to which the databases apply them may also differ significantly. The aim of this study is to empirically explore the number of subject headings assigned to publications in two databases over time and to determine if publication characteristics are associated with the number of subject headings. Articles and reviews in MEDLINE and Embase from 1990 to 2019 assigned with one of the subject headings from six subject areas are included in this study. Each of the retrieved publications in Embase is matched with a similar publication in MEDLINE. Furthermore, multivariable linear regressions are used to explore the association of the number of subject headings in MEDLINE and Embase with six prespecified publication characteristics. The average number of assigned subject headings in MEDLINE is stable or even slightly decreasing over time. In Embase, the average number of assigned subject headings was stable until about 2000 where the average number increased dramatically during the next 3 years. Furthermore, linear regressions show that the average number of subject headings in MEDLINE and Embase is higher for publications in English, publications with longer abstract, recent publications and if it belongs to specific subject areas. However, reviews are assigned with more subject headings in Embase and fewer in MEDLINE. The implications of the results are discussed. Â© The Author(s) 2022.","Many bibliographic databases describe the content of a publication using a thesaurus. The vocabularies vary and the extent to which the databases apply them may also differ significantly. The aim of this study is to empirically explore the number of subject headings assigned to publications in two databases over time and to determine if publication characteristics are associated with the number of subject headings. Articles and reviews in MEDLINE and Embase from 1990 to 2019 assigned with one of the subject headings from six subject areas are included in this study. Each of the retrieved publications in Embase is matched with a similar publication in MEDLINE. Furthermore, multivariable linear regressions are used to explore the association of the number of subject headings in MEDLINE and Embase with six prespecified publication characteristics. The average number of assigned subject headings in MEDLINE is stable or even slightly decreasing over time. In Embase, the average number of assigned subject headings was stable until about 2000 where the average number increased dramatically during the next 3 years. Furthermore, linear regressions show that the average number of subject headings in MEDLINE and Embase is higher for publications in English, publications with longer abstract, recent publications and if it belongs to specific subject areas. However, reviews are assigned with more subject headings in Embase and fewer in MEDLINE. The implications of the results are discussed."
Extractive text summarisation using Bayesian state estimation of sentences: A Markovian framework,"Identifying and extracting valuable information from textual documents in the form of cohesively and appropriately developed summaries is one of the most challenging tasks in text mining and natural language processing. In this article, we present a sequential Markov model, equipped with Bayesian inference, to estimate the degree of importance of sentences in a document and thereby address the text summarisation problem. The proposed methodology models the extractive sentence summarisation as a Bayesian state estimation problem, where the system state is the importance degree of each sentence in a document. The transition and observation models are derived using a nonlinear dynamical system identification based on a recurrent feedback neural model that predicts the sentence observation using the sentence input data. In the end, the transition and observation probability density functions are modelled using a mixture density network. The performance assessment of the system has been carried out by investigating the optimal feature dimensionality and the impact of the model parameters on the system accuracy, using entropy-based risk and loss-based risk measures. Finally, the superiority of the proposed methodology over the state of the art in extractive summarisation is discussed and verified by reporting the recall, precision and accuracy on the real-world benchmark data sets. Â© The Author(s) 2022.","Identifying and extracting valuable information from textual documents in the form of cohesively and appropriately developed summaries is one of the most challenging tasks in text mining and natural language processing. In this article, we present a sequential Markov model, equipped with Bayesian inference, to estimate the degree of importance of sentences in a document and thereby address the text summarisation problem. The proposed methodology models the extractive sentence summarisation as a Bayesian state estimation problem, where the system state is the importance degree of each sentence in a document. The transition and observation models are derived using a nonlinear dynamical system identification based on a recurrent feedback neural model that predicts the sentence observation using the sentence input data. In the end, the transition and observation probability density functions are modelled using a mixture density network. The performance assessment of the system has been carried out by investigating the optimal feature dimensionality and the impact of the model parameters on the system accuracy, using entropy-based risk and loss-based risk measures. Finally, the superiority of the proposed methodology over the state of the art in extractive summarisation is discussed and verified by reporting the recall, precision and accuracy on the real-world benchmark data sets."
Modelling the effect of perceived organisational policies on knowledge management in libraries: Focus on the moderating role of transformational leadership and professional commitment,"The purpose of this study was to examine the effect of perceived organisational policies (POPs) on knowledge management (KM) with regard to the moderating role of transformational leadership (TL) and librariansâ professional commitment. The study was conducted in three stages. First, the research moderating variables were chosen through an explorative study and surveying the librarians. Following the design of the theoretical model, the Delphi method was employed to validate it. Finally, the model was tested with a sample of 205 librarians working at Iranian state universities. To examine the causal relationships between the research instrument variables, the structural equation modelling technique and Smart PLS software were used. The results of the study revealed that POP was moderated through TL and the librariansâ professional commitment and affected the processes of knowledge acquisition and identification, knowledge generation and sharing, and knowledge application. The results, further, confirmed the strong effect of POP on professional commitment and the effect of TL on KM. The findings indicate that moderating the negative effects of POP in academic libraries could pave the way for the improvement of librariansâ job performance. To date, no empirical investigations have examined the effect of POP on KM in libraries with regard to the moderating role of TL and the librariansâ professional commitment. This study is assumed to fill this gap. Â© The Author(s) 2022.","The purpose of this study was to examine the effect of perceived organisational policies (POPs) on knowledge management (KM) with regard to the moderating role of transformational leadership (TL) and librarians professional commitment. The study was conducted in three stages. First, the research moderating variables were chosen through an explorative study and surveying the librarians. Following the design of the theoretical model, the Delphi method was employed to validate it. Finally, the model was tested with a sample of 205 librarians working at Iranian state universities. To examine the causal relationships between the research instrument variables, the structural equation modelling technique and Smart PLS software were used. The results of the study revealed that POP was moderated through TL and the librarians professional commitment and affected the processes of knowledge acquisition and identification, knowledge generation and sharing, and knowledge application. The results, further, confirmed the strong effect of POP on professional commitment and the effect of TL on KM. The findings indicate that moderating the negative effects of POP in academic libraries could pave the way for the improvement of librarians job performance. To date, no empirical investigations have examined the effect of POP on KM in libraries with regard to the moderating role of TL and the librarians professional commitment. This study is assumed to fill this gap."
Why a user prefers an artwork: A deep attention model for artwork recommendation,"The combination of art market and emerging e-commerce has brought new trade opportunities and has achieved continuous growth in recent years. More and more people, especially young people, are keen to browse art information and buy artwork on the Internet. Therefore, designing an effective method of recommending artworks may not only effectively enhance the user experience in the art market but also benefit the greater transaction growth. The artwork recommendation task in e-commerce has not received much attention. Previous research works often regard the artworks as ordinary pictures and do not take into account the particularity of the artwork. To solve this problem, we modelled the aesthetic features into artwork recommendation and used the attention mechanism to learn user preferences for various features. We proposed a DAAR (Deep Attention Artwork Recommendation) model and used the attention mechanism to model the userâs preference weights for different features (including content features, aesthetic features and authors). To verify the validity of the proposed model, we collected data and conducted experiments on a real artwork community website. The experimental results show that the proposed DAAR model was better than the current state-of-the-art recommendation methods. Â© The Author(s) 2022.","The combination of art market and emerging e-commerce has brought new trade opportunities and has achieved continuous growth in recent years. More and more people, especially young people, are keen to browse art information and buy artwork on the Internet. Therefore, designing an effective method of recommending artworks may not only effectively enhance the user experience in the art market but also benefit the greater transaction growth. The artwork recommendation task in e-commerce has not received much attention. Previous research works often regard the artworks as ordinary pictures and do not take into account the particularity of the artwork. To solve this problem, we modelled the aesthetic features into artwork recommendation and used the attention mechanism to learn user preferences for various features. We proposed a DAAR (Deep Attention Artwork Recommendation) model and used the attention mechanism to model the users preference weights for different features (including content features, aesthetic features and authors). To verify the validity of the proposed model, we collected data and conducted experiments on a real artwork community website. The experimental results show that the proposed DAAR model was better than the current state-of-the-art recommendation methods."
Personalised publication recommendation service for open-access digital archives,"Increase in the number of open-access academic publications and open-access institutional academic archives led more researchers use these archives. No model offers personalised publication suggestions in academic archives. A central service architecture has been proposed towards personalised academic article recommendations for open-access digital archives. Thus, it has been possible to make personalised suggestions for open-access digital archives and enable researchers to discover new publications. A service based on the centralised micro-service architecture model was proposed in the study. Also, TF-IDF and article classification methods were used together for the personalised publication suggestion system. For the first time globally, the proposed method was used with 1464 real users in 49 open-access archives. F-measure success value was found to be higher than 0.8 for recommended publications. Â© The Author(s) 2022.","Increase in the number of open-access academic publications and open-access institutional academic archives led more researchers use these archives. No model offers personalised publication suggestions in academic archives. A central service architecture has been proposed towards personalised academic article recommendations for open-access digital archives. Thus, it has been possible to make personalised suggestions for open-access digital archives and enable researchers to discover new publications. A service based on the centralised micro-service architecture model was proposed in the study. Also, TF-IDF and article classification methods were used together for the personalised publication suggestion system. For the first time globally, the proposed method was used with 1464 real users in 49 open-access archives. F-measure success value was found to be higher than 0.8 for recommended publications."
A modified LSTM network to predict the citation counts of papers,"Quantifiable predictability in the citation counts of articles is significant in scientometrics and informetrics. Many metrics based on the citation counts can evaluate the scientific impact of research articles and journals. Utilising time series models, an articleâs citation counts up to the yth year after publication can be predicted by those up to the previous years. However, the typically used models cannot predict the fat tail of the actual citation distributions. Thus, based on cumulative advantage of the citation behaviour, we propose a method to predict the accumulated citation counts, by using a random number sampled from a power-law distribution to modify the results given by a recurrent neural network (RNN), long short-term memory. Extensive experiments on the data set including 17 journals in information science verified the effectiveness of our method by the good fittings on distributions and evolutionary trends of the citation counts of articles. Our method has the potential to be extended to predict other popular assessment measures such as impact factor and h-index for journals. Â© The Author(s) 2022.","Quantifiable predictability in the citation counts of articles is significant in scientometrics and informetrics. Many metrics based on the citation counts can evaluate the scientific impact of research articles and journals. Utilising time series models, an articles citation counts up to the yth year after publication can be predicted by those up to the previous years. However, the typically used models cannot predict the fat tail of the actual citation distributions. Thus, based on cumulative advantage of the citation behaviour, we propose a method to predict the accumulated citation counts, by using a random number sampled from a power-law distribution to modify the results given by a recurrent neural network (RNN), long short-term memory. Extensive experiments on the data set including 17 journals in information science verified the effectiveness of our method by the good fittings on distributions and evolutionary trends of the citation counts of articles. Our method has the potential to be extended to predict other popular assessment measures such as impact factor and h-index for journals."
"Research on the discourse power evaluation of academic journals from the perspective of multiple fusion: Taking Medicine, General and Internal journals as an example","In the open science environment, this article evaluates the discourse power of academic journals from the perspective of multiple integration. It is conducive to scientific research management and provides a reference for enriching and perfecting the evaluation theory and indicators system of academic journals. Based on the theory of evaluation science, discourse power theory and communication theory, first, this article discusses the basic issues of the discourse power evaluation for academic journals. It defines the academic discourse power and the discourse power of academic journals. It is proposed that the discourse power of academic journals is composed of discourse influence and discourse leading. Discourse influence is composed of discourse influence ability and discourse power, and discourse leading is composed of social media discourse, news and policy discourse, encyclopaedia discourse, peer-review discourse and video discourse leading. This article explores the formation process and operation mechanism of the discourse power for academic journals. Then, this article constructs the discourse power evaluation model of academic journals. Second, this article integrates multi-source heterogeneous data, then adopts correlation analysis, integrated factor analysis, entropy weight method, Technique for Order Preference by Similarity to an Ideal Solution (TOPSIS) method and two-dimensional four-quadrant mapping method to conduct empirical research on the discourse power evaluation of Medicine, General and Internal journals from the perspectives of multi-dimensions, multi-factors, multi-indicators and multi-methods fusion. The results show that the research on the discourse power evaluation for academic journals based on the theory, method and application logic is practical, comprehensive and reliable. Â© The Author(s) 2022.","In the open science environment, this article evaluates the discourse power of academic journals from the perspective of multiple integration. It is conducive to scientific research management and provides a reference for enriching and perfecting the evaluation theory and indicators system of academic journals. Based on the theory of evaluation science, discourse power theory and communication theory, first, this article discusses the basic issues of the discourse power evaluation for academic journals. It defines the academic discourse power and the discourse power of academic journals. It is proposed that the discourse power of academic journals is composed of discourse influence and discourse leading. Discourse influence is composed of discourse influence ability and discourse power, and discourse leading is composed of social media discourse, news and policy discourse, encyclopaedia discourse, peer-review discourse and video discourse leading. This article explores the formation process and operation mechanism of the discourse power for academic journals. Then, this article constructs the discourse power evaluation model of academic journals. Second, this article integrates multi-source heterogeneous data, then adopts correlation analysis, integrated factor analysis, entropy weight method, Technique for Order Preference by Similarity to an Ideal Solution (TOPSIS) method and two-dimensional four-quadrant mapping method to conduct empirical research on the discourse power evaluation of Medicine, General and Internal journals from the perspectives of multi-dimensions, multi-factors, multi-indicators and multi-methods fusion. The results show that the research on the discourse power evaluation for academic journals based on the theory, method and application logic is practical, comprehensive and reliable."
Locality sensitive blocking (LSB): A robust blocking technique for data deduplication,"Data deduplication is process of discovering multiple representations of same entity in an information system. Blocking has been a benchmark technique for avoiding the pair-wise record comparisons in data deduplication. Standard blocking (SB) aims at putting the potential duplicate records in the same block on the basis of a blocking key. Afterwards, the detailed comparisons are made only among the records residing in the same block. The selection of blocking key is a tedious process that involves exponential alternatives. The outcome of SB varies considerably with a change in blocking key. To this end, we have proposed a robust blocking technique called Locality Sensitive Blocking (LSB) that does not require the selection of blocking key. The experimental results show an increase of up to 0.448 in F-score as compared with SB. Furthermore, it is found that LSB is more robust towards blocking parameters and data noise. Â© The Author(s) 2022.","Data deduplication is process of discovering multiple representations of same entity in an information system. Blocking has been a benchmark technique for avoiding the pair-wise record comparisons in data deduplication. Standard blocking (SB) aims at putting the potential duplicate records in the same block on the basis of a blocking key. Afterwards, the detailed comparisons are made only among the records residing in the same block. The selection of blocking key is a tedious process that involves exponential alternatives. The outcome of SB varies considerably with a change in blocking key. To this end, we have proposed a robust blocking technique called Locality Sensitive Blocking (LSB) that does not require the selection of blocking key. The experimental results show an increase of up to 0.448 in F-score as compared with SB. Furthermore, it is found that LSB is more robust towards blocking parameters and data noise."
Sentiment analysis using lexico-semantic features,"Sentiment analysis of the text deals with the mining of the opinions of people from their written communication. With the increasing usage of online social media platforms for user interactions, abundant opinionated textual data emerges. Therefore, it leads to increased mining of opinions and sentiments and hence greater interest in sentiment analysis. The article introduces the novel Lexico-Semantic features and their use in the sentiment polarity task of English language text. These features are derived using the semantic extension of the lexicons by employing sentiment lexicons and semantic models. These features make data sample size consistent when used in deep learning settings, thereby eliminating the zero padding. For evaluation, we use different semantic models and lexicons to determine the role and impact of Lexico-Semantic features in classification performance. These features, along with the other features, are used to train the different classifiers. Our experimental evaluation shows that introducing Lexico-Semantic features to various state-of-the-art methods of both machine and deep learning improves the overall performance of classifiers. Â© The Author(s) 2022.","Sentiment analysis of the text deals with the mining of the opinions of people from their written communication. With the increasing usage of online social media platforms for user interactions, abundant opinionated textual data emerges. Therefore, it leads to increased mining of opinions and sentiments and hence greater interest in sentiment analysis. The article introduces the novel Lexico-Semantic features and their use in the sentiment polarity task of English language text. These features are derived using the semantic extension of the lexicons by employing sentiment lexicons and semantic models. These features make data sample size consistent when used in deep learning settings, thereby eliminating the zero padding. For evaluation, we use different semantic models and lexicons to determine the role and impact of Lexico-Semantic features in classification performance. These features, along with the other features, are used to train the different classifiers. Our experimental evaluation shows that introducing Lexico-Semantic features to various state-of-the-art methods of both machine and deep learning improves the overall performance of classifiers."
Investigating country-focused studies in Library and Information Science journals,"A country-focused study can be defined as a type of study in which the articleâs topic is limited to a particular country. Based on the assumption that a country-focused article contains one or more country names in the title or keywords (CNtk), the objective of this study was to investigate the characteristics of country-focused articles in Library and Information Science (LIS) journals. For this study, we selected 30 journals from the Scimago Journal Rank with the highest h-index scores to empirically investigate LIS articles with CNtk. The respective bibliographic records of these journals from 2010 to 2020 were downloaded from Scopus. In terms of subject categories, journals that addressed scientific and technical topics published fewer country-focused articles. In contrast, those on culture, society and government published a higher number of country-focused articles. One exception to this generalisation is that country-focused studies were found to be highly prevalent in certain technical subjects, such as bibliometrics. Although additional empirical evidence is needed for other fields of study, the proposed method seems valuable in analysing journal characteristics since it provides country-specific information about the published articles. Â© The Author(s) 2022.","A country-focused study can be defined as a type of study in which the articles topic is limited to a particular country. Based on the assumption that a country-focused article contains one or more country names in the title or keywords (CNtk), the objective of this study was to investigate the characteristics of country-focused articles in Library and Information Science (LIS) journals. For this study, we selected 30 journals from the Scimago Journal Rank with the highest h-index scores to empirically investigate LIS articles with CNtk. The respective bibliographic records of these journals from 2010 to 2020 were downloaded from Scopus. In terms of subject categories, journals that addressed scientific and technical topics published fewer country-focused articles. In contrast, those on culture, society and government published a higher number of country-focused articles. One exception to this generalisation is that country-focused studies were found to be highly prevalent in certain technical subjects, such as bibliometrics. Although additional empirical evidence is needed for other fields of study, the proposed method seems valuable in analysing journal characteristics since it provides country-specific information about the published articles."
Professional benefits and challenges of health information documentation on Instagram for Iranian physicians,"Social media are new tools that can be used for facilitating health information management. However, social mediaâs benefits and challenges for information documentation have not been identified well. This study sought to identify the benefits and challenges of Instagram to information documentation based on Iranian physiciansâ viewpoints. In this qualitative study, a semi-structure interview was administered to 28 Iranian physicians, and data were collected using purposive and snowball sampling method. The data analysis was done using the thematic analysis method by MAXQDA 10. Based on the findings of this study, six benefits for information documentation over Instagram were identified include sharing lessons learned, developing incidental learning, empowering communications, supporting decision-making, managing personal brand and translating knowledge. Also, based on Iranian physiciansâ viewpoints, three challenges were found for information documentation in Instagram include ethical and privacy challenges, poor information quality and damaging professional image. Iranian physicians have different attitudes towards using Instagram for information documentation. For control challenges, regulatory and security issues must be addressed to protect physiciansâ privacy and more education is required for the health professionals to make them more aware of the nature of using social media. Â© The Author(s) 2022.","Social media are new tools that can be used for facilitating health information management. However, social medias benefits and challenges for information documentation have not been identified well. This study sought to identify the benefits and challenges of Instagram to information documentation based on Iranian physicians viewpoints. In this qualitative study, a semi-structure interview was administered to 28 Iranian physicians, and data were collected using purposive and snowball sampling method. The data analysis was done using the thematic analysis method by MAXQDA 10. Based on the findings of this study, six benefits for information documentation over Instagram were identified include sharing lessons learned, developing incidental learning, empowering communications, supporting decision-making, managing personal brand and translating knowledge. Also, based on Iranian physicians viewpoints, three challenges were found for information documentation in Instagram include ethical and privacy challenges, poor information quality and damaging professional image. Iranian physicians have different attitudes towards using Instagram for information documentation. For control challenges, regulatory and security issues must be addressed to protect physicians privacy and more education is required for the health professionals to make them more aware of the nature of using social media."
To know or not to know? Exploring COVID-19 information seeking with the risk information seeking and processing model,"To cope with the COVID-19 pandemic and reduce uncertainty, the public needs accurate and timely information. Inspired by the risk information seeking and processing (RISP) model, this article discovers the significant predictors of individualsâ COVID-19 information-seeking intention and behaviour. Overall, 394 adult participants from 47 states completed this studyâs online survey. The hierarchical regression analysis reveals that risk experience and informational subjective norms are the most substantial predictors of individualsâ online information-seeking behaviour about COVID-19. Information insufficiency did not predict information seeking, and participants tend to overestimate their knowledge about COVID-19. RISP variables tend to share power in explaining the variances of information-seeking behaviour. Moreover, both channel beliefs and perceived information gathering capacity moderate information insufficiencyâs prediction of information-seeking intention. These findings will assist researchers in discovering the fundamental motivation of information seeking. This article can guide pragmatic interventions to reduce the publicâs uncertainty and mitigate the risk. Â© The Author(s) 2022.","To cope with the COVID-19 pandemic and reduce uncertainty, the public needs accurate and timely information. Inspired by the risk information seeking and processing (RISP) model, this article discovers the significant predictors of individuals COVID-19 information-seeking intention and behaviour. Overall, 394 adult participants from 47 states completed this studys online survey. The hierarchical regression analysis reveals that risk experience and informational subjective norms are the most substantial predictors of individuals online information-seeking behaviour about COVID-19. Information insufficiency did not predict information seeking, and participants tend to overestimate their knowledge about COVID-19. RISP variables tend to share power in explaining the variances of information-seeking behaviour. Moreover, both channel beliefs and perceived information gathering capacity moderate information insufficiencys prediction of information-seeking intention. These findings will assist researchers in discovering the fundamental motivation of information seeking. This article can guide pragmatic interventions to reduce the publics uncertainty and mitigate the risk."
Quality of reporting of literature search strategies in systematic reviews published on the role of telehealth during COVID-19,"A comprehensive and reproducible search strategy for systematic reviews especially about COVID-19 plays a pivotal role in conducting a reliable and unbiased review. The primary aim of this study was to investigate the quality of the search strategy reporting in systematic reviews conducted on the role of telehealth during COVID-19. The secondary aim of study was to explore some affecting factor in the quality of search strategy. The study evaluated the quality of the search strategy reporting with PRISMA-S checklist. The search was performed in MEDLINE, Embase, CINAHL, and other related databases. Systematic reviews were included. There was no language restriction. The correlation of the PRISMA-S scores with journal impact factor, CiteScore, and librariansâ role were evaluated using Spearmanâs rank correlation coefficient. A total of 85 articles were included in the review. The overall mean score of PRISMA-S checklist was 6.12 Â± 1.46. PubMed was the most popular database for search. More than half of the studies did not provide a full search strategy. There was a significant positive correlation between PRISMA-S score and the journal impact factor (Spearmanâs rho = 0.217; P = 0.46) and CiteScore (Spearmanâs rho = 0.235; P = 0.03). The quality of literature search was poor in the included studies. Using the PRISMA-S as a search reporting guideline can be a helpful tool for authors. A professional librarian can be beneficial in improving the quality of the search. It is recommended to use a new pattern in COVID-19-related searches, such as preprint sources. Â© The Author(s) 2022.","A comprehensive and reproducible search strategy for systematic reviews especially about COVID-19 plays a pivotal role in conducting a reliable and unbiased review. The primary aim of this study was to investigate the quality of the search strategy reporting in systematic reviews conducted on the role of telehealth during COVID-19. The secondary aim of study was to explore some affecting factor in the quality of search strategy. The study evaluated the quality of the search strategy reporting with PRISMA-S checklist. The search was performed in MEDLINE, Embase, CINAHL, and other related databases. Systematic reviews were included. There was no language restriction. The correlation of the PRISMA-S scores with journal impact factor, CiteScore, and librarians role were evaluated using Spearmans rank correlation coefficient. A total of 85 articles were included in the review. The overall mean score of PRISMA-S checklist was 6.12 1.46. PubMed was the most popular database for search. More than half of the studies did not provide a full search strategy. There was a significant positive correlation between PRISMA-S score and the journal impact factor (Spearmans rho = 0.217; P = 0.46) and CiteScore (Spearmans rho = 0.235; P = 0.03). The quality of literature search was poor in the included studies. Using the PRISMA-S as a search reporting guideline can be a helpful tool for authors. A professional librarian can be beneficial in improving the quality of the search. It is recommended to use a new pattern in COVID-19-related searches, such as preprint sources."
Measuring knowledge contribution performance of physicians in online health communities: A BP neural network approach,"Extant literature on measuring the performance of physiciansâ knowledge contribution in an online health community (OHC) is limited. To address this gap, this article aims to (1) develop a measurement model for physiciansâ knowledge contribution performance; (2) use BP neural network to assign reasonable weight to each indicator of the model; and (3) explore the status and differences of knowledge contribution performance among a group of physicians. Based on the sample of 5407 infectious disease physicians in a Chinese OHC, we propose the measurement model by integrating physiciansâ active knowledge contribution (AKC) and responsive knowledge contribution (RKC), covering 11 dimensions of contribution quantity and quality. We employ the BP neural network to optimise the model weights using the initial weight of the model obtained by the entropy method. The Technique for Order of Preference by Similarity to Ideal Solution (TOPSIS) method is used to evaluate the performance of physiciansâ knowledge contribution in the OHC. The results show that it is feasible to use BP neural network to assign model weights. The distribution of physiciansâ knowledge contribution performance is uneven; only a few have a high-level knowledge contribution performance. Meanwhile, a significant positive correlation exists between a physicianâs title and respective knowledge contribution performance. Our research may contribute to related literature and practices by offering a fine-grained understanding of the performance of physiciansâ knowledge contribution. Â© The Author(s) 2022.","Extant literature on measuring the performance of physicians knowledge contribution in an online health community (OHC) is limited. To address this gap, this article aims to develop a measurement model for physicians knowledge contribution performance; use BP neural network to assign reasonable weight to each indicator of the model; and explore the status and differences of knowledge contribution performance among a group of physicians. Based on the sample of 5407 infectious disease physicians in a Chinese OHC, we propose the measurement model by integrating physicians active knowledge contribution (AKC) and responsive knowledge contribution (RKC), covering 11 dimensions of contribution quantity and quality. We employ the BP neural network to optimise the model weights using the initial weight of the model obtained by the entropy method. The Technique for Order of Preference by Similarity to Ideal Solution (TOPSIS) method is used to evaluate the performance of physicians knowledge contribution in the OHC. The results show that it is feasible to use BP neural network to assign model weights. The distribution of physicians knowledge contribution performance is uneven; only a few have a high-level knowledge contribution performance. Meanwhile, a significant positive correlation exists between a physicians title and respective knowledge contribution performance. Our research may contribute to related literature and practices by offering a fine-grained understanding of the performance of physicians knowledge contribution."
Factors affecting the journal choice for manuscript submission: A qualitative study on Turkish medical researchers,"The factors that affect journal choice for manuscript submission vary, depending on the researcherâs career and field. Although several guidelines are available, information is limited on which criteria are used by Turkish authors. We aimed to evaluate the factors that affect Turkish medical researchersâ journal choice decisions using semi-structured in-depth video conference interviews. The participants identified journal prestige as the major factor, mainly Science Citation Index-Expanded (SCIE) indexing and journal impact factor (JIF), along with acceptance and rejection rates, the age of the journal, and journal audience. Participants identified high publishing costs and mandatory paid open access policy as negative factors. Future policies on covering publishing costs institutionally would be helpful to remove obstacles during journal choice. Turkish medical researchers should be informed on using various indexes and scientometric data to better understand journal performance, rather than only SCIE and JIF. Â© The Author(s) 2022.","The factors that affect journal choice for manuscript submission vary, depending on the researchers career and field. Although several guidelines are available, information is limited on which criteria are used by Turkish authors. We aimed to evaluate the factors that affect Turkish medical researchers journal choice decisions using semi-structured in-depth video conference interviews. The participants identified journal prestige as the major factor, mainly Science Citation Index-Expanded (SCIE) indexing and journal impact factor (JIF), along with acceptance and rejection rates, the age of the journal, and journal audience. Participants identified high publishing costs and mandatory paid open access policy as negative factors. Future policies on covering publishing costs institutionally would be helpful to remove obstacles during journal choice. Turkish medical researchers should be informed on using various indexes and scientometric data to better understand journal performance, rather than only SCIE and JIF."
Questionable conferences and presenters from top-ranked universities,"This article aims to investigate the structures of 935 conferences organised by OMICS and 296 conferences organised by WASET from 2015 through 2017. These conferences are characterised in existing literature as so-called predatory or questionable conferences that provide low-quality academic meetings. We analyse 40,224 presenters, focusing on top-ranked institutions according to three global university ranking systems (Academic Ranking of World Universities, Times Higher Education World University Rankings, and QS World University Rankings). Our analysis shows that participants in OMICS events were primarily researchers from the United States, India, the United Kingdom, and China. WASET attracted more researchers from Turkey, India, and South Korea. We found that 11.0% of OMICS and 5.7% of WASET presenters were affiliated with institutions ranked in the top 100 in one of the three aforementioned rankings. We also found that both companies mostly organised conferences in cities that were top tourist destinations. Â© The Author(s) 2022.","This article aims to investigate the structures of 935 conferences organised by OMICS and 296 conferences organised by WASET from 2015 through 2017. These conferences are characterised in existing literature as so-called predatory or questionable conferences that provide low-quality academic meetings. We analyse 40,224 presenters, focusing on top-ranked institutions according to three global university ranking systems (Academic Ranking of World Universities, Times Higher Education World University Rankings, and QS World University Rankings). Our analysis shows that participants in OMICS events were primarily researchers from the United States, India, the United Kingdom, and China. WASET attracted more researchers from Turkey, India, and South Korea. We found that 11.0% of OMICS and 5.7% of WASET presenters were affiliated with institutions ranked in the top 100 in one of the three aforementioned rankings. We also found that both companies mostly organised conferences in cities that were top tourist destinations."
"Examination of digital citizenship, online information searching strategy and information literacy depending on changing state of experience in using digital technologies during COVID-19 pandemic","As educational processes are adapted to the online environment due to COVID-19 pandemic, digital citizenship and online information searching strategies came into prominence. In this context, the aim of this study is the examination of digital citizenship awareness, online information searching strategies and differentiation of the level of information literacy depending on changing state of experience in using digital technologies before and during COVID-19 pandemic. Also, it is aimed to examine the presence of relations between variables of the research. The study was conducted with the participation of 255 university students. Analyses were performed using multivariate analysis of variance (MANOVA) and partial least squares structural equation modelling (PLS-SEM). According to the results of the study, during COVID-19 pandemic, there was an increase in digital citizenship awareness of students who spend more time in social media and the ones who improved digital technology usage skills developed online information searching strategies. According to the model analysed, online information searching strategies and information literacy have an effect on digital citizenship while online information searching strategies have an effect on information literacy. The results of the study showed that the students with higher level of information literacy and online information searching strategies may help them develop digital citizenship awareness. Â© The Author(s) 2022.","As educational processes are adapted to the online environment due to COVID-19 pandemic, digital citizenship and online information searching strategies came into prominence. In this context, the aim of this study is the examination of digital citizenship awareness, online information searching strategies and differentiation of the level of information literacy depending on changing state of experience in using digital technologies before and during COVID-19 pandemic. Also, it is aimed to examine the presence of relations between variables of the research. The study was conducted with the participation of 255 university students. Analyses were performed using multivariate analysis of variance (MANOVA) and partial least squares structural equation modelling (PLS-SEM). According to the results of the study, during COVID-19 pandemic, there was an increase in digital citizenship awareness of students who spend more time in social media and the ones who improved digital technology usage skills developed online information searching strategies. According to the model analysed, online information searching strategies and information literacy have an effect on digital citizenship while online information searching strategies have an effect on information literacy. The results of the study showed that the students with higher level of information literacy and online information searching strategies may help them develop digital citizenship awareness."
SEA-PS: Semantic embedding with attention to measuring patent similarity by leveraging various text fields,"Similarity metrics are critical to identifying the relationships between patents. While many bibliometric methods such as co-citation and co-classification fail to use the vast majority of technical information existing in the text, most text mining methods focus on keywords in only one text field of the patent document. This article aims to leverage various text fields to measure pairwise patent similarity according to their technological bases. A novel approach called semantic embedding with attention for patent similarity (SEA-PS) is proposed. First, the method identifies technological bases and models the semantic relatedness. To achieve this, we put forward an additional patent stop-word list to help extract technical terms with an n-gram-based statistical method. The technical terms are then mapped into a vector space using word embedding. Second, we propose a graph-based method to allocate weights to distinguish the technical focus, considering the linkages between technologies. Finally, we assess the feasibility of the text fields, and integrate their semantics at the patent-level with an attention layer to conduct similarity metrics. The validations are from two perspectives: content validity (coverage of technical information, the validity of semantic representations and effectiveness of text field combinations), and external validity against existing methods via an expert panel. The results demonstrate the superiority of SEA-PS to existing methods, and suggest that âabstractsâ, âclaimsâ and âtechnical descriptionsâ are more effective than âtitlesâ. SEA-PS is a fundamental tool for patent retrieval and classification. It also has a broad range of practical applications in innovation and strategy studies, including identifying technological frontiers and studying knowledge spillovers. Â© The Author(s) 2022.","Similarity metrics are critical to identifying the relationships between patents. While many bibliometric methods such as co-citation and co-classification fail to use the vast majority of technical information existing in the text, most text mining methods focus on keywords in only one text field of the patent document. This article aims to leverage various text fields to measure pairwise patent similarity according to their technological bases. A novel approach called semantic embedding with attention for patent similarity (SEA-PS) is proposed. First, the method identifies technological bases and models the semantic relatedness. To achieve this, we put forward an additional patent stop-word list to help extract technical terms with an n-gram-based statistical method. The technical terms are then mapped into a vector space using word embedding. Second, we propose a graph-based method to allocate weights to distinguish the technical focus, considering the linkages between technologies. Finally, we assess the feasibility of the text fields, and integrate their semantics at the patent-level with an attention layer to conduct similarity metrics. The validations are from two perspectives: content validity (coverage of technical information, the validity of semantic representations and effectiveness of text field combinations), and external validity against existing methods via an expert panel. The results demonstrate the superiority of SEA-PS to existing methods, and suggest that abstracts, claims and technical descriptions are more effective than titles. SEA-PS is a fundamental tool for patent retrieval and classification. It also has a broad range of practical applications in innovation and strategy studies, including identifying technological frontiers and studying knowledge spillovers."
Exploring hotspots and user information behaviour of WeChat official accounts: An empirical study based on stimulusâresponse model,"The aim of this study was to investigate the hotspots of WeChat official accounts and the impact of their pushes on user information behaviour including reading rate, sharing rate, number of comments or collections and fan growth rate. Using nine official accounts provided by the Sootoo Network, this study collected data on more than 10,000 pushes released from January to December 2017. In this study, a second-order user information behaviour model using the collected data was constructed. Based on empirical research, a prediction model of user information behaviour was built using a backpropagation neural network and random forest algorithm, and two variable sets were used for training. Then, the effect of different prediction models was analysed to determine the main factors affecting user information behaviour. This study addresses gaps in the field of WeChat research, and the results are of great practical significance for the operators of WeChat official accounts: they can help them optimise operation effects and enhance the influence of official accounts. Â© The Author(s) 2022.","The aim of this study was to investigate the hotspots of WeChat official accounts and the impact of their pushes on user information behaviour including reading rate, sharing rate, number of comments or collections and fan growth rate. Using nine official accounts provided by the Sootoo Network, this study collected data on more than 10,000 pushes released from January to December 2017. In this study, a second-order user information behaviour model using the collected data was constructed. Based on empirical research, a prediction model of user information behaviour was built using a backpropagation neural network and random forest algorithm, and two variable sets were used for training. Then, the effect of different prediction models was analysed to determine the main factors affecting user information behaviour. This study addresses gaps in the field of WeChat research, and the results are of great practical significance for the operators of WeChat official accounts: they can help them optimise operation effects and enhance the influence of official accounts."
Studentsâ perception of academic databases as recognition of learning and research during the COVID-19 pandemic,"Consistent with their goal of becoming a centre for educational excellence in teaching, learning and research, the authorities of Chinese higher education introduced their academic database system as one of their e-library services. However, the existing literature exhibits inadequate empirical measurements of academic databases in all aspects of higher education throughout the world during the COVID-19 pandemic. To address this gap, this study aims to validate the technology satisfaction model (TSM) for measuring studentsâ satisfaction in using academic databases for their learning and research purposes. This study also analysed local and international academic databases to explore whether these databases could play a moderating role in shaping learnersâ satisfaction. The data were collected through a survey of 500 respondents studying at a research university in Shanghai. The results, which were analysed by structural equation modelling and the Rasch model, showed that studentsâ satisfaction is determined by three valid predictors: computer self-efficacy, perceived usefulness and ease of use, and causal direct and indirect relationships among these variables in the use of local and international academic databases. Our new findings on the moderating effect of local (i.e. Chinese) and international (e.g. English) academic databases highlighted that the TSM has successfully estimated dual databases and produced insignificant, dissimilar results. This study could aid local and international educators, researchers, information science professionals and others in measuring the perception of academic databases for learning and research. This research could also serve as a guideline for researchers and psychometricians in measuring innovative learning technologies using structural equation modelling and the Rasch model. This is the unique contribution of this study, which concludes that local and international academic databases are almost equally important for postgraduate students at a research university in China. Moreover, these students are satisfied in using these academic databases to learn and do research. Â© The Author(s) 2022.","Consistent with their goal of becoming a centre for educational excellence in teaching, learning and research, the authorities of Chinese higher education introduced their academic database system as one of their e-library services. However, the existing literature exhibits inadequate empirical measurements of academic databases in all aspects of higher education throughout the world during the COVID-19 pandemic. To address this gap, this study aims to validate the technology satisfaction model (TSM) for measuring students satisfaction in using academic databases for their learning and research purposes. This study also analysed local and international academic databases to explore whether these databases could play a moderating role in shaping learners satisfaction. The data were collected through a survey of 500 respondents studying at a research university in Shanghai. The results, which were analysed by structural equation modelling and the Rasch model, showed that students satisfaction is determined by three valid predictors: computer self-efficacy, perceived usefulness and ease of use, and causal direct and indirect relationships among these variables in the use of local and international academic databases. Our new findings on the moderating effect of local ( Chinese) and international ( English) academic databases highlighted that the TSM has successfully estimated dual databases and produced insignificant, dissimilar results. This study could aid local and international educators, researchers, information science professionals and others in measuring the perception of academic databases for learning and research. This research could also serve as a guideline for researchers and psychometricians in measuring innovative learning technologies using structural equation modelling and the Rasch model. This is the unique contribution of this study, which concludes that local and international academic databases are almost equally important for postgraduate students at a research university in China. Moreover, these students are satisfied in using these academic databases to learn and do research."
Components of reading culture: Insights from bibliometric analysis of 1991â2020 research,"In the digital age, the reading habits are gradually declining and poor reading skills are causing various informational and social challenges include lack of critical thinking and intellectual irresponsibility. The main objective of this study is to consolidate the published works to comprehend the influencing factors of reading culture. We examine the reading skills, reading habits and reading behaviour literature from 1991 to 2020 to measure the scope and depth of reading culture (RC) through bibliometric analysis and visualisation by utilising a VOSviewer software based on Web of Science database. These selected 1139 articles are classified according to year of publication, author, the country of publications and author affiliation. The analysis provided insights about the RC publication trends as the most productive years are 2017 and 2019. The United States, Spain and China are the leading countries contributing to reading culture-related publications. JyvÃ¤skylÃ¤ University and Florida State University serve as the most productive institutions. Furthermore, we also conduct a thematic analysis based on the keywords to identify the components and influencing factors contributing to the reading culture. The four main components of the reading culture are identified as reading skills, behaviour, habits and factors. At the end, we provided propositions to support the reading culture and future research directions for the interest of Library and Information Science (LIS) professionals, academicians and researchers. Â© The Author(s) 2022.","In the digital age, the reading habits are gradually declining and poor reading skills are causing various informational and social challenges include lack of critical thinking and intellectual irresponsibility. The main objective of this study is to consolidate the published works to comprehend the influencing factors of reading culture. We examine the reading skills, reading habits and reading behaviour literature from 1991 to 2020 to measure the scope and depth of reading culture (RC) through bibliometric analysis and visualisation by utilising a VOSviewer software based on Web of Science database. These selected 1139 articles are classified according to year of publication, author, the country of publications and author affiliation. The analysis provided insights about the RC publication trends as the most productive years are 2017 and 2019. The United States, Spain and China are the leading countries contributing to reading culture-related publications. Jyvskyl University and Florida State University serve as the most productive institutions. Furthermore, we also conduct a thematic analysis based on the keywords to identify the components and influencing factors contributing to the reading culture. The four main components of the reading culture are identified as reading skills, behaviour, habits and factors. At the end, we provided propositions to support the reading culture and future research directions for the interest of Library and Information Science (LIS) professionals, academicians and researchers."
Data sharing practices across knowledge domains: A dynamic examination of data availability statements in PLOS ONE publications,"As the importance of research data gradually grows in sciences, data sharing has come to be encouraged and even mandated by journals and funders in recent years. Following this trend, the data availability statement has been increasingly embraced by academic communities as a means of sharing research data as part of research articles. This article presents a quantitative study of which mechanisms and repositories are used to share research data in PLOS ONE articles. We offer a dynamic examination of this topic from the disciplinary and temporal perspectives based on all statements in English-language research articles published between 2014 and 2020 in the journal. We find a slow yet steady growth in the use of data repositories to share data over time, as opposed to sharing data in the article and/or supplementary materials; this indicates improved compliance with the journalâs data sharing policies. We also find that multidisciplinary data repositories have been increasingly used over time, whereas some disciplinary repositories show a decreasing trend. Our findings can help academic publishers and funders to improve their data sharing policies and serve as an important baseline dataset for future studies on data sharing activities. Â© The Author(s) 2022.","As the importance of research data gradually grows in sciences, data sharing has come to be encouraged and even mandated by journals and funders in recent years. Following this trend, the data availability statement has been increasingly embraced by academic communities as a means of sharing research data as part of research articles. This article presents a quantitative study of which mechanisms and repositories are used to share research data in PLOS ONE articles. We offer a dynamic examination of this topic from the disciplinary and temporal perspectives based on all statements in English-language research articles published between 2014 and 2020 in the journal. We find a slow yet steady growth in the use of data repositories to share data over time, as opposed to sharing data in the article and/or supplementary materials; this indicates improved compliance with the journals data sharing policies. We also find that multidisciplinary data repositories have been increasingly used over time, whereas some disciplinary repositories show a decreasing trend. Our findings can help academic publishers and funders to improve their data sharing policies and serve as an important baseline dataset for future studies on data sharing activities."
A new multi-document summarisation approach using saplings growing-up optimisation algorithms: Simultaneously optimised coverage and diversity,"Automatic text summarisation is obtaining a subset that accurately represents the main text. A quality summary should contain the maximum amount of information while avoiding redundant information. Redundancy is a severe deficiency that causes unnecessary repetition of information within sentences and should not occur in summarisation studies. Although many optimisation-based text summarisation methods have been proposed in recent years, there exists a lack of research on the simultaneous optimisation of scope and redundancy. In this context, this study presents an approach in which maximum coverage and minimum redundancy, which form the two key features of a rich summary, are modelled as optimisation targets. In optimisation-based text summarisation studies, different conflicting objectives are generally weighted or formulated and transformed into single-objective problems. However, this transformation can directly affect the quality of the solution. In this study, the optimisation goals are met simultaneously without transformation or formulation. In addition, the multi-objective saplings growing-up algorithm (MO-SGuA) is implemented and modified for text summarisation. The presented approach, called Pareto optimal, achieves an optimal solution with simultaneous optimisation. Experimentation with the MO-SGuA method was tested using open-access (document understanding conference; DUC) data sets. Performance success of the MO-SGuA approach was calculated using the recall-oriented understudy for gisting evaluation (ROUGE) metrics and then compared with the competitive practices used in the literature. Testing achieved a 26.6% summarisation result for the ROUGE-2 metric and 65.96% for ROUGE-L, which represents an improvement of 11.17% and 20.54%, respectively. The experimental results showed that good-quality summaries were achieved using the proposed approach. Â© The Author(s) 2022.","Automatic text summarisation is obtaining a subset that accurately represents the main text. A quality summary should contain the maximum amount of information while avoiding redundant information. Redundancy is a severe deficiency that causes unnecessary repetition of information within sentences and should not occur in summarisation studies. Although many optimisation-based text summarisation methods have been proposed in recent years, there exists a lack of research on the simultaneous optimisation of scope and redundancy. In this context, this study presents an approach in which maximum coverage and minimum redundancy, which form the two key features of a rich summary, are modelled as optimisation targets. In optimisation-based text summarisation studies, different conflicting objectives are generally weighted or formulated and transformed into single-objective problems. However, this transformation can directly affect the quality of the solution. In this study, the optimisation goals are met simultaneously without transformation or formulation. In addition, the multi-objective saplings growing-up algorithm (MO-SGuA) is implemented and modified for text summarisation. The presented approach, called Pareto optimal, achieves an optimal solution with simultaneous optimisation. Experimentation with the MO-SGuA method was tested using open-access (document understanding conference; DUC) data sets. Performance success of the MO-SGuA approach was calculated using the recall-oriented understudy for gisting evaluation (ROUGE) metrics and then compared with the competitive practices used in the literature. Testing achieved a 26.6% summarisation result for the ROUGE-2 metric and 65.96% for ROUGE-L, which represents an improvement of 11.17% and 20.54%, respectively. The experimental results showed that good-quality summaries were achieved using the proposed approach."
RETRACTED: Information skills and literacy in investigative journalism in the social media era,"The study aims to determine the preferred information skills and evaluate information literacy and skills in the social media era on the example of investigative journalism to surmount the majority of challenges it faces. The challenge-based survey conducted among 281 reporting journalists from various countries revealed that their information skills and literacy are average. The survey results show that half of the respondents (52%) recognise the importance of these characteristics, while 38% are sure that information literacy and skills are not necessary and 10% chose the variant âI am not sureâ. As the indicators show, the main aim for investigative journalistsâ writing is to influence the societyâs mind about some facts presented in the reports (40%). Only 25% of respondents write with the purpose to present reliable information that indicates the level of their information literacy. The latest strategies in the professional development of investigative journalists in modern social media era allow overcoming the major challenges, including those related to the influence and interests of third parties. The practical significance and prospects of further research are explained by the possibility of using the obtained statistical data to increase the level of information literacy and skills not only of investigative journalists, but also other specialists who work with information. Â© The Author(s) 2022.","The study aims to determine the preferred information skills and evaluate information literacy and skills in the social media era on the example of investigative journalism to surmount the majority of challenges it faces. The challenge-based survey conducted among 281 reporting journalists from various countries revealed that their information skills and literacy are average. The survey results show that half of the respondents (52%) recognise the importance of these characteristics, while 38% are sure that information literacy and skills are not necessary and 10% chose the variant I am not sure. As the indicators show, the main aim for investigative journalists writing is to influence the societys mind about some facts presented in the reports (40%). Only 25% of respondents write with the purpose to present reliable information that indicates the level of their information literacy. The latest strategies in the professional development of investigative journalists in modern social media era allow overcoming the major challenges, including those related to the influence and interests of third parties. The practical significance and prospects of further research are explained by the possibility of using the obtained statistical data to increase the level of information literacy and skills not only of investigative journalists, but also other specialists who work with information."
"Assessing journals through a three-dimensional framework based on article citation, author and institution influence","Journal assessment is of great significance to promote the development of academic platforms. Citation analysis is a recognised tool to assess the citation performance of journals. However, due to the shortcomings such as the inflated Journal Impact Factor and heterogeneity of the citations, additional dimensions are necessary to be considered to balance with the article citation. This article constructs a three-dimensional journal assessment framework to measure the comprehensive influence of a journal based on article citation, author and institution influence. The CRITIC-Entropy weighting method is employed to calculate the weighted average scores for the three dimensions, respectively. Then, a newly defined Pareto-dominated set-based sum of TOPSIS score (PDS-based STS) approach is developed to assess the comprehensive influence of journals. A sample of 76 journals in Economics field is selected to demonstrate the effectiveness and validity of the proposed assessment method. The Chartered Association of Business Schoolsâ Academic Journal Guide 2021 (CABS-AJG 2021) which is an expert-based journal rating is chosen as a baseline model. It has been found that the assessment method using PDS-based STS shows a more rational journal ranking than that based on the Journal Impact Factor if using the CABS-AJG 2021âs ratings as the benchmark model. Â© The Author(s) 2024.","Journal assessment is of great significance to promote the development of academic platforms. Citation analysis is a recognised tool to assess the citation performance of journals. However, due to the shortcomings such as the inflated Journal Impact Factor and heterogeneity of the citations, additional dimensions are necessary to be considered to balance with the article citation. This article constructs a three-dimensional journal assessment framework to measure the comprehensive influence of a journal based on article citation, author and institution influence. The CRITIC-Entropy weighting method is employed to calculate the weighted average scores for the three dimensions, respectively. Then, a newly defined Pareto-dominated set-based sum of TOPSIS score (PDS-based STS) approach is developed to assess the comprehensive influence of journals. A sample of 76 journals in Economics field is selected to demonstrate the effectiveness and validity of the proposed assessment method. The Chartered Association of Business Schools Academic Journal Guide 2021 (CABS-AJG 2021) which is an expert-based journal rating is chosen as a baseline model. It has been found that the assessment method using PDS-based STS shows a more rational journal ranking than that based on the Journal Impact Factor if using the CABS-AJG 2021s ratings as the benchmark model."
Chinaâs policy similarity evaluation using LDA model: An experimental analysis in Hebei province,"This article proposes a combination model, which is composed of latent Dirichlet allocation model, TF-IDF feature extraction algorithm and Euclidean distance measurement method, to identify and judge whether the similarities between multiple policy texts exist or not. With the help of actual data result, this will drive the relevant government agencies to figure out problems in a timely manner and provide a decision-making basis for them to formulate and optimise appropriate economic policies. To this end, this article analyses and studies the four types of economic texts that are classified as Insurance, Banking, Tax and Finance from the Central Government of Hebei province and Shijiazhuang city levels. Also, we consider Beijing, Shanghai and Guangdong. Experimental results show that (1) the combination model can quickly and effectively recognise and determine whether there are similarities between multiple economic policy texts; (2) similarities exist or not between the central, provincial and municipal level policy texts depending on the comparison of the distance values across them; (3) the smaller the distance value between economic policy texts of the same kind, the higher the similarity in them; and (4) the distance values between the six policy texts in Finance, Insurance, Bank and Tax categories are ranked from low to high. In terms of similarity, the Finance category is the highest, followed by Insurance and Bank, and the Tax category is the lowest. Â© The Author(s) 2022.","This article proposes a combination model, which is composed of latent Dirichlet allocation model, TF-IDF feature extraction algorithm and Euclidean distance measurement method, to identify and judge whether the similarities between multiple policy texts exist or not. With the help of actual data result, this will drive the relevant government agencies to figure out problems in a timely manner and provide a decision-making basis for them to formulate and optimise appropriate economic policies. To this end, this article analyses and studies the four types of economic texts that are classified as Insurance, Banking, Tax and Finance from the Central Government of Hebei province and Shijiazhuang city levels. Also, we consider Beijing, Shanghai and Guangdong. Experimental results show that the combination model can quickly and effectively recognise and determine whether there are similarities between multiple economic policy texts; similarities exist or not between the central, provincial and municipal level policy texts depending on the comparison of the distance values across them; the smaller the distance value between economic policy texts of the same kind, the higher the similarity in them; and the distance values between the six policy texts in Finance, Insurance, Bank and Tax categories are ranked from low to high. In terms of similarity, the Finance category is the highest, followed by Insurance and Bank, and the Tax category is the lowest."
Investigating community evolutions in TikTok dangerous and non-dangerous challenges,"In just few years, TikTok has become a major player in the social media environment, especially with regard to teenagers. One of the key factors of this success is the idea of challenges, that is, video competitions/emulations on a certain topic, which a user can launch and other ones can join. Most of the challenges are fun and harmless. However, there are also users who launch challenges that are dangerous, or at least suitable only for an adult audience (and TikTok is the most popular social network for teenagers). This article focuses primarily on this kind of challenge. In particular, it investigates an aspect not yet studied in the literature, which is the different characteristics and evolutionary dynamics of the communities of users participating in non-dangerous and dangerous challenges. Its final goal is the identification of evolutionary patterns that distinguish the communities of users participating in the two types of challenges. The knowledge of these patterns could be a first step in implementing an approach to the early detection of dangerous challenges in TikTok. Â© The Author(s) 2022.","In just few years, TikTok has become a major player in the social media environment, especially with regard to teenagers. One of the key factors of this success is the idea of challenges, that is, video competitions/emulations on a certain topic, which a user can launch and other ones can join. Most of the challenges are fun and harmless. However, there are also users who launch challenges that are dangerous, or at least suitable only for an adult audience (and TikTok is the most popular social network for teenagers). This article focuses primarily on this kind of challenge. In particular, it investigates an aspect not yet studied in the literature, which is the different characteristics and evolutionary dynamics of the communities of users participating in non-dangerous and dangerous challenges. Its final goal is the identification of evolutionary patterns that distinguish the communities of users participating in the two types of challenges. The knowledge of these patterns could be a first step in implementing an approach to the early detection of dangerous challenges in TikTok."
Comprehensive selective improvements in agri-informatics semantics,"The advent of information technology re-innovates all sectors of bio-sciences. Researchers use Semantic Web to improve web searching, mining and integration, which alleviates the time-consuming task of finding relevant and high-quality content. Semantics is improved through ontology engineering in any domain. Amended and developed ontologies will be uploaded to existing standardised and approved biomedical repositories. The establishment of a World Wide Web Consortium (W3C) approved and standardised ontology repository is the most ambitious goal. This work will solely focus on some selected agri-ontologies. The main objective is to promote outcome-based research and transformation styles of relevant expertise sharing. The intended goal is to win project funding to train and equip students with relevant skills and expertise. Need-based and market-oriented training and professional grooming are a tangible asset for students. The majority of traditional Web development freelancers are unaware of ontology or semantic web market demand. Freelancing is another option for expert Ontology developers. However, agriculture students are used to all the research vocabulary and terminologies in their area, but they do not know how to contribute their expertise to improve the efficiency of the Semantic Web in their domain. If the improvement in relevant ontology becomes a part of the Semantic Web, then it is termed âReal-time Web semantics enhancementâ. In other words, the target ontology becomes a part of the future Web of meaning. Â© The Author(s) 2022.","The advent of information technology re-innovates all sectors of bio-sciences. Researchers use Semantic Web to improve web searching, mining and integration, which alleviates the time-consuming task of finding relevant and high-quality content. Semantics is improved through ontology engineering in any domain. Amended and developed ontologies will be uploaded to existing standardised and approved biomedical repositories. The establishment of a World Wide Web Consortium (W3C) approved and standardised ontology repository is the most ambitious goal. This work will solely focus on some selected agri-ontologies. The main objective is to promote outcome-based research and transformation styles of relevant expertise sharing. The intended goal is to win project funding to train and equip students with relevant skills and expertise. Need-based and market-oriented training and professional grooming are a tangible asset for students. The majority of traditional Web development freelancers are unaware of ontology or semantic web market demand. Freelancing is another option for expert Ontology developers. However, agriculture students are used to all the research vocabulary and terminologies in their area, but they do not know how to contribute their expertise to improve the efficiency of the Semantic Web in their domain. If the improvement in relevant ontology becomes a part of the Semantic Web, then it is termed Real-time Web semantics enhancement. In other words, the target ontology becomes a part of the future Web of meaning."
Gender inequality in applying research project and funding,"Gender equality in scientific research has gradually attracted attention from many countries. However, the possible interaction effect of gender on research funding has not been fully disclosed. This study conducts an empirical study to examine the possible influences of gender on research funding and its interaction effect with applicantsâ social influences, including their external cumulative advantages (Matthew effect) and internal cognition (halo effect). In total, 1465 research projects from 2015 to 2021 are analysed to examine the proposed hypotheses. The results reveal that there is no gender inequality in the association between the Matthew effect and future research funding. However, the halo effect is easily affected by gender. For male scientists, higher institutional reputation and past research performance lead to higher future research funding. However, female scientists have no such benefits. According to the findings, this study suggests that female scientists should give priority to accumulating their own external resource advantages and participate in academic activities more frequently to activate womenâs participation in scientific research and academia. Â© The Author(s) 2022.","Gender equality in scientific research has gradually attracted attention from many countries. However, the possible interaction effect of gender on research funding has not been fully disclosed. This study conducts an empirical study to examine the possible influences of gender on research funding and its interaction effect with applicants social influences, including their external cumulative advantages (Matthew effect) and internal cognition (halo effect). In total, 1465 research projects from 2015 to 2021 are analysed to examine the proposed hypotheses. The results reveal that there is no gender inequality in the association between the Matthew effect and future research funding. However, the halo effect is easily affected by gender. For male scientists, higher institutional reputation and past research performance lead to higher future research funding. However, female scientists have no such benefits. According to the findings, this study suggests that female scientists should give priority to accumulating their own external resource advantages and participate in academic activities more frequently to activate womens participation in scientific research and academia."
Finding answers to COVID-19-specific questions: An information retrieval system based on latent keywords and adapted TF-IDF,"The scientific community has reacted to the COVID-19 outbreak by producing a high number of literary works that are helping us to understand a variety of topics related to the pandemic from different perspectives. Dealing with this large amount of information can be challenging, especially when researchers need to find answers to complex questions about specific topics. We present an Information Retrieval System that uses latent information to select relevant works related to specific concepts. By applying Latent Dirichlet Allocation (LDA) models to documents, we can identify key concepts related to a specific query and a corpus. Our method is iterative in that, from an initial input query defined by the user, the original query is expanded for each subsequent iteration. In addition, our method is able to work with a limited amount of information per article. We have tested the performance of our proposal using human validation and two evaluation strategies, achieving good results in both of them. Concerning the first strategy, we performed two surveys to determine the performance of our model. For all the categories that were studied, precision was always greater than 0.6, while accuracy was always greater than 0.8. The second strategy also showed good results, achieving a precision of 1.0 for one category and scoring over 0.7 points overall. Â© The Author(s) 2022.","The scientific community has reacted to the COVID-19 outbreak by producing a high number of literary works that are helping us to understand a variety of topics related to the pandemic from different perspectives. Dealing with this large amount of information can be challenging, especially when researchers need to find answers to complex questions about specific topics. We present an Information Retrieval System that uses latent information to select relevant works related to specific concepts. By applying Latent Dirichlet Allocation (LDA) models to documents, we can identify key concepts related to a specific query and a corpus. Our method is iterative in that, from an initial input query defined by the user, the original query is expanded for each subsequent iteration. In addition, our method is able to work with a limited amount of information per article. We have tested the performance of our proposal using human validation and two evaluation strategies, achieving good results in both of them. Concerning the first strategy, we performed two surveys to determine the performance of our model. For all the categories that were studied, precision was always greater than 0.6, while accuracy was always greater than 0.8. The second strategy also showed good results, achieving a precision of 1.0 for one category and scoring over 0.7 points overall."
A formal study of co-opetition in scholarly publishing,"In this article, we model and study a cooperative peer-review scenario in scholarly publishing. In this scenario, the peer-reviewed journals cooperate in the necessary investment for the peer-review system. However, the final decision on what to publish in each journal would rest with the journalâs editor, and the journals still compete in their quality standards for accepting papers. This simultaneously cooperative and competitive relationship between peer-reviewed journals is co-opetition in scholarly publishing. From the comparison between a benchmark scenario of competition between journals and a cooperative peer-review setting, we find that by sharing the cost of providing a common peer-review system, the peer-reviewed journals could offer a higher review quality in the manuscript evaluation process than they would otherwise be able to achieve individually. Furthermore, we find the conditions under which the competing academic journals using cooperative peer review could increase their expected quality levels, their standards for accepting articles, and their peer-review quality, which establishes the benefit from co-opetition between peer-reviewed journals. Nevertheless, a threshold cost-sharing factor exists above which the benefit from cooperative peer review disappears. Â© The Author(s) 2022.","In this article, we model and study a cooperative peer-review scenario in scholarly publishing. In this scenario, the peer-reviewed journals cooperate in the necessary investment for the peer-review system. However, the final decision on what to publish in each journal would rest with the journals editor, and the journals still compete in their quality standards for accepting papers. This simultaneously cooperative and competitive relationship between peer-reviewed journals is co-opetition in scholarly publishing. From the comparison between a benchmark scenario of competition between journals and a cooperative peer-review setting, we find that by sharing the cost of providing a common peer-review system, the peer-reviewed journals could offer a higher review quality in the manuscript evaluation process than they would otherwise be able to achieve individually. Furthermore, we find the conditions under which the competing academic journals using cooperative peer review could increase their expected quality levels, their standards for accepting articles, and their peer-review quality, which establishes the benefit from co-opetition between peer-reviewed journals. Nevertheless, a threshold cost-sharing factor exists above which the benefit from cooperative peer review disappears."
Knowledge-graph-based explainable AI: A systematic review,"In recent years, knowledge graphs (KGs) have been widely applied in various domains for different purposes. The semantic model of KGs can represent knowledge through a hierarchical structure based on classes of entities, their properties, and their relationships. The construction of large KGs can enable the integration of heterogeneous information sources and help Artificial Intelligence (AI) systems be more explainable and interpretable. This systematic review examines a selection of recent publications to understand how KGs are currently being used in eXplainable AI systems. To achieve this goal, we design a framework and divide the use of KGs into four categories: extracting features, extracting relationships, constructing KGs, and KG reasoning. We also identify where KGs are mostly used in eXplainable AI systems (pre-model, in-model, and post-model) according to the aforementioned categories. Based on our analysis, KGs have been mainly used in pre-model XAI for feature and relation extraction. They were also utilised for inference and reasoning in post-model XAI. We found several studies that leveraged KGs to explain the XAI models in the healthcare domain. Â© The Author(s) 2022.","In recent years, knowledge graphs (KGs) have been widely applied in various domains for different purposes. The semantic model of KGs can represent knowledge through a hierarchical structure based on classes of entities, their properties, and their relationships. The construction of large KGs can enable the integration of heterogeneous information sources and help Artificial Intelligence (AI) systems be more explainable and interpretable. This systematic review examines a selection of recent publications to understand how KGs are currently being used in eXplainable AI systems. To achieve this goal, we design a framework and divide the use of KGs into four categories: extracting features, extracting relationships, constructing KGs, and KG reasoning. We also identify where KGs are mostly used in eXplainable AI systems (pre-model, in-model, and post-model) according to the aforementioned categories. Based on our analysis, KGs have been mainly used in pre-model XAI for feature and relation extraction. They were also utilised for inference and reasoning in post-model XAI. We found several studies that leveraged KGs to explain the XAI models in the healthcare domain."
Multimodal sentiment analysis of intangible cultural heritage songs with strengthened audio features-guided attention,"Intangible cultural heritage (ICH) songs convey folk lives and stories from different communities and nations through touching melodies and lyrics, which are rich in sentiments. Currently, researches about the sentiment analysis of songs are mainly based on lyrics, audios and lyric-audio. Recent studies have shown that deep spectrum features extracted from the spectrogram, generated from the audio, perform well in several speech-based tasks. However, studies combining spectrum features in multimodal sentiment analysis of songs are in a lack. Hence, we propose to combine the audio, lyric and spectrogram to conduct multimodal sentiment analysis for ICH songs, in a tri-modal fusion way. In addition, the correlations and interactions between different modalities are not considered fully. Here, we propose a multimodal song sentiment analysis model (MSSAM), including a strengthened audio features-guided attention (SAFGA) mechanism, which can learn intra- and inter-modal information effectively. First, we obtain strengthened audio features through the fusion of acoustic and spectrum features. Then, the strengthened audio features are used to guide the attention weights distribution of words in the lyric with help of SAFGA, which can make the model focus on the important words with sentiments and related with the sentiment of strengthened audio features, capturing modal interactions and complementary information. We take two world-level ICH lists, Jingju (äº¬å§) and Kunqu (ææ²), as examples, and build sentiment analysis datasets. We compare the proposed model with other state-of-the-arts baselines in Jingju and Kunqu datasets. Experimental results demonstrate the superiority of our proposed model. Â© The Author(s) 2022.","Intangible cultural heritage (ICH) songs convey folk lives and stories from different communities and nations through touching melodies and lyrics, which are rich in sentiments. Currently, researches about the sentiment analysis of songs are mainly based on lyrics, audios and lyric-audio. Recent studies have shown that deep spectrum features extracted from the spectrogram, generated from the audio, perform well in several speech-based tasks. However, studies combining spectrum features in multimodal sentiment analysis of songs are in a lack. Hence, we propose to combine the audio, lyric and spectrogram to conduct multimodal sentiment analysis for ICH songs, in a tri-modal fusion way. In addition, the correlations and interactions between different modalities are not considered fully. Here, we propose a multimodal song sentiment analysis model (MSSAM), including a strengthened audio features-guided attention (SAFGA) mechanism, which can learn intra- and inter-modal information effectively. First, we obtain strengthened audio features through the fusion of acoustic and spectrum features. Then, the strengthened audio features are used to guide the attention weights distribution of words in the lyric with help of SAFGA, which can make the model focus on the important words with sentiments and related with the sentiment of strengthened audio features, capturing modal interactions and complementary information. We take two world-level ICH lists, Jingju () and Kunqu (), as examples, and build sentiment analysis datasets. We compare the proposed model with other state-of-the-arts baselines in Jingju and Kunqu datasets. Experimental results demonstrate the superiority of our proposed model."
A comparative analysis of Inventor Patent Classification Coupling between the first-inventor and all-inventor: Taking 3D printing as an example,"This article takes 43,753 patents collected in the Derwent database from 2011 to 2020 as data samples and studies the intellectual structure and evolution of the three-dimensional (3D) printing field in two time periods of 2011â2015 and 2016â2020. We compare the performance of the first-inventor patent classification codes coupling analysis and all-inventor patent classification codes coupling analysis in detecting the intellectual structure of the technical domain. We obtain the following findings: (1) Both methods, the first-inventor patent classification codes coupling analysis and the all-inventor patent classification codes coupling analysis, can show the intellectual structure of the 3D printing field. However, the first-inventor patent classification codes coupling analysis outperforms all-inventor patent classification codes coupling analysis in the detection of basic research; while all-inventor patent classification codes coupling analysis is more sensitive in emerging and interdisciplinary topics. (2) Three-dimensional printing has been evolving in the last decade. While the field has retained the research themes of the previous phase in the last 5 years, several new research themes have also emerged. The most prominent feature of this field is the development and integration of the previous phaseâs topics. (3) The research direction of the top 20 inventors in the average coupling frequency ranking is consistent. A small number of researchers continue to work on their previous research, while most inventors move to popular technical topics or combine multiple topics at the same time. Â© The Author(s) 2022.","This article takes 43,753 patents collected in the Derwent database from 2011 to 2020 as data samples and studies the intellectual structure and evolution of the three-dimensional (3D) printing field in two time periods of 20112015 and 20162020. We compare the performance of the first-inventor patent classification codes coupling analysis and all-inventor patent classification codes coupling analysis in detecting the intellectual structure of the technical domain. We obtain the following Both methods, the first-inventor patent classification codes coupling analysis and the all-inventor patent classification codes coupling analysis, can show the intellectual structure of the 3D printing field. However, the first-inventor patent classification codes coupling analysis outperforms all-inventor patent classification codes coupling analysis in the detection of basic research; while all-inventor patent classification codes coupling analysis is more sensitive in emerging and interdisciplinary topics. Three-dimensional printing has been evolving in the last decade. While the field has retained the research themes of the previous phase in the last 5 years, several new research themes have also emerged. The most prominent feature of this field is the development and integration of the previous phases topics. The research direction of the top 20 inventors in the average coupling frequency ranking is consistent. A small number of researchers continue to work on their previous research, while most inventors move to popular technical topics or combine multiple topics at the same time."
"Young informal carersâ information needs communicated online: Professional and personal growth, finance, health and relationships","Young informal carers (YICs) are non-professional young individuals providing care and support in various forms, usually to immediate family members, afflicted from a diverse range of both long- and short-term health conditions. Although there is significant knowledge about the information needs of adult carers in general, information needs and information seeking characteristics of the YICsâ community are understudied and are different. This study aims to identify the information needs of YICs communicated over the Internet and understanding their information seeking characteristics through a three-stage qualitative content analysis of posts written by YICs on two notable Internet forums. The analysis of 323 posts dated between March 2010 and April 2019 finds YICsâ needs are categorised by two types of online expression of needs, situational and information. Situational needs are illustrations of current difficult conditions and information needs are direct requests for information. Under situational and information needs, we identify four types of needs expressed: personal and professional growth, health (self and caree), finance and relationships. In addition, the findings indicate 94.36% posts in the sample as situational needs, which depict the uncertainty experienced by YICs under caring circumstances. The findings can assist government organisations and charities by improving the indexing of advice pages of their websites appropriate to the YICsâ search words, better availability of information and advertising, in addition to building quality mobile applications or digital support tools. Â© The Author(s) 2022.","Young informal carers (YICs) are non-professional young individuals providing care and support in various forms, usually to immediate family members, afflicted from a diverse range of both long- and short-term health conditions. Although there is significant knowledge about the information needs of adult carers in general, information needs and information seeking characteristics of the YICs community are understudied and are different. This study aims to identify the information needs of YICs communicated over the Internet and understanding their information seeking characteristics through a three-stage qualitative content analysis of posts written by YICs on two notable Internet forums. The analysis of 323 posts dated between March 2010 and April 2019 finds YICs needs are categorised by two types of online expression of needs, situational and information. Situational needs are illustrations of current difficult conditions and information needs are direct requests for information. Under situational and information needs, we identify four types of needs expressed: personal and professional growth, health (self and caree), finance and relationships. In addition, the findings indicate 94.36% posts in the sample as situational needs, which depict the uncertainty experienced by YICs under caring circumstances. The findings can assist government organisations and charities by improving the indexing of advice pages of their websites appropriate to the YICs search words, better availability of information and advertising, in addition to building quality mobile applications or digital support tools."
The cross-subsidy and buy-one-give-one models of compensated peer review: A comparative study for mission-driven journals,"A financial compensation model could incentivise peer reviewers to provide the optimal amount of effort by rewarding them for their quality reports. Therefore, in this article, we consider peer-reviewed journals with the mission to financially compensate reviewers for delivering quality reports and to ensure affordable access to peer review for every author. In a cross-subsidy scenario of compensated peer review, the mission-driven journals expect all authors to pay for either the standard or a premium peer review. However, using this strategy, the journals offer standard peer review to low-income authors at lower prices, compared with the premium price high-income authors pay for higher-quality peer review. In addition, we also present a buy-one-give-one model of compensated peer review for the mission-driven journals. In this alternative setting, when a high-income author pays for a premium peer review, the journals would donate a certain number of free peer reviews to the low-income authors who are unable to afford them. In this two-tiered system, scholars with access to more funding receive premium treatment, while low-income authors can still access standard peer review similar to what authors currently receive for free. In this article, we show a comparative study between the cross-subsidy and buy-one-give-one models of compensated peer review. We find that a buy-one-give-one scenario provides a higher total sum of financial and social gain than the cross-subsidy scenario either when the mission-driven journals are highly socially responsible or when the social gap between the high-income and the low-income authors is large enough. Â© The Author(s) 2022.","A financial compensation model could incentivise peer reviewers to provide the optimal amount of effort by rewarding them for their quality reports. Therefore, in this article, we consider peer-reviewed journals with the mission to financially compensate reviewers for delivering quality reports and to ensure affordable access to peer review for every author. In a cross-subsidy scenario of compensated peer review, the mission-driven journals expect all authors to pay for either the standard or a premium peer review. However, using this strategy, the journals offer standard peer review to low-income authors at lower prices, compared with the premium price high-income authors pay for higher-quality peer review. In addition, we also present a buy-one-give-one model of compensated peer review for the mission-driven journals. In this alternative setting, when a high-income author pays for a premium peer review, the journals would donate a certain number of free peer reviews to the low-income authors who are unable to afford them. In this two-tiered system, scholars with access to more funding receive premium treatment, while low-income authors can still access standard peer review similar to what authors currently receive for free. In this article, we show a comparative study between the cross-subsidy and buy-one-give-one models of compensated peer review. We find that a buy-one-give-one scenario provides a higher total sum of financial and social gain than the cross-subsidy scenario either when the mission-driven journals are highly socially responsible or when the social gap between the high-income and the low-income authors is large enough."
A novel developmental trajectory discovery approach by integrating main path analysis and intermediacy,"As a widely used technique for discovering developmental trajectory of a specific field of science and technology, main path analysis armed with global search strategy prefers longer citation paths rather than shorter ones. An obvious feature of longer main paths is that the theme of documents may not be so coherent, though longer paths may provide more details on the development of a field than shorter ones. Thereupon, a new measure, named as intermediacy, was proposed in the literature for recognising important scientific publications. However, the intermediacy is only applicable to the citation network with one single target node and one single source node. For purpose of loosening this limitation of the intermediacy and benefitting from main path analysis and intermediacy, this work raises an alternative approach for discovering developmental trajectory by combining node importance and edge importance via edge and node integrated modes. Extensive experimental results on the weak signals and education fields indicate that similar trajectories can be obtained through these two integrated modes, and richer implications can be encoded in our discovered trajectories than those from main path analysis and intermediacy. In addition, our framework is able to scale very well to a large citation network. Â© The Author(s) 2022.","As a widely used technique for discovering developmental trajectory of a specific field of science and technology, main path analysis armed with global search strategy prefers longer citation paths rather than shorter ones. An obvious feature of longer main paths is that the theme of documents may not be so coherent, though longer paths may provide more details on the development of a field than shorter ones. Thereupon, a new measure, named as intermediacy, was proposed in the literature for recognising important scientific publications. However, the intermediacy is only applicable to the citation network with one single target node and one single source node. For purpose of loosening this limitation of the intermediacy and benefitting from main path analysis and intermediacy, this work raises an alternative approach for discovering developmental trajectory by combining node importance and edge importance via edge and node integrated modes. Extensive experimental results on the weak signals and education fields indicate that similar trajectories can be obtained through these two integrated modes, and richer implications can be encoded in our discovered trajectories than those from main path analysis and intermediacy. In addition, our framework is able to scale very well to a large citation network."
Algorithm metadata vocabulary: A representational model and metadata vocabulary for describing and maintaining algorithms,"Metadata vocabularies are used in various domains of study. It provides an in-depth description of the resources. In this work, we develop algorithm metadata vocabulary (AMV), a vocabulary for capturing and storing the metadata about the algorithms (a procedure or a set of rules that is followed step-by-step to solve a problem, especially by a computer). The snag faced by the researchers in the current time is the failure of getting relevant results when searching for algorithms in any search engine. The designed vocabulary can be used by the algorithm repository developers, managers, and application developers. Besides, AMV is represented as a semantic model and produced OWL file, and it can be directly used by anyone interested to create and publish algorithm metadata as a knowledge graph, or to provide metadata service through the SPARQL endpoint. To design the vocabulary, we propose a well-defined methodology, which considers factual issues faced by the algorithm users and the practitioners. The evaluation shows promising results. Â© The Author(s) 2022.","Metadata vocabularies are used in various domains of study. It provides an in-depth description of the resources. In this work, we develop algorithm metadata vocabulary (AMV), a vocabulary for capturing and storing the metadata about the algorithms (a procedure or a set of rules that is followed step-by-step to solve a problem, especially by a computer). The snag faced by the researchers in the current time is the failure of getting relevant results when searching for algorithms in any search engine. The designed vocabulary can be used by the algorithm repository developers, managers, and application developers. Besides, AMV is represented as a semantic model and produced OWL file, and it can be directly used by anyone interested to create and publish algorithm metadata as a knowledge graph, or to provide metadata service through the SPARQL endpoint. To design the vocabulary, we propose a well-defined methodology, which considers factual issues faced by the algorithm users and the practitioners. The evaluation shows promising results."
Koreaâs national approach to Open Science: Present and possible future,"Open Science (OS) â an emerging global trend driven by advances in digital technologies and governmentâs commitment to greater transparency and value for money of publicly funded research â is at its early stages, even in countries with high R&D expenditures, such as South Korea. This study provides a comprehensive overview and analysis of Koreaâs national OS approach, with a focus on exploring the current OS regulatory and technological environments it operates under, and uncovering its SWOT â strengths, weaknesses, opportunities and threats. It concludes that internal weaknesses, such as insufficient political will to promote OS, dominate other SWOT characteristics of Koreaâs national OS approach. Thus, the highest priority should be given to strategies attempting to minimise both internal weaknesses and external threats, such as reinforcing domestic Open Access publishing ecosystem to mitigate Korean researchersâ dependency on large international commercial publishers. Â© The Author(s) 2022.","Open Science (OS) an emerging global trend driven by advances in digital technologies and governments commitment to greater transparency and value for money of publicly funded research is at its early stages, even in countries with high R&D expenditures, such as South Korea. This study provides a comprehensive overview and analysis of Koreas national OS approach, with a focus on exploring the current OS regulatory and technological environments it operates under, and uncovering its SWOT strengths, weaknesses, opportunities and threats. It concludes that internal weaknesses, such as insufficient political will to promote OS, dominate other SWOT characteristics of Koreas national OS approach. Thus, the highest priority should be given to strategies attempting to minimise both internal weaknesses and external threats, such as reinforcing domestic Open Access publishing ecosystem to mitigate Korean researchers dependency on large international commercial publishers."
Sentiment analysis of Indian Tweets about Covid-19 vaccines,"People are becoming more reliant on social media networks to express their opinions about various topics and obtain health information. The study is intended to explore and analyse the sentiments of Indian people related to Covid-19 vaccines as well as to visualise the top most frequently occurring terms individuals have used to communicate their ideas on Twitter about Covid-19 vaccines in India. The Tweet Archiver was used to retrieve the Tweets against âCovid19vaccineâ and âCoronavirusvaccineâ hashtags for the period of 2 months 18 days (4 January 2021â22 March 2021). After collecting data, the Orange software and VOSviewer were used for further analysis. The Tweets were posted across the country, with an immense contribution from Maharashtra (223, 15.58%), followed by Delhi (220, 15.37%) and Tamil Nadu (73, 5.10%). The majority (639, 44.65%) of the Tweets reflect positive sentiments, followed by neutral (521, 38.50%) and negative (241, 16.84%) sentiments, respectively. This signifies that most Twitter users have a favourable opinion towards Covid vaccines in India. Based on the relevance score of the words, the words âDelhi heartâ, âLung instituteâ, âGiftâ, âUnite2fightcoronaâ, and âCovid-19 Vaccineâ are the leading words appearing in Tweets. The study illustrates the sentiments of the Indian people towards âCovid-19 vaccinesâ, gains some insights into overall public communication about the topic and complements the existing literature. It can assist health policymakers and administrators in better understanding the polarity (positive, negative, and neutral) of Tweets about Covid-19 vaccines on Twitter to raise public awareness about health concerns and misinformation about the vaccine. Â© The Author(s) 2022.","People are becoming more reliant on social media networks to express their opinions about various topics and obtain health information. The study is intended to explore and analyse the sentiments of Indian people related to Covid-19 vaccines as well as to visualise the top most frequently occurring terms individuals have used to communicate their ideas on Twitter about Covid-19 vaccines in India. The Tweet Archiver was used to retrieve the Tweets against Covid19vaccine and Coronavirusvaccine hashtags for the period of 2 months 18 days (4 January 202122 March 2021). After collecting data, the Orange software and VOSviewer were used for further analysis. The Tweets were posted across the country, with an immense contribution from Maharashtra (223, 15.58%), followed by Delhi (220, 15.37%) and Tamil Nadu (73, 5.10%). The majority (639, 44.65%) of the Tweets reflect positive sentiments, followed by neutral (521, 38.50%) and negative (241, 16.84%) sentiments, respectively. This signifies that most Twitter users have a favourable opinion towards Covid vaccines in India. Based on the relevance score of the words, the words Delhi heart, Lung institute, Gift, Unite2fightcorona, and Covid-19 Vaccine are the leading words appearing in Tweets. The study illustrates the sentiments of the Indian people towards Covid-19 vaccines, gains some insights into overall public communication about the topic and complements the existing literature. It can assist health policymakers and administrators in better understanding the polarity (positive, negative, and neutral) of Tweets about Covid-19 vaccines on Twitter to raise public awareness about health concerns and misinformation about the vaccine."
Is this question going to be closed? Answering question closibility on Stack Exchange,"Community question answering sites (CQAs) are often flooded with questions that are never answered. To cope with the problem, experienced users of Stack Exchange are now allowed to mark newly posted questions as closed if they are of poor quality. Once closed, a question is no longer eligible to receive answers. However, identifying and closing subpar questions takes time. Therefore, the purpose of this article is to develop a supervised machine learning system that predicts question closibility, the possibility of a newly posted question to be eventually closed. Building on extant research on CQA question quality, the supervised machine learning system uses 17 features that were grouped into four categories, namely, asker features, community features, question content features and textual features. The performance of the developed system was tested on questions posted on Stack Exchange from 11 randomly chosen topics. The classification performance was generally promising and outperformed the baseline. Most of the measures of precision, recall, F1-score and area under the receiver operating characteristic curve (AUC) were above 0.90 irrespective of the topic of questions. By conceptualising question closibility, the article extends previous CQA research on question quality. Unlike previous studies, which were mostly limited to programming-related questions from Stack Overflow, this one empirically tests question closibility on questions from 11 randomly selected topics. The set of features used for classification offers a framework of question closibility that is not only more comprehensive but also more parsimonious compared with prior works. Â© The Author(s) 2022.","Community question answering sites (CQAs) are often flooded with questions that are never answered. To cope with the problem, experienced users of Stack Exchange are now allowed to mark newly posted questions as closed if they are of poor quality. Once closed, a question is no longer eligible to receive answers. However, identifying and closing subpar questions takes time. Therefore, the purpose of this article is to develop a supervised machine learning system that predicts question closibility, the possibility of a newly posted question to be eventually closed. Building on extant research on CQA question quality, the supervised machine learning system uses 17 features that were grouped into four categories, namely, asker features, community features, question content features and textual features. The performance of the developed system was tested on questions posted on Stack Exchange from 11 randomly chosen topics. The classification performance was generally promising and outperformed the baseline. Most of the measures of precision, recall, F1-score and area under the receiver operating characteristic curve (AUC) were above 0.90 irrespective of the topic of questions. By conceptualising question closibility, the article extends previous CQA research on question quality. Unlike previous studies, which were mostly limited to programming-related questions from Stack Overflow, this one empirically tests question closibility on questions from 11 randomly selected topics. The set of features used for classification offers a framework of question closibility that is not only more comprehensive but also more parsimonious compared with prior works."
Agency and liminality during the COVID-19 pandemic: Why information literacy cannot fix vaccine hesitancy,"This article employs a sociological and dialogical information perspective to identify what shape information literacy practice takes for people who are hesitant about the COVID-19 vaccine. An information perspective places information and peopleâs relations with information at the centre of the inquiry. The study carried out 14 semi-structured interviews with UK adults who had not yet received or taken up their invitation to have the COVID-19 vaccine. Outcomes of this study suggest that information literacy practices related to vaccine hesitancy emerged through the liminal space and in relation to agentic performance, which was catalysed through engagement with experiential, corporeal and social information. This study has implications for the teaching of information literacy, in particular, the idea that being informed is an affirmative action that will automatically empower learners to make appropriate choices. Â© The Author(s) 2022.","This article employs a sociological and dialogical information perspective to identify what shape information literacy practice takes for people who are hesitant about the COVID-19 vaccine. An information perspective places information and peoples relations with information at the centre of the inquiry. The study carried out 14 semi-structured interviews with UK adults who had not yet received or taken up their invitation to have the COVID-19 vaccine. Outcomes of this study suggest that information literacy practices related to vaccine hesitancy emerged through the liminal space and in relation to agentic performance, which was catalysed through engagement with experiential, corporeal and social information. This study has implications for the teaching of information literacy, in particular, the idea that being informed is an affirmative action that will automatically empower learners to make appropriate choices."
Improved PageRank and New Indices for Academic Impact Evaluation Using AI Papers as Case Studies,"Evaluating academic papers and groups is important in scholar evaluation and literature retrieval. However, current evaluation indices, which pay excessive attention to the citation number rather than the citation importance and unidirectionality, are relatively simple. This study proposes new evaluation indices for papers and groups. First, an improved PageRank (PR) algorithm introducing citation importance is proposed to obtain a new citation-based paper index (CPI) via a pre-ranking and fine-tuning strategy. Second, to evaluate the paperâs influence inside and outside its research field, the focus citation-based paper index (FCPI) and diversity citation-based paper index (DCPI) are proposed based on topic similarity and diversity, respectively. Third, aside from the statistical indices for academic papers, we propose a foreign academic degree of dependence (FAD) to characterise the dependence between two academic groups. Finally, artificial intelligence (AI) papers from 2005 to 2019 are utilised for a case study. Â© The Author(s) 2022.","Evaluating academic papers and groups is important in scholar evaluation and literature retrieval. However, current evaluation indices, which pay excessive attention to the citation number rather than the citation importance and unidirectionality, are relatively simple. This study proposes new evaluation indices for papers and groups. First, an improved PageRank (PR) algorithm introducing citation importance is proposed to obtain a new citation-based paper index (CPI) via a pre-ranking and fine-tuning strategy. Second, to evaluate the papers influence inside and outside its research field, the focus citation-based paper index (FCPI) and diversity citation-based paper index (DCPI) are proposed based on topic similarity and diversity, respectively. Third, aside from the statistical indices for academic papers, we propose a foreign academic degree of dependence (FAD) to characterise the dependence between two academic groups. Finally, artificial intelligence (AI) papers from 2005 to 2019 are utilised for a case study."
"Tweets on a horror movie: An investigation into relationships between sentiment strength, cognitive language and tweet virality","This article studies how sentiment strength and cognitive language may influence the levels of tweet virality. A total of 11,381 tweets about a horror movie (âMother!â) were collected. Based on the definitions of two independent variables: sentiment strength and cognitive language use, and the dependent variable: tweet virality, the data descriptive statistics and analysis of variance (ANOVA) analysis were applied to reveal the relationships between tweet virality and sentiment and cognitive factors. The results indicate that a high tweet virality is associated with either a lower level of sentiment strength or/and a higher level of cognitive language use by a statistically significant margin. This finding is more evident for negative tweets. The study findings help improve the understanding about sentimental and cognitive factors impacting tweet virality and guide the movie industry to improve marketing movie content to achieve high virality on social media. The conclusions can also be applied to other industries, government agencies, organisations and individuals who intend to quickly disseminate specific information on social media platforms. Â© The Author(s) 2022.","This article studies how sentiment strength and cognitive language may influence the levels of tweet virality. A total of 11,381 tweets about a horror movie (Mother!) were collected. Based on the definitions of two independent variables: sentiment strength and cognitive language use, and the dependent variable: tweet virality, the data descriptive statistics and analysis of variance (ANOVA) analysis were applied to reveal the relationships between tweet virality and sentiment and cognitive factors. The results indicate that a high tweet virality is associated with either a lower level of sentiment strength or/and a higher level of cognitive language use by a statistically significant margin. This finding is more evident for negative tweets. The study findings help improve the understanding about sentimental and cognitive factors impacting tweet virality and guide the movie industry to improve marketing movie content to achieve high virality on social media. The conclusions can also be applied to other industries, government agencies, organisations and individuals who intend to quickly disseminate specific information on social media platforms."
DRMM: A novel data mining-based emotion transfer detecting method for emotion prediction of social media,"With the progress of the Internet and information technology, emotion analysis has been applied to analyse the emotional orientation and evolution trend of online public opinion of online tweets. At present, most of the existing methods use econometric model and machine learning algorithm to predict the trend of online public opinion. Although these methods have achieved good prediction results, they do not take into account the influence of internal factors on network public opinion prediction, such as mutual migration among emotion classes. The emotion may change dynamically because different events trigger it in the evolution process. In this view, this article proposes a novel method, called Deviation Rule Markov Model (DRMM), to predict the emotional change trend of Internet users in online public opinion by analysing the correlation between Internet usersâ emotional categories. Structurally, the proposed DRMM involves various processes such as pre-processing, emotion classification, data mining and transfer prediction. For the processing of network comment data, the proposed model initially undergoes pre-processing to delete unnecessary data. Then, the extended fuzzy emotion ontology is used to annotate the emotion class of the comment data. Besides, an extended association rule mining algorithm is used in the emotion association analysis process to obtain the transfer probability between emotion classes. Moreover, Markov chain is used to construct an emotional state transition matrix to predict the transition probability of positive or negative emotions. According to the predicted single emotion transfer probability results, the analytic hierarchy process is used to assign values to different emotion classes, and finally, the transfer probability of the overall emotion in a certain period is obtained. Compared with the actual case, the mean absolute error (MAE) and root mean square error (RMSE) of the proposed model are 2.7119 and 3.7254, respectively, which has good prediction performance. Â© The Author(s) 2022.","With the progress of the Internet and information technology, emotion analysis has been applied to analyse the emotional orientation and evolution trend of online public opinion of online tweets. At present, most of the existing methods use econometric model and machine learning algorithm to predict the trend of online public opinion. Although these methods have achieved good prediction results, they do not take into account the influence of internal factors on network public opinion prediction, such as mutual migration among emotion classes. The emotion may change dynamically because different events trigger it in the evolution process. In this view, this article proposes a novel method, called Deviation Rule Markov Model (DRMM), to predict the emotional change trend of Internet users in online public opinion by analysing the correlation between Internet users emotional categories. Structurally, the proposed DRMM involves various processes such as pre-processing, emotion classification, data mining and transfer prediction. For the processing of network comment data, the proposed model initially undergoes pre-processing to delete unnecessary data. Then, the extended fuzzy emotion ontology is used to annotate the emotion class of the comment data. Besides, an extended association rule mining algorithm is used in the emotion association analysis process to obtain the transfer probability between emotion classes. Moreover, Markov chain is used to construct an emotional state transition matrix to predict the transition probability of positive or negative emotions. According to the predicted single emotion transfer probability results, the analytic hierarchy process is used to assign values to different emotion classes, and finally, the transfer probability of the overall emotion in a certain period is obtained. Compared with the actual case, the mean absolute error (MAE) and root mean square error (RMSE) of the proposed model are 2.7119 and 3.7254, respectively, which has good prediction performance."
JSON document clustering based on schema embeddings,"The growing popularity of JSON as the data storage and interchange format increases the availability of massive multi-structured data collections. Clustering JSON documents has become a significant issue in organising large data collections. Existing research uses various structural similarity measures to perform clustering. However, differently annotated JSON structures may also encode semantic relatedness, necessitating the use of both syntactic and semantic properties of heterogeneous JSON schemas. Using the SchemaEmbed model, this paper proposes an embedding-based clustering approach for grouping contextually similar JSON documents. The SchemaEmbed model is designed using the pre-trained Word2Vec model and a deep autoencoder that considers both syntactic and semantic information of JSON schemas for clustering the documents. The Word2Vec model learns the attribute embeddings, and a deep autoencoder is designed to generate context-aware schema embeddings. Finally, the context-based similar JSON documents are grouped using a clustering algorithm. The effectiveness of the proposed work is evaluated using both real and synthetic datasets. The results and findings show that the proposed approach improves clustering quality significantly, with a high NMI score of 75%. In addition, we demonstrate that clustering results obtained by contextual similarity are superior to those obtained by traditional semantic similarity models. Â© The Author(s) 2022.","The growing popularity of JSON as the data storage and interchange format increases the availability of massive multi-structured data collections. Clustering JSON documents has become a significant issue in organising large data collections. Existing research uses various structural similarity measures to perform clustering. However, differently annotated JSON structures may also encode semantic relatedness, necessitating the use of both syntactic and semantic properties of heterogeneous JSON schemas. Using the SchemaEmbed model, this paper proposes an embedding-based clustering approach for grouping contextually similar JSON documents. The SchemaEmbed model is designed using the pre-trained Word2Vec model and a deep autoencoder that considers both syntactic and semantic information of JSON schemas for clustering the documents. The Word2Vec model learns the attribute embeddings, and a deep autoencoder is designed to generate context-aware schema embeddings. Finally, the context-based similar JSON documents are grouped using a clustering algorithm. The effectiveness of the proposed work is evaluated using both real and synthetic datasets. The results and findings show that the proposed approach improves clustering quality significantly, with a high NMI score of 75%. In addition, we demonstrate that clustering results obtained by contextual similarity are superior to those obtained by traditional semantic similarity models."
The information behaviour of individuals changing health insurance plans and an exploration of health insurance priorities,"This study investigated why individuals change their health insurance plans, factors that influence their health insurance plan choices and information sources used to compare and select their desired plans. Semi-structured interviews and card sorting exercises with state university employees in the Midwest region were performed. Saving money was the main reason for switching health insurance plans. Health insurance plan coverage and cost, past experiences with the plans and coverage, health saving accounts, personal and/or family health status and forecasting health care demands for the upcoming year determined their choice of health insurance plan. Human Resource departments, printed materials, health insurance companies, online tools for comparing plans and interpersonal communications were the primary information sources for comparing and selecting health insurance plans. The study suggests that although individuals evaluate various factors and refer to multiple information sources when choosing a plan, they still experience uncertainty regarding selected plans for the coming year. Â© The Author(s) 2022.","This study investigated why individuals change their health insurance plans, factors that influence their health insurance plan choices and information sources used to compare and select their desired plans. Semi-structured interviews and card sorting exercises with state university employees in the Midwest region were performed. Saving money was the main reason for switching health insurance plans. Health insurance plan coverage and cost, past experiences with the plans and coverage, health saving accounts, personal and/or family health status and forecasting health care demands for the upcoming year determined their choice of health insurance plan. Human Resource departments, printed materials, health insurance companies, online tools for comparing plans and interpersonal communications were the primary information sources for comparing and selecting health insurance plans. The study suggests that although individuals evaluate various factors and refer to multiple information sources when choosing a plan, they still experience uncertainty regarding selected plans for the coming year."
Information overload and misinformation sharing behaviour of social media users: Testing the moderating role of cognitive ability,"Sharing of misinformation on social media platforms is a global concern, with research offering little insight into the motives behind such sharing. Drawing from the cognitive load theory and literature on cognitive ability, we developed and tested a research model hypothesising why people share misinformation. We also tested the moderating role of cognitive ability. We obtained data from 385 social media users in Nigeria using a chain referral technique with an online questionnaire as the instrument for data collection. Our findings suggest that information overload and social media fatigue are strong predictors of misinformation sharing. Information stress also contributed to misinformation sharing behaviour. Furthermore, cognitive ability moderated and weakened the effect information strain and information overload have on misinformation sharing in such a way that this effect is more pronounced among those with low cognitive ability. This indicates that those with low cognitive ability have a higher tendency to share misinformation. However, cognitive ability had no effect on the effect social media fatigue has on misinformation sharing behaviour. The study concluded with some theoretical and practical implications. Â© The Author(s) 2022.","Sharing of misinformation on social media platforms is a global concern, with research offering little insight into the motives behind such sharing. Drawing from the cognitive load theory and literature on cognitive ability, we developed and tested a research model hypothesising why people share misinformation. We also tested the moderating role of cognitive ability. We obtained data from 385 social media users in Nigeria using a chain referral technique with an online questionnaire as the instrument for data collection. Our findings suggest that information overload and social media fatigue are strong predictors of misinformation sharing. Information stress also contributed to misinformation sharing behaviour. Furthermore, cognitive ability moderated and weakened the effect information strain and information overload have on misinformation sharing in such a way that this effect is more pronounced among those with low cognitive ability. This indicates that those with low cognitive ability have a higher tendency to share misinformation. However, cognitive ability had no effect on the effect social media fatigue has on misinformation sharing behaviour. The study concluded with some theoretical and practical implications."
Node classifications with DjCaNE: Disjoint content and network embedding,"Machine learning approaches have become a crucial tool in graph analysis. Despite the accurate results of the existing approaches, most of them are not scalable enough to be used in real-world problems. Networks provide two different kinds of information, nodes contents and nodes relations (network structure). Training deep graph neural networks (GNN) over large-scale graphs is challenging due to the limitation of the message passing framework. Graph Convolutional Networks (GCN) work on all node neighbours at once. Furthermore, it is usual to transform node features with a deep neural network before the GC operation. Therefore, the deep transform operation may apply up to hundreds of times for each target node which is heavy computation and hard to batch. This paper presents an abstract framework with two embedding components, the first component embeds node relations, and the second one embeds node contents. The model makes predictions by aggregating these embeddings through a combination component. The presented approach limits the deep transform only to the target node and uses random walk-based embedding instead of the GC operator to reduce the cost. The main goal of the proposed approach is to provide a light framework for the task. To this aim, node relations are embedded based on node neighbourhood structure by a biased variant of the DeepWalk model, called GuidedWalk, and an autoencoder embeds node contents. The experimental results on three well-known datasets show the superiority of the proposed model compared to the state-of-the-art GraphSAGE and TADW models with less computational complexity. On the Citeseer, Cora, and PubMed datasets, the model has achieved 3.23%, 0.88%, and 7.63% improvement in Macro-F1 and 3.25%, 0.7%, and 6.34% improvement in Micro-F1, respectively. Although GNNs are state-of-the-art models, considering node content is their main advantage. This paper shows that even a simple integration of node content to available random walk-based methods improves their performance up to GCNs without increasing the complexity. Â© The Author(s) 2022.","Machine learning approaches have become a crucial tool in graph analysis. Despite the accurate results of the existing approaches, most of them are not scalable enough to be used in real-world problems. Networks provide two different kinds of information, nodes contents and nodes relations (network structure). Training deep graph neural networks (GNN) over large-scale graphs is challenging due to the limitation of the message passing framework. Graph Convolutional Networks (GCN) work on all node neighbours at once. Furthermore, it is usual to transform node features with a deep neural network before the GC operation. Therefore, the deep transform operation may apply up to hundreds of times for each target node which is heavy computation and hard to batch. This paper presents an abstract framework with two embedding components, the first component embeds node relations, and the second one embeds node contents. The model makes predictions by aggregating these embeddings through a combination component. The presented approach limits the deep transform only to the target node and uses random walk-based embedding instead of the GC operator to reduce the cost. The main goal of the proposed approach is to provide a light framework for the task. To this aim, node relations are embedded based on node neighbourhood structure by a biased variant of the DeepWalk model, called GuidedWalk, and an autoencoder embeds node contents. The experimental results on three well-known datasets show the superiority of the proposed model compared to the state-of-the-art GraphSAGE and TADW models with less computational complexity. On the Citeseer, Cora, and PubMed datasets, the model has achieved 3.23%, 0.88%, and 7.63% improvement in Macro-F1 and 3.25%, 0.7%, and 6.34% improvement in Micro-F1, respectively. Although GNNs are state-of-the-art models, considering node content is their main advantage. This paper shows that even a simple integration of node content to available random walk-based methods improves their performance up to GCNs without increasing the complexity."
Peopleâs perceptions on social media archiving by the National Library of Japan,"Social media content can be considered an unprecedented historical resource that reflects present-day ordinary life. However, although private data are publicly available on social media, the preserving of such personal content by a third party entails legal and ethical concerns. We report on a nationwide questionnaire survey conducted to obtain the responses of people to hypothetical scenarios of social media archiving by the National Diet Library in Japan. Within our survey sample, 35% of respondents (n = 1126) disagreed with scenarios involving the preserving of blogs and public tweets. Moreover, we found that the agreement rate for the archiving of government websites already collected under the current legislation was 44%. Ordered logistic analysis clarified that privacy-sensitive respondents tend to resist archival scenarios, and content analysis showed that the disagreement reasons involve concerns over information privacy. Our findings suggest that informed consent and data anonymisation could be effective means to mitigate such concerns. Â© The Author(s) 2022.","Social media content can be considered an unprecedented historical resource that reflects present-day ordinary life. However, although private data are publicly available on social media, the preserving of such personal content by a third party entails legal and ethical concerns. We report on a nationwide questionnaire survey conducted to obtain the responses of people to hypothetical scenarios of social media archiving by the National Diet Library in Japan. Within our survey sample, 35% of respondents (n = 1126) disagreed with scenarios involving the preserving of blogs and public tweets. Moreover, we found that the agreement rate for the archiving of government websites already collected under the current legislation was 44%. Ordered logistic analysis clarified that privacy-sensitive respondents tend to resist archival scenarios, and content analysis showed that the disagreement reasons involve concerns over information privacy. Our findings suggest that informed consent and data anonymisation could be effective means to mitigate such concerns."
Studying the cognitive relatedness between topics in the global science landscape: The case of Big Data research,"Taking Big Data research as a case study, this article intends to investigate the cognitive relatedness of research topics across the global science landscape to a focal topic. Several levels of cognitive relatedness are established depending on the citation distance between the citing publications and a core set of publications. The concept of citation generation is adopted for identifying and classifying other publications with different levels of relatedness to the core set. The micro publication-level classification system of Centre for Science and Technology Studies (CWTS) is applied for determining clusters of publication sets at the topic level. The overall cognitive relatedness of micro clusters to Big Data core publications are measured based on the mean citation generation of all the publications in corresponding clusters. In addition to the given clusters, this study also explores the âtopicsâ relatedness from a semantic point of view, by extracting high-frequency title terms of publications in each generation. Results show that data analysis methods and technologies are the topics with the strongest cognitive relatedness to Big Data research, while topics on physics and astronomy studies present the weakest relatedness. This approach allows assessment of relatedness between research topics by considering the citations distribution across multiple citation generations, and can provide useful insights to study and characterise topics with fuzzy boundaries or are difficult to delineate, thus representing a novel toolset relevant in the context of studying interdisciplinary research. Â© The Author(s) 2022.","Taking Big Data research as a case study, this article intends to investigate the cognitive relatedness of research topics across the global science landscape to a focal topic. Several levels of cognitive relatedness are established depending on the citation distance between the citing publications and a core set of publications. The concept of citation generation is adopted for identifying and classifying other publications with different levels of relatedness to the core set. The micro publication-level classification system of Centre for Science and Technology Studies (CWTS) is applied for determining clusters of publication sets at the topic level. The overall cognitive relatedness of micro clusters to Big Data core publications are measured based on the mean citation generation of all the publications in corresponding clusters. In addition to the given clusters, this study also explores the topics relatedness from a semantic point of view, by extracting high-frequency title terms of publications in each generation. Results show that data analysis methods and technologies are the topics with the strongest cognitive relatedness to Big Data research, while topics on physics and astronomy studies present the weakest relatedness. This approach allows assessment of relatedness between research topics by considering the citations distribution across multiple citation generations, and can provide useful insights to study and characterise topics with fuzzy boundaries or are difficult to delineate, thus representing a novel toolset relevant in the context of studying interdisciplinary research."
Text-based experiment retrieval in genomic databases,"With the growing number of genomic data in public repositories, efficient search methodologies have become a basic need to reach the relevant genomic data. However, this need cannot be fulfilled with the current repositories because they offer a limited search option which is a lexical matching of textual descriptions or metadata of the experiments. This technique is insufficient to get the required information needed to detect similarities between experiments within a large data collection. Due to the limitation of the existing repositories, in this study, we develop a text-based experiment retrieval framework by using both lexical and semantic similarity approaches to find similarities between experiments, and their retrieval performance was compared. This study is the first attempt to use text-driven semantic analysis approaches for developing a retrieval framework for experiments. An empirical study was conducted on a large textual description of Arabidopsis microarray experiments from the Gene Expression Omnibus database. In the proposed model, Jaccard similarity was used as a lexical similarity approach; Latent Semantic Analysis, Probabilistic Latent Semantic Analysis and Latent Dirichlet allocation were used as semantic similarity approaches to detect similarities between the textual descriptions of the experiments. According to the experimental results, relevant experiments can be retrieved successfully by text-driven semantic similarity approaches compared with the lexical similarity approach. Â© The Author(s) 2022.","With the growing number of genomic data in public repositories, efficient search methodologies have become a basic need to reach the relevant genomic data. However, this need cannot be fulfilled with the current repositories because they offer a limited search option which is a lexical matching of textual descriptions or metadata of the experiments. This technique is insufficient to get the required information needed to detect similarities between experiments within a large data collection. Due to the limitation of the existing repositories, in this study, we develop a text-based experiment retrieval framework by using both lexical and semantic similarity approaches to find similarities between experiments, and their retrieval performance was compared. This study is the first attempt to use text-driven semantic analysis approaches for developing a retrieval framework for experiments. An empirical study was conducted on a large textual description of Arabidopsis microarray experiments from the Gene Expression Omnibus database. In the proposed model, Jaccard similarity was used as a lexical similarity approach; Latent Semantic Analysis, Probabilistic Latent Semantic Analysis and Latent Dirichlet allocation were used as semantic similarity approaches to detect similarities between the textual descriptions of the experiments. According to the experimental results, relevant experiments can be retrieved successfully by text-driven semantic similarity approaches compared with the lexical similarity approach."
Listening to the unheard and unseen: Information literacy perspectives of the rural bi/multilinguals in Nigeria,"This article aims to investigate information literacy perspectives of bi/multilinguals in Nigeria. The study aims to identify bilingual and multilingual definitions of information literacy, determine factors that influence their information literacy needs and at what point bi/multilinguals need it. This implies that individuals who can speak up to two or three native languages would be used as the population of the study. Despite being a multicultural country, some languages are more recognised in Nigeria and as such, those groups of people are recognised. This research aims to listen to the unheard and unseen individuals who are the rural indigenes while considering a major influencing factor in the study which is language. A focus group interview was used and a transcript-based analysis was used for the study. The results indicate that information literacy is being able to communicate with their environment but more so involves giving back to the society through languages. These findings can provide a solid ground on which inclusion of rural indigenes can be formed. It provides a platform on which library advocacy for the inclusion of the rural community indigenes to enable them to express their form of information literacy and produce intellectual works in indigenous languages can be formed. Â© The Author(s) 2022.","This article aims to investigate information literacy perspectives of bi/multilinguals in Nigeria. The study aims to identify bilingual and multilingual definitions of information literacy, determine factors that influence their information literacy needs and at what point bi/multilinguals need it. This implies that individuals who can speak up to two or three native languages would be used as the population of the study. Despite being a multicultural country, some languages are more recognised in Nigeria and as such, those groups of people are recognised. This research aims to listen to the unheard and unseen individuals who are the rural indigenes while considering a major influencing factor in the study which is language. A focus group interview was used and a transcript-based analysis was used for the study. The results indicate that information literacy is being able to communicate with their environment but more so involves giving back to the society through languages. These findings can provide a solid ground on which inclusion of rural indigenes can be formed. It provides a platform on which library advocacy for the inclusion of the rural community indigenes to enable them to express their form of information literacy and produce intellectual works in indigenous languages can be formed."
Hermos: An annotated image dataset for visual detection of grape leaf diseases,"Powdery mildew, dead arm and vineyard downy mildew diseases are frequently seen in the vineyards in the Gediz River Basin, West Anatolia of Turkey. These diseases can be detected early using artificial intelligence (AI)âbased systems that can contribute to crop yields and also reduce the labour of the farmer and the amount of pesticides used. This article presents a dataset â namely, Hermos â for use in such AI-based systems. Hermos contains four classes of grape leaf images: leaves with powdery mildew, leaves with dead arm, leaves with downy mildew and healthy leaves. We have currently 492 images and 13,913 labels in the dataset. We have published Hermos in the Linked Open Data (LOD) cloud in order to make it easier for consumers to access, process and manipulate the data. Â© The Author(s) 2022.","Powdery mildew, dead arm and vineyard downy mildew diseases are frequently seen in the vineyards in the Gediz River Basin, West Anatolia of Turkey. These diseases can be detected early using artificial intelligence (AI)based systems that can contribute to crop yields and also reduce the labour of the farmer and the amount of pesticides used. This article presents a dataset namely, Hermos for use in such AI-based systems. Hermos contains four classes of grape leaf images: leaves with powdery mildew, leaves with dead arm, leaves with downy mildew and healthy leaves. We have currently 492 images and 13,913 labels in the dataset. We have published Hermos in the Linked Open Data (LOD) cloud in order to make it easier for consumers to access, process and manipulate the data."
Considerations in releasing public data: The case of local governments in Korea,"Local governments play a very important role in providing and disseminating public data; with the assistance of the national government, they also strive to promote effective policies for accessing this data. However, local governments face challenges in independently promoting public data policies owing to budget, manpower and technical constraints. This study analyses public data provided by local governments in South Korea from the perspective of management and use, apart from suggesting considerations for the public data policies of local governments. Public data provided by local governments were also collected, and both the data management method and use of such data were evaluated. Data management measures the currentness in complying with the update policy for each dataset, and the data utilisation measures the relationship between view and download as usefulness. A clustering analysis was conducted to analyse the common characteristics of individual local governments. According to the research results, most local governments do not systematically manage their datasets, and the use of the data provided is extremely low. It is therefore necessary to establish an effective data policy that considers the characteristics of local governments. Â© The Author(s) 2022.","Local governments play a very important role in providing and disseminating public data; with the assistance of the national government, they also strive to promote effective policies for accessing this data. However, local governments face challenges in independently promoting public data policies owing to budget, manpower and technical constraints. This study analyses public data provided by local governments in South Korea from the perspective of management and use, apart from suggesting considerations for the public data policies of local governments. Public data provided by local governments were also collected, and both the data management method and use of such data were evaluated. Data management measures the currentness in complying with the update policy for each dataset, and the data utilisation measures the relationship between view and download as usefulness. A clustering analysis was conducted to analyse the common characteristics of individual local governments. According to the research results, most local governments do not systematically manage their datasets, and the use of the data provided is extremely low. It is therefore necessary to establish an effective data policy that considers the characteristics of local governments."
Extending ontology pitfalls for better ontology evaluation,This article presents a framework that detects potential ontology building errors to improve the ontology quality. These potential errors are called ontology pitfalls in the literature. This work extends the existing ontology pitfall set in the literature and suggests new solutions for ontology repair. We have also developed a Java implementation for detection of the proposed pitfalls. These pitfalls were evaluated with well-known ontologies using this implementation. Â© The Author(s) 2022.,This article presents a framework that detects potential ontology building errors to improve the ontology quality. These potential errors are called ontology pitfalls in the literature. This work extends the existing ontology pitfall set in the literature and suggests new solutions for ontology repair. We have also developed a Java implementation for detection of the proposed pitfalls. These pitfalls were evaluated with well-known ontologies using this implementation.
Information behaviour in high risk decision making: Study of international postgraduates,"This article explores the role of information in high risk consumer decision making. Forty-two qualitative interviews were undertaken with international non-EU postgraduates when making the high risk decision to study in a UK Business School. Prospective international postgraduates moved iteratively through the stages in Kuhlthauâs Information Search Process model and learnt from the search process they had undertaken in a continuous cyclical manner. Word-of-mouth recommendations were the most influential sources of information gathered, and online sources were perceived to be credible regardless of their origins. The perception of risk impacted the rigour of the information search process. An iterative decision making cycle model is proposed with Kuhlthauâs model and word of mouth information at its core, which reflects the connectedness of individuals in this digital era. This study provides new insights by combining both marketing and LIS models and extends Kuhlthauâs research into a new context. Â© The Author(s) 2022.","This article explores the role of information in high risk consumer decision making. Forty-two qualitative interviews were undertaken with international non-EU postgraduates when making the high risk decision to study in a UK Business School. Prospective international postgraduates moved iteratively through the stages in Kuhlthaus Information Search Process model and learnt from the search process they had undertaken in a continuous cyclical manner. Word-of-mouth recommendations were the most influential sources of information gathered, and online sources were perceived to be credible regardless of their origins. The perception of risk impacted the rigour of the information search process. An iterative decision making cycle model is proposed with Kuhlthaus model and word of mouth information at its core, which reflects the connectedness of individuals in this digital era. This study provides new insights by combining both marketing and LIS models and extends Kuhlthaus research into a new context."
Exploring the diversity and consistency of Chinaâs information technology policy,"Information technology (IT) policies have played an indispensable role in Chinaâs IT research and development (R&D) and industrial development. However, there has been a lack of quantitative and clear understanding of the consistency and diversity of Chinaâs IT policy mixes. In this article, we constructed policy target network to simulate the real policy mixes. Based on the distribution and evolution of networks, we identify and analyse the âdiversityâ and âconsistency & continuityâ of Chinaâs IT policy. Our results show that Chinaâs IT policies cover 12 different demands, and each demand corresponds to 5â10 core policy goals. At the same time, we divide the history of IT policy into seven periods and compare the characteristics of each period. Despite the scale of the policy mixes continuing to expand, the synergy between different policy targets has become clearer, forming unique communities. Among them, there are 25 core policy targets that reflect varying degrees of consistency and continuity. This study not only deepens the understanding of Chinaâs policy characteristics and patterns, but also provides a quantitative framework for the assessment of policy consistency and diversity. Â© The Author(s) 2022.","Information technology (IT) policies have played an indispensable role in Chinas IT research and development (R&D) and industrial development. However, there has been a lack of quantitative and clear understanding of the consistency and diversity of Chinas IT policy mixes. In this article, we constructed policy target network to simulate the real policy mixes. Based on the distribution and evolution of networks, we identify and analyse the diversity and consistency & continuity of Chinas IT policy. Our results show that Chinas IT policies cover 12 different demands, and each demand corresponds to 510 core policy goals. At the same time, we divide the history of IT policy into seven periods and compare the characteristics of each period. Despite the scale of the policy mixes continuing to expand, the synergy between different policy targets has become clearer, forming unique communities. Among them, there are 25 core policy targets that reflect varying degrees of consistency and continuity. This study not only deepens the understanding of Chinas policy characteristics and patterns, but also provides a quantitative framework for the assessment of policy consistency and diversity."
Ensemble correction model for aspect-level sentiment classification,"The aspect-level sentiment analysis is widely used in public opinion analysis. However, the problem of context information loss and distortion with the increase of the model depth is rarely considered in previous research. Few studies have attempted to combine the feature extracted from different embedding models. Based on the correction strategy, the ensemble correction (EC) model proposed in this study can correct context information loss and distortion. Based on the ensemble learning strategy and the weight sharing strategy, EC can extract features from different word embedding models and can reduce computational complexity. Experiments on the resturant14, laptop14, resturant16 and twitter datasets show that the accuracies of the EC model are 0.8848, 0.8213, 0.9301 and 0.7731, respectively. The accuracy of the EC model is higher than state-of-the-art models. Ablation studies and case studies are used to verify the model structure. The optimal number of graph convolutional network (GCN) layers is also verified. Â© The Author(s) 2022.","The aspect-level sentiment analysis is widely used in public opinion analysis. However, the problem of context information loss and distortion with the increase of the model depth is rarely considered in previous research. Few studies have attempted to combine the feature extracted from different embedding models. Based on the correction strategy, the ensemble correction (EC) model proposed in this study can correct context information loss and distortion. Based on the ensemble learning strategy and the weight sharing strategy, EC can extract features from different word embedding models and can reduce computational complexity. Experiments on the resturant14, laptop14, resturant16 and twitter datasets show that the accuracies of the EC model are 0.8848, 0.8213, 0.9301 and 0.7731, respectively. The accuracy of the EC model is higher than state-of-the-art models. Ablation studies and case studies are used to verify the model structure. The optimal number of graph convolutional network (GCN) layers is also verified."
Do retraction practices work effectively? Evidence from citations of psychological retracted articles,"Scientific retraction practices are intended to help purge the continued use of flawed research and assist in maintaining the integrity, credibility and quality of scientific literature. However, the practical effect of retraction is still vague and needs to be further explored. In this study, we analysed the citation counts and sentiments (positive/negative) of retracted articles in psychology journals from Web of Science to explore the effect of retraction. Causal inference strategies were used to measure the net effect of retractions on citation. Results show that the retraction practices induced the citation counts to reduce as expected. However, the proportion of negative citations also decreased because of retraction, indicating an unsatisfied effect. The retraction practice of high-impact factors and open access journals was more effective than other journals. The study integrated an understanding of the dissemination of erroneous publications and provided implications for liabilities involved in the whole retraction process. Â© The Author(s) 2022.","Scientific retraction practices are intended to help purge the continued use of flawed research and assist in maintaining the integrity, credibility and quality of scientific literature. However, the practical effect of retraction is still vague and needs to be further explored. In this study, we analysed the citation counts and sentiments (positive/negative) of retracted articles in psychology journals from Web of Science to explore the effect of retraction. Causal inference strategies were used to measure the net effect of retractions on citation. Results show that the retraction practices induced the citation counts to reduce as expected. However, the proportion of negative citations also decreased because of retraction, indicating an unsatisfied effect. The retraction practice of high-impact factors and open access journals was more effective than other journals. The study integrated an understanding of the dissemination of erroneous publications and provided implications for liabilities involved in the whole retraction process."
Inequality of authorsâ reference reuse,"This brief communication finds a clear and universal inequality of authorsâ reference reuse behaviour. We observe that a few references are reused many times in an authorâs oeuvre while most of his or her references only occur in the reference list for quite a limited number of times. A power law distribution depicts such an inequality. We particularly utilise the power value, (Formula presented.), to characterise the nuanced difference of such inequalities. A pilot study based upon Microsoft Academic Graph (MAG) shows that the (Formula presented.) tends to be normally distributed, regardless of whether it is from a citing or a cited perspective. Our empirical study also reveals that the (Formula presented.) of highly cited publications tends to be greater than that of lowly cited ones, yet we also observe a saturation when the number of citations increases. Â© The Author(s) 2022.","This brief communication finds a clear and universal inequality of authors reference reuse behaviour. We observe that a few references are reused many times in an authors oeuvre while most of his or her references only occur in the reference list for quite a limited number of times. A power law distribution depicts such an inequality. We particularly utilise the power value, (Formula presented.), to characterise the nuanced difference of such inequalities. A pilot study based upon Microsoft Academic Graph (MAG) shows that the (Formula presented.) tends to be normally distributed, regardless of whether it is from a citing or a cited perspective. Our empirical study also reveals that the (Formula presented.) of highly cited publications tends to be greater than that of lowly cited ones, yet we also observe a saturation when the number of citations increases."
Classifying the Mexican epidemiological semaphore colour from the Covid-19 text Spanish news,"This work aims to generate classification models that help determine the colour of an epidemiological semaphore (ES) by analysing online news and being better prepared for the different changes in the evolution of the pandemic. To accomplish this, we introduce Cov-NES-Mex corpus, a collection of 77,983 news (labelled with the Mexican ES system) related to Covid-19 for the 32 regions of Mexico. Also, we showed measures that describe the corpus as imbalanced and with a high vocabulary overlap between classes. In addition, evaluation measurements of the pandemic by region are proposed. Furthermore, a classification model, based on a transformer architecture specialised for the Spanish language, achieved up to 0.83 of F-measure. Thus, this work provides evidence that there is essential information in the news that can be used to determine the colour of the ES up to 4 weeks in advance. Finally, the presented results could be applied to other Spanish-speaking countries, which do not have an ES system, thus inferring and comparing their situation concerning the Mexican ES. Â© The Author(s) 2022.","This work aims to generate classification models that help determine the colour of an epidemiological semaphore (ES) by analysing online news and being better prepared for the different changes in the evolution of the pandemic. To accomplish this, we introduce Cov-NES-Mex corpus, a collection of 77,983 news (labelled with the Mexican ES system) related to Covid-19 for the 32 regions of Mexico. Also, we showed measures that describe the corpus as imbalanced and with a high vocabulary overlap between classes. In addition, evaluation measurements of the pandemic by region are proposed. Furthermore, a classification model, based on a transformer architecture specialised for the Spanish language, achieved up to 0.83 of F-measure. Thus, this work provides evidence that there is essential information in the news that can be used to determine the colour of the ES up to 4 weeks in advance. Finally, the presented results could be applied to other Spanish-speaking countries, which do not have an ES system, thus inferring and comparing their situation concerning the Mexican ES."
An intelligent system for multi-topic social spam detection in microblogging,"The communication revolution has perpetually reshaped the means through which people send and receive information. Social media is an important pillar of this revolution and has brought profound changes to various aspects of our lives. However, the open environment and popularity of these platforms inaugurate windows of opportunities for various cyber threats, thus social networks have become a fertile venue for spammers and other illegitimate users to execute their malicious activities. These activities include phishing hot and trendy topics and posting a wide range of contents in many topics. Hence, it is crucial to continuously introduce new techniques and approaches to detect and stop this category of users. This article proposes a novel and effective approach to detect social spammers. An investigation into several attributes to measure topic-dependent and topic-independent usersâ behaviours on Twitter is carried out. The experiments of this study are undertaken on various machine learning classifiers. The performance of these classifiers is compared and their effectiveness is measured via a number of robust evaluation measures. Furthermore, the proposed approach is benchmarked against state-of-the-art social spam and anomalous detection techniques. These experiments report the effectiveness and utility of the proposed approach and embedded modules. Â© The Author(s) 2022.","The communication revolution has perpetually reshaped the means through which people send and receive information. Social media is an important pillar of this revolution and has brought profound changes to various aspects of our lives. However, the open environment and popularity of these platforms inaugurate windows of opportunities for various cyber threats, thus social networks have become a fertile venue for spammers and other illegitimate users to execute their malicious activities. These activities include phishing hot and trendy topics and posting a wide range of contents in many topics. Hence, it is crucial to continuously introduce new techniques and approaches to detect and stop this category of users. This article proposes a novel and effective approach to detect social spammers. An investigation into several attributes to measure topic-dependent and topic-independent users behaviours on Twitter is carried out. The experiments of this study are undertaken on various machine learning classifiers. The performance of these classifiers is compared and their effectiveness is measured via a number of robust evaluation measures. Furthermore, the proposed approach is benchmarked against state-of-the-art social spam and anomalous detection techniques. These experiments report the effectiveness and utility of the proposed approach and embedded modules."
How do information overload and message fatigue reduce information processing in the era of COVID-19? An abilityâmotivation approach,"The global outbreak of COVID-19 in 2020 has led to the dominance of COVID-19 prevention information on all media channels. Drawing on the abilityâmotivation model of information processing, this study examined how such an information overabundance hampered individualsâ ability and motivation to process in the era of COVID-19. With a survey conducted from 493 participants, we found that less message elaboration of COVID-19 prevention information was predicted by greater message fatigue, a state of low motivation due to information overabundance. In addition, greater message fatigue was accompanied by greater information overload, a state of low ability due to information overabundance. Moreover, certain motivation-related (i.e. health status, trait reactance and frequency of information seeking) and abilityârelated factors (i.e. health literacy, health status, trait anxiety and information quality) were found to be associated with message fatigue and information overload, respectively. The theoretical and practical implications are discussed. Â© The Author(s) 2022.","The global outbreak of COVID-19 in 2020 has led to the dominance of COVID-19 prevention information on all media channels. Drawing on the abilitymotivation model of information processing, this study examined how such an information overabundance hampered individuals ability and motivation to process in the era of COVID-19. With a survey conducted from 493 participants, we found that less message elaboration of COVID-19 prevention information was predicted by greater message fatigue, a state of low motivation due to information overabundance. In addition, greater message fatigue was accompanied by greater information overload, a state of low ability due to information overabundance. Moreover, certain motivation-related ( health status, trait reactance and frequency of information seeking) and abilityrelated factors ( health literacy, health status, trait anxiety and information quality) were found to be associated with message fatigue and information overload, respectively. The theoretical and practical implications are discussed."
Discovering the fundamental strategic indicators of the use of Internet of Things in libraries: A grounded theory study,"The aim of this study is to identify the strategic indicators Internet of Things (IoT) application in libraries and to present a conceptual model. The research was performed qualitatively and the data method of the foundation. Data were collected through documentary methods and interviews with a statistical sample of 13 professors of information science and the snowball sampling method. Interview data were analysed in three steps of open, axial and selective coding. Validity assessment was determined by the responsive method and reliability of two coding tests. After analysing the findings, 35 main codes of subcategories and concepts were discovered. The main categories were divided into eight categories: control and supervision, providing advanced services, accessibility, intelligence, maintaining security, new thinking and development, information literacy and method of use and satisfaction, and based on this, a paradigmatic and theoretical model was presented. In the theoretical model of the main phenomenon, the strategic indicators are the use of IoT in libraries, and the classes and the main phenomenon are related to the main class of application strategies. Strategic indicators of IoT application are leading in the growth and development of libraries. These indicators are important in the IoT use in libraries and their development. The eight categories identified in the conceptual model were considered significant by the interviewees. Thus, the indicators discovered are effective and necessary in the success of libraries that use the IoT. Â© The Author(s) 2022.","The aim of this study is to identify the strategic indicators Internet of Things (IoT) application in libraries and to present a conceptual model. The research was performed qualitatively and the data method of the foundation. Data were collected through documentary methods and interviews with a statistical sample of 13 professors of information science and the snowball sampling method. Interview data were analysed in three steps of open, axial and selective coding. Validity assessment was determined by the responsive method and reliability of two coding tests. After analysing the findings, 35 main codes of subcategories and concepts were discovered. The main categories were divided into eight categories: control and supervision, providing advanced services, accessibility, intelligence, maintaining security, new thinking and development, information literacy and method of use and satisfaction, and based on this, a paradigmatic and theoretical model was presented. In the theoretical model of the main phenomenon, the strategic indicators are the use of IoT in libraries, and the classes and the main phenomenon are related to the main class of application strategies. Strategic indicators of IoT application are leading in the growth and development of libraries. These indicators are important in the IoT use in libraries and their development. The eight categories identified in the conceptual model were considered significant by the interviewees. Thus, the indicators discovered are effective and necessary in the success of libraries that use the IoT."
Improving text relationship modelling with artificial data,"Data augmentation uses artificially created examples to support supervised machine learning, adding robustness to the resulting models and helping to account for limited availability of labelled data. We apply and evaluate a synthetic data approach to relationship classification in digital libraries, generating artificial books with relationships that are common in digital libraries but not easier inferred from existing metadata. Artificial books are generated by remixing existing texts into synthetically constructed formats. We find that for classification on wholeâpart relationships between books, synthetic data improves a deep neural network classifier by 91%. Furthermore, we consider the ability of synthetic data to learn a useful new text relationship class from fully artificial training data. Â© The Author(s) 2022.","Data augmentation uses artificially created examples to support supervised machine learning, adding robustness to the resulting models and helping to account for limited availability of labelled data. We apply and evaluate a synthetic data approach to relationship classification in digital libraries, generating artificial books with relationships that are common in digital libraries but not easier inferred from existing metadata. Artificial books are generated by remixing existing texts into synthetically constructed formats. We find that for classification on wholepart relationships between books, synthetic data improves a deep neural network classifier by 91%. Furthermore, we consider the ability of synthetic data to learn a useful new text relationship class from fully artificial training data."
The effect of co-opinion on the cocitation-based information retrieval systemsâ effectiveness evaluated by semantic similarity,"The co-opinionatedness measure, that is, the similarity of cociting documents in their opinions about their cocited articles, has been recently proposed. The present study uses a wider range of baselines and benchmarks to investigate the measureâs effectiveness in retrieval ranking that was previously confirmed in a pilot study. A test collection was built including 30 seed documents and their 4702 cocited articles. Their citances and full-texts were analysed using natural language processing (NLP) and opinion mining techniques. Cocitation values, syntactical similarity and contexts similarity were used as baselines. The distributional semantic similarity and the linear and hierarchical Medical Subject Headings (MeSH) similarities served as benchmarks to evaluate the effect of the co-opinionatedness as a boosting factor on the performance of the baselines. The improvements in the rankings were measured by normalised discounted cumulative gain (nDCG). According to the findings, there existed significant differences between the nDCG mean values obtained before and after weighting the baselines by the co-opinionatedness measures. The results of the generalisability study corroborated the reliability and generalisability of the systems. Accordingly, the similarity in the opinions of the cociting papers towards their cocited articles can explain the cocitation relation in the scientific papers network and can be effectively utilised for improving the results of the cocitation-based retrieval systems. Â© The Author(s) 2022.","The co-opinionatedness measure, that is, the similarity of cociting documents in their opinions about their cocited articles, has been recently proposed. The present study uses a wider range of baselines and benchmarks to investigate the measures effectiveness in retrieval ranking that was previously confirmed in a pilot study. A test collection was built including 30 seed documents and their 4702 cocited articles. Their citances and full-texts were analysed using natural language processing (NLP) and opinion mining techniques. Cocitation values, syntactical similarity and contexts similarity were used as baselines. The distributional semantic similarity and the linear and hierarchical Medical Subject Headings (MeSH) similarities served as benchmarks to evaluate the effect of the co-opinionatedness as a boosting factor on the performance of the baselines. The improvements in the rankings were measured by normalised discounted cumulative gain (nDCG). According to the findings, there existed significant differences between the nDCG mean values obtained before and after weighting the baselines by the co-opinionatedness measures. The results of the generalisability study corroborated the reliability and generalisability of the systems. Accordingly, the similarity in the opinions of the cociting papers towards their cocited articles can explain the cocitation relation in the scientific papers network and can be effectively utilised for improving the results of the cocitation-based retrieval systems."
Knowledge as a theoretical object: Implications for knowledge management,"The thesis of this paper is that, despite its substantial accomplishments over the past 40 years as a professional practice, knowledge management (KM) has yet to mature as an intellectual discipline. The goal of this conceptual paper is to accelerate this maturation by generalizing KM. The paper does so by identifying KMâs generic social phenomena and how these phenomena can be better understood through the development of theory oriented around knowledge that defines, predicts, and explains them. KM theory is examined through an extensive review of the existing literature. Four levels of organizational KM are offered, including two that examine an organizationâs basis (resources) and essence (ideas and actions), while the second two build upon these to examine resource management in general, and the management of knowledge resources specifically. Treating KM as a theoretical object will lead to greater understanding of KM. This enriched understanding will lead to a more effective performance of KM. The achievement of this paperâs goal (i.e., generalizing KM) will facilitate the maturation of KM into a discipline and practice that consistently contributes to organizational effectiveness. Little KM literature exists on the conceptualization of knowledge as a theoretical object, which offers the opportunity not only to develop theory that defines, predicts, and explains KM within organizations more systematically but also ultimately to understand better the ways in which KM contributes to individual happiness, community cohesion, and social prosperity. Â© The Author(s) 2022.","The thesis of this paper is that, despite its substantial accomplishments over the past 40 years as a professional practice, knowledge management (KM) has yet to mature as an intellectual discipline. The goal of this conceptual paper is to accelerate this maturation by generalizing KM. The paper does so by identifying KMs generic social phenomena and how these phenomena can be better understood through the development of theory oriented around knowledge that defines, predicts, and explains them. KM theory is examined through an extensive review of the existing literature. Four levels of organizational KM are offered, including two that examine an organizations basis (resources) and essence (ideas and actions), while the second two build upon these to examine resource management in general, and the management of knowledge resources specifically. Treating KM as a theoretical object will lead to greater understanding of KM. This enriched understanding will lead to a more effective performance of KM. The achievement of this papers goal (, generalizing KM) will facilitate the maturation of KM into a discipline and practice that consistently contributes to organizational effectiveness. Little KM literature exists on the conceptualization of knowledge as a theoretical object, which offers the opportunity not only to develop theory that defines, predicts, and explains KM within organizations more systematically but also ultimately to understand better the ways in which KM contributes to individual happiness, community cohesion, and social prosperity."
A novel integrated framework based on multi-view features for multidimensional social bot detection,"The ravage of malicious bots in online social networks has profoundly affected normal users. In this article, we propose a novel integrated framework to detect social bots. Specifically, social bot detection is performed from two dimensions: binary-class and fine-grained detection. Moreover, 35 features from three views are extracted to detect social bots, including eight newly defined features. Then, a category balancing based on resampling technology is designed to balance the training data. Finally, a divide-and-conquer strategy is integrated into Random Forest, and the interference of noise in the training process is reduced. Feature effectiveness evaluation found that extracting features from multi-views can describe bots more comprehensively. It is also noted from the category imbalance test that the balanced data set can prevent the detection result from tilting. Comparative experiments show that the integrated framework is more effective than the baseline both in social bot detection and the type detection of bots. Â© The Author(s) 2022.","The ravage of malicious bots in online social networks has profoundly affected normal users. In this article, we propose a novel integrated framework to detect social bots. Specifically, social bot detection is performed from two dimensions: binary-class and fine-grained detection. Moreover, 35 features from three views are extracted to detect social bots, including eight newly defined features. Then, a category balancing based on resampling technology is designed to balance the training data. Finally, a divide-and-conquer strategy is integrated into Random Forest, and the interference of noise in the training process is reduced. Feature effectiveness evaluation found that extracting features from multi-views can describe bots more comprehensively. It is also noted from the category imbalance test that the balanced data set can prevent the detection result from tilting. Comparative experiments show that the integrated framework is more effective than the baseline both in social bot detection and the type detection of bots."
Towards a typology development of crowdsourcing in science,"Crowdsourcing in science as collaborative online process through which non-professional and/or professional scientists incorporate a group of individuals of varying, diversity knowledge and skills, via an open call to the Internet and/or online platforms, to undertaking of a task in science, is an important strategy to support scientific research that has gained attention in academia and practitioners. While research efforts to date have focused on the benefits of crowdsourcing in science, its typology has yet to mature. Typologies are important in describing complex, multidisciplinary organisational forms such as crowdsourcing in science. The main purpose of this article is to identify and provide a typology of crowdsourcing in science. Based on the thematic analysis of publications collected in a systematic manner and focused group interviews, 12 types of crowdsourcing in science are identified. The proposed crowdsourcing in science typology matrix may be a starting point for future research and decision-making by practitioners regarding the choice of a specific type of crowdsourcing in science. Â© The Author(s) 2022.","Crowdsourcing in science as collaborative online process through which non-professional and/or professional scientists incorporate a group of individuals of varying, diversity knowledge and skills, via an open call to the Internet and/or online platforms, to undertaking of a task in science, is an important strategy to support scientific research that has gained attention in academia and practitioners. While research efforts to date have focused on the benefits of crowdsourcing in science, its typology has yet to mature. Typologies are important in describing complex, multidisciplinary organisational forms such as crowdsourcing in science. The main purpose of this article is to identify and provide a typology of crowdsourcing in science. Based on the thematic analysis of publications collected in a systematic manner and focused group interviews, 12 types of crowdsourcing in science are identified. The proposed crowdsourcing in science typology matrix may be a starting point for future research and decision-making by practitioners regarding the choice of a specific type of crowdsourcing in science."
Query expansion using Haar wavelet transform,"Novice users are unable to express their information needs properly, due to this it is difficult to retrieve all the desired relevant documents from the test collection. The problem of word mismatch is fundamental to information retrieval. Query expansion is a technique in which additional terms are added to retrieve relevant documents. In this article, we have expanded the query using pseudo-relevance feedback and Haar wavelet transform. The performance of the proposed technique is evaluated on FIRE 2011 ad hoc English test Collection and Robust dataset. The mean average precision of the proposed model on the FIRE dataset and Robust Track dataset is 0.3334 and 0.2724, respectively. Â© The Author(s) 2022.","Novice users are unable to express their information needs properly, due to this it is difficult to retrieve all the desired relevant documents from the test collection. The problem of word mismatch is fundamental to information retrieval. Query expansion is a technique in which additional terms are added to retrieve relevant documents. In this article, we have expanded the query using pseudo-relevance feedback and Haar wavelet transform. The performance of the proposed technique is evaluated on FIRE 2011 ad hoc English test Collection and Robust dataset. The mean average precision of the proposed model on the FIRE dataset and Robust Track dataset is 0.3334 and 0.2724, respectively."
Do papers of high interdisciplinarity have an advantage in terms of citations? A case study of the top five Economic journals,"This study examines the interdisciplinarity scores of papers published in five major Economic journals by analysing their references. It also explores the relationship between interdisciplinarity and citation. The study considers the influence of the citation time window on accumulating citations and investigates the source of citation advantage for high interdisciplinarity papers. Empirical findings reveal a U-shaped curve relationship between the interdisciplinarity of papers and their citation frequency. Papers with high interdisciplinarity do enjoy a citation advantage, which primarily stems from the attention and citations from distant disciplinary papers and multidisciplinary journals. However, it often takes a longer time for the value of interdisciplinary papers to be recognised. Based on these findings, the study discusses the necessity and effectiveness of incentives for interdisciplinary research and provides recommendations for evaluating and managing interdisciplinary research. Â© The Author(s) 2024.","This study examines the interdisciplinarity scores of papers published in five major Economic journals by analysing their references. It also explores the relationship between interdisciplinarity and citation. The study considers the influence of the citation time window on accumulating citations and investigates the source of citation advantage for high interdisciplinarity papers. Empirical findings reveal a U-shaped curve relationship between the interdisciplinarity of papers and their citation frequency. Papers with high interdisciplinarity do enjoy a citation advantage, which primarily stems from the attention and citations from distant disciplinary papers and multidisciplinary journals. However, it often takes a longer time for the value of interdisciplinary papers to be recognised. Based on these findings, the study discusses the necessity and effectiveness of incentives for interdisciplinary research and provides recommendations for evaluating and managing interdisciplinary research."
The economics of libraries,"The present article describes recent developments related to the study of libraries in the field of economics. In terms of scope, there is considerable variation in economic applications, with research themes ranging from costâbenefit analyses to the impacts of libraries on educational outcomes, for example. In terms of approaches, the vast majority of articles in the field correspond to empirical studies employing library data, although there is some variation in terms of aggregation. In general, a first look at this burgeoning literature divides its main contributions into two broad sets: (1) one focused on the long-term effects of public libraries over economic outcomes and (2) another focused on the use of libraries as naturally occurring laboratories for the test of economic theories. Although there is not a common theme underlying the majority of contributions here surveyed, there are still sizable opportunities for economists â and social scientists, in general â who want to explore the research potential of libraries. Â© The Author(s) 2024.","The present article describes recent developments related to the study of libraries in the field of economics. In terms of scope, there is considerable variation in economic applications, with research themes ranging from costbenefit analyses to the impacts of libraries on educational outcomes, for example. In terms of approaches, the vast majority of articles in the field correspond to empirical studies employing library data, although there is some variation in terms of aggregation. In general, a first look at this burgeoning literature divides its main contributions into two broad sets: one focused on the long-term effects of public libraries over economic outcomes and another focused on the use of libraries as naturally occurring laboratories for the test of economic theories. Although there is not a common theme underlying the majority of contributions here surveyed, there are still sizable opportunities for economists and social scientists, in general who want to explore the research potential of libraries."
Open research data and privacy violations,"Open research data refer to the practice of sharing research data with others to enhance science innovations and discoveries. Despite the great potentials of open research data, it comes with certain limitations, especially with regards to the privacy of research participantsâ data. In this article, we examine the tension between public data repository policy and the General Data Protection Regulation (GDPR). To achieve that, we draw on privacy as contextual integrity theory. We further enrich our research by interviewing 12 researchers from European institutions to examine their perception of whether open research data have privacy challenges or not. Our findings reveal that according to the heuristics steps of privacy as contextual integrity and the GDPR requirements, open research data may entail a violation to research participantsâ informational privacy. Moreover, data repositoryâs policy is geared towards protecting the confidentiality of research participantsâ data rather than their privacy. We further reveal that researchers conflate privacy and anonymity and lack knowledge of sharing research data practices. Â© The Author(s) 2024.","Open research data refer to the practice of sharing research data with others to enhance science innovations and discoveries. Despite the great potentials of open research data, it comes with certain limitations, especially with regards to the privacy of research participants data. In this article, we examine the tension between public data repository policy and the General Data Protection Regulation (GDPR). To achieve that, we draw on privacy as contextual integrity theory. We further enrich our research by interviewing 12 researchers from European institutions to examine their perception of whether open research data have privacy challenges or not. Our findings reveal that according to the heuristics steps of privacy as contextual integrity and the GDPR requirements, open research data may entail a violation to research participants informational privacy. Moreover, data repositorys policy is geared towards protecting the confidentiality of research participants data rather than their privacy. We further reveal that researchers conflate privacy and anonymity and lack knowledge of sharing research data practices."
"How are global university rankings adjusted for erroneous science, fraud and misconduct? Posterior reduction or adjustment in rankings in response to retractions and invalidation of scientific findings","Global university rankings (GURs), such as the Times Higher Education World University Ranking (THE WUR), Quacquarelli Symonds University World Rankings (QS UWR) and the Academic Ranking of World Universities (ARWU) are positively incremental, that is, they do not reflect any level of penalisation in response to unscholarly activity, especially in the field of research and publication. In the light of an increasing trend in fraud, such as the use of paper mills and authorship-for-sale schemes, this letter proposes that GURs need to be reduced, or penalised, in response to cases of misconduct and instances of retractions. In the absence of a transparent corrective system, GURs will be further criticised for being unfair, biased and not reflective of an evolving and unstable academic publishing ecosystem. Â© The Author(s) 2024.","Global university rankings (GURs), such as the Times Higher Education World University Ranking (THE WUR), Quacquarelli Symonds University World Rankings (QS UWR) and the Academic Ranking of World Universities (ARWU) are positively incremental, that is, they do not reflect any level of penalisation in response to unscholarly activity, especially in the field of research and publication. In the light of an increasing trend in fraud, such as the use of paper mills and authorship-for-sale schemes, this letter proposes that GURs need to be reduced, or penalised, in response to cases of misconduct and instances of retractions. In the absence of a transparent corrective system, GURs will be further criticised for being unfair, biased and not reflective of an evolving and unstable academic publishing ecosystem."
Exploring the topic evolution of Dunhuang murals through image classification,"Dunhuang is a unique art treasure and a world heritage site. In order to organise and manage Dunhuang cultural heritage resources, this article studies the classification of Dunhuang murals in different dynasties, and explores the topic distribution characteristics and evolution rules of them. First, image features are extracted through scale-invariant feature transform (SIFT) and Canny and scale-invariant feature transform (CSIFT), a visual dictionary is generated through the k-means clustering algorithm, and the term frequencyâinverse document frequency (TF-IDF) vector is calculated and combined with the colour feature vector extracted via hue, saturation and value (HSV). Second, Dunhuang mural images are collected and the support vector machine (SVM) classifier is built. Finally, the knowledge graph-based topic maps are constructed, and graph theory is introduced to analyse the topic distribution and evolution of Dunhuang murals in different dynasties. The results show that the Dunhuang murals of different dynasties can be effectively classified through the bag of words, HSV and support vector machine (BOW_HSV_SVM) based on their visual features. Through topic maps, the topic distribution characteristics and evolution rules of Dunhuang murals with the dynasties are revealed. Â© The Author(s) 2022.","Dunhuang is a unique art treasure and a world heritage site. In order to organise and manage Dunhuang cultural heritage resources, this article studies the classification of Dunhuang murals in different dynasties, and explores the topic distribution characteristics and evolution rules of them. First, image features are extracted through scale-invariant feature transform (SIFT) and Canny and scale-invariant feature transform (CSIFT), a visual dictionary is generated through the k-means clustering algorithm, and the term frequencyinverse document frequency (TF-IDF) vector is calculated and combined with the colour feature vector extracted via hue, saturation and value (HSV). Second, Dunhuang mural images are collected and the support vector machine (SVM) classifier is built. Finally, the knowledge graph-based topic maps are constructed, and graph theory is introduced to analyse the topic distribution and evolution of Dunhuang murals in different dynasties. The results show that the Dunhuang murals of different dynasties can be effectively classified through the bag of words, HSV and support vector machine (BOW_HSV_SVM) based on their visual features. Through topic maps, the topic distribution characteristics and evolution rules of Dunhuang murals with the dynasties are revealed."
The relationship between sentiment score and COVID-19 cases in the United States,"The coronavirus disease (COVID-19) continues to have devastating effects across the globe. No nation has been free from the uncertainty brought by this pandemic. The health, social and economic tolls associated with it are causing strong emotions and spreading fear in people of all ages, genders and races. Since the beginning of the COVID-19 pandemic, many have expressed their feelings and opinions related to a wide range of aspects of their lives via Twitter. In this study, we consider a framework for extracting sentiment scores and opinions from COVID-19ârelated tweets. We connect usersâ sentiment with COVID-19 cases across the United States and investigate the effect of specific COVID-19 milestones on public sentiment. The results of this work may help with the development of pandemic-related legislation, serve as a guide for scientific work, as well as inform and educate the public on core issues related to the pandemic. Â© The Author(s) 2022.","The coronavirus disease (COVID-19) continues to have devastating effects across the globe. No nation has been free from the uncertainty brought by this pandemic. The health, social and economic tolls associated with it are causing strong emotions and spreading fear in people of all ages, genders and races. Since the beginning of the COVID-19 pandemic, many have expressed their feelings and opinions related to a wide range of aspects of their lives via Twitter. In this study, we consider a framework for extracting sentiment scores and opinions from COVID-19related tweets. We connect users sentiment with COVID-19 cases across the United States and investigate the effect of specific COVID-19 milestones on public sentiment. The results of this work may help with the development of pandemic-related legislation, serve as a guide for scientific work, as well as inform and educate the public on core issues related to the pandemic."
Cross-domain corpus selection for cold-start context,"Sentiment analysis is a powerful tool for monitoring attitudes towards companies, products or services and identifying specific features that drive positive or negative sentiment. However, collecting labelled data for training sentiment analysis models in a specific domain can be challenging in practical applications. One promising solution to this âcold-startâ problem is domain adaptation, which leverages labelled data from a related source domain to train a model for the target domain. A critical yet often neglected aspect in prior research is the measurement of similarity between the source and target domains, a factor that greatly impacts the success of domain adaptation. To fill this gap, we propose a novel measure that combines semantic, syntactic and lexical features to assess corpus-level similarity between two domains. Our experimental results demonstrate that our method achieves high precision (0.91) and recall (0.75), outperforming traditional methods. Moreover, our proposed measure can assist new domain products in selecting the most suitable training data set for their sentiment analysis tasks. Â© The Author(s) 2024.","Sentiment analysis is a powerful tool for monitoring attitudes towards companies, products or services and identifying specific features that drive positive or negative sentiment. However, collecting labelled data for training sentiment analysis models in a specific domain can be challenging in practical applications. One promising solution to this cold-start problem is domain adaptation, which leverages labelled data from a related source domain to train a model for the target domain. A critical yet often neglected aspect in prior research is the measurement of similarity between the source and target domains, a factor that greatly impacts the success of domain adaptation. To fill this gap, we propose a novel measure that combines semantic, syntactic and lexical features to assess corpus-level similarity between two domains. Our experimental results demonstrate that our method achieves high precision (0.91) and recall (0.75), outperforming traditional methods. Moreover, our proposed measure can assist new domain products in selecting the most suitable training data set for their sentiment analysis tasks."
"How has academia responded to the urgent needs created by COVID-19? A multi-level global, regional and national analysis","In the context of the COVID-19 pandemic, gaining insights into how academia has responded to this urgent challenge is of great significance. This article presents academic response patterns at a global, regional and national level from an analysis of publication volume versus reported cases of COVID-19, scientific collaboration and research focus. We also compare academic activity associated with this newly emerging infection to that related to long-standing infections. Our results show that the research community has responded quickly to COVID-19. The highly developed countries, which have the highest number of confirmed cases, are also the major academic contributors. National-level analysis reveals diverse response patterns from different countries. Specifically, academic research in the United Kingdom remained at a relatively constant level throughout the whole year (2020), while the global share of Chinaâs research output was prone to shift as its domestic pandemic status changed. Strong alliances have formed among countries with academic capabilities in response to the COVID-19 pandemic. The distribution of disciplines is relatively decentralised, indicating that a diverse and broad knowledge base contributes to the COVID-19 literature. Most of the analysed countries show dynamic patterns of research focus that vary over time as the pandemic evolves, except India. As one of the worldâs biggest suppliers of vaccines, India makes consistent efforts on vaccine research, especially those related to pharmaceutical preparations. Our findings may serve as resources for fostering strategies to respond to future threats of pandemics. Â© The Author(s) 2022.","In the context of the COVID-19 pandemic, gaining insights into how academia has responded to this urgent challenge is of great significance. This article presents academic response patterns at a global, regional and national level from an analysis of publication volume versus reported cases of COVID-19, scientific collaboration and research focus. We also compare academic activity associated with this newly emerging infection to that related to long-standing infections. Our results show that the research community has responded quickly to COVID-19. The highly developed countries, which have the highest number of confirmed cases, are also the major academic contributors. National-level analysis reveals diverse response patterns from different countries. Specifically, academic research in the United Kingdom remained at a relatively constant level throughout the whole year , while the global share of Chinas research output was prone to shift as its domestic pandemic status changed. Strong alliances have formed among countries with academic capabilities in response to the COVID-19 pandemic. The distribution of disciplines is relatively decentralised, indicating that a diverse and broad knowledge base contributes to the COVID-19 literature. Most of the analysed countries show dynamic patterns of research focus that vary over time as the pandemic evolves, except India. As one of the worlds biggest suppliers of vaccines, India makes consistent efforts on vaccine research, especially those related to pharmaceutical preparations. Our findings may serve as resources for fostering strategies to respond to future threats of pandemics."
What financial topics do people search for? An analysis of search queries using text mining,"Billions of web searches are recorded every day; however, little is known about the types of financial information that users search for. While many studies have investigated information exchanges in financial forums, this is the first study to identify the financial information needs of Internet users in Malaysia through an analysis of search queries. We identified financial topics and discovered subtopics of interest using text mining. We found that topics with high search volume were related to financial products and services, and very little was related to concepts and information that would increase financial knowledge. The results of this study can be used to develop more strategic online financial education content that not only meets usersâ financial information needs but also increases their financial knowledge, especially when the financial knowledge of the global population has remained low over the years. Â© The Author(s) 2024.","Billions of web searches are recorded every day; however, little is known about the types of financial information that users search for. While many studies have investigated information exchanges in financial forums, this is the first study to identify the financial information needs of Internet users in Malaysia through an analysis of search queries. We identified financial topics and discovered subtopics of interest using text mining. We found that topics with high search volume were related to financial products and services, and very little was related to concepts and information that would increase financial knowledge. The results of this study can be used to develop more strategic online financial education content that not only meets users financial information needs but also increases their financial knowledge, especially when the financial knowledge of the global population has remained low over the years."
H-index and research evaluation: A suggested set of components for developing a comprehensive author-level index,"The H-index has been investigated in various studies; this index has many strengths that have made it popular. However, it also has weaknesses, due to which other indicators have been developed. This study aims to identify the strengths and weaknesses of the H-index and provide the minimum set of necessary components for developing a comprehensive author-level index. In this systematic literature review, Scopus, PubMed, Web of Science, Emerald, and ProQuest databases were searched to identify relevant studies. From the number of 14,253 retrieved studies, after two stages of screening, 81 studies were selected according to the eligibility criteria for data extraction. The findings of the study led to the identification of 15 strengths in the three categories of Quality Features, Simplicity, and Suitability, and 13 weaknesses in the six categories of Publications, Citations, Academic Age, Author Credit Allocation, Variety of Fields, and mathematical calculation for H-index. Finally, 28 components were identified as the minimum set of necessary components to develop a comprehensive author-level index to help evaluate researchers more realistically and fairly. The minimum components that need to be considered in developing a comprehensive author-level index can be proposed as follows: Quality Features, Simplicity, Suitability, Publications, Citations, Academic Age, Author Credit Allocation, Variety of Fields, and mathematical calculation. Â© The Author(s) 2024.","The H-index has been investigated in various studies; this index has many strengths that have made it popular. However, it also has weaknesses, due to which other indicators have been developed. This study aims to identify the strengths and weaknesses of the H-index and provide the minimum set of necessary components for developing a comprehensive author-level index. In this systematic literature review, Scopus, PubMed, Web of Science, Emerald, and ProQuest databases were searched to identify relevant studies. From the number of 14,253 retrieved studies, after two stages of screening, 81 studies were selected according to the eligibility criteria for data extraction. The findings of the study led to the identification of 15 strengths in the three categories of Quality Features, Simplicity, and Suitability, and 13 weaknesses in the six categories of Publications, Citations, Academic Age, Author Credit Allocation, Variety of Fields, and mathematical calculation for H-index. Finally, 28 components were identified as the minimum set of necessary components to develop a comprehensive author-level index to help evaluate researchers more realistically and fairly. The minimum components that need to be considered in developing a comprehensive author-level index can be proposed as follows: Quality Features, Simplicity, Suitability, Publications, Citations, Academic Age, Author Credit Allocation, Variety of Fields, and mathematical calculation."
Utilising crowdsourcing and text mining to enhance information extraction from social media: A case study in handling COVID-19 supply requests in Thailand,"Social media platforms are critical for disaster communication and relief efforts. Rapid and precise social media post analysis is required for effective disaster response. This article presents a comprehensive study of a framework that combines crowdsourcing and text mining techniques to enhance data extraction from social media. The research focuses on a particular case study of COVID-19 pandemic medical supply request, which shows several key findings. First, the incorporation of domain-specific data during the training of named entity recognition (NER) models is essential for accurately identifying and retrieving important entities, such as the names of medical supplies and hospitals. Second, the implementation of a hybrid system leads to improvement in the extraction of information from social media posts. Finally, the involvement of crowdsourcing is found to be significant in the validation, verification, and filtering of disorganised information within the hybrid system. Our performance analysis demonstrates that the use of hybrid models has the potential to significantly improve the extraction of supply names (by up to 37%) and hospital names (by up to 66%), especially in the absence of a comprehensive vocabulary or specially trained NER models. During the COVID-19 supply shortage in Thailand, volunteers utilised hybrid models to expedite the identification of the necessary information. Experiment results demonstrated significant improvement in the accuracy of extracted data, the ability to acquire relevant information in real-time, the capacity to handle a substantial number of posts and the practical benefit of the proposed framework. Â© The Author(s) 2024.","Social media platforms are critical for disaster communication and relief efforts. Rapid and precise social media post analysis is required for effective disaster response. This article presents a comprehensive study of a framework that combines crowdsourcing and text mining techniques to enhance data extraction from social media. The research focuses on a particular case study of COVID-19 pandemic medical supply request, which shows several key findings. First, the incorporation of domain-specific data during the training of named entity recognition (NER) models is essential for accurately identifying and retrieving important entities, such as the names of medical supplies and hospitals. Second, the implementation of a hybrid system leads to improvement in the extraction of information from social media posts. Finally, the involvement of crowdsourcing is found to be significant in the validation, verification, and filtering of disorganised information within the hybrid system. Our performance analysis demonstrates that the use of hybrid models has the potential to significantly improve the extraction of supply names (by up to 37%) and hospital names (by up to 66%), especially in the absence of a comprehensive vocabulary or specially trained NER models. During the COVID-19 supply shortage in Thailand, volunteers utilised hybrid models to expedite the identification of the necessary information. Experiment results demonstrated significant improvement in the accuracy of extracted data, the ability to acquire relevant information in real-time, the capacity to handle a substantial number of posts and the practical benefit of the proposed framework."
A matter of time: Publication dates in Scopus,"Similar to the Web of Science, Scopus is also a widely used abstract and citation database. Researchers typically employ the Year of Publication or Date of Publication field in Scopus to retrieve, filter and analyse indexed records. However, the inconsistent retrieval results obtained by these two fields in Scopus, which was occasionally observed in this study, may cause confusion among users. In this brief research article, we seek to elucidate this phenomenon by utilising indexed records in Scopus from the past 50 years. Empirical evidence indicates that inconsistent retrieval results retrieved by these two search fields are attributable to discrepancies in the publication year information provided in the Year of Publication and Date of Publication fields in Scopus. Specifically, missing year information in the Date of Publication field, incorrect year information in the Date of Publication field or in the Year of Publication field, and inconsistent use of different versions of publication dates in these two fields are four representative causes for the observed inconsistencies in retrieval results in Scopus. This article concludes by outlining the potential consequences of these issues and suggesting ways to effectively address them. Â© The Author(s) 2024.","Similar to the Web of Science, Scopus is also a widely used abstract and citation database. Researchers typically employ the Year of Publication or Date of Publication field in Scopus to retrieve, filter and analyse indexed records. However, the inconsistent retrieval results obtained by these two fields in Scopus, which was occasionally observed in this study, may cause confusion among users. In this brief research article, we seek to elucidate this phenomenon by utilising indexed records in Scopus from the past 50 years. Empirical evidence indicates that inconsistent retrieval results retrieved by these two search fields are attributable to discrepancies in the publication year information provided in the Year of Publication and Date of Publication fields in Scopus. Specifically, missing year information in the Date of Publication field, incorrect year information in the Date of Publication field or in the Year of Publication field, and inconsistent use of different versions of publication dates in these two fields are four representative causes for the observed inconsistencies in retrieval results in Scopus. This article concludes by outlining the potential consequences of these issues and suggesting ways to effectively address them."
Predicting the technological impact of papers: Exploring optimal models and most important features,"Patent citations received by a paper are considered one of the most appropriate indicators for quantifying the technological impact of scientific research. In light of the large number of published research outcomes, technology developers need an effective method to identify academic work with potential technological impact and so as to provide scientific theories for the generation of relevant technologies. Focusing on the technical field of artificial intelligence (AI), this study constructs a set of 47 features from seven dimensions and uses feature selection and machine learning models to accurately predict how research papers impact AI technology. The results show that the random forest model is superior to the other tested models in predicting AI patent citations of papers, with citation-related features (such as âPaperCitationsâ and âBackgroundâ) playing a vital role in the prediction. Â© The Author(s) 2024.","Patent citations received by a paper are considered one of the most appropriate indicators for quantifying the technological impact of scientific research. In light of the large number of published research outcomes, technology developers need an effective method to identify academic work with potential technological impact and so as to provide scientific theories for the generation of relevant technologies. Focusing on the technical field of artificial intelligence (AI), this study constructs a set of 47 features from seven dimensions and uses feature selection and machine learning models to accurately predict how research papers impact AI technology. The results show that the random forest model is superior to the other tested models in predicting AI patent citations of papers, with citation-related features (such as PaperCitations and Background) playing a vital role in the prediction."
âAsking for a friendâ: Mediation needs in Israeli social media conversations on benefits and rights,"Benefits and rights are integral to modern citizenship in modern society, and information is essential to their utilisation (take-up) process. Such information is often unclear and difficult to find and understand, thus requiring mediation. Who requires mediation, and under what circumstances? Does the right type matter? This study addresses these questions by computerised and manual analysis of 69,696 Israeli social media conversation segments about 29 issues. According to the results, some people need mediation to get information about benefits and rights, regardless of the right type or circumstance they face. We also developed a computerised way to identify the need for mediation in a social media conversation. Future research is needed to deepen understanding of what plays a more significant role in mediation needs â the right type, life circumstances or other factors. Â© The Author(s) 2024.","Benefits and rights are integral to modern citizenship in modern society, and information is essential to their utilisation (take-up) process. Such information is often unclear and difficult to find and understand, thus requiring mediation. Who requires mediation, and under what circumstances? Does the right type matter? This study addresses these questions by computerised and manual analysis of 69,696 Israeli social media conversation segments about 29 issues. According to the results, some people need mediation to get information about benefits and rights, regardless of the right type or circumstance they face. We also developed a computerised way to identify the need for mediation in a social media conversation. Future research is needed to deepen understanding of what plays a more significant role in mediation needs the right type, life circumstances or other factors."
Gender differences in citation sentiment: A case study in life sciences and biomedicine,"In this study, we investigated whether female and male authors in the field of life sciences and biomedicine differed in their tendency for citation and citation sentiment. The data comprised two sets, cited set and citing set. Cited set comprised 17,237 articles whereas citing set comprised 115,935 articles. The cited set which is from the area Life Sciences & Biomedicine and published during 2012â2016 was retrieved from the Web of Science Medline. The citing set and its citation contexts were retrieved using the Colil database. The analysis was done using a combination of homophily analysis, regression analysis and a chi-square test. The covariates in the regression analyses were features related to authors, journal, institution, country and abstract readability. The homophily analysis showed a significant tendency for female (8%) and male (14%) authorship teams to cite papers by the same gender composition teams. In addition, the results of regression analysis (Model 1) and pairwise comparisons showed that male-authored papers received a significant higher positive sentiment compared with female-authored papers. The results of regression analysis (Model 2) showed a small significant positive association between gender similarity of cited and citing authorship teams and the sentiment score. However, further analysis using the chi-square test showed a significant lower tendency for women to use positive terms when citing the research findings of papers with the same gender composition. Men, in contrast, used significantly more positive terms when citing papers with the same gender composition. Finally, lay summary for a cited paper, country similarity and the venue of cited publication when it was a mega journal had a positive significant association with the sentiment score received. Â© The Author(s) 2022.","In this study, we investigated whether female and male authors in the field of life sciences and biomedicine differed in their tendency for citation and citation sentiment. The data comprised two sets, cited set and citing set. Cited set comprised 17,237 articles whereas citing set comprised 115,935 articles. The cited set which is from the area Life Sciences & Biomedicine and published during 20122016 was retrieved from the Web of Science Medline. The citing set and its citation contexts were retrieved using the Colil database. The analysis was done using a combination of homophily analysis, regression analysis and a chi-square test. The covariates in the regression analyses were features related to authors, journal, institution, country and abstract readability. The homophily analysis showed a significant tendency for female (8%) and male (14%) authorship teams to cite papers by the same gender composition teams. In addition, the results of regression analysis (Model 1) and pairwise comparisons showed that male-authored papers received a significant higher positive sentiment compared with female-authored papers. The results of regression analysis (Model 2) showed a small significant positive association between gender similarity of cited and citing authorship teams and the sentiment score. However, further analysis using the chi-square test showed a significant lower tendency for women to use positive terms when citing the research findings of papers with the same gender composition. Men, in contrast, used significantly more positive terms when citing papers with the same gender composition. Finally, lay summary for a cited paper, country similarity and the venue of cited publication when it was a mega journal had a positive significant association with the sentiment score received."
Short text classification using semantically enriched topic model,"Modelling short text is challenging due to the small number of word co-occurrence and insufficient semantic information that affects downstream Natural Language Processing (NLP) tasks, for example, text classification. Gathering information from external sources is expensive and may increase noise. For efficient short text classification without depending on external knowledge sources, we propose Expressive Short text Classification (EStC). EStC consists of a novel document context-aware semantically enriched topic model called the Short text Topic Model (StTM) that captures words, topics and documents semantics in a joint learning framework. In StTM, the probability of predicting a context word involves the topic distribution of word embeddings and the document vector as the global context, which obtains by weighted averaging of word embeddings on the fly simultaneously with the topic distribution of words without requiring an additional inference method for the document embedding. EStC represents documents in an expressive (number of topics Ã number of word embedding features) embedding space and uses a linear support vector machine (SVM) classifier for their classification. Experimental results demonstrate that EStC outperforms many state-of-the-art language models in short text classification using several publicly available short text data sets. Â© The Author(s) 2024.","Modelling short text is challenging due to the small number of word co-occurrence and insufficient semantic information that affects downstream Natural Language Processing (NLP) tasks, for example, text classification. Gathering information from external sources is expensive and may increase noise. For efficient short text classification without depending on external knowledge sources, we propose Expressive Short text Classification (EStC). EStC consists of a novel document context-aware semantically enriched topic model called the Short text Topic Model (StTM) that captures words, topics and documents semantics in a joint learning framework. In StTM, the probability of predicting a context word involves the topic distribution of word embeddings and the document vector as the global context, which obtains by weighted averaging of word embeddings on the fly simultaneously with the topic distribution of words without requiring an additional inference method for the document embedding. EStC represents documents in an expressive (number of topics number of word embedding features) embedding space and uses a linear support vector machine (SVM) classifier for their classification. Experimental results demonstrate that EStC outperforms many state-of-the-art language models in short text classification using several publicly available short text data sets."
"A âfield transformation and social integrationâ: Settlement information practices of relocated ethnic minorities with small populations in poverty-alleviation areas of Yunnan, China","This study investigates the settlement information practices of ethnic minorities with small populations (EMSPs) who were relocated to poverty-alleviation areas in Yunnan, China. Data were gathered using interviews and surveys. A face-to-face survey was administered to 126 and 147 EMSPs before and after relocation, respectively, focusing on their information needs, acquisition, and sharing. In addition, in-depth interviews were conducted with 16 relocated EMSP participants to identify the factors influencing their settlement information practices. This studyâs findings showed that ethnic characteristics, spatial reconstruction and social inclusion were the primary factors affecting the context of changing living spaces and social communication relationships. Furthermore, the results contribute to a conceptual framework for the settlement information practices of EMSPs and provide valuable insights for research on settlement information practices of newcomer populations across cultures and ethnicities. Â© The Author(s) 2024.","This study investigates the settlement information practices of ethnic minorities with small populations (EMSPs) who were relocated to poverty-alleviation areas in Yunnan, China. Data were gathered using interviews and surveys. A face-to-face survey was administered to 126 and 147 EMSPs before and after relocation, respectively, focusing on their information needs, acquisition, and sharing. In addition, in-depth interviews were conducted with 16 relocated EMSP participants to identify the factors influencing their settlement information practices. This studys findings showed that ethnic characteristics, spatial reconstruction and social inclusion were the primary factors affecting the context of changing living spaces and social communication relationships. Furthermore, the results contribute to a conceptual framework for the settlement information practices of EMSPs and provide valuable insights for research on settlement information practices of newcomer populations across cultures and ethnicities."
COVID-19 effect on the gender gap in academic publishing,"The authors wanted to verify a popular belief that women scholars have been disproportionately affected by the COVID-19 pandemic. We studied the first names of authors of 266,409 articles from 2813 journals in 21 disciplines, and we found no significant differences between men and women in publication patterns between 2021, 2020, and 2019 overall. However, we found significant differences in publication patterns between gender in different disciplines. In addition, in disciplines where the proportion of women authors is higher, there are fewer single-authored articles. In the multi-author articles if the first author is female, there is more gender balance among authors, although there are still fewer women co-authors. Â© The Author(s) 2022.","The authors wanted to verify a popular belief that women scholars have been disproportionately affected by the COVID-19 pandemic. We studied the first names of authors of 266,409 articles from 2813 journals in 21 disciplines, and we found no significant differences between men and women in publication patterns between 2021, 2020, and 2019 overall. However, we found significant differences in publication patterns between gender in different disciplines. In addition, in disciplines where the proportion of women authors is higher, there are fewer single-authored articles. In the multi-author articles if the first author is female, there is more gender balance among authors, although there are still fewer women co-authors."
A multilayer network-based framework for handling and comparing user histories in X,"In this article, we propose a framework that uses a multimodal multilayer network to build, manage and compare User Histories in X. A User History not only considers the contents of interest to the user, as is generally the case with a User Profile. In fact, it also records the set of interactions she has made with contents, the timestamps when they happened, and, ultimately, the history of her actions and behaviour. This provides a more comprehensive view of the user, her preferences and needs and paves the way to a number of additional applications. Most of them need to compare two User Histories to calculate their similarity degree. Our framework proposes two approaches, one naive and one refined, to perform this task. We also mention an application that, along with others already proposed in the literature, can greatly benefit from User Histories with the aim of fostering people inclusiveness. Finally, we describe a set of experiments we conducted to evaluate the goodness of the proposed framework. Â© The Author(s) 2024.","In this article, we propose a framework that uses a multimodal multilayer network to build, manage and compare User Histories in A User History not only considers the contents of interest to the user, as is generally the case with a User Profile. In fact, it also records the set of interactions she has made with contents, the timestamps when they happened, and, ultimately, the history of her actions and behaviour. This provides a more comprehensive view of the user, her preferences and needs and paves the way to a number of additional applications. Most of them need to compare two User Histories to calculate their similarity degree. Our framework proposes two approaches, one naive and one refined, to perform this task. We also mention an application that, along with others already proposed in the literature, can greatly benefit from User Histories with the aim of fostering people inclusiveness. Finally, we describe a set of experiments we conducted to evaluate the goodness of the proposed framework."
The adoption footprints of Koha as a library management system in university libraries of Pakistan,"The study investigated the adoption footprints of Koha in university libraries in Pakistan by using the Unified Theory of Acceptance and Use of Technology (UTAUT) model. The UTAUT model suggests that the decision to use technology is influenced by performance expectancy, effort expectancy, social influence and facilitating conditions. The study used a survey questionnaire to collect data from 250 librarians working in libraries and analysed the results using a quantitative research approach. The findings of the study revealed that performance expectancy, effort expectancy and facilitating conditions positively influenced the intention to adopt Koha. Interestingly, social influence had no significant effect on the adoption of Koha, indicating that the opinions of others did not play a significant role in the decision to adopt Koha. Personal innovativeness had a negative impact on the behavioural intention of librarians to use Koha. Personal innovativeness refers to the willingness of individuals to adopt new technology, and this finding suggests that librarians who are less willing to adopt new technology may be less probably to adopt Koha. The study also found that the significance of information and communication technology (ICT) background among librarians in Pakistan was low. This can be attributed to inadequate perceived social influence and facilitating conditions towards adopting new information technology (IT) solutions. To increase the use of Koha among librarians, the study recommends that libraries improve social influence and provide better conditions for adoption. This study has important implications for library managers and policymakers who are seeking to enhance the use of open-source library management system (LMS) in university libraries. Â© The Author(s) 2024.","The study investigated the adoption footprints of Koha in university libraries in Pakistan by using the Unified Theory of Acceptance and Use of Technology (UTAUT) model. The UTAUT model suggests that the decision to use technology is influenced by performance expectancy, effort expectancy, social influence and facilitating conditions. The study used a survey questionnaire to collect data from 250 librarians working in libraries and analysed the results using a quantitative research approach. The findings of the study revealed that performance expectancy, effort expectancy and facilitating conditions positively influenced the intention to adopt Koha. Interestingly, social influence had no significant effect on the adoption of Koha, indicating that the opinions of others did not play a significant role in the decision to adopt Koha. Personal innovativeness had a negative impact on the behavioural intention of librarians to use Koha. Personal innovativeness refers to the willingness of individuals to adopt new technology, and this finding suggests that librarians who are less willing to adopt new technology may be less probably to adopt Koha. The study also found that the significance of information and communication technology (ICT) background among librarians in Pakistan was low. This can be attributed to inadequate perceived social influence and facilitating conditions towards adopting new information technology (IT) solutions. To increase the use of Koha among librarians, the study recommends that libraries improve social influence and provide better conditions for adoption. This study has important implications for library managers and policymakers who are seeking to enhance the use of open-source library management system (LMS) in university libraries."
Vocabulary mapping for archaeological infrastructure,"This article reports and reflects on vocabulary mapping techniques, tools and experience from the ARIADNE European archaeological infrastructure projects, where the widely differing terminology of subject indexing in the different partner languages posed significant challenges for effective data integration. The Getty Art & Architecture Thesaurus is employed as a central spine vocabulary for partners to map their native vocabularies and term lists â a hub structure enables a multilingual search capability via vocabulary mapping. Mappings are expressed via SKOS mapping relationships and output as structured JSON for use in the overall data aggregation process and in the ARIADNE portal. The approach followed offers some automatic support for final intellectual judgement. The method can be characterised as providing lexical support in an interactive tool that aims to intuitively visualise semantic context. The experience of partners in producing the vocabulary mappings is discussed in light of previous work in this area. Reflections on lessons learned both for the immediate project and for vocabulary mapping in general contribute to the conclusions. Future search functionality could take account of available vocabulary mappings via a range of search options, such as query expansion including compound mappings and mapping types. Further work on mapping guidelines and metadata is recommended. Â© The Author(s) 2024.","This article reports and reflects on vocabulary mapping techniques, tools and experience from the ARIADNE European archaeological infrastructure projects, where the widely differing terminology of subject indexing in the different partner languages posed significant challenges for effective data integration. The Getty Art & Architecture Thesaurus is employed as a central spine vocabulary for partners to map their native vocabularies and term lists a hub structure enables a multilingual search capability via vocabulary mapping. Mappings are expressed via SKOS mapping relationships and output as structured JSON for use in the overall data aggregation process and in the ARIADNE portal. The approach followed offers some automatic support for final intellectual judgement. The method can be characterised as providing lexical support in an interactive tool that aims to intuitively visualise semantic context. The experience of partners in producing the vocabulary mappings is discussed in light of previous work in this area. Reflections on lessons learned both for the immediate project and for vocabulary mapping in general contribute to the conclusions. Future search functionality could take account of available vocabulary mappings via a range of search options, such as query expansion including compound mappings and mapping types. Further work on mapping guidelines and metadata is recommended."
Identifying artificial intelligenceâgenerated content in online Q&A communities through interpretable machine learning,"This study aims to construct a comprehensive feature system for identifying artificial intelligenceâgenerated content (AIGC) in online Q&A communities, thus uncovering the key factors and mechanisms influencing the identification of AIGC. First, based on the theory of systemic functional linguistics (SFL) and information quality (IQ), this article extracts vocabulary, content, structure, and emotional features from the text, and identifies the AIGC through nine mainstream machine learning algorithms. Subsequently, three widely used resampling strategies are exploited to address the category imbalance problem. The grid search optimisation algorithm fine-tunes different combinations of parameters to improve the performance of the identification classifier. Finally, SHAP values are introduced to evaluate and elucidate the global feature importance and feature influence mechanism. A Chinese corpus from the Zhihu Q&A community is constructed to verify the validity of these methods. The experimental results show that the eXtreme Gradient Boosting (XGBoost) model optimised with hybrid sampling and grid search parameters exhibits excellent performance in identifying AI-generated text, which achieves an F1-score of 0.9935, an improvement of 0.11 percentage points over the original model. In addition, all four dimensions of features constructed in this article contribute to AI-generated text identification, and the results of feature interpretability analysis show the greatest impact of features that focus on content readability. The study facilitates the identification and labelling of AIGC in online Q&A communities, thereby enhancing transparency and accountability of information shared online. Â© The Author(s) 2024.","This study aims to construct a comprehensive feature system for identifying artificial intelligencegenerated content (AIGC) in online Q&A communities, thus uncovering the key factors and mechanisms influencing the identification of AIGC. First, based on the theory of systemic functional linguistics (SFL) and information quality (IQ), this article extracts vocabulary, content, structure, and emotional features from the text, and identifies the AIGC through nine mainstream machine learning algorithms. Subsequently, three widely used resampling strategies are exploited to address the category imbalance problem. The grid search optimisation algorithm fine-tunes different combinations of parameters to improve the performance of the identification classifier. Finally, SHAP values are introduced to evaluate and elucidate the global feature importance and feature influence mechanism. A Chinese corpus from the Zhihu Q&A community is constructed to verify the validity of these methods. The experimental results show that the eXtreme Gradient Boosting (XGBoost) model optimised with hybrid sampling and grid search parameters exhibits excellent performance in identifying AI-generated text, which achieves an F1-score of 0.9935, an improvement of 0.11 percentage points over the original model. In addition, all four dimensions of features constructed in this article contribute to AI-generated text identification, and the results of feature interpretability analysis show the greatest impact of features that focus on content readability. The study facilitates the identification and labelling of AIGC in online Q&A communities, thereby enhancing transparency and accountability of information shared online."
A new method of calculating the disruption index based on open citation data,"This article discusses the method of calculating the disruption index (D index) based on COCI (the OpenCitations Index of Crossref open DOI-to-DOI citations), breaks through the difficulties brought by the acquisition of massive citation data and verifies the reliability of the method of calculating the disruption index based on open citation data based on empirical research. Through empirical research, we found that (1) there is little difference in the number of citation data of focus papers in Web of Science (WoS) and COCI; (2) the levels of disruptive innovation of the papers calculated based on the WoS and COCI are significantly strongly correlated; (3) among the D index and related extended indicators calculated based on COCI, (Formula presented.) has the strongest correlation with peer-review indicators, which is consistent with the calculation results based on WoS. Given the broad disciplinary coverage of COCI, although it has not yet been able to fully replace the function of commercial citation databases in research assessment, it can undoubtedly serve as an important source of citation data for further the disruption index and other scientometrics research thereafter. Â© The Author(s) 2024.","This article discusses the method of calculating the disruption index (D index) based on COCI (the OpenCitations Index of Crossref open DOI-to-DOI citations), breaks through the difficulties brought by the acquisition of massive citation data and verifies the reliability of the method of calculating the disruption index based on open citation data based on empirical research. Through empirical research, we found that there is little difference in the number of citation data of focus papers in Web of Science (WoS) and COCI; the levels of disruptive innovation of the papers calculated based on the WoS and COCI are significantly strongly correlated; among the D index and related extended indicators calculated based on COCI, (Formula presented.) has the strongest correlation with peer-review indicators, which is consistent with the calculation results based on WoS. Given the broad disciplinary coverage of COCI, although it has not yet been able to fully replace the function of commercial citation databases in research assessment, it can undoubtedly serve as an important source of citation data for further the disruption index and other scientometrics research thereafter."
"How could the Library and Information Studies curriculum better prepare graduates to address equity, diversity and inclusion issues in their workplace?","Equity, diversity and inclusion (EDI) practices in the library and information professions can be linked to the curriculum of the professional qualification, which plays an important role in preparing students for practice. The aim of this small, non-generalisable survey of recent graduates at one UK library school, a collaboration between two academic staff and two current and recent students, was to identify how the curriculum could better prepare graduates to address EDI issues in their workplace. Approaches for cultivating effective pedagogical strategies included the importance of recognising and exploring personal identity; group work and community building and embedding an EDI ethos, approach and method within the curriculum. Important gaps relating to the preparation of students for EDI practices that were noted included management and leadership; fostering learner positionality and addressing the broad scope of EDI work including all protected and other characteristics, alongside tensions between individual and structural approaches to change. Â© The Author(s) 2024.","Equity, diversity and inclusion (EDI) practices in the library and information professions can be linked to the curriculum of the professional qualification, which plays an important role in preparing students for practice. The aim of this small, non-generalisable survey of recent graduates at one UK library school, a collaboration between two academic staff and two current and recent students, was to identify how the curriculum could better prepare graduates to address EDI issues in their workplace. Approaches for cultivating effective pedagogical strategies included the importance of recognising and exploring personal identity; group work and community building and embedding an EDI ethos, approach and method within the curriculum. Important gaps relating to the preparation of students for EDI practices that were noted included management and leadership; fostering learner positionality and addressing the broad scope of EDI work including all protected and other characteristics, alongside tensions between individual and structural approaches to change."
"Erroneous concepts of prominent scientists: C. F. WeizsÃ¤cker, J. A. Wheeler, S. Wolfram, S. Lloyd, J. Schmidhuber, and M. Vopson, resulting from misunderstanding of information and complexity","The common use of Shannonâs information, specified for the needs of telecommunications, gives rise to many misunderstandings outside of this context. (e.g. in conceptions of such well-known theorists as C.F. WeizsÃ¤cker and J. A. Wheeler). This article shows that the terms of the general definition of information meet the structural information, and Shannonâs information is a special case of it. Similarly, complexity is misunderstood today as exemplified by the concepts of reputable computer scientists, such as S. Lloyd, S. Wolfram and J. Schmidhuber. These theorists use an algorithmic definition of complexity and the so-called logical depth, neither of which meets the intuitive criterion of complexity. Hence, their misconceptions of beauty, art and the universe. It will be shown that the intuitive criterion is met by Abstract Complexity Definition. This definition also fulfils the criterion for a general complexity definition, as it defines complexity of the most general/abstract structure of our reality, that is, binary structure. It also explains such fundamental issues of information theory as the informationâenergy relationship and the value of information, which are still discussed and need to be clarified. Â© The Author(s) 2024.","The common use of Shannons information, specified for the needs of telecommunications, gives rise to many misunderstandings outside of this context. ( in conceptions of such well-known theorists as Weizscker and Wheeler). This article shows that the terms of the general definition of information meet the structural information, and Shannons information is a special case of it. Similarly, complexity is misunderstood today as exemplified by the concepts of reputable computer scientists, such as Lloyd, Wolfram and Schmidhuber. These theorists use an algorithmic definition of complexity and the so-called logical depth, neither of which meets the intuitive criterion of complexity. Hence, their misconceptions of beauty, art and the universe. It will be shown that the intuitive criterion is met by Abstract Complexity Definition. This definition also fulfils the criterion for a general complexity definition, as it defines complexity of the most general/abstract structure of our reality, that is, binary structure. It also explains such fundamental issues of information theory as the informationenergy relationship and the value of information, which are still discussed and need to be clarified."
Agent-based model: A method worthy of promotion in Library and Information Science,"Agent-based model (ABM) is a branch of artificial intelligence. Its specialty is to construct a complex macro-system model by describing the perception, decision, learning and action of micro-agents. This method is widely used in many fields from natural science to social science. We discuss ABM by collecting relevant academic papers which apply to the field of Library and Information Science (LIS). This article systematically reviews how ABM is applied to the LIS field and argues that ABM can provide an exploratory tool with quantifiability, repeatability, interpretability, contingency, adaptability and other types of advantages. Finally, it is pointed out that this method is a research tool worthy of careful exploration. Â© The Author(s) 2022.","Agent-based model (ABM) is a branch of artificial intelligence. Its specialty is to construct a complex macro-system model by describing the perception, decision, learning and action of micro-agents. This method is widely used in many fields from natural science to social science. We discuss ABM by collecting relevant academic papers which apply to the field of Library and Information Science (LIS). This article systematically reviews how ABM is applied to the LIS field and argues that ABM can provide an exploratory tool with quantifiability, repeatability, interpretability, contingency, adaptability and other types of advantages. Finally, it is pointed out that this method is a research tool worthy of careful exploration."
Detecting agro: Korean trolling and clickbaiting behaviour in online environments,"This article presents one of the first approaches to provide the understanding of agro (one of the unique eye-attracting cues) headlines and thumbnails in online video sharing platform, YouTube. We annotated 1881 headlines and thumbnails, based on agro and the type of agro. Then, we experimented with machine learning models to classify agro data from the non-agro data. With a bidirectional long short-term memory (Bi-LSTM) model, we achieved 84.35% of accuracy in detecting agro headlines and 82.80% of accuracy in detecting agro thumbnails. We believe that the automatic detection of agro headlines can allow users to have better experience in browsing through and getting the content that they want online. Â© The Author(s) 2022.","This article presents one of the first approaches to provide the understanding of agro (one of the unique eye-attracting cues) headlines and thumbnails in online video sharing platform, YouTube. We annotated 1881 headlines and thumbnails, based on agro and the type of agro. Then, we experimented with machine learning models to classify agro data from the non-agro data. With a bidirectional long short-term memory (Bi-LSTM) model, we achieved 84.35% of accuracy in detecting agro headlines and 82.80% of accuracy in detecting agro thumbnails. We believe that the automatic detection of agro headlines can allow users to have better experience in browsing through and getting the content that they want online."
Exploiting tweet sentiments in altmetrics large-scale data,"This article aims to exploit social exchanges on scientific literature, specifically tweets, to analyse social media usersâ sentiments towards publications within a research field. First, we employ the SentiStrength tool, extended with newly created lexicon terms, to classify the sentiments of 6,482,260 tweets associated with 1,083,535 publications provided by Altmetric.com. Then, we propose harmonic means-based statistical measures to generate a specialised lexicon, using positive and negative sentiment scores and frequency metrics. Next, we adopt a novel article-level summarisation approach to domain-level sentiment analysis to gauge the opinion of social media users on Twitter about the scientific literature. Last, we propose and employ an aspect-based analytical approach to mine usersâ expressions relating to various aspects of the article, such as tweets on its title, abstract, methodology, conclusion or results section. We show that research communities exhibit dissimilar sentiments towards their respective fields. The analysis of the field-wise distribution of article aspects shows that in Medicine, Economics, Business and Decision Sciences, tweet aspects are focused on the results section. In contrast, in Physics and Astronomy, Materials Sciences and Computer Science, these aspects are focused on the methodology section. Overall, the study helps us to understand the sentiments of online social exchanges of the scientific community on scientific literature. Specifically, such a fine-grained analysis may help research communities in improving their social media exchanges about the scientific articles to disseminate their scientific findings effectively and to further increase their societal impact. Â© The Author(s) 2022.","This article aims to exploit social exchanges on scientific literature, specifically tweets, to analyse social media users sentiments towards publications within a research field. First, we employ the SentiStrength tool, extended with newly created lexicon terms, to classify the sentiments of 6,482,260 tweets associated with 1,083,535 publications provided by Altmetric.com. Then, we propose harmonic means-based statistical measures to generate a specialised lexicon, using positive and negative sentiment scores and frequency metrics. Next, we adopt a novel article-level summarisation approach to domain-level sentiment analysis to gauge the opinion of social media users on Twitter about the scientific literature. Last, we propose and employ an aspect-based analytical approach to mine users expressions relating to various aspects of the article, such as tweets on its title, abstract, methodology, conclusion or results section. We show that research communities exhibit dissimilar sentiments towards their respective fields. The analysis of the field-wise distribution of article aspects shows that in Medicine, Economics, Business and Decision Sciences, tweet aspects are focused on the results section. In contrast, in Physics and Astronomy, Materials Sciences and Computer Science, these aspects are focused on the methodology section. Overall, the study helps us to understand the sentiments of online social exchanges of the scientific community on scientific literature. Specifically, such a fine-grained analysis may help research communities in improving their social media exchanges about the scientific articles to disseminate their scientific findings effectively and to further increase their societal impact."
One-step multi-view clustering via deep-level semantics exploiting,"Multi-view clustering (MVC) has gained promising performance improvement compared with traditional signal-view clustering due to the complementary information of multiple views. However, existing MVC methods exploit clustering structure by utilising signal-layer mapping, such that they cannot exploit the underlying deep-level semantic information in complex and interleaved multi-view data. Moreover, existing methods usually conduct multi-view fusion and clustering separately, which results in unpromising performance. To address the above problems, one-step MVC via deep-level semantics exploiting (DLSE) is proposed to exploit deep-level semantic information and learn the indicator matrix using a one-step manner. To be specific, a novel deep matrix factorisation (DMF) paradigm is designed to exploit the hierarchical semantics via a layer-wise scheme, so that samples from the same clusters are forced to be closer in the low-dimensional space layer by layer. Furthermore, to make the learned representation preserve the local geometric structure of data, DLSE introduces a local preservation regularisation to guide DMF. Meanwhile, by employing spectral rotating fusion, the cluster indicator can be obtained directly. Extensive experiments demonstrate the superiority of DLSE in contrast with some state-of-the-art methods. Â© The Author(s) 2024.","Multi-view clustering has gained promising performance improvement compared with traditional signal-view clustering due to the complementary information of multiple views. However, existing MVC methods exploit clustering structure by utilising signal-layer mapping, such that they cannot exploit the underlying deep-level semantic information in complex and interleaved multi-view data. Moreover, existing methods usually conduct multi-view fusion and clustering separately, which results in unpromising performance. To address the above problems, one-step MVC via deep-level semantics exploiting (DLSE) is proposed to exploit deep-level semantic information and learn the indicator matrix using a one-step manner. To be specific, a novel deep matrix factorisation (DMF) paradigm is designed to exploit the hierarchical semantics via a layer-wise scheme, so that samples from the same clusters are forced to be closer in the low-dimensional space layer by layer. Furthermore, to make the learned representation preserve the local geometric structure of data, DLSE introduces a local preservation regularisation to guide DMF. Meanwhile, by employing spectral rotating fusion, the cluster indicator can be obtained directly. Extensive experiments demonstrate the superiority of DLSE in contrast with some state-of-the-art methods."
Axiology of Covid-19 as a linguistic phenomenon,"This workâs aim was to investigate what verbal means are used by English-speaking Twitter accounts to describe the pandemic while focusing on extralinguistic factors that are the primary catalysts for linguistic transformations in society. A critical discourse analysis of the lexeme âCovid-19â and words accompanying it was applied. A total of 1736 English-language tweets (6844 lexical units) posted during March to April 2020 were selected for the analysis. Functional discourse analysis allowed systematising and commenting on sampling results as well as provided the opportunity to make the following conclusions. In tweets, the lexeme âCovid-19â is combined not only with the actual name of the virus. This lexeme became a productive ground for derivation into various linguistic structures: substantive word combinations, abbreviations, neologisms and anthropomorphic metaphors. The research results application in international practice will allow linguists to interpret neologisms that emerged as a result of the pandemic and foster the understanding of axiological indicators of native speakers. Â© The Author(s) 2022.","This works aim was to investigate what verbal means are used by English-speaking Twitter accounts to describe the pandemic while focusing on extralinguistic factors that are the primary catalysts for linguistic transformations in society. A critical discourse analysis of the lexeme Covid-19 and words accompanying it was applied. A total of 1736 English-language tweets (6844 lexical units) posted during March to April 2020 were selected for the analysis. Functional discourse analysis allowed systematising and commenting on sampling results as well as provided the opportunity to make the following conclusions. In tweets, the lexeme Covid-19 is combined not only with the actual name of the virus. This lexeme became a productive ground for derivation into various linguistic structures: substantive word combinations, abbreviations, neologisms and anthropomorphic metaphors. The research results application in international practice will allow linguists to interpret neologisms that emerged as a result of the pandemic and foster the understanding of axiological indicators of native speakers."
Reuse of the transparency-related information posted on Spanish library and archive websites,"This study addresses the types of formats and ease of reuse of transparency-related information available on the websites of 53 national public libraries and 53 provincial historic archives. Further to Spainâs Transparency Act, reuse of public sector information is one of the elements comprising the right of access to information. Access and use must consequently be ensured to enable citizens and businesses to reuse all available data. The working methodology deployed here consisted in searching for, identifying and analysing the transparency-related documents carried on library and archive websites and the legal warnings governing their reuse. The findings revealed a wide variety of formats and rules governing reuse and indications of scant interest in these institutions in fostering the transparency and reuse of public information. Even when available, reusable information was normally found to be posted either separately from the data furnished by libraries and archives directly or positioned on pages or sections with complex access paths. Â© The Author(s) 2021.","This study addresses the types of formats and ease of reuse of transparency-related information available on the websites of 53 national public libraries and 53 provincial historic archives. Further to Spains Transparency Act, reuse of public sector information is one of the elements comprising the right of access to information. Access and use must consequently be ensured to enable citizens and businesses to reuse all available data. The working methodology deployed here consisted in searching for, identifying and analysing the transparency-related documents carried on library and archive websites and the legal warnings governing their reuse. The findings revealed a wide variety of formats and rules governing reuse and indications of scant interest in these institutions in fostering the transparency and reuse of public information. Even when available, reusable information was normally found to be posted either separately from the data furnished by libraries and archives directly or positioned on pages or sections with complex access paths."
#fridaysforfuture â What does Instagram tell us about a social movement?,"Understanding social movement structures is important for political decision-makers to enable them to recognise the various motivating factors behind these movements. The Fridays for Future movement characterises a political group that has a majority of young people, frequently using social media to organise actions. By conducting a social network analysis on hashtags, this study contributes to the understanding of the global Fridays for Future movement. Particularly, we focus on the use and connection of hashtags on Instagram. We collected 59,112 posts tagged with #fridaysforfuture and analysed 91,172 hashtags used therein. Subsequently, the 140 most used hashtags were divided into 11 clusters, which provide not only information about the organisation of the social movement via social media, but also insights into lifestyle-related aspects. The clusters include the topics: climate; nutrition, lifestyle and health; memes; cycling; art; sustainable consumption and the Earth Day. The article shows that the motives of the Fridays for Future movement are broad. We can demonstrate that Fridays for Future is connected to other social movements and gain insights into the everyday life of the Fridays for Future stakeholders. Â© The Author(s) 2022.","Understanding social movement structures is important for political decision-makers to enable them to recognise the various motivating factors behind these movements. The Fridays for Future movement characterises a political group that has a majority of young people, frequently using social media to organise actions. By conducting a social network analysis on hashtags, this study contributes to the understanding of the global Fridays for Future movement. Particularly, we focus on the use and connection of hashtags on Instagram. We collected 59,112 posts tagged with #fridaysforfuture and analysed 91,172 hashtags used therein. Subsequently, the 140 most used hashtags were divided into 11 clusters, which provide not only information about the organisation of the social movement via social media, but also insights into lifestyle-related aspects. The clusters include the topics: climate; nutrition, lifestyle and health; memes; cycling; art; sustainable consumption and the Earth Day. The article shows that the motives of the Fridays for Future movement are broad. We can demonstrate that Fridays for Future is connected to other social movements and gain insights into the everyday life of the Fridays for Future stakeholders."
Scientistsâ behaviour towards information disorder: A systematic review,"How are scientists coping with misinformation and disinformation? Focusing on the triangle scientists/mis-disinformation/behaviour, this study aims to systematically review the literature to answer three research questions: What are the main approaches described in the literature concerning scientistsâ behaviour towards mis-disinformation? Which techniques or strategies are discussed to tackle information disorder? Is there a research gap in including scientists as subjects of research projects concerning information disorder tackling strategies? Following PRISMA 2020 statement, a checklist and flow diagram for reporting systematic reviews, a set of 14 documents was analysed. Findings revealed that the literature might be interpreted following Wilson and Maceviciuteâs model as creation, acceptance and dissemination categories. Crossing over these categories, we advanced three standing points to analyse scientistsâ positions towards mis-disinformation: inside, inside-out and outside-in. The stage âCreation/facilitationâ was the least present in our sample, but âUse/rejection/acceptanceâ and âDisseminationâ were depicted in the literature retrieved. Most of the literature approaches were about inside-out perspectives, meaning that the topic is mainly studied concerning communication issues. Regarding the strategies against the information disorder, findings suggest that preventive and reactive strategies are simultaneously used. A strong appeal to a multidisciplinary effort against mis-disinformation is widely present, but there is a gap in including scientists as subjects of research projects. Â© The Author(s) 2024.","How are scientists coping with misinformation and disinformation? Focusing on the triangle scientists/mis-disinformation/behaviour, this study aims to systematically review the literature to answer three research questions: What are the main approaches described in the literature concerning scientists behaviour towards mis-disinformation? Which techniques or strategies are discussed to tackle information disorder? Is there a research gap in including scientists as subjects of research projects concerning information disorder tackling strategies? Following PRISMA 2020 statement, a checklist and flow diagram for reporting systematic reviews, a set of 14 documents was analysed. Findings revealed that the literature might be interpreted following Wilson and Maceviciutes model as creation, acceptance and dissemination categories. Crossing over these categories, we advanced three standing points to analyse scientists positions towards mis-disinformation: inside, inside-out and outside-in. The stage Creation/facilitation was the least present in our sample, but Use/rejection/acceptance and Dissemination were depicted in the literature retrieved. Most of the literature approaches were about inside-out perspectives, meaning that the topic is mainly studied concerning communication issues. Regarding the strategies against the information disorder, findings suggest that preventive and reactive strategies are simultaneously used. A strong appeal to a multidisciplinary effort against mis-disinformation is widely present, but there is a gap in including scientists as subjects of research projects."
Knowledge diffusion trajectories of PageRank: A main path analysis,"In the era of the Internet and big data, the PageRank (PR) algorithm is a constantly evolving research field. However, there is no systematic research to explore the overall development trend of the PR domain. This article evaluates 1446 articles related to the PR algorithm and provides a thorough understanding of the PR field through the main path analysis (MPA). Through two basic main paths, a number of papers that play a leading role have been identified, which outline the backbone of the PR domain. Based on the analysis of multiple main paths, four main subareas have been investigated. There are accelerating the computation of PR, comprehensive applications of PR, researches on academic impact assessment and age preference in network evolution. Finally, this article discusses the research findings and the future directions of the PR field. It is the first attempt to identify the development trend of the PR domain through MPA, thus providing an insight into the knowledge evolution of the PR field over the past two decades. Â© The Author(s) 2023.","In the era of the Internet and big data, the PageRank (PR) algorithm is a constantly evolving research field. However, there is no systematic research to explore the overall development trend of the PR domain. This article evaluates 1446 articles related to the PR algorithm and provides a thorough understanding of the PR field through the main path analysis (MPA). Through two basic main paths, a number of papers that play a leading role have been identified, which outline the backbone of the PR domain. Based on the analysis of multiple main paths, four main subareas have been investigated. There are accelerating the computation of PR, comprehensive applications of PR, researches on academic impact assessment and age preference in network evolution. Finally, this article discusses the research findings and the future directions of the PR field. It is the first attempt to identify the development trend of the PR domain through MPA, thus providing an insight into the knowledge evolution of the PR field over the past two decades."
TRSAv1: A new benchmark dataset for classifying user reviews on Turkish e-commerce websites,"The amount of data produced significantly increased with the development of Internet technologies. Accordingly, the importance of natural language processing studies increased, and this topic became one of the most studied artificial intelligence subjects. Even though it is a popular topic that is widely studied on, not enough studies have been conducted on the Turkish language. Even the studies conducted in Turkey are primarily on English and other natural languages instead of Turkish. The lack of a Turkish dataset is the most crucial reason for the lack of studies. Therefore, to create a solution, user reviews on e-commerce websites were collected and labelled reviews as positive, negative and neutral, and a new and unique dataset consisting of 150,000 reviews was created. This dataset was named TRSAv1, which was publicly shared with the researchers will contribute to the Turkish natural language processing studies; however, the effect of different word representation methods on algorithm performance was examined in detail, and the results were compared. Â© The Author(s) 2022.","The amount of data produced significantly increased with the development of Internet technologies. Accordingly, the importance of natural language processing studies increased, and this topic became one of the most studied artificial intelligence subjects. Even though it is a popular topic that is widely studied on, not enough studies have been conducted on the Turkish language. Even the studies conducted in Turkey are primarily on English and other natural languages instead of Turkish. The lack of a Turkish dataset is the most crucial reason for the lack of studies. Therefore, to create a solution, user reviews on e-commerce websites were collected and labelled reviews as positive, negative and neutral, and a new and unique dataset consisting of 150,000 reviews was created. This dataset was named TRSAv1, which was publicly shared with the researchers will contribute to the Turkish natural language processing studies; however, the effect of different word representation methods on algorithm performance was examined in detail, and the results were compared."
Journal of Information Science: A gender-based bibliometric study (2015â2020),"Scientific publication is one of the main channels for disseminating research results and one of the most important means of determining the presence of women in scientific research. This article aims to conduct a bibliometric analysis in the Library and Information Science (LIS) field, from a gender perspective, by analysing Journal of Information Science (2015â2020). To reach this goal, the research has been developed in several stages (data collection, gender authorsâ identification, validation of authorities, contact by email and analysis of results) to identify 326 contributions and 697 authors finally. Analysis patterns showed outcomes on gender (single and multiple authors), scientific collaboration, authorship time-course, authorship productivity as well as institutional and geographic affiliation. Some conclusions show that male and female authors are not equally represented in the journal, with a great difference in the case of collective authorship. Overall, there is a clear trend of single and multiple male authorship. Â© The Author(s) 2022.","Scientific publication is one of the main channels for disseminating research results and one of the most important means of determining the presence of women in scientific research. This article aims to conduct a bibliometric analysis in the Library and Information Science (LIS) field, from a gender perspective, by analysing Journal of Information Science (20152020). To reach this goal, the research has been developed in several stages (data collection, gender authors identification, validation of authorities, contact by email and analysis of results) to identify 326 contributions and 697 authors finally. Analysis patterns showed outcomes on gender (single and multiple authors), scientific collaboration, authorship time-course, authorship productivity as well as institutional and geographic affiliation. Some conclusions show that male and female authors are not equally represented in the journal, with a great difference in the case of collective authorship. Overall, there is a clear trend of single and multiple male authorship."
Assessing the impact of health information orientation and health information literacy on patientsâ engagement with health information,"Health information engagement can help individuals to find and use reliable sources of health information to make informed decisions about their health. This helps to improve their health outcomes and prevent unnecessary healthcare costs. Drawing upon the cognitive behavioural theory, this pilot study postulated a model to understand that the consequences of information orientation in terms of information engagement (behaviour), information literacy (cognition) and information avoidance (behaviour) in post-COVID era under health context. Furthermore, the moderation effects of health information literacy (HIL) are also calculated in managing health information avoidance beahvior. This pilot study is conducted in the context of social media exposure to health information by diabetic patients in Pakistani community. The proposed model was tested using Partial Lease Square Structural Equational Modelling (PLS-SEM). The data were collected from 166 diabetic patients (active social media users) through a survey. The study findings suggest that health information orientation on social media leads to HIL and engagement. Whereas, it has significant negative impact towards health information avoidance behaviour. Furthermore, HIL significantly increases health information engagement of diabetic patients. Also, HIL moderates the relationship between health information orientation and information engagement positively, whereas between health information orientation and health information avoidance negatively. Â© The Author(s) 2024.","Health information engagement can help individuals to find and use reliable sources of health information to make informed decisions about their health. This helps to improve their health outcomes and prevent unnecessary healthcare costs. Drawing upon the cognitive behavioural theory, this pilot study postulated a model to understand that the consequences of information orientation in terms of information engagement (behaviour), information literacy (cognition) and information avoidance (behaviour) in post-COVID era under health context. Furthermore, the moderation effects of health information literacy (HIL) are also calculated in managing health information avoidance beahvior. This pilot study is conducted in the context of social media exposure to health information by diabetic patients in Pakistani community. The proposed model was tested using Partial Lease Square Structural Equational Modelling (PLS-SEM). The data were collected from 166 diabetic patients (active social media users) through a survey. The study findings suggest that health information orientation on social media leads to HIL and engagement. Whereas, it has significant negative impact towards health information avoidance behaviour. Furthermore, HIL significantly increases health information engagement of diabetic patients. Also, HIL moderates the relationship between health information orientation and information engagement positively, whereas between health information orientation and health information avoidance negatively."
A comprehensive bibliometric analysis on opinion mining and sentiment analysis global research output,"The rise of the Internet and social media (i.e. reviews, forum discussions, blogs and social networks) constituted an interesting source to detect user opinion trends. This study examines the global publication output on opinion mining and sentiment analysis from documents published in 2000 to 2020. Bibliometric indicators on the trends, most cited papers, authors, institutions, countries, funding agencies and research subject areas were independently screened and analysed using bibliometrix package in R. A total of 7603 eligible documents were identified from 2000 to 2020. The total number of citations for all publications was 129,251, with an average of 17.0 citations per publication. About 14,629 authors wrote those documents with 1.93 authors per document and a collaboration index of 1.98. The most prolific author was Cambria Erik, with 47 publications and h-index of 42. The leading countries for research were China with n = 824, India with n = 576 and the United States with n = 244 publications. Lecture Notes in Computer Science proceedings was the top-ranked venue for publications with n = 434, h-index of 32 and 4598 total citation scores. National Natural Science Foundation of China was the top-ranked funding agency for research, and most of the publications were computer science (n = 6320) documents. The study provides an in-depth assessment of the landmark of the hot research topic and acknowledges the contribution of the most productive and active authors across different countries in the world. In addition, the findings could support the younger scholars in their future research direction and improve the efficiency in potential future research collaborations and projects. Â© The Author(s) 2021.","The rise of the Internet and social media ( reviews, forum discussions, blogs and social networks) constituted an interesting source to detect user opinion trends. This study examines the global publication output on opinion mining and sentiment analysis from documents published in 2000 to 2020. Bibliometric indicators on the trends, most cited papers, authors, institutions, countries, funding agencies and research subject areas were independently screened and analysed using bibliometrix package in A total of 7603 eligible documents were identified from 2000 to 2020. The total number of citations for all publications was 129,251, with an average of 17.0 citations per publication. About 14,629 authors wrote those documents with 1.93 authors per document and a collaboration index of 1.98. The most prolific author was Cambria Erik, with 47 publications and h-index of 42. The leading countries for research were China with n = 824, India with n = 576 and the United States with n = 244 publications. Lecture Notes in Computer Science proceedings was the top-ranked venue for publications with n = 434, h-index of 32 and 4598 total citation scores. National Natural Science Foundation of China was the top-ranked funding agency for research, and most of the publications were computer science (n = 6320) documents. The study provides an in-depth assessment of the landmark of the hot research topic and acknowledges the contribution of the most productive and active authors across different countries in the world. In addition, the findings could support the younger scholars in their future research direction and improve the efficiency in potential future research collaborations and projects."
Peer Review Metrics and their influence on the Journal Impact,"The manuscript processing timeline, a necessary facet of the publishing process, varies from journal to journal, and its influence on the journal impact needs to be studied. The current research looks into the correlation between the âPeer Review Metricsâ (submission to first editorial decision; submission to first post-review decision and submission to accept) and the âJournal Impact Dataâ (2-year Impact Factor; 5-year Impact Factor; Immediacy Index; Eigenfactor Score and Article Influence Score). The data related to âPeer Review Metricsâ (submission to first editorial decision; submission to first post-review decision and submission to accept) and âJournal Impact Dataâ (2-year Impact Factor; 5-year Impact Factor; Immediacy Index; Eigenfactor Score and Article Influence Score) were downloaded from the âNature Researchâ journals website https://www.nature.com/nature-portfolio/about/journal-metrics. Accordingly, correlations were drawn between the âPeer Review Metricsâ and the âJournal Impact Dataâ. If the time from âsubmission to first editorial decisionâ decreases, the âJournal Impact Dataâ increases and vice versa. However, an increase or decrease in the time from âsubmission to first editorial decisionâ does not affect the âEigenfactor Scoreâ of the journal and vice versa. An increase or decrease in the time from âsubmission to first post-review decisionâ does not affect any âJournal Impact Dataâ and vice versa. If the time from âsubmission to acceptanceâ increases, the âJournal Impact Dataâ (2-year Impact Factor, 5-year Impact Factor, Immediacy Index and Article Influence Score) also increases, and if the time from âsubmission to acceptanceâ decreases, so will the âJournal Impact Dataâ. However, an increase or decrease in the time from âsubmission to acceptanceâ does not affect the âEigenfactor Scoreâ of the journal and vice versa. The study will act as a ready reference tool for the scholars to select the most appropriate submitting platforms for their scholarly endeavours. Furthermore, the performance and evaluative indicators responsible for a journalâs overall research performance can also be understood from a micro-analytical view, which will help the researchers select appropriate journals for their future scholarly submissions. Lengthy publication timelines are a big problem for the researchers because they are not able to get the credit for their research on time. Since the study validates a relationship between the âPeer Review Metricsâ and âJournal Impact Dataâ, the findings will be of great help in making an appropriate journalâs choice. The study can be an eye opener for the journal administrators who vocalise a speed-up publication process by enhancing certain areas of publication timeline. The study is the first of its kind that correlates the âPeer Review Metricsâ of the journals and the âJournal Impact Dataâ. The studyâs findings are limited to the data retrieved from the âNature Researchâ journals and cannot be generalised to the full score of journals. The study can be extended across other publishers to generalise the findings. Even the articlesâ early access availability concerning âPeer Review Metricsâ of the journals and the âJournal Impact Dataâ can be studied. Â© The Author(s) 2021.","The manuscript processing timeline, a necessary facet of the publishing process, varies from journal to journal, and its influence on the journal impact needs to be studied. The current research looks into the correlation between the Peer Review Metrics (submission to first editorial decision; submission to first post-review decision and submission to accept) and the Journal Impact Data (2-year Impact Factor; 5-year Impact Factor; Immediacy Index; Eigenfactor Score and Article Influence Score). The data related to Peer Review Metrics (submission to first editorial decision; submission to first post-review decision and submission to accept) and Journal Impact Data (2-year Impact Factor; 5-year Impact Factor; Immediacy Index; Eigenfactor Score and Article Influence Score) were downloaded from the Nature Research journals website https://www.nature.com/nature-portfolio/about/journal-metrics. Accordingly, correlations were drawn between the Peer Review Metrics and the Journal Impact Data. If the time from submission to first editorial decision decreases, the Journal Impact Data increases and vice versa. However, an increase or decrease in the time from submission to first editorial decision does not affect the Eigenfactor Score of the journal and vice versa. An increase or decrease in the time from submission to first post-review decision does not affect any Journal Impact Data and vice versa. If the time from submission to acceptance increases, the Journal Impact Data (2-year Impact Factor, 5-year Impact Factor, Immediacy Index and Article Influence Score) also increases, and if the time from submission to acceptance decreases, so will the Journal Impact Data. However, an increase or decrease in the time from submission to acceptance does not affect the Eigenfactor Score of the journal and vice versa. The study will act as a ready reference tool for the scholars to select the most appropriate submitting platforms for their scholarly endeavours. Furthermore, the performance and evaluative indicators responsible for a journals overall research performance can also be understood from a micro-analytical view, which will help the researchers select appropriate journals for their future scholarly submissions. Lengthy publication timelines are a big problem for the researchers because they are not able to get the credit for their research on time. Since the study validates a relationship between the Peer Review Metrics and Journal Impact Data, the findings will be of great help in making an appropriate journals choice. The study can be an eye opener for the journal administrators who vocalise a speed-up publication process by enhancing certain areas of publication timeline. The study is the first of its kind that correlates the Peer Review Metrics of the journals and the Journal Impact Data. The studys findings are limited to the data retrieved from the Nature Research journals and cannot be generalised to the full score of journals. The study can be extended across other publishers to generalise the findings. Even the articles early access availability concerning Peer Review Metrics of the journals and the Journal Impact Data can be studied."
Social capital and individual motivations for information sharing: A theory of reasoned action perspective,"Based on the theory of reasoned action, this study examined the impact of social capital and individual motivations on information sharing in the context of higher education. The research conducted an online survey of 277 academic technicians in five academic institutions in public university in Iraq. The model was developed using the structural equation modelling technique with AMOS v.27 and conditional hypotheses were tested. The findings suggest that social connection, trust, reciprocity, shared language, vision and a positive attitude towards assisting others influence techniciansâ willingness to share information. It is also shown that attitude and subjective norms significantly affect information-sharing intentions. The results provide insights into understanding the social capital processes and individual motivations that contribute to information sharing among academic technicians in developing countries, particularly Iraq. Therefore, lab managers can implement practical plans to support these factors. Â© The Author(s) 2021.","Based on the theory of reasoned action, this study examined the impact of social capital and individual motivations on information sharing in the context of higher education. The research conducted an online survey of 277 academic technicians in five academic institutions in public university in Iraq. The model was developed using the structural equation modelling technique with AMOS 27 and conditional hypotheses were tested. The findings suggest that social connection, trust, reciprocity, shared language, vision and a positive attitude towards assisting others influence technicians willingness to share information. It is also shown that attitude and subjective norms significantly affect information-sharing intentions. The results provide insights into understanding the social capital processes and individual motivations that contribute to information sharing among academic technicians in developing countries, particularly Iraq. Therefore, lab managers can implement practical plans to support these factors."
Breakthrough potential of emerging research topics based on citation diffusion features,"This article uses the characteristics of citation curves in emerging research topics (ERTs) and combines them with the ERTsâ knowledge bases to draw conclusions by comparing their development patterns. The goal of this study is to enrich the toolset for predicting breakthroughs in scientific research. A set of multidimensional and practical bibliometric indicators is used to identify ERTs, to further identify the knowledge bases of ERTs and construct citation curves for both ERTs and their knowledge bases. The development trends of the citation curves of ERTs and their knowledge bases in different time periods are compared and analysed from two dimensions: knowledge transition and continuous growth. We use the field of stem cell research to test our method. Based on the outcome of the analysis, we can assess the breakthrough potential of ERTs. The stratification, transition and recent changes of the citation curve can be used as a basis for analysing and assessing the ERTsâ breakthrough potential. The combination of different citation diffusion patterns of ERTs and their knowledge bases can improve the effectiveness of identifying ERTs that can become breakthrough innovations. Â© The Author(s) 2021.","This article uses the characteristics of citation curves in emerging research topics (ERTs) and combines them with the ERTs knowledge bases to draw conclusions by comparing their development patterns. The goal of this study is to enrich the toolset for predicting breakthroughs in scientific research. A set of multidimensional and practical bibliometric indicators is used to identify ERTs, to further identify the knowledge bases of ERTs and construct citation curves for both ERTs and their knowledge bases. The development trends of the citation curves of ERTs and their knowledge bases in different time periods are compared and analysed from two dimensions: knowledge transition and continuous growth. We use the field of stem cell research to test our method. Based on the outcome of the analysis, we can assess the breakthrough potential of ERTs. The stratification, transition and recent changes of the citation curve can be used as a basis for analysing and assessing the ERTs breakthrough potential. The combination of different citation diffusion patterns of ERTs and their knowledge bases can improve the effectiveness of identifying ERTs that can become breakthrough innovations."
Detecting multiple coexisting emotions from public emergency opinions,"To detect multiple coexisting emotions from public emergency opinions, this article proposes a novel two-stage multiple coexisting emotion-detection model. First, the text semantic feature extracted through bidirectional encoder representation from transformers (BERT) and the emotion lexicon feature extracted through the emotion dictionary are fused. Then, the emotion subjectivity judgement and multiple coexisting emotion detection are performed in two separate stages. In the first stage, we introduce synthetic minority oversampling technique (SMOTE) to enhance the balance of data distribution and select the optimal classifier to recognise opinion texts with emotion. In the second stage, the label powerset (LP)-SMOTE is proposed to increase the number of the minority category samples, and multichannel emotion classifiers and the decision mechanism are employed to recognise different types of emotions and determine the final coexisting emotion labels. Finally, the Weibo data about coronavirus disease 2019 (COVID-19) are collected to verify the effectiveness of the proposed model. Experiment results indicate that the proposed model outperforms state-of-the-art models, with the F1_macro of 0.8532, the F1_micro of 0.8333, and the hamming loss of 0.0476. The emotion detection results are conducive to decision-making for public emergency departments. Â© The Author(s) 2024.","To detect multiple coexisting emotions from public emergency opinions, this article proposes a novel two-stage multiple coexisting emotion-detection model. First, the text semantic feature extracted through bidirectional encoder representation from transformers (BERT) and the emotion lexicon feature extracted through the emotion dictionary are fused. Then, the emotion subjectivity judgement and multiple coexisting emotion detection are performed in two separate stages. In the first stage, we introduce synthetic minority oversampling technique (SMOTE) to enhance the balance of data distribution and select the optimal classifier to recognise opinion texts with emotion. In the second stage, the label powerset (LP)-SMOTE is proposed to increase the number of the minority category samples, and multichannel emotion classifiers and the decision mechanism are employed to recognise different types of emotions and determine the final coexisting emotion labels. Finally, the Weibo data about coronavirus disease 2019 (COVID-19) are collected to verify the effectiveness of the proposed model. Experiment results indicate that the proposed model outperforms state-of-the-art models, with the F1_macro of 0.8532, the F1_micro of 0.8333, and the hamming loss of 0.0476. The emotion detection results are conducive to decision-making for public emergency departments."
A linguistically asymmetric similarity decision model integrating item tendency for rating predictions,"Neighbourhood-based collaborative filtering (CF) methods typically rely only on user rating information for similarity calculation, without considering linguistic concepts (terms) that reflect user fuzzy preferences. However, in real-world decision-making processes, users often prefer to express their preferences for items linguistically rather than numerically. Inspired by this, we propose a probabilistic linguistic term setâbased item similarity method that transforms absolute ratings into linguistic terms to capture the degree of importance users place on explicit aspects and opinions. Furthermore, we take into account the positive impact of usersâ preferred consistency towards items on similarity results and introduce a Bhattacharyya coefficientâbased item tendency to adjust semantic similarities, enhancing the reliability of predictions. In addition, we account for the asymmetric relation between items when selecting appropriate neighbours to optimise rating predictions. The experiments on two benchmark data sets indicate that our method outperforms existing similarity methods across various evaluation metrics. Specifically, compared with the state-of-the-art method, intuitionistic fuzzy setâbased hybrid similarity model (IFS-HSM), the proposed model improves the performance by at least 2.1% and 1.9%, respectively, within the metrics mean absolute error (MAE) and F1. Moreover, our approach provides a new insight for measuring similarity between items from both qualitative and quantitative perspectives. Â© The Author(s) 2024.","Neighbourhood-based collaborative filtering (CF) methods typically rely only on user rating information for similarity calculation, without considering linguistic concepts (terms) that reflect user fuzzy preferences. However, in real-world decision-making processes, users often prefer to express their preferences for items linguistically rather than numerically. Inspired by this, we propose a probabilistic linguistic term setbased item similarity method that transforms absolute ratings into linguistic terms to capture the degree of importance users place on explicit aspects and opinions. Furthermore, we take into account the positive impact of users preferred consistency towards items on similarity results and introduce a Bhattacharyya coefficientbased item tendency to adjust semantic similarities, enhancing the reliability of predictions. In addition, we account for the asymmetric relation between items when selecting appropriate neighbours to optimise rating predictions. The experiments on two benchmark data sets indicate that our method outperforms existing similarity methods across various evaluation metrics. Specifically, compared with the state-of-the-art method, intuitionistic fuzzy setbased hybrid similarity model (IFS-HSM), the proposed model improves the performance by at least 2.1% and 1.9%, respectively, within the metrics mean absolute error (MAE) and F1. Moreover, our approach provides a new insight for measuring similarity between items from both qualitative and quantitative perspectives."
Towards an agenda for information education and research for sustainable development,"Education for sustainable development (ESD) has been identified by the United Nations Educational, Scientific and Cultural Organization (UNESCO) as a core requirement for achieving success in the UN Sustainable Development Goals (SDGs). Research around data, information and people for achieving success in different SDGs shows how important ESD is. Research also shows that the library and information sector can contribute in many ways to achieve the UN SDGs. Therefore, it is crucial that a strategic approach is taken to embed the concepts of SDGs and their targets and indicators, and the corresponding data and information required to achieve those, within the information science curricula, so that the SDGs form the foundation of information science education, research and professional activities. This article aims to develop a research agenda for education and research in information sciences for promoting and achieving success in different SDGs. First, taking the approach of a metareview, this article shows the trends, as well as challenges, of research and development activities around information for sustainable development. This article demonstrates how the different activities of the LIS (Library and Information Science) sector can be mapped onto some specific targets and indicators of different SDGs, and based on this, it develops an agenda for education and research in information for sustainable development. The research agenda will lead to the development of new information sciences curricula to accommodate the SDGs for training and research in specific LIS activities. This article discusses how the research agenda will also lead to the development of trained professionals in information science for promoting the concepts, and achieving the targets, of the SDGs for a sustainable future. Â© The Author(s) 2024.","Education for sustainable development (ESD) has been identified by the United Nations Educational, Scientific and Cultural Organization (UNESCO) as a core requirement for achieving success in the UN Sustainable Development Goals (SDGs). Research around data, information and people for achieving success in different SDGs shows how important ESD is. Research also shows that the library and information sector can contribute in many ways to achieve the UN SDGs. Therefore, it is crucial that a strategic approach is taken to embed the concepts of SDGs and their targets and indicators, and the corresponding data and information required to achieve those, within the information science curricula, so that the SDGs form the foundation of information science education, research and professional activities. This article aims to develop a research agenda for education and research in information sciences for promoting and achieving success in different SDGs. First, taking the approach of a metareview, this article shows the trends, as well as challenges, of research and development activities around information for sustainable development. This article demonstrates how the different activities of the LIS (Library and Information Science) sector can be mapped onto some specific targets and indicators of different SDGs, and based on this, it develops an agenda for education and research in information for sustainable development. The research agenda will lead to the development of new information sciences curricula to accommodate the SDGs for training and research in specific LIS activities. This article discusses how the research agenda will also lead to the development of trained professionals in information science for promoting the concepts, and achieving the targets, of the SDGs for a sustainable future."
Knowing within multispecies families: An information experience study,"The transition of a companion animal and a human companion into a shared family context is an everyday yet complex process that involves information interactions. Concerned with the cognitive information that resides within humansâ and animalsâ minds, this article aims to explore the knowings (having knowledge or awareness about something) of all multispecies family members. Building upon an information experience approach, the research process consisted of experiential material gathering with multispecies ethnography, followed by phenomenological reflections and writing. Findings are organised into three main sections: animal knowing, human knowing and their engaged knowing. The cognitive information presented in this study is sometimes unconventional, yet innovative within the field of Information Science. the article contributes to the cognitive view of information by showing how diverse information from both humans and animals interweaves to shape a harmonious understanding in everyday life and provides implications for information research, practice and design. Â© The Author(s) 2024.","The transition of a companion animal and a human companion into a shared family context is an everyday yet complex process that involves information interactions. Concerned with the cognitive information that resides within humans and animals minds, this article aims to explore the knowings (having knowledge or awareness about something) of all multispecies family members. Building upon an information experience approach, the research process consisted of experiential material gathering with multispecies ethnography, followed by phenomenological reflections and writing. Findings are organised into three main sections: animal knowing, human knowing and their engaged knowing. The cognitive information presented in this study is sometimes unconventional, yet innovative within the field of Information Science. the article contributes to the cognitive view of information by showing how diverse information from both humans and animals interweaves to shape a harmonious understanding in everyday life and provides implications for information research, practice and design."
"Smart library: Reflections on concepts, aspects and technologies","Recent developments in the information ecosystem and the changes of the knowledge organisations have resulted in a growing tendency towards a new generation of libraries. This study intends to reflect the characteristics, necessities and challenges of smart libraries using the documentary research method. In total, 78 research articles from the top 17 databases were reviewed. A total of 128 concepts were identified in different aspects, such as technology (n = 53), services (n = 36), people (n = 19), management (n = 7), space and place (n = 9), governance (n = 2), and moral and legal matters (n = 2). The characteristics, necessities, reasons, challenges and obstacles of smart libraries are multidimensional, complex and varied. Smart libraries employ various technologies to facilitate the interaction between people and resources and between people and libraries, while also enabling intelligent administration. This work assists researchers, designers and librarians in how to develop and improve smart libraries. Â© The Author(s) 2024.","Recent developments in the information ecosystem and the changes of the knowledge organisations have resulted in a growing tendency towards a new generation of libraries. This study intends to reflect the characteristics, necessities and challenges of smart libraries using the documentary research method. In total, 78 research articles from the top 17 databases were reviewed. A total of 128 concepts were identified in different aspects, such as technology (n = 53), services (n = 36), people (n = 19), management (n = 7), space and place (n = 9), governance (n = 2), and moral and legal matters (n = 2). The characteristics, necessities, reasons, challenges and obstacles of smart libraries are multidimensional, complex and varied. Smart libraries employ various technologies to facilitate the interaction between people and resources and between people and libraries, while also enabling intelligent administration. This work assists researchers, designers and librarians in how to develop and improve smart libraries."
Academic collaboration recommendation based on graph neural network and multi-attribute embedding,"Academic collaboration recommendation (ACR) can help researchers find potential partners for research and thus promote academic innovation. Recent works mostly use graph learning-based methods to explore various ways of combining node information with topology, which consists of multiple steps, including network construction, node feature extraction, network representation learning and link prediction. One limitation is that they only conduct research on co-authorship networks and ignore citation relationship between publications. Besides, they tend to use attributes from a single dimension of researchers and do not take attributes of researchers from multiple dimensions into consideration at the same time. To address the above issues, we present the multi-dimensional attributes enhanced heterogeneous (MAH) network representation learning method, which constructs heterogeneous networks with both co-authorship and citation information and makes use of multi-dimensional attributes. Three research questions are addressed in this work: (RQ1) Is our proposed method effective on academic collaboration recommendation compared with existing state-of-the-art methods? (RQ2) Does incorporating citation information into co-authorship network help improve the performance of academic collaboration recommendation? (RQ3) How does fusing multi-dimensional attributes affect the performance of academic collaboration recommendation? A publicly available real-world data set is used in our experiments. The superior performance of MAH compared with baseline methods demonstrates that the proposed multi-dimensional feature-based researcher profile can enrich node information in academic network and effective researcher representations can be learned by applying graph representation learning methods on the network. Â© The Author(s) 2024.","Academic collaboration recommendation (ACR) can help researchers find potential partners for research and thus promote academic innovation. Recent works mostly use graph learning-based methods to explore various ways of combining node information with topology, which consists of multiple steps, including network construction, node feature extraction, network representation learning and link prediction. One limitation is that they only conduct research on co-authorship networks and ignore citation relationship between publications. Besides, they tend to use attributes from a single dimension of researchers and do not take attributes of researchers from multiple dimensions into consideration at the same time. To address the above issues, we present the multi-dimensional attributes enhanced heterogeneous (MAH) network representation learning method, which constructs heterogeneous networks with both co-authorship and citation information and makes use of multi-dimensional attributes. Three research questions are addressed in this work: (RQ1) Is our proposed method effective on academic collaboration recommendation compared with existing state-of-the-art methods? (RQ2) Does incorporating citation information into co-authorship network help improve the performance of academic collaboration recommendation? (RQ3) How does fusing multi-dimensional attributes affect the performance of academic collaboration recommendation? A publicly available real-world data set is used in our experiments. The superior performance of MAH compared with baseline methods demonstrates that the proposed multi-dimensional feature-based researcher profile can enrich node information in academic network and effective researcher representations can be learned by applying graph representation learning methods on the network."
A graph convolutional network to improve item recommendation by incorporating bundle-based side-information with multi-level propagations,"Side-information, such as bundle, type, or brand, can be used to improve item recommendations. Use of graph convolutional networks (GCN) to calculate embeddings of side-information through user-side-item heterogeneous networks is common in the recommendation domain. However, current GCN-based methods largely ignore the limitations of bundle side-information. This is for two reasons: in some bundles, interaction with users or items is sparse; while in others, contributions of items cannot be estimated accurately due to irrelevant and noisy interactions. To overcome these limitations, we propose Graph Convolutional Network incorporating Bundle-based Side-Information (GCN-BSI). Unlike earlier studies, which model user, item and side-information into a unified graph, this model reduces the negative influence of bundle side-information by splitting the graph into three-level (lower, middle and upper) propagation models and incorporating these models into a unified framework by adopting different propagation strategies at different levels. This framework can make better use of bundle semantic information by iteratively optimising models from lower to upper levels, thereby controlling the quality of propagated information. This refined approach can further improve the performance of item recommendations. In a series of experiments, GCN-BSI was compared with eight state-of-the-art baselines using data from NetEase and SteamGame. GCN-BSI showed a significant improvement. An ablation test and case studies further indicated that the optimised solution was better at capturing userâitem correlations from specific side-information. The code and data can be visited at: https://github.com/zhongshsh/GCN-BSI Â© The Author(s) 2024.","Side-information, such as bundle, type, or brand, can be used to improve item recommendations. Use of graph convolutional networks (GCN) to calculate embeddings of side-information through user-side-item heterogeneous networks is common in the recommendation domain. However, current GCN-based methods largely ignore the limitations of bundle side-information. This is for two reasons: in some bundles, interaction with users or items is sparse; while in others, contributions of items cannot be estimated accurately due to irrelevant and noisy interactions. To overcome these limitations, we propose Graph Convolutional Network incorporating Bundle-based Side-Information (GCN-BSI). Unlike earlier studies, which model user, item and side-information into a unified graph, this model reduces the negative influence of bundle side-information by splitting the graph into three-level (lower, middle and upper) propagation models and incorporating these models into a unified framework by adopting different propagation strategies at different levels. This framework can make better use of bundle semantic information by iteratively optimising models from lower to upper levels, thereby controlling the quality of propagated information. This refined approach can further improve the performance of item recommendations. In a series of experiments, GCN-BSI was compared with eight state-of-the-art baselines using data from NetEase and SteamGame. GCN-BSI showed a significant improvement. An ablation test and case studies further indicated that the optimised solution was better at capturing useritem correlations from specific side-information. The code and data can be visited at: https://github.com/zhongshsh/GCN-BSI"
MathUSE: Mathematical information retrieval system using universal sentence encoder model,"In the scientific field, mathematical formulae are a significant factor in communicating the ideas and the fundamental principles of any scientific knowledge. Nowadays, the scientific research community generates a huge number of documents that comprise both textual and mathematical formulae. For the retrieval of textual information, numerous retrieval systems are present that generate excellent results. Nevertheless, these textual information retrieval systems are insufficient to handle the structure and scripting styles of the mathematical formulae. The recent past has perceived the research, which intends to retrieve the textual and mathematical formulae, but their impoverished results are symptomatic to the scope of improvement. In this article, we have implemented the formula-embedding approach, which encodes the formulae into fixed dimensional embedding vectors. For encoding of formula, we have used universal sentence encoderâbased sentence-embedding model, which relies on transformer architecture and deep averaging network. The proposed models take the latex formula as an input and produce an output of fixed dimensional embedding representation. To achieve more promising results, the transformer model follows stacked self-attentions, point-wise fully connected layers and positional encoding for both the encoder and decoder. The obtained results have been compared with state-of-the-art existing approaches, and the comparison study revealed that the proposed approach offers better retrieval accuracy in terms of (Formula presented.) = 0.217, (Formula presented.) = 0.178 and P@10 = 0.378 measures. Â© The Author(s) 2022.","In the scientific field, mathematical formulae are a significant factor in communicating the ideas and the fundamental principles of any scientific knowledge. Nowadays, the scientific research community generates a huge number of documents that comprise both textual and mathematical formulae. For the retrieval of textual information, numerous retrieval systems are present that generate excellent results. Nevertheless, these textual information retrieval systems are insufficient to handle the structure and scripting styles of the mathematical formulae. The recent past has perceived the research, which intends to retrieve the textual and mathematical formulae, but their impoverished results are symptomatic to the scope of improvement. In this article, we have implemented the formula-embedding approach, which encodes the formulae into fixed dimensional embedding vectors. For encoding of formula, we have used universal sentence encoderbased sentence-embedding model, which relies on transformer architecture and deep averaging network. The proposed models take the latex formula as an input and produce an output of fixed dimensional embedding representation. To achieve more promising results, the transformer model follows stacked self-attentions, point-wise fully connected layers and positional encoding for both the encoder and decoder. The obtained results have been compared with state-of-the-art existing approaches, and the comparison study revealed that the proposed approach offers better retrieval accuracy in terms of (Formula presented.) = 0.217, (Formula presented.) = 0.178 and P@10 = 0.378 measures."
Predictors affecting personal digital information management activities: A hierarchical regression analysis,"The study aimed to investigate the influence of demographic characteristics, Internet use, computer knowledge and technology self-efficacy on personal digital information management (PIM) activities â information finding, information keeping, information organising and information re-finding. The design of the study was quantitative and a survey method was used to get the objectives of the study. Three independent institutes of art and design â the Institute of Art and Culture, the National College of Arts and Design and the Pakistan Institute of Fashion and Design â were chosen as the research setting of the study. The population of the study was an academic community of three art and design institutes. The questionnaire was distributed to faculty and students of respective institutes and 229 responses were received after follow-up. The findings of the study indicated that both demographic characteristics and technology-related factors influenced the arts and design academic communityâs PIM activities. However, the second set of variables â Internet use, computer knowledge and technology self-efficacy influenced more than demographic variables on PIM activities. Academic role, university and technology self-efficacy appeared significant predictors of all PIM activities. The findings might be helpful for arts and design institutes librarians to make strategies to improve academic communityâs personal information management skills. In arts and design institutes, better efficiency in faculty and studentsâ PIM could be achieved if PIM literacy programmes are designed paying attention to differentials in demographic factors and technology-related factors, as revealed in this study. Â© The Author(s) 2022.","The study aimed to investigate the influence of demographic characteristics, Internet use, computer knowledge and technology self-efficacy on personal digital information management (PIM) activities information finding, information keeping, information organising and information re-finding. The design of the study was quantitative and a survey method was used to get the objectives of the study. Three independent institutes of art and design the Institute of Art and Culture, the National College of Arts and Design and the Pakistan Institute of Fashion and Design were chosen as the research setting of the study. The population of the study was an academic community of three art and design institutes. The questionnaire was distributed to faculty and students of respective institutes and 229 responses were received after follow-up. The findings of the study indicated that both demographic characteristics and technology-related factors influenced the arts and design academic communitys PIM activities. However, the second set of variables Internet use, computer knowledge and technology self-efficacy influenced more than demographic variables on PIM activities. Academic role, university and technology self-efficacy appeared significant predictors of all PIM activities. The findings might be helpful for arts and design institutes librarians to make strategies to improve academic communitys personal information management skills. In arts and design institutes, better efficiency in faculty and students PIM could be achieved if PIM literacy programmes are designed paying attention to differentials in demographic factors and technology-related factors, as revealed in this study."
KNNHI: Resilient KNN algorithm for heterogeneous incomplete data classification and K identification using rough set theory,"The original K-nearest neighbour (KNN) algorithm was meant to classify homogeneous complete data, that is, data with only numerical features whose values exist completely. Thus, it faces problems when used with heterogeneous incomplete (HI) data, which has also categorical features and is plagued with missing values. Many solutions have been proposed over the years but most have pitfalls. For example, some solve heterogeneity by converting categorical features into numerical ones, inflicting structural damage. Others solve incompleteness by imputation or elimination, causing semantic disturbance. Almost all use the same K for all query objects, leading to misclassification. In the present work, we introduce KNNHI, a KNN-based algorithm for HI data classification that avoids all these pitfalls. Leveraging rough set theory, KNNHI preserves both categorical and numerical features, leaves missing values untouched and uses a different K for each query. The end result is an accurate classifier, as demonstrated by extensive experimentation on nine datasets mostly from the University of California Irvine repository, using a 10-fold cross-validation technique. We show that KNNHI outperforms six recently published KNN-based algorithms, in terms of precision, recall, accuracy and F-Score. In addition to its function as a mighty classifier, KNNHI can also serve as a K calculator, helping KNN-based algorithms that use a single K value for all queries that find the best such value. Sure enough, we show how four such algorithms improve their performance using the K obtained by KNNHI. Finally, KNNHI exhibits impressive resilience to the degree of incompleteness, degree of heterogeneity and the metric used to measure distance. Â© The Author(s) 2022.","The original K-nearest neighbour (KNN) algorithm was meant to classify homogeneous complete data, that is, data with only numerical features whose values exist completely. Thus, it faces problems when used with heterogeneous incomplete (HI) data, which has also categorical features and is plagued with missing values. Many solutions have been proposed over the years but most have pitfalls. For example, some solve heterogeneity by converting categorical features into numerical ones, inflicting structural damage. Others solve incompleteness by imputation or elimination, causing semantic disturbance. Almost all use the same K for all query objects, leading to misclassification. In the present work, we introduce KNNHI, a KNN-based algorithm for HI data classification that avoids all these pitfalls. Leveraging rough set theory, KNNHI preserves both categorical and numerical features, leaves missing values untouched and uses a different K for each query. The end result is an accurate classifier, as demonstrated by extensive experimentation on nine datasets mostly from the University of California Irvine repository, using a 10-fold cross-validation technique. We show that KNNHI outperforms six recently published KNN-based algorithms, in terms of precision, recall, accuracy and F-Score. In addition to its function as a mighty classifier, KNNHI can also serve as a K calculator, helping KNN-based algorithms that use a single K value for all queries that find the best such value. Sure enough, we show how four such algorithms improve their performance using the K obtained by KNNHI. Finally, KNNHI exhibits impressive resilience to the degree of incompleteness, degree of heterogeneity and the metric used to measure distance."
"How do gender, Internet activity and learning beliefs predict sixth-grade studentsâ self-efficacy beliefs in and attitudes towards online inquiry?","Todayâs students search, evaluate and actively use Web information in their school assignments, that is, they conduct an online inquiry. This current survey study addresses sixth-grade studentsâ self-efficacy beliefs in and attitudes towards online inquiry, and to what extent free-time and school-related Internet activity, gender and learning beliefs explain these. The questionnaire was administered in 10 schools to 340 sixth-graders in Finland. Exploratory and confirmatory factor analyses revealed three elements of self-efficacy beliefs: self-efficacy in Web searching, the evaluation of sources and synthesising information. Furthermore, attitudes towards online inquiry loaded into two factors: a positive and a negative attitude towards online inquiry. A structural equation model was used to analyse the effects of the explanatory variables on the factors. The results of this work suggest that gender and free-time Internet use predict most sixth-gradersâ self-efficacy beliefs in and attitudes towards online inquiry. Â© The Author(s) 2021.","Todays students search, evaluate and actively use Web information in their school assignments, that is, they conduct an online inquiry. This current survey study addresses sixth-grade students self-efficacy beliefs in and attitudes towards online inquiry, and to what extent free-time and school-related Internet activity, gender and learning beliefs explain these. The questionnaire was administered in 10 schools to 340 sixth-graders in Finland. Exploratory and confirmatory factor analyses revealed three elements of self-efficacy beliefs: self-efficacy in Web searching, the evaluation of sources and synthesising information. Furthermore, attitudes towards online inquiry loaded into two factors: a positive and a negative attitude towards online inquiry. A structural equation model was used to analyse the effects of the explanatory variables on the factors. The results of this work suggest that gender and free-time Internet use predict most sixth-graders self-efficacy beliefs in and attitudes towards online inquiry."
"Linked data for libraries: Creating a global knowledge space, a systematic literature review","The Semantic Web in general and the Linked Open Data Initiative, in particular, are a growing movement for organisations to make their existing data available in a machine-readable format. Thus, institutions are highly encouraged to publish, share and interlink their data publicly. The more data are opened on the Web (Open Data), the more integrated sets of data will be connected in the Semantic Web (Linked Open Data). Within this context, libraries can complement their data by linking it to other, external data sources. The purpose of this article is to identify papers that refer to linked data in libraries, emphasising the ways that linked data empower libraries to put their knowledge in the context of the open-world, thus enhancing semantic technology innovations. The study considered papers published between 2008 and 2019 in English and presents the collected literature by grouping it according to the topic each paper refers to. The results show that libraries are facing a period of continuing change which present several challenges and indicate that they are moving towards developing new practices, policies and services. Â© The Author(s) 2022.","The Semantic Web in general and the Linked Open Data Initiative, in particular, are a growing movement for organisations to make their existing data available in a machine-readable format. Thus, institutions are highly encouraged to publish, share and interlink their data publicly. The more data are opened on the Web (Open Data), the more integrated sets of data will be connected in the Semantic Web (Linked Open Data). Within this context, libraries can complement their data by linking it to other, external data sources. The purpose of this article is to identify papers that refer to linked data in libraries, emphasising the ways that linked data empower libraries to put their knowledge in the context of the open-world, thus enhancing semantic technology innovations. The study considered papers published between 2008 and 2019 in English and presents the collected literature by grouping it according to the topic each paper refers to. The results show that libraries are facing a period of continuing change which present several challenges and indicate that they are moving towards developing new practices, policies and services."
All roads lead to Rome: Understanding the diffusion trajectories of innovation twins,"This study conducts a comparison of the references and citations of two discoveries that were made simultaneously yet independently. The two discoveries, specifically âInference of Population Structure Using Multilocus Genotype Data (IPSUMGD)â and âLatent Dirichlet Allocation (LDA)â, are both significant academic publications in their respective fields. Although they share similar underlying concepts, they originate from different disciplines. Our objective is to analyse similarities and differences in the knowledge foundation and diffusion trajectories of these simultaneous discoveries, IPSUMGD and LDA, to further determine if a general pattern of successful innovation diffusion exists. The results indicate that the considerable similarity in the core ideas of IPSUMGD and LDA may be attributed to a strong disciplinary connection in their knowledge foundation, leading to overlapping diffusion processes. However, the divergence in thematic volatility and discipline distribution implies that IPSUMGD and LDA occupy distinct and independent diffusion spaces, which is crucial for their success. The citation cascade networks highlight the unique diffusion patterns of IPSUMGD and LDA, with IPSUMGD originating from the emergence of multiple high-impact nodes and LDA evolving through iterative innovation. The main path analysis reveals that both articles feature several key nodes in their diffusion processes, and the original authors have made substantial contributions to their long-term citation trajectories. Â© The Author(s) 2024.","This study conducts a comparison of the references and citations of two discoveries that were made simultaneously yet independently. The two discoveries, specifically Inference of Population Structure Using Multilocus Genotype Data (IPSUMGD) and Latent Dirichlet Allocation (LDA), are both significant academic publications in their respective fields. Although they share similar underlying concepts, they originate from different disciplines. Our objective is to analyse similarities and differences in the knowledge foundation and diffusion trajectories of these simultaneous discoveries, IPSUMGD and LDA, to further determine if a general pattern of successful innovation diffusion exists. The results indicate that the considerable similarity in the core ideas of IPSUMGD and LDA may be attributed to a strong disciplinary connection in their knowledge foundation, leading to overlapping diffusion processes. However, the divergence in thematic volatility and discipline distribution implies that IPSUMGD and LDA occupy distinct and independent diffusion spaces, which is crucial for their success. The citation cascade networks highlight the unique diffusion patterns of IPSUMGD and LDA, with IPSUMGD originating from the emergence of multiple high-impact nodes and LDA evolving through iterative innovation. The main path analysis reveals that both articles feature several key nodes in their diffusion processes, and the original authors have made substantial contributions to their long-term citation trajectories."
Research on interdisciplinarity of five-metrics in China based on Chinese Citation Data under the background of open science,"The theories, methods and techniques of bibliometrics, scientometrics, informetrics, webometrics and knowledgometrics together constitute Five-Metrics. Five-Metrics is one of the most active research fields in Chinaâs library and information science (LIS), and the research on Five-Metrics in China is characterised by the diversity of disciplines. Quantitative analysis of interdisciplinary research in Five-Metrics of China reveals the disciplinary origin and knowledge structure of Chinese Five-Metrics, grasps the interdisciplinary patterns and laws of Five-Metrics, and helps promote international exchange and cooperation, innovation and development of Five-Metrics research in the context of open science. Based on the theory of knowledge flow, this study uses a combination of citation analysis, mathematical modelling analysis, social network analysis and statistical analysis. We study the interdisciplinary degree of Five-Metrics based on 20,528 publications and corresponding 207,530 reference records and 111,823 citing article records, using a combination of python, gephi, origin and other tools. The results show that the interdisciplinarity of Five-Metrics publications and knowledge flow at the macroscopic level is high, and interdisciplinarity of the cited references and citing articles of Five-Metrics is higher. At the microscopic level, there is a wide gap in the interdisciplinarity of Five-Metrics in different disciplines. In addition, this study identifies three interdisciplinary knowledge flow patterns of Five-Metrics of China. This study conducts a comprehensive analysis of the interdisciplinary Five-Metrics study in China based on the cited references, publications and citing articles. Â© The Author(s) 2024.","The theories, methods and techniques of bibliometrics, scientometrics, informetrics, webometrics and knowledgometrics together constitute Five-Metrics. Five-Metrics is one of the most active research fields in Chinas library and information science (LIS), and the research on Five-Metrics in China is characterised by the diversity of disciplines. Quantitative analysis of interdisciplinary research in Five-Metrics of China reveals the disciplinary origin and knowledge structure of Chinese Five-Metrics, grasps the interdisciplinary patterns and laws of Five-Metrics, and helps promote international exchange and cooperation, innovation and development of Five-Metrics research in the context of open science. Based on the theory of knowledge flow, this study uses a combination of citation analysis, mathematical modelling analysis, social network analysis and statistical analysis. We study the interdisciplinary degree of Five-Metrics based on 20,528 publications and corresponding 207,530 reference records and 111,823 citing article records, using a combination of python, gephi, origin and other tools. The results show that the interdisciplinarity of Five-Metrics publications and knowledge flow at the macroscopic level is high, and interdisciplinarity of the cited references and citing articles of Five-Metrics is higher. At the microscopic level, there is a wide gap in the interdisciplinarity of Five-Metrics in different disciplines. In addition, this study identifies three interdisciplinary knowledge flow patterns of Five-Metrics of China. This study conducts a comprehensive analysis of the interdisciplinary Five-Metrics study in China based on the cited references, publications and citing articles."
Information literacy and research support services in academic libraries: A bibliometric analysis from 2001 to 2020,"This article attempted to examine research support services, information services, print collections, digital resources and information literacy using bibliometric analysis from 2001 to 2020. The main aim was to consolidate the published studies on the research support services in academic libraries in the Web of Science (WoS) indexed documents. However, there has been a lack of quantitative measurements on the subject. Thus, we used the bibliometric method and found a total of 4079 published documents. The study findings revealed that the topic of âinformation literacy and libraryâ was on top with a total number of 2168 publications, 3047 articles as a type of published documents, 3662 publications in English and a considerable increase in publications as per years were found. The top author named Fourie I was found with 106 citations and 22 articles started in 2001. Similarly, the University of Illinois found on top organisations out of 2609, United States on top out of 113 countries and information literacy as a keyword out of 6179. Furthermore, the Journal of Academic Librarianship placed at top of sources out of 726 and the National Institutes of Health NIH USA as a top funding agency. Â© The Author(s) 2022.","This article attempted to examine research support services, information services, print collections, digital resources and information literacy using bibliometric analysis from 2001 to 2020. The main aim was to consolidate the published studies on the research support services in academic libraries in the Web of Science (WoS) indexed documents. However, there has been a lack of quantitative measurements on the subject. Thus, we used the bibliometric method and found a total of 4079 published documents. The study findings revealed that the topic of information literacy and library was on top with a total number of 2168 publications, 3047 articles as a type of published documents, 3662 publications in English and a considerable increase in publications as per years were found. The top author named Fourie I was found with 106 citations and 22 articles started in 2001. Similarly, the University of Illinois found on top organisations out of 2609, United States on top out of 113 countries and information literacy as a keyword out of 6179. Furthermore, the Journal of Academic Librarianship placed at top of sources out of 726 and the National Institutes of Health NIH USA as a top funding agency."
Research knowledge utilisation for societal impact: Information practices based on abductive topic modelling,"Information science researchers are increasingly seeking to understand the utilisation of knowledge generated through scientific research outside of academia. Although the conceptual levels of knowledge utilisation are well established, our understanding of the various information practices for knowledge utilisation employed by researchers remains limited. This study identified such information practices by text-mining 6637 case studies documented under the United Kingdomâs Research Excellence Framework. The results were augmented with expert judgement to develop a framework consisting of nine types based on the theoretical framework of research knowledge utilisation. Three emerging types were identified: deliberation, co-creation and foresighting. They indicate the rise of information practices leveraging social media and analytical capabilities to engage potential beneficiaries in using and realising the value of research. Â© The Author(s) 2022.","Information science researchers are increasingly seeking to understand the utilisation of knowledge generated through scientific research outside of academia. Although the conceptual levels of knowledge utilisation are well established, our understanding of the various information practices for knowledge utilisation employed by researchers remains limited. This study identified such information practices by text-mining 6637 case studies documented under the United Kingdoms Research Excellence Framework. The results were augmented with expert judgement to develop a framework consisting of nine types based on the theoretical framework of research knowledge utilisation. Three emerging types were identified: deliberation, co-creation and foresighting. They indicate the rise of information practices leveraging social media and analytical capabilities to engage potential beneficiaries in using and realising the value of research."
Characterising usersâ task completion process in learning-related tasks:A search pace model,"This study investigated usersâ searching, reading and writing interactions and their activity transitions during task completion process when users were collecting information for learning-related search tasks. Task completion process was defined as the process users started to search till the time when they have collected enough information to accomplish the search tasks. The data analysis was conducted from a new process perspective through synthesising macro- and micro-process levels. Four evenly distributed stages were divided according to the total task completion time in each search session. Our results demonstrated that users generally experienced three sub-processes during task completion process: exploration, accumulation and composition/reporting. Exploration sub-process is basically the first quarter of the total task completion time, during which users often issue more queries and visit more search engine result pages (SERPs) to collect information, and the dominant activity transition is switching between searching and reading; accumulation sub-process is mainly the second and third quarters of the total task completion time, during which they visit more unique content pages, have more revisits per content page, and they switch between reading and writing activities frequently; the last stage is composition/reporting sub-process, which is dominated by writing, and users often switch between writing and reading, and between writing and searching. Based on these findings, we propose a search pace model to describe how users proceed from the beginning to the end of task completion process in these three sub-processes. The methodology applied has been proved to be effective to examine usersâ interaction behaviours from the process perspective on both the micro- and macro-levels. The findings of this article help us understand how users proceed their dynamic searching, reading and writing behaviours for learning-related tasks, and also have implications for the design of search systems that support learning-related tasks. Â© The Author(s) 2021.","This study investigated users searching, reading and writing interactions and their activity transitions during task completion process when users were collecting information for learning-related search tasks. Task completion process was defined as the process users started to search till the time when they have collected enough information to accomplish the search tasks. The data analysis was conducted from a new process perspective through synthesising macro- and micro-process levels. Four evenly distributed stages were divided according to the total task completion time in each search session. Our results demonstrated that users generally experienced three sub-processes during task completion process: exploration, accumulation and composition/reporting. Exploration sub-process is basically the first quarter of the total task completion time, during which users often issue more queries and visit more search engine result pages (SERPs) to collect information, and the dominant activity transition is switching between searching and reading; accumulation sub-process is mainly the second and third quarters of the total task completion time, during which they visit more unique content pages, have more revisits per content page, and they switch between reading and writing activities frequently; the last stage is composition/reporting sub-process, which is dominated by writing, and users often switch between writing and reading, and between writing and searching. Based on these findings, we propose a search pace model to describe how users proceed from the beginning to the end of task completion process in these three sub-processes. The methodology applied has been proved to be effective to examine users interaction behaviours from the process perspective on both the micro- and macro-levels. The findings of this article help us understand how users proceed their dynamic searching, reading and writing behaviours for learning-related tasks, and also have implications for the design of search systems that support learning-related tasks."
Research on aspect-based sentiment analysis of movie reviews based on deep learning,"Aspect-based sentiment analysis aims to extract the sentiment polarity of different aspects within a text. In recent years, most methods have relied on pre-trained language models such as BERT and Roberta to learn semantic representations from the context. However, in texts with ambiguous sentiment expression, the absence of domain knowledge guidance may lead pre-trained language models to miss critical information, and the attention mechanism might incorrectly focus on text that is irrelevant to the aspect categories. To address these issues, this study integrates the ontology of movie reviews to construct an aspect-based sentiment analysis model based on the ERNIE(OMR-EBA). We annotated a new Chinese data set focused on movie reviews to evaluate the modelâs performance. Experimental results show that our model achieves 86% accuracy in aspectual sentiment analysis, which is better than other baseline models. The movie review domain ontology and aspect-based sentiment analysis model proposed in this study can provide valuable reference and guidance for research in the field of online movie reviews. It can also help movie production teams understand genuine user sentiments, aiding in subsequent marketing and production efforts. Â© The Author(s) 2024.","Aspect-based sentiment analysis aims to extract the sentiment polarity of different aspects within a text. In recent years, most methods have relied on pre-trained language models such as BERT and Roberta to learn semantic representations from the context. However, in texts with ambiguous sentiment expression, the absence of domain knowledge guidance may lead pre-trained language models to miss critical information, and the attention mechanism might incorrectly focus on text that is irrelevant to the aspect categories. To address these issues, this study integrates the ontology of movie reviews to construct an aspect-based sentiment analysis model based on the ERNIE(OMR-EBA). We annotated a new Chinese data set focused on movie reviews to evaluate the models performance. Experimental results show that our model achieves 86% accuracy in aspectual sentiment analysis, which is better than other baseline models. The movie review domain ontology and aspect-based sentiment analysis model proposed in this study can provide valuable reference and guidance for research in the field of online movie reviews. It can also help movie production teams understand genuine user sentiments, aiding in subsequent marketing and production efforts."
Collaboration of issuing agencies and topic evolution of health informatisation policies in China,"Digital transformation in the Chinese healthcare industry has led national government agencies to issue a series of policies to guide the construction of health informatisation. However, little is known about the issuing agencies and the topics of health informatisation policies. This study aimed to explore the collaboration of policies issuing and the evolution of policy topics. In this study, a total of 156 policy documents were identified. AuthorâTopic model and pre-discretised method based on Latent Dirichlet Allocation model were employed to mine the correlation between the issuing agencies and policy topics, and the evolution of policy contents. Findings suggest that the development of health informatisation policies can be divided into three stages. The number of policies has been increasing constantly, among which the policy of opinion and notification accounts for the vast majority. Many government agencies are involved in formulating policies collaboratively. On the whole, the topics changed constantly over time. From 2003 to 2008, policy topics focused on standards and specifications, with the phenomenon of splitting and development. From 2009 to 2014, policies were predominantly related to the construction of regional health informatisation, with some emerging topics generating. Internet + medical and new information technology gained attention from 2015 to 2020; most topics in this period were inherited, split or merged from the previous period. This study is helpful to research and formulation of the health informatisation-related policies. Â© The Author(s) 2022.","Digital transformation in the Chinese healthcare industry has led national government agencies to issue a series of policies to guide the construction of health informatisation. However, little is known about the issuing agencies and the topics of health informatisation policies. This study aimed to explore the collaboration of policies issuing and the evolution of policy topics. In this study, a total of 156 policy documents were identified. AuthorTopic model and pre-discretised method based on Latent Dirichlet Allocation model were employed to mine the correlation between the issuing agencies and policy topics, and the evolution of policy contents. Findings suggest that the development of health informatisation policies can be divided into three stages. The number of policies has been increasing constantly, among which the policy of opinion and notification accounts for the vast majority. Many government agencies are involved in formulating policies collaboratively. On the whole, the topics changed constantly over time. From 2003 to 2008, policy topics focused on standards and specifications, with the phenomenon of splitting and development. From 2009 to 2014, policies were predominantly related to the construction of regional health informatisation, with some emerging topics generating. Internet + medical and new information technology gained attention from 2015 to 2020; most topics in this period were inherited, split or merged from the previous period. This study is helpful to research and formulation of the health informatisation-related policies."
Adoption and uses of cloud computing in academic libraries: A systematic literature,"This study aims to synthesise the findings of research on cloud computing adoption and use in libraries. This systematic literature review is based on Preferred Reporting Items for Systematic Reviews and Meta-Analyses method and comprises publications in the English language, published in the four world-renowned databases. This study identified various cloud computing practices, including library automation systems on clouds, email services, applications of social media, cloud storage (Dropbox), consortium services, digital library and file sharing. The libraries adopted cloud computing due to cost-effectiveness, storage facility, ease-to-use, flexibility and scalability, time-saving, lack of in-house skill set and ubiquitous nature of the technologies. Several factors, for example, security issues, privacy of data, slow Internet connectivity and high subscription rate affect the adoption of cloud computing. The critical adoption, usage factors and various challenges identified would provide valuable insight to library professionals to decide how to employ cloud-based practices to offer innovative services in academic libraries. Â© The Author(s) 2024.","This study aims to synthesise the findings of research on cloud computing adoption and use in libraries. This systematic literature review is based on Preferred Reporting Items for Systematic Reviews and Meta-Analyses method and comprises publications in the English language, published in the four world-renowned databases. This study identified various cloud computing practices, including library automation systems on clouds, email services, applications of social media, cloud storage (Dropbox), consortium services, digital library and file sharing. The libraries adopted cloud computing due to cost-effectiveness, storage facility, ease-to-use, flexibility and scalability, time-saving, lack of in-house skill set and ubiquitous nature of the technologies. Several factors, for example, security issues, privacy of data, slow Internet connectivity and high subscription rate affect the adoption of cloud computing. The critical adoption, usage factors and various challenges identified would provide valuable insight to library professionals to decide how to employ cloud-based practices to offer innovative services in academic libraries."
The impact of social noise on social media and the original intended message: BLM as a case study,"Social media has become a platform for information diffusion, voicing concerns of existing inequalities and raising public awareness of various social and societal issues. Despite the social good, social media has become a fertile ground for spreading misinformation, hate speech and conspiracy theories. The death of George Floyd in May 2020 triggered a series of protests worldwide in support of the Black Lives Matter (BLM) movement and triggered a debate about equity, inclusion and social justice. The purpose of this study is to examine the impact of misinformation and social noise on the original intended message of BLM using data from the Twitter hashtag âBLMâ. Results from topic modelling have shown the strong presence of misinformation and social noise. Such information was most probably intended to influence, mislead and dilute the original intended message. However, despite the effort to distort the original message of BLM, results from sentiment analysis show that usersâ opinions of the BLM movement remained positive. Â© The Author(s) 2022.","Social media has become a platform for information diffusion, voicing concerns of existing inequalities and raising public awareness of various social and societal issues. Despite the social good, social media has become a fertile ground for spreading misinformation, hate speech and conspiracy theories. The death of George Floyd in May 2020 triggered a series of protests worldwide in support of the Black Lives Matter (BLM) movement and triggered a debate about equity, inclusion and social justice. The purpose of this study is to examine the impact of misinformation and social noise on the original intended message of BLM using data from the Twitter hashtag BLM. Results from topic modelling have shown the strong presence of misinformation and social noise. Such information was most probably intended to influence, mislead and dilute the original intended message. However, despite the effort to distort the original message of BLM, results from sentiment analysis show that users opinions of the BLM movement remained positive."
MeSHelper: Predicting the evolution of Medical Subject Headings based on knowledge graph dynamics,"The Medical Subject Headings (MeSH) thesaurus is a controlled vocabulary widely used in biomedical knowledge systems. We propose a novel framework, termed MeSHelper, that employs a dynamic knowledge graph to predict whether the MeSH main headings (MHs) will evolve while also predicting their corresponding revision type. We parsed the whole PubMed database and all MeSH releases to construct a dynamic semantic tree (DST) and a dynamic knowledge network (DKN) to characterise the evolutionary patterns of MHs and create prediction models. Our results show that DST-related features play a major role in predicting whether the MHs will be revised. Our prediction performance achieved an F1 score of 92.07%. Both DST- and DKN-related features play a crucial role in predicting which types of MHs will evolve. The prediction performance achieved a Macro-F1 score of 72.15%, a Micro-F1 score of 84.09% and a Weighted-F1 score of 84.55%. The findings of this work aid both in constructing an automatic update model for domain thesauruses and in detecting evolutionary trends of the domain knowledge system. Â© The Author(s) 2024.","The Medical Subject Headings (MeSH) thesaurus is a controlled vocabulary widely used in biomedical knowledge systems. We propose a novel framework, termed MeSHelper, that employs a dynamic knowledge graph to predict whether the MeSH main headings (MHs) will evolve while also predicting their corresponding revision type. We parsed the whole PubMed database and all MeSH releases to construct a dynamic semantic tree (DST) and a dynamic knowledge network (DKN) to characterise the evolutionary patterns of MHs and create prediction models. Our results show that DST-related features play a major role in predicting whether the MHs will be revised. Our prediction performance achieved an F1 score of 92.07%. Both DST- and DKN-related features play a crucial role in predicting which types of MHs will evolve. The prediction performance achieved a Macro-F1 score of 72.15%, a Micro-F1 score of 84.09% and a Weighted-F1 score of 84.55%. The findings of this work aid both in constructing an automatic update model for domain thesauruses and in detecting evolutionary trends of the domain knowledge system."
Unveiling cognitive structure and comparative advantages of countries in knowledge domains,"Mapping and depicting the structure, dynamics and national specialisation profiles of scientific fields at the country level affords a better understanding of national developments and changes in a given field, particularly when these changes may serve as an aid in decision-making with regard to research management. This article looks at the cognitive structure of a field over time to characterise its development across countries and to appraise the competitiveness of countries in terms of research specialisation. Based on a dataset extracted from the Scopus database, we conducted a co-word analysis and studied the degree of specialisation based on publications and on keywords, in the Nanoscience and Nanotechnology field (NST). The results reveal that NST research tends to focus on nano applications and devices. According to the keyword activity index, the countries studied centre their specialisation on electronic, biotechnology and biomedical research, certain countries showing a more competitive edge in the global realm of output. Accordingly, implications that could contribute to decision-making regarding the economy and research policies are described. Â© The Author(s) 2022.","Mapping and depicting the structure, dynamics and national specialisation profiles of scientific fields at the country level affords a better understanding of national developments and changes in a given field, particularly when these changes may serve as an aid in decision-making with regard to research management. This article looks at the cognitive structure of a field over time to characterise its development across countries and to appraise the competitiveness of countries in terms of research specialisation. Based on a dataset extracted from the Scopus database, we conducted a co-word analysis and studied the degree of specialisation based on publications and on keywords, in the Nanoscience and Nanotechnology field (NST). The results reveal that NST research tends to focus on nano applications and devices. According to the keyword activity index, the countries studied centre their specialisation on electronic, biotechnology and biomedical research, certain countries showing a more competitive edge in the global realm of output. Accordingly, implications that could contribute to decision-making regarding the economy and research policies are described."
A benchmark of Spanish language datasets for computationally driven research,"In the domain of Galleries, Libraries, Archives and Museums (GLAM) institutions, creative and innovative tools and methodologies for content delivery and user engagement have recently gained international attention. New methods have been proposed to publish digital collections as datasets amenable to computational use. Standardised benchmarks can be useful to broaden the scope of machine-actionable collections and to promote cultural and linguistic diversity. In this article, we propose a methodology to select datasets for computationally driven research applied to Spanish text corpora. This work seeks to encourage Spanish and Latin American institutions to publish machine-actionable collections based on best practices and avoiding common mistakes. Â© The Author(s) 2021.","In the domain of Galleries, Libraries, Archives and Museums (GLAM) institutions, creative and innovative tools and methodologies for content delivery and user engagement have recently gained international attention. New methods have been proposed to publish digital collections as datasets amenable to computational use. Standardised benchmarks can be useful to broaden the scope of machine-actionable collections and to promote cultural and linguistic diversity. In this article, we propose a methodology to select datasets for computationally driven research applied to Spanish text corpora. This work seeks to encourage Spanish and Latin American institutions to publish machine-actionable collections based on best practices and avoiding common mistakes."
Documenting the ephemeral: An ontology for the performing arts,"The performing arts domain, addressing the ephemeral nature of its main study subject, the performance, calls upon Digital Humanitiesâ techniques to document, record and analyse the performative event, thus offering a new perspective to research and education. In this context, many digital projects have been completed, including digitising art archives, creating databases of art productions and developing metadata schemas and conceptual models. The study of these efforts revealed a need for a universal and comprehensive way of documenting performance. This article, which benefitted from the experience gained thus far in the performing arts in the digital world, attempts to unify the domainâs acquired knowledge and organise it in an ontology. While the overall success of such a project proved to be greater than the scope of this research, we propose a core ontology for the performing arts, which has the potential to evolve in the future. Â© The Author(s) 2024.","The performing arts domain, addressing the ephemeral nature of its main study subject, the performance, calls upon Digital Humanities techniques to document, record and analyse the performative event, thus offering a new perspective to research and education. In this context, many digital projects have been completed, including digitising art archives, creating databases of art productions and developing metadata schemas and conceptual models. The study of these efforts revealed a need for a universal and comprehensive way of documenting performance. This article, which benefitted from the experience gained thus far in the performing arts in the digital world, attempts to unify the domains acquired knowledge and organise it in an ontology. While the overall success of such a project proved to be greater than the scope of this research, we propose a core ontology for the performing arts, which has the potential to evolve in the future."
"A review of challenges, algorithms and evaluation methods in news recommendation","News reading is an important social activity and to help readers quickly find news articles of their interest, news content providers and aggregators use recommender systems. Such systems are designed to address a variety of challenges. Inspiration for algorithmic design is taken from various domains which has resulted in the creation of an enormous body of literature. Also, different methods are used for evaluation of the recommendation algorithms. In this study, we review these developments and present three major components in news recommendation research. First, we list and categorise the challenges faced while designing news recommender systems. We especially list the different algorithmic designs used for generating personalised and non-personalised recommendations. We discuss the major neural network architectures that are being increasingly used for both collaborative and content-based recommender systems. Next, we list the two major evaluation methods and also list some popular datasets used in evaluation. Finally, we identify the emerging trends in news recommender research. We find that the issues related to fake news, trust and use of personal data for news recommendation are gaining wider attention, and deep learning methods are being increasingly used to address these issues. Â© The Author(s) 2024.","News reading is an important social activity and to help readers quickly find news articles of their interest, news content providers and aggregators use recommender systems. Such systems are designed to address a variety of challenges. Inspiration for algorithmic design is taken from various domains which has resulted in the creation of an enormous body of literature. Also, different methods are used for evaluation of the recommendation algorithms. In this study, we review these developments and present three major components in news recommendation research. First, we list and categorise the challenges faced while designing news recommender systems. We especially list the different algorithmic designs used for generating personalised and non-personalised recommendations. We discuss the major neural network architectures that are being increasingly used for both collaborative and content-based recommender systems. Next, we list the two major evaluation methods and also list some popular datasets used in evaluation. Finally, we identify the emerging trends in news recommender research. We find that the issues related to fake news, trust and use of personal data for news recommendation are gaining wider attention, and deep learning methods are being increasingly used to address these issues."
AI anxiety and fear: A look at perspectives of information science students and professionals towards artificial intelligence,"The rapid integration of artificial intelligence (AI) within society and the emergence of the fourth industrial revolution (4IR), has ignited a spectrum of emotions in society, ranging from enthusiasm to anxiety. This study investigates the depths of AI anxiety and fear among a population of information science students and professionals. Utilising a survey of over 200 current students and professionals, this study explores the connections between age, gender identity, ethnicity, geographic location, educational attainment and residence, and the levels of anxiety and fear associated with AI and the 4IR. The findings reveal nuanced relationships, with age, ethnicity, academic achievement and regional context serving as critical differentiators in 4IR and AI anxiety within this population. Students and professionals alike may benefit from seeking further education about this emerging technology. Â© The Author(s) 2024.","The rapid integration of artificial intelligence (AI) within society and the emergence of the fourth industrial revolution (4IR), has ignited a spectrum of emotions in society, ranging from enthusiasm to anxiety. This study investigates the depths of AI anxiety and fear among a population of information science students and professionals. Utilising a survey of over 200 current students and professionals, this study explores the connections between age, gender identity, ethnicity, geographic location, educational attainment and residence, and the levels of anxiety and fear associated with AI and the 4IR. The findings reveal nuanced relationships, with age, ethnicity, academic achievement and regional context serving as critical differentiators in 4IR and AI anxiety within this population. Students and professionals alike may benefit from seeking further education about this emerging technology."
"Expert-recommended biomedical journal articles: Their retractions or corrections, and post-retraction citing","Faculty Opinions has provided recommendations of important biomedical publications by domain experts (FMs) since 2001. The purpose of this study is two-fold: (1) identify the characteristics of the expert-recommended articles that were subsequently retracted and (2) investigate what happened after retraction. We examined a set of 232 recommended, later retracted or corrected articles. These articles were classified as New Finding (43%), Interesting Hypothesis (16%), and so on. More than 71% of the articles acknowledged funding support; the National Institutes of Health, USA (NIH) was a top funder (64%). The top reasons for retractions were Errors of various types (28%); Falsification/fabrication of data, image, or results (20%); Unreliable data, image, or results (16%); and Results not reproducible (16%). Retractions took from less than 2 months to more than 15 years. Only 15% of recommendations were withdrawn either after dissents were made by other FMs or after retractions. Most of the retracted articles continue to be cited post-retraction, especially those published in Nature, Science, and Cell. Significant positive correlations were observed between post-retraction citations and pre-retraction citations, between post-retraction citations and peak citations, and between post-retraction citations and the post-retraction citing span. A significant negative correlation was also observed between the post-retraction citing span and years taken to reach peak citations. Literature recommendation systems need to update the changing status of the recommended articles in a timely manner; invite the recommending experts to update their recommendations; and provide a personalised mechanism to alert users who have accessed the recommended articles on their subsequent retractions, concerns, or corrections. Â© The Author(s) 2022.","Faculty Opinions has provided recommendations of important biomedical publications by domain experts (FMs) since 2001. The purpose of this study is two-fold: identify the characteristics of the expert-recommended articles that were subsequently retracted and investigate what happened after retraction. We examined a set of 232 recommended, later retracted or corrected articles. These articles were classified as New Finding (43%), Interesting Hypothesis (16%), and so on. More than 71% of the articles acknowledged funding support; the National Institutes of Health, USA (NIH) was a top funder (64%). The top reasons for retractions were Errors of various types (28%); Falsification/fabrication of data, image, or results (20%); Unreliable data, image, or results (16%); and Results not reproducible (16%). Retractions took from less than 2 months to more than 15 years. Only 15% of recommendations were withdrawn either after dissents were made by other FMs or after retractions. Most of the retracted articles continue to be cited post-retraction, especially those published in Nature, Science, and Cell. Significant positive correlations were observed between post-retraction citations and pre-retraction citations, between post-retraction citations and peak citations, and between post-retraction citations and the post-retraction citing span. A significant negative correlation was also observed between the post-retraction citing span and years taken to reach peak citations. Literature recommendation systems need to update the changing status of the recommended articles in a timely manner; invite the recommending experts to update their recommendations; and provide a personalised mechanism to alert users who have accessed the recommended articles on their subsequent retractions, concerns, or corrections."
Two fast algorithms for computation of h-index,"Despite its many limitations, h-index is one of the popular productivity indicators used by many scholarly indexing databases and search engine tools. Apart from indicating scientific and inventive productivity at individual level, it can be used for other actors such as journals, institutions, countries and assignees. Though h-index is relatively simple to compute than most of the h-type indicators for a single actor profile, efficient algorithms are always useful to manage large databases or repositories that require handling and frequent updating of large number of such profiles. However, there are very few attempts to improve the computation of h-index. In this work, we introduce two new estimation-based algorithms for computation of h-index on sorted profile and compare it with two existing approaches using (1) the real dataset of scholarly profiles of 50 eminent researchers in the area of information science and scientometrics, (2) a real dataset of 4177 scholars with h-index greater than 100 and (3) three sets of simulated profiles of actors. Both the algorithms are found to be performing better than their existing counterparts. Furthermore, we attempt to provide guidelines for the choice of strategies to implement these algorithms to ensure maximisation of speed. Â© The Author(s) 2024.","Despite its many limitations, h-index is one of the popular productivity indicators used by many scholarly indexing databases and search engine tools. Apart from indicating scientific and inventive productivity at individual level, it can be used for other actors such as journals, institutions, countries and assignees. Though h-index is relatively simple to compute than most of the h-type indicators for a single actor profile, efficient algorithms are always useful to manage large databases or repositories that require handling and frequent updating of large number of such profiles. However, there are very few attempts to improve the computation of h-index. In this work, we introduce two new estimation-based algorithms for computation of h-index on sorted profile and compare it with two existing approaches using the real dataset of scholarly profiles of 50 eminent researchers in the area of information science and scientometrics, a real dataset of 4177 scholars with h-index greater than 100 and three sets of simulated profiles of actors. Both the algorithms are found to be performing better than their existing counterparts. Furthermore, we attempt to provide guidelines for the choice of strategies to implement these algorithms to ensure maximisation of speed."
The role of institutional policies in the sustainability of institutional repositories in Africa: A reflection from Ghana,"Institutional repositories (IRs) are increasingly gaining prominence among African academic institutions, and Ghana is no exception. This can largely be attributed to the enduring value of hosting research outputs from institutional and individual depositors. Despite its increasing adoption, there is a growing concern about the sustainability of open access IRs, particularly in Africa. However, most of these factors that threaten the sustainability of IRs on the continent can be mitigated by enacting comprehensive institutional policies. Thus, this study sought to examine the role of institutional policies in the sustainability of IRs in Ghana. A total of 830 respondents comprised of IR managers, library staff, postgraduate students, lecturers and university librarians (management) from five public universities in Ghana took part in this study. Questionnaires, semi-structured interviews and document analysis were the main instruments used for data collection. The study yielded an overall response rate of 92.8%. The study findings revealed that public universities in Ghana have institutional IR policies that guide the operation, usage and management of their IRs. However, these policies were persuasive in nature and mainly focused on content submission and generation issues. The study underscored the IR policyâs importance in addressing content generation, awareness, advocacy and copyright restriction challenges. The study recommends the necessity of IR policies to focus on other factors such as technical requirements, expertise and others to ensure the sustainability of these repositories. Â© The Author(s) 2024.","Institutional repositories (IRs) are increasingly gaining prominence among African academic institutions, and Ghana is no exception. This can largely be attributed to the enduring value of hosting research outputs from institutional and individual depositors. Despite its increasing adoption, there is a growing concern about the sustainability of open access IRs, particularly in Africa. However, most of these factors that threaten the sustainability of IRs on the continent can be mitigated by enacting comprehensive institutional policies. Thus, this study sought to examine the role of institutional policies in the sustainability of IRs in Ghana. A total of 830 respondents comprised of IR managers, library staff, postgraduate students, lecturers and university librarians (management) from five public universities in Ghana took part in this study. Questionnaires, semi-structured interviews and document analysis were the main instruments used for data collection. The study yielded an overall response rate of 92.8%. The study findings revealed that public universities in Ghana have institutional IR policies that guide the operation, usage and management of their IRs. However, these policies were persuasive in nature and mainly focused on content submission and generation issues. The study underscored the IR policys importance in addressing content generation, awareness, advocacy and copyright restriction challenges. The study recommends the necessity of IR policies to focus on other factors such as technical requirements, expertise and others to ensure the sustainability of these repositories."
Research on the strategy for improving the utility of government social media information based on a multi-agent game model,"Government social media (GSM) has become an important tool for government departments to open information, guide public opinion and interact with the government and the people. However, the operation and maintenance of some GSM are not standardised, and the content published is inconsistent with identity positioning, resulting in the realistic dilemma of low utility of GSM information. The purpose of this study is to explore the effective strategies to improve the effectiveness of GSM information. The research is from the perspective of information economics, this article uses evolutionary game theory to build a tripartite evolutionary game model comprising GSM operations departments, government regulators and users in order to explore the evolution process of tripartite game behaviours and the influence of subject behaviour selection on information utility. It subsequently conducts a solution and numerical simulation to demonstrate the influence of different factors on the game results. The experimental results show that there are four situations in which the utility of GSM information affects the evolution and stability strategy of the subject and that changes in different parameter values have significant effects on the results of the three-party game. The evolution trend of the subject behaviour can be changed by increasing the regulatory means of rewards and punishments and establishing an efficient operation mechanism for GSM, thus promoting system convergence to the ideal state. The results of this study can provide references and suggestions for government departments to effectively enhance the effectiveness of GSM information and promote the healthy development of GSM. Â© The Author(s) 2024.","Government social media (GSM) has become an important tool for government departments to open information, guide public opinion and interact with the government and the people. However, the operation and maintenance of some GSM are not standardised, and the content published is inconsistent with identity positioning, resulting in the realistic dilemma of low utility of GSM information. The purpose of this study is to explore the effective strategies to improve the effectiveness of GSM information. The research is from the perspective of information economics, this article uses evolutionary game theory to build a tripartite evolutionary game model comprising GSM operations departments, government regulators and users in order to explore the evolution process of tripartite game behaviours and the influence of subject behaviour selection on information utility. It subsequently conducts a solution and numerical simulation to demonstrate the influence of different factors on the game results. The experimental results show that there are four situations in which the utility of GSM information affects the evolution and stability strategy of the subject and that changes in different parameter values have significant effects on the results of the three-party game. The evolution trend of the subject behaviour can be changed by increasing the regulatory means of rewards and punishments and establishing an efficient operation mechanism for GSM, thus promoting system convergence to the ideal state. The results of this study can provide references and suggestions for government departments to effectively enhance the effectiveness of GSM information and promote the healthy development of GSM."
"Information in the personal collections of writers and artists: Practices, challenges and preservation","This article presents findings from interviews with 18 writers and artists in New Zealand, whose lives and work have potential heritage value. The objective was to investigate the perceived value of participantsâ personal collections, the relevant management practices and challenges, and their potential effects on preservation and (re)use. The findings provide a characterisation of the personal information management (PIM) practices of writers and artists, revealed challenges common to organising personal collections across time and devices as well as those caused or increased by the nature of writersâ and artistsâ work, and produce insights into the impact of perceived collection value and PIM practices on future access, preservation and (re)use of such collections. Â© The Author(s) 2022.","This article presents findings from interviews with 18 writers and artists in New Zealand, whose lives and work have potential heritage value. The objective was to investigate the perceived value of participants personal collections, the relevant management practices and challenges, and their potential effects on preservation and (re)use. The findings provide a characterisation of the personal information management (PIM) practices of writers and artists, revealed challenges common to organising personal collections across time and devices as well as those caused or increased by the nature of writers and artists work, and produce insights into the impact of perceived collection value and PIM practices on future access, preservation and (re)use of such collections."
"You change the way you talk: Examining the network, toxicity and discourse of cross-platform users on Twitter and Parler during the 2020 US Presidential Election","This study examines code-switching behaviours of cross-platform social media users specifically between Twitter and Parler during the 2020 US Presidential Election. Utilising social identity theory as a framework, we examine messages related to voter fraud by users who migrated from Twitter to Parler following Twitter bans. Our analysis covers 38,798 accounts active on both platforms, analysing 1.5 million tweets and more than 100,000 parleys. The key findings of the study are as follows: First, we discovered differing levels of network homophily between high degree centrality and low-degree centrality cross-platform users, illustrating how individuals with varying degrees of influence engage differently across platforms. Second, we observed higher toxicity levels in heterogeneous networks, which include both in-group and out-group members, compared with homogeneous networks that are primarily composed of in-group members. This suggests the level of toxicity in online spaces correlates with the level of group diversity. Third, we found that cross-platform users created distinctive discourse community with in-group and out-group members, indicating that content and discussions within these networks are influenced by the social identity dynamics of the users. Our study contributes to the current research in political communication and information science by proposing comparative user analyses across multiple social media platforms. Focusing on a critical period of platform transition during a contentious political event, our study offers insights into the dynamics of online communities and the shifting nature of political language used by social media users. Â© The Author(s) 2024.","This study examines code-switching behaviours of cross-platform social media users specifically between Twitter and Parler during the 2020 US Presidential Election. Utilising social identity theory as a framework, we examine messages related to voter fraud by users who migrated from Twitter to Parler following Twitter bans. Our analysis covers 38,798 accounts active on both platforms, analysing 1.5 million tweets and more than 100,000 parleys. The key findings of the study are as follows: First, we discovered differing levels of network homophily between high degree centrality and low-degree centrality cross-platform users, illustrating how individuals with varying degrees of influence engage differently across platforms. Second, we observed higher toxicity levels in heterogeneous networks, which include both in-group and out-group members, compared with homogeneous networks that are primarily composed of in-group members. This suggests the level of toxicity in online spaces correlates with the level of group diversity. Third, we found that cross-platform users created distinctive discourse community with in-group and out-group members, indicating that content and discussions within these networks are influenced by the social identity dynamics of the users. Our study contributes to the current research in political communication and information science by proposing comparative user analyses across multiple social media platforms. Focusing on a critical period of platform transition during a contentious political event, our study offers insights into the dynamics of online communities and the shifting nature of political language used by social media users."
Benefits of open access to researchers from lower-income countries: A global analysis of reference patterns in 1980â2020,"The main objective of the open access (OA) movement is to make scientific literature freely available to everyone. This may be of particular importance to researchers in lower-income countries, who often face barriers due to high subscription costs. In this article, we address this issue by analysing over time the reference lists of scientific publications around the world. Our study focuses on key issues, including whether researchers from lower-income countries reference fewer publications in their research and how this trend evolves over time. We also investigate whether researchers from lower-income countries rely more on the literature that is openly available through different OA routes compared with other researchers. Our study revealed that the proportion of OA references has increased over time for all publications and country groups. However, publications from lower-income countries have seen a higher growth rate of OA-based references, suggesting that the emergence of OA publishing has been particularly advantageous to researchers in these countries. Â© The Author(s) 2024.","The main objective of the open access (OA) movement is to make scientific literature freely available to everyone. This may be of particular importance to researchers in lower-income countries, who often face barriers due to high subscription costs. In this article, we address this issue by analysing over time the reference lists of scientific publications around the world. Our study focuses on key issues, including whether researchers from lower-income countries reference fewer publications in their research and how this trend evolves over time. We also investigate whether researchers from lower-income countries rely more on the literature that is openly available through different OA routes compared with other researchers. Our study revealed that the proportion of OA references has increased over time for all publications and country groups. However, publications from lower-income countries have seen a higher growth rate of OA-based references, suggesting that the emergence of OA publishing has been particularly advantageous to researchers in these countries."
Evaluating public sector employee perceptions towards artificial intelligence and generative artificial intelligence integration,"This study investigates the emerging field of innovative technology applications for public usage, focusing on employee perspectives. The research employs a questionnaire-based approach, collecting responses from 439 participants and examining demographics, technological proficiency, utility perceptions, personal data concerns, attitudes towards artificial intelligence and generative artificial intelligence, and willingness to endorse technology adoption. The data analysis minimises discrepancies between predicted and actual values through multiple linear regression. In addition, statistical methods such as Spearmanâs Ï, the WilcoxonâMannâWhitney test and chi-square statistics are employed to consolidate the findings, ensuring the thoroughness and validity of the research process. The results indicate a positive inclination among participants to perceive artificial intelligence as augmentative rather than a replacement in public usage contexts. The researchâs originality lies in the unique contribution of employees to technology adoption and strategic knowledge asset renewal for the management in the public domain. Â© The Author(s) 2024.","This study investigates the emerging field of innovative technology applications for public usage, focusing on employee perspectives. The research employs a questionnaire-based approach, collecting responses from 439 participants and examining demographics, technological proficiency, utility perceptions, personal data concerns, attitudes towards artificial intelligence and generative artificial intelligence, and willingness to endorse technology adoption. The data analysis minimises discrepancies between predicted and actual values through multiple linear regression. In addition, statistical methods such as Spearmans , the WilcoxonMannWhitney test and chi-square statistics are employed to consolidate the findings, ensuring the thoroughness and validity of the research process. The results indicate a positive inclination among participants to perceive artificial intelligence as augmentative rather than a replacement in public usage contexts. The researchs originality lies in the unique contribution of employees to technology adoption and strategic knowledge asset renewal for the management in the public domain."
Research perspectives of supply chain knowledge management: A bibliometrics study,"The purpose of this article is to present a bibliometric analysis of scientific publications on knowledge management (KM) and supply chain (SC), provide an overview of research activities in this field and recognise its most substantial and fundamental aspects. In addition, this study aims to quantitatively analyse KM in SC in other words supply chain knowledge management (SCKM) research trends, forecasts and citations from 1999 to 2021 in Web of Science (WOS). A total of 499 documents related to SCKM research were collected from the following databases: SCI-EXPAND, SSCI, AHCI and ESCI. These documents were carefully reviewed and subjected to bibliometric data analysis techniques. We map the time trend, disciplinary distribution, high-frequency keywords and topics, major authors and influential publications to show emerging topics. Based on the findings of other researchers, many implications emerged that improve oneâs understanding of the identity of SCKM as a distinct multi-discipline scientific field, and its publication has grown significantly since 2018. SC papers were published in engineering and Operations Research and Management Science journals, while the coverage of research topics related to KM and SCKM is broader and more interdisciplinary than those of SC papers. High-frequency keywords associated with SCKM research are listed. From the citation of 499 papers with both KM and SC as keywords, we find the most popular one. Â© The Author(s) 2024.","The purpose of this article is to present a bibliometric analysis of scientific publications on knowledge management (KM) and supply chain (SC), provide an overview of research activities in this field and recognise its most substantial and fundamental aspects. In addition, this study aims to quantitatively analyse KM in SC in other words supply chain knowledge management (SCKM) research trends, forecasts and citations from 1999 to 2021 in Web of Science (WOS). A total of 499 documents related to SCKM research were collected from the following databases: SCI-EXPAND, SSCI, AHCI and ESCI. These documents were carefully reviewed and subjected to bibliometric data analysis techniques. We map the time trend, disciplinary distribution, high-frequency keywords and topics, major authors and influential publications to show emerging topics. Based on the findings of other researchers, many implications emerged that improve ones understanding of the identity of SCKM as a distinct multi-discipline scientific field, and its publication has grown significantly since 2018. SC papers were published in engineering and Operations Research and Management Science journals, while the coverage of research topics related to KM and SCKM is broader and more interdisciplinary than those of SC papers. High-frequency keywords associated with SCKM research are listed. From the citation of 499 papers with both KM and SC as keywords, we find the most popular one."
AI- and LLM-driven search tools: A paradigm shift in information access for education and research,"The article reports on an exploratory study that assesses the results produced by emerging artificial intelligence (AI)- and large language model-driven search tools in response to a series of queries and prompts based on four scenarios of information-intensive tasks of university students and researchers. Sixteen questions and prompts were created based on four scenarios of information-intensive tasks of university students. Each of these questions and prompts was presented to six AI-driven search tools, and the results were manually checked to assess their suitability for specific user needs and contexts. Based on the findings, it was argued that while the AI-driven tools bring a paradigm shift in information access for education and research, outputs generated by these tools vary quite significantly. Choice of the right tool, framing the question and further prompting play a key role. Also, users need to scrutinise each output to check their quality and reliability in the context of the specific search tasks. It was concluded that further research is needed involving different user groups, scenarios and search tasks and different AI-driven search tools. Implications of the use of AI-driven search tools for libraries and scholarly databases, as well as for research and scholarship in different areas of information science, are discussed. Â© The Author(s) 2024.","The article reports on an exploratory study that assesses the results produced by emerging artificial intelligence (AI)- and large language model-driven search tools in response to a series of queries and prompts based on four scenarios of information-intensive tasks of university students and researchers. Sixteen questions and prompts were created based on four scenarios of information-intensive tasks of university students. Each of these questions and prompts was presented to six AI-driven search tools, and the results were manually checked to assess their suitability for specific user needs and contexts. Based on the findings, it was argued that while the AI-driven tools bring a paradigm shift in information access for education and research, outputs generated by these tools vary quite significantly. Choice of the right tool, framing the question and further prompting play a key role. Also, users need to scrutinise each output to check their quality and reliability in the context of the specific search tasks. It was concluded that further research is needed involving different user groups, scenarios and search tasks and different AI-driven search tools. Implications of the use of AI-driven search tools for libraries and scholarly databases, as well as for research and scholarship in different areas of information science, are discussed."
"The international dissemination of Chinaâs English academic journals on Twitter: Structure, theme and impact","Objective/Significance: To measure the international communication capacity of Chinaâs English academic journal papers based on social media platforms. Method/Process: Social network analysis, content analysis and emotion analysis were used to describe the diffusion structure of Chinaâs English academic journal papers on Twitter from the aspects of network structure and identify important node users, explore the identity characteristics and emotion characteristics of users as well as diffusion capability of journals of different disciplines. Conclusion/Discovery: Journal articles in the fields of natural sciences and engineering as well as humanities and social sciences exhibit a strong positive correlation between their diffusion structure and network size. Networks with a mixed diffusion structure typically have larger network sizes. The users occupying key positions in the diffusion network typically have profile information that is academically relevant. The retweets and comments are mainly statements without emotional inclination, and the comments are mainly summary descriptions of the papers. The distribution of diffusion width, diffusion strength and diffusion speed of each discipline is significantly different. There is a significant positive correlation between the diffusion structure of papers and their diffusion capabilities. Innovation/Value: To reveal the diffusion structure and characteristics of Chinaâs English academic journal papers on social media platforms, so as to provide reference for expanding the visibility of Chinaâs English academic journals and improving the international dissemination capacity of Chinaâs English journals. Â© The Author(s) 2024.","Objective/Significance: To measure the international communication capacity of Chinas English academic journal papers based on social media platforms. Method/Process: Social network analysis, content analysis and emotion analysis were used to describe the diffusion structure of Chinas English academic journal papers on Twitter from the aspects of network structure and identify important node users, explore the identity characteristics and emotion characteristics of users as well as diffusion capability of journals of different disciplines. Conclusion/Discovery: Journal articles in the fields of natural sciences and engineering as well as humanities and social sciences exhibit a strong positive correlation between their diffusion structure and network size. Networks with a mixed diffusion structure typically have larger network sizes. The users occupying key positions in the diffusion network typically have profile information that is academically relevant. The retweets and comments are mainly statements without emotional inclination, and the comments are mainly summary descriptions of the papers. The distribution of diffusion width, diffusion strength and diffusion speed of each discipline is significantly different. There is a significant positive correlation between the diffusion structure of papers and their diffusion capabilities. Innovation/Value: To reveal the diffusion structure and characteristics of Chinas English academic journal papers on social media platforms, so as to provide reference for expanding the visibility of Chinas English academic journals and improving the international dissemination capacity of Chinas English journals."
The digital ecosystem information framework: Insights from action design research,"Digital ecosystem (DE) is a dynamic configuration of informational organisms, individual and organisational actors, which interact in the digitally networked and federated environment. Traditional approaches are challenged by the need for handling information in complex DE where information flows beyond the boundary of a single actor. This article presents the informational organism-interaction centric digital ecosystem information (DEi) framework for information operations, management, and governance. The DEi framework emerged based on the insights obtained through the application of well-known thematic network analysis and abstraction, reflection and learning techniques to 15 action design research projects across nine different industry partners in Australia. The DEi framework includes 27 topics that are organised into nine key knowledge and three focus areas. The DEi framework can be used by researchers and practitioners as a resource for designing digital information capabilities as appropriate to their context. Â© The Author(s) 2022.","Digital ecosystem (DE) is a dynamic configuration of informational organisms, individual and organisational actors, which interact in the digitally networked and federated environment. Traditional approaches are challenged by the need for handling information in complex DE where information flows beyond the boundary of a single actor. This article presents the informational organism-interaction centric digital ecosystem information (DEi) framework for information operations, management, and governance. The DEi framework emerged based on the insights obtained through the application of well-known thematic network analysis and abstraction, reflection and learning techniques to 15 action design research projects across nine different industry partners in Australia. The DEi framework includes 27 topics that are organised into nine key knowledge and three focus areas. The DEi framework can be used by researchers and practitioners as a resource for designing digital information capabilities as appropriate to their context."
Attitudes and practices of educational researchers towards the use of social media to disseminate science,"In recent years, there has been a notable increase in the use of digital platforms in higher education and science. This tendency has impacted how knowledge is produced, accessed and disseminated, considering the Internet and social media strategies. This study seeks to investigate the attitudes and practices of educational researchers when it comes to sharing science on social media. An online survey (N = 487) was used to measure participantsâ motivations for using or not social media, frequency of use, attitudes and practices for sharing scientific research and sociodemographic characteristics. Overall, findings reveal that there is high support for the use of social media for academic purposes. Most researchers prefer to publish full results over partial results. The researcherâs perception of the importance of social media is greater than the actual use of them. Finally, we identify some of the main reasons that facilitate or limit the academic use of social media, thus contributing a contextualised reflection on such use. Â© The Author(s) 2024.","In recent years, there has been a notable increase in the use of digital platforms in higher education and science. This tendency has impacted how knowledge is produced, accessed and disseminated, considering the Internet and social media strategies. This study seeks to investigate the attitudes and practices of educational researchers when it comes to sharing science on social media. An online survey (N = 487) was used to measure participants motivations for using or not social media, frequency of use, attitudes and practices for sharing scientific research and sociodemographic characteristics. Overall, findings reveal that there is high support for the use of social media for academic purposes. Most researchers prefer to publish full results over partial results. The researchers perception of the importance of social media is greater than the actual use of them. Finally, we identify some of the main reasons that facilitate or limit the academic use of social media, thus contributing a contextualised reflection on such use."
Infusing factual knowledge into pre-trained model for finding the contributions from the research articles,"The growing volume of scientific literature makes it difficult for researchers to identify the key contributions of a research paper. Automating this process would facilitate efficient understanding, faster literature surveys and comparisons. The automated process may help researchers to identify relevant and impactful information in less time and effort. In this article, we address the challenge of identifying the contributions in research articles. We propose a method that infuses factual knowledge from a scientific knowledge graph into a pre-trained model. We divide the knowledge graph into mutually exclusive subgroups and infuse the knowledge in the pre-trained model using adapters. We also construct a scientific knowledge graph consisting of 3,600 Natural Language Processing (NLP) papers to acquire factual knowledge. In addition, we annotate a new test set to evaluate the modelâs ability to identify sentences that make significant contributions to the papers. Our model achieves the best performance in comparison to previous methods with a relative improvement of 40.06% and 25.28% in terms of F1 score for identifying contributing sentences in the NLPContributionGraph (NCG) test set and the newly annotated test set, respectively. Â© The Author(s) 2024.","The growing volume of scientific literature makes it difficult for researchers to identify the key contributions of a research paper. Automating this process would facilitate efficient understanding, faster literature surveys and comparisons. The automated process may help researchers to identify relevant and impactful information in less time and effort. In this article, we address the challenge of identifying the contributions in research articles. We propose a method that infuses factual knowledge from a scientific knowledge graph into a pre-trained model. We divide the knowledge graph into mutually exclusive subgroups and infuse the knowledge in the pre-trained model using adapters. We also construct a scientific knowledge graph consisting of 3,600 Natural Language Processing (NLP) papers to acquire factual knowledge. In addition, we annotate a new test set to evaluate the models ability to identify sentences that make significant contributions to the papers. Our model achieves the best performance in comparison to previous methods with a relative improvement of 40.06% and 25.28% in terms of F1 score for identifying contributing sentences in the NLPContributionGraph (NCG) test set and the newly annotated test set, respectively."
An empirical study of factors influencing usage intention for generative artificial intelligence products: A case study of China,"The rapid advancement of generative artificial intelligence technologies has sparked widespread interest, yet understanding of user adoption patterns for these tools remains limited. While the unified theory of acceptance and use of technology model has been widely applied to various technologies, its applicability to generative artificial intelligence products, which present unique challenges and opportunities, has not been thoroughly explored. This gap in knowledge hinders the development of effective strategies for promoting user acceptance and optimising the design of generative artificial intelligence tools. This study extends the unified theory of acceptance and use of technology model to investigate the factors influencing usage intention for generative artificial intelligence products. The study incorporates additional variables such as digital literacy, perceived risk and perceived trust to provide a more comprehensive framework. Using structural equation modelling to analyse survey data, the study found that effort expectancy, performance expectancy, social influence, and perceived trust significantly and positively impact usage intention. In addition, digital literacy indirectly enhances usage intention through effort expectancy, while perceived risk negatively influences usage intention through reduced trust. Notably, facilitating conditions did not exhibit a significant effect on usage intention. These findings offer valuable insights for developers and researchers in the field of generative artificial intelligence, highlighting the importance of user-friendly design, performance optimisation, and trust-building measures. By identifying key factors that drive user adoption, this study contributes to a more nuanced understanding of technology acceptance in the context of advanced artificial intelligence systems, paving the way for more effective development and implementation strategies in this rapidly evolving field. Â© The Author(s) 2024.","The rapid advancement of generative artificial intelligence technologies has sparked widespread interest, yet understanding of user adoption patterns for these tools remains limited. While the unified theory of acceptance and use of technology model has been widely applied to various technologies, its applicability to generative artificial intelligence products, which present unique challenges and opportunities, has not been thoroughly explored. This gap in knowledge hinders the development of effective strategies for promoting user acceptance and optimising the design of generative artificial intelligence tools. This study extends the unified theory of acceptance and use of technology model to investigate the factors influencing usage intention for generative artificial intelligence products. The study incorporates additional variables such as digital literacy, perceived risk and perceived trust to provide a more comprehensive framework. Using structural equation modelling to analyse survey data, the study found that effort expectancy, performance expectancy, social influence, and perceived trust significantly and positively impact usage intention. In addition, digital literacy indirectly enhances usage intention through effort expectancy, while perceived risk negatively influences usage intention through reduced trust. Notably, facilitating conditions did not exhibit a significant effect on usage intention. These findings offer valuable insights for developers and researchers in the field of generative artificial intelligence, highlighting the importance of user-friendly design, performance optimisation, and trust-building measures. By identifying key factors that drive user adoption, this study contributes to a more nuanced understanding of technology acceptance in the context of advanced artificial intelligence systems, paving the way for more effective development and implementation strategies in this rapidly evolving field."
"Essential elements, conceptual foundations and workflow design in crowd-powered projects","Crowdsourcing arose as a problem-solving strategy that uses a large number of workers to achieve tasks and solve specific problems. Although there are many studies that explore crowdsourcing platforms and systems, little attention has been paid to define what a crowd-powered project is. To address this issue, this article introduces a general-purpose conceptual model that represents the essential elements involved in this kind of project and how they relate to each other. We consider that the workflow in crowdsourcing projects is context-oriented and should represent the planning and coordination by the crowdsourcer in the project, instead of only facilitating decomposing a complex task into subtask sets. Since structural models are limited to cannot properly represent the execution flow, we also introduce the use of behavioural conceptual models, specifically Unified Modeling Language (UML) activity diagrams, to represent the user, tasks, assets, control activities and products involved in a specific project. Â© The Author(s) 2022.","Crowdsourcing arose as a problem-solving strategy that uses a large number of workers to achieve tasks and solve specific problems. Although there are many studies that explore crowdsourcing platforms and systems, little attention has been paid to define what a crowd-powered project is. To address this issue, this article introduces a general-purpose conceptual model that represents the essential elements involved in this kind of project and how they relate to each other. We consider that the workflow in crowdsourcing projects is context-oriented and should represent the planning and coordination by the crowdsourcer in the project, instead of only facilitating decomposing a complex task into subtask sets. Since structural models are limited to cannot properly represent the execution flow, we also introduce the use of behavioural conceptual models, specifically Unified Modeling Language (UML) activity diagrams, to represent the user, tasks, assets, control activities and products involved in a specific project."
Government chatbot: Empowering smart conversations with enhanced contextual understanding and reasoning,"Currently, an increasing number of governments have adopted question answering systems (QASs) in public service delivery. As some citizens with limited information literacy often express their questions vaguely when interacting with a chatbot, it is necessary to improve the contextual understanding and reasoning ability of government chatbots (G-chatbots). This goal can be achieved through the optimisation of the matching between question, answer and context. By incorporating the Relational Graph Convolutional Networks (R-GCNs) and fuzzy logic, this study proposes a multi-turn dialogue model that introduces a re-question mechanism and a subgraph matching algorithm. The experiment results show that the model can improve the contextual reasoning ability of G-chatbots by about 10% and generate answers in a more explainable way. This study innovatively integrates a questionâanswerâcontext matching approach, re-question mechanism into the MTRF-G-chatbot model, reducing barriers to citizensâ access to government services and enhancing contextual reasoning abilities. Â© The Author(s) 2024.","Currently, an increasing number of governments have adopted question answering systems (QASs) in public service delivery. As some citizens with limited information literacy often express their questions vaguely when interacting with a chatbot, it is necessary to improve the contextual understanding and reasoning ability of government chatbots (G-chatbots). This goal can be achieved through the optimisation of the matching between question, answer and context. By incorporating the Relational Graph Convolutional Networks (R-GCNs) and fuzzy logic, this study proposes a multi-turn dialogue model that introduces a re-question mechanism and a subgraph matching algorithm. The experiment results show that the model can improve the contextual reasoning ability of G-chatbots by about 10% and generate answers in a more explainable way. This study innovatively integrates a questionanswercontext matching approach, re-question mechanism into the MTRF-G-chatbot model, reducing barriers to citizens access to government services and enhancing contextual reasoning abilities."
Development overview and research hotspots of Information Science & Library Science based on SSCI data,"This study uses the scientometric method to analyse the literature to summarise the development overview and research hotspots in the field of Library and Information Science (LIS), aiming at sorting out the knowledge structure of the discipline and providing references and information for deepening its research and development. This article takes 85 Social Science Citation Index (SSCI) journals in the field of LIS as the research object, from which 116,570 articles from 1900 to 2023 are collected as the basis of data analysis. It employs the scientometric method based on visualisation software to analyse four dimensions of the LIS discipline: annual publications, journal outputs distribution, production and cooperation, and present focus and research hotspots, finding that the overall development of the discipline is stable and shows some new topics. Â© The Author(s) 2024.","This study uses the scientometric method to analyse the literature to summarise the development overview and research hotspots in the field of Library and Information Science (LIS), aiming at sorting out the knowledge structure of the discipline and providing references and information for deepening its research and development. This article takes 85 Social Science Citation Index (SSCI) journals in the field of LIS as the research object, from which 116,570 articles from 1900 to 2023 are collected as the basis of data analysis. It employs the scientometric method based on visualisation software to analyse four dimensions of the LIS discipline: annual publications, journal outputs distribution, production and cooperation, and present focus and research hotspots, finding that the overall development of the discipline is stable and shows some new topics."
Knowledge management activities: Conceptual foundations and research issues,"Knowledge is a broad concept whose epistemological construct has been debated since the days of the early Greek philosophers. Knowledge was discussed extensively during the Renaissance, became a central area of study during the Scientific Revolution and was applied extensively within organisations throughout the Industrial Revolution. Knowledge became an organisational resource of significant interest, emerging over the past 25 years as a unique field of study called knowledge management (KM). Much of the KM literature addresses matters of practice and application; what is missing is a deep and conceptual analysis of the activities that drive KM processes. This article provides a conceptualisation of KM activities focusing on the underlying foundations of these activities. The result is a rich framework of KM activities that can be used to pursue important research areas involved in studying KM processes, including theory development, areas of overlap and where further research is needed. Â© The Author(s) 2022.","Knowledge is a broad concept whose epistemological construct has been debated since the days of the early Greek philosophers. Knowledge was discussed extensively during the Renaissance, became a central area of study during the Scientific Revolution and was applied extensively within organisations throughout the Industrial Revolution. Knowledge became an organisational resource of significant interest, emerging over the past 25 years as a unique field of study called knowledge management (KM). Much of the KM literature addresses matters of practice and application; what is missing is a deep and conceptual analysis of the activities that drive KM processes. This article provides a conceptualisation of KM activities focusing on the underlying foundations of these activities. The result is a rich framework of KM activities that can be used to pursue important research areas involved in studying KM processes, including theory development, areas of overlap and where further research is needed."
The impact of being selected as a cover paper: Evidence from high-impact materials science journals,"In the era of print reading, being selected as a cover paper holds a crucial role in attracting greater attention and bolstering academic influence. It is important to assess its effect on scholarly attention and academic influence, particularly in light of the evolving reading habits among researchers. In this study, we empirically estimate the impact of âbeing selected as a cover paperâ on scholarly online attention (proxied by altmetric score) and academic influence (measured by citation counts). This analysis is based on a data set comprising 25,238 papers selected from 10 high-impact materials science journals (with journal impact factors exceeding 10) published between 2016 and 2020. Our findings indicate a positive correlation between âbeing selected as a cover paperâ and scholarly online attention, while its impact on academic influence is insignificant. Our results remain robust even when excluding the top 1% mostly cited papers, employing the negative binomial model and considering various time windows for estimation. Heterogeneity analysis indicates that the impact of âbeing selected as a cover paperâ on scholarly online attention holds across nearly all topics, consistent with the baseline result. In addition, online platforms, such as Twitter and News outlets, exhibit a higher frequency of sharing research featured as cover papers. We offer suggestive evidence that âbeing selected as a cover paperâ is not solely contingent on its quality. These findings contribute to the development of a precise, dynamic and multi-dimensional evaluation framework, crucial for navigating the revolution of science communication. Â© The Author(s) 2024.","In the era of print reading, being selected as a cover paper holds a crucial role in attracting greater attention and bolstering academic influence. It is important to assess its effect on scholarly attention and academic influence, particularly in light of the evolving reading habits among researchers. In this study, we empirically estimate the impact of being selected as a cover paper on scholarly online attention (proxied by altmetric score) and academic influence (measured by citation counts). This analysis is based on a data set comprising 25,238 papers selected from 10 high-impact materials science journals (with journal impact factors exceeding 10) published between 2016 and 2020. Our findings indicate a positive correlation between being selected as a cover paper and scholarly online attention, while its impact on academic influence is insignificant. Our results remain robust even when excluding the top 1% mostly cited papers, employing the negative binomial model and considering various time windows for estimation. Heterogeneity analysis indicates that the impact of being selected as a cover paper on scholarly online attention holds across nearly all topics, consistent with the baseline result. In addition, online platforms, such as Twitter and News outlets, exhibit a higher frequency of sharing research featured as cover papers. We offer suggestive evidence that being selected as a cover paper is not solely contingent on its quality. These findings contribute to the development of a precise, dynamic and multi-dimensional evaluation framework, crucial for navigating the revolution of science communication."
The imitation game: Detecting human and AI-generated texts in the era of ChatGPT and BARD,"The potential of artificial intelligence (AI)-based large language models (LLMs) holds considerable promise in revolutionising education, research and practice. However, distinguishing between human-written and AI-generated text has become a significant task. This article presents a comparative study, introducing a novel dataset of human-written and LLM-generated texts in different genres: essays, stories, poetry and Python code. We employ several machine learning models to classify the texts. Results demonstrate the efficacy of these models in discerning between human and AI-generated text, despite the datasetâs limited sample size. However, the task becomes more challenging when classifying GPT-generated text, particularly in story writing. The results indicate that the models exhibit superior performance in binary classification tasks, such as distinguishing human-generated text from a specific LLM, compared with the more complex multiclass tasks that involve discerning among human-generated and multiple LLMs. Our findings provide insightful implications for AI text detection, while our dataset paves the way for future research in this evolving area. Â© The Author(s) 2024.","The potential of artificial intelligence (AI)-based large language models (LLMs) holds considerable promise in revolutionising education, research and practice. However, distinguishing between human-written and AI-generated text has become a significant task. This article presents a comparative study, introducing a novel dataset of human-written and LLM-generated texts in different genres: essays, stories, poetry and Python code. We employ several machine learning models to classify the texts. Results demonstrate the efficacy of these models in discerning between human and AI-generated text, despite the datasets limited sample size. However, the task becomes more challenging when classifying GPT-generated text, particularly in story writing. The results indicate that the models exhibit superior performance in binary classification tasks, such as distinguishing human-generated text from a specific LLM, compared with the more complex multiclass tasks that involve discerning among human-generated and multiple LLMs. Our findings provide insightful implications for AI text detection, while our dataset paves the way for future research in this evolving area."
Realising the potential of information acquisition in promoting sustainable agriculture: A systematic review,"A growing body of literature identifies information constraints as a potential barrier to farmersâ adoption of sustainable agricultural practices (SAPs) and information acquisition (IA) as the key to breaking the dilemma. Due to the heterogeneity of information, channels and farmers, IA has different effects on farmersâ sustainable behaviour in different SAP scenarios. This study systematically reviews the mechanisms, empirical cases, models and methods of the impact of IA on farmersâ adoption of SAPs. Results show that IA plays a crucial role in promoting SAPs, and exposure to different information sources/channels usually has differential effects on farmersâ intention or behaviour to adopt SAPs. It is worth noting that the positive effect of the use of modern information channels (especially the Internet and smartphone) on SAPs is gradually being demonstrated in empirical studies across countries, and the effect of Internet or smartphone use on farmers with different resource endowment characteristics may also be heterogeneous. According to further analysis, traditional and modern information channels should complement each other and be fully integrated into SAPs extensions. This study also discusses the gaps that need to be bridged in IA-to-SAPs and describes possible future research directions. These findings will contribute to the scientific design of information interventions and environmental policies in SAPs extension services. Â© The Author(s) 2024.","A growing body of literature identifies information constraints as a potential barrier to farmers adoption of sustainable agricultural practices (SAPs) and information acquisition (IA) as the key to breaking the dilemma. Due to the heterogeneity of information, channels and farmers, IA has different effects on farmers sustainable behaviour in different SAP scenarios. This study systematically reviews the mechanisms, empirical cases, models and methods of the impact of IA on farmers adoption of SAPs. Results show that IA plays a crucial role in promoting SAPs, and exposure to different information sources/channels usually has differential effects on farmers intention or behaviour to adopt SAPs. It is worth noting that the positive effect of the use of modern information channels (especially the Internet and smartphone) on SAPs is gradually being demonstrated in empirical studies across countries, and the effect of Internet or smartphone use on farmers with different resource endowment characteristics may also be heterogeneous. According to further analysis, traditional and modern information channels should complement each other and be fully integrated into SAPs extensions. This study also discusses the gaps that need to be bridged in IA-to-SAPs and describes possible future research directions. These findings will contribute to the scientific design of information interventions and environmental policies in SAPs extension services."
Ranking of Iranian medical universities based on altmetric indices,"This study was aimed at evaluating the Iranian medical universitiesâ rankings and altmetric indices in ResearchGate and Academia.edu. This cross-sectional analytical study was conducted using a scientometric method. Social networking measures were collected in MS Excel from January to February 2017. Data were analysed using SPSS software and the Spearman, chi-square and Kendall rank coefficient tests. Ranking information and altmetric indices of 50 Iranian medical universities were collected and analysed. All of the type-1 medical universities have been presented in the Academia.edu and ResearchGate social networks. A statistically significant relationship (P < 0.05) has been found between the ranking systems of the universities with altmetric indicators, such as the number of members in both social networks, the number of publications and the total RG score in ResearchGate. In addition, there was a significant correlation between the ranking of medical universities based on the H-index of science metrics with their presence in the ResearchGate social network (P < 0.05). The results of this study reveal a relationship between the universitiesâ ranking systems and altmetric indicators. These findings emphasise the necessity of increasing the presence of faculty members in social network activities in disseminating and sharing knowledge. Â© The Author(s) 2022.","This study was aimed at evaluating the Iranian medical universities rankings and altmetric indices in ResearchGate and Academia.edu. This cross-sectional analytical study was conducted using a scientometric method. Social networking measures were collected in MS Excel from January to February 2017. Data were analysed using SPSS software and the Spearman, chi-square and Kendall rank coefficient tests. Ranking information and altmetric indices of 50 Iranian medical universities were collected and analysed. All of the type-1 medical universities have been presented in the Academia.edu and ResearchGate social networks. A statistically significant relationship (P < 0.05) has been found between the ranking systems of the universities with altmetric indicators, such as the number of members in both social networks, the number of publications and the total RG score in ResearchGate. In addition, there was a significant correlation between the ranking of medical universities based on the H-index of science metrics with their presence in the ResearchGate social network (P < 0.05). The results of this study reveal a relationship between the universities ranking systems and altmetric indicators. These findings emphasise the necessity of increasing the presence of faculty members in social network activities in disseminating and sharing knowledge."
Investigation of information security policy violations among oil and gas employees: A security-related stress and avoidance coping perspective,"Information security is one of the most crucial considerations in digitising Oil and Gas (O&G) organisations. For ensuring information security policy compliance, O&G organisations enforce heavy security requirements. The purpose of this article is to assess how O&G employees cope with stressful information security tasks and how security-related stress (SRS) is related to information security policy violations among O&G employees in developing countries. Based on the coping theory, this article develops a theoretical framework to examine O&G employeesâ intention to violate information security policies. The framework is tested using a survey of 270 managers/executives from 150 Malaysian O&G organisations. The results indicated that O&G employees perceive security requirements as stressful to follow and adopt avoidance coping strategies that lead them to violate organisational information security policies. For practitioners, the study findings demonstrate the prevalence of technostress in O&G organisations and suggest alternative mechanisms to address the stressful effects of information security requirements. This article contributes to the information system security literature by testing procrastination and psychological detachment with SRS in the context of developing countries' O&G organisationsâ employees and provides an understanding of how O&G employees adopt avoidance coping. Â© The Author(s) 2022.","Information security is one of the most crucial considerations in digitising Oil and Gas (O&G) organisations. For ensuring information security policy compliance, O&G organisations enforce heavy security requirements. The purpose of this article is to assess how O&G employees cope with stressful information security tasks and how security-related stress (SRS) is related to information security policy violations among O&G employees in developing countries. Based on the coping theory, this article develops a theoretical framework to examine O&G employees intention to violate information security policies. The framework is tested using a survey of 270 managers/executives from 150 Malaysian O&G organisations. The results indicated that O&G employees perceive security requirements as stressful to follow and adopt avoidance coping strategies that lead them to violate organisational information security policies. For practitioners, the study findings demonstrate the prevalence of technostress in O&G organisations and suggest alternative mechanisms to address the stressful effects of information security requirements. This article contributes to the information system security literature by testing procrastination and psychological detachment with SRS in the context of developing countries' O&G organisations employees and provides an understanding of how O&G employees adopt avoidance coping."
E-reading consumption among Pakistani digital immigrants: A mixed-methods approach,"Digital information adoption among the older generation is becoming interesting, and e-reading consumption is an important phenomenon. The current study explores the e-reading consumption experience among Pakistani generation X readers (Xers) through the theory of planned behaviour (TPB), along with TPB model validation through a larger sample. A mixed-method research design (exploratory sequential) was employed. The study was completed in two phases; the first phase was qualitative based on nine (n = 9) in-depth face-to-face interviews. In the second phase, a quantitative research design was employed. A survey questionnaire was developed based on the TPB model and outcomes of the first phase, and the data were collected from 250 Xers from Pakistani public libraries. The first phase outlined numerous positive consequences and challenges specific to the behavioural beliefs. The circle of friends, colleagues and supervisors encourage e-reading consumption given the benefits, speed, and saver of time, cost, and effort, to name a few. Notably, e-reading consumption intention leaves no alternative for Xers in the digital information era. The results of second phase show that seven out of nine proposed hypotheses were supported significantly H2 (Î² = 0.33, p =.00), H3 (Î² = 0.20, p =.02), H4 (Î² = 0.27, p =.00), H6 (Î² = 0.22, p =.01), H7 (Î² = 0.18, p =.03), H8 (Î² =.28, p =.00) and H9 (Î² = â0.15, p =.04), whereas H1 (Î² = â0.03, p =.66) and H5 (Î² = â0.02, p =.73) were rejected. The current study extends the theoretical foundations of TPB in the age of digital information consumption by exploring dimensions qualitatively and tested that proposed relationship quantitatively from a developing country context, Pakistani Xers. Â© The Author(s) 2022.","Digital information adoption among the older generation is becoming interesting, and e-reading consumption is an important phenomenon. The current study explores the e-reading consumption experience among Pakistani generation X readers (Xers) through the theory of planned behaviour (TPB), along with TPB model validation through a larger sample. A mixed-method research design (exploratory sequential) was employed. The study was completed in two phases; the first phase was qualitative based on nine (n = 9) in-depth face-to-face interviews. In the second phase, a quantitative research design was employed. A survey questionnaire was developed based on the TPB model and outcomes of the first phase, and the data were collected from 250 Xers from Pakistani public libraries. The first phase outlined numerous positive consequences and challenges specific to the behavioural beliefs. The circle of friends, colleagues and supervisors encourage e-reading consumption given the benefits, speed, and saver of time, cost, and effort, to name a few. Notably, e-reading consumption intention leaves no alternative for Xers in the digital information era. The results of second phase show that seven out of nine proposed hypotheses were supported significantly H2 ( = 0.33, p =.00), H3 ( = 0.20, p =.02), H4 ( = 0.27, p =.00), H6 ( = 0.22, p =.01), H7 ( = 0.18, p =.03), H8 ( =.28, p =.00) and H9 ( = 0.15, p =.04), whereas H1 ( = 0.03, p =.66) and H5 ( = 0.02, p =.73) were rejected. The current study extends the theoretical foundations of TPB in the age of digital information consumption by exploring dimensions qualitatively and tested that proposed relationship quantitatively from a developing country context, Pakistani Xers."
The impact of mobile social media on knowledge sharing among vocational school teachers: A social cognitive career perspective,"In the rapidly evolving field of information systems, the role of mobile-based social media as a platform for knowledge sharing among vocational schoolteachers presents both opportunities and challenges. This study addresses a critical gap in the understanding of how psychological factors (such as self-efficacy) and contextual factors (such as trust environments) influence knowledge-sharing behaviours in information systems. This study includes 332 vocational schoolteachers and employs structural equation modelling to examine how psychological and contextual factors enhance or inhibit sharing intentions. The results revealed that psychological factors significantly impact sharing intentions, whereas contextual factors bolster self-efficacy and behaviours. The findings offer valuable insights into the optimisation of information systems environments to facilitate effective knowledge sharing, thereby contributing to enhanced collaborative practices and technology adoption in educational settings. This research underscores the need to integrate social cognitive career theory into information systems to better understand the complex dynamics of knowledge sharing on mobile platforms. Â© The Author(s) 2024.","In the rapidly evolving field of information systems, the role of mobile-based social media as a platform for knowledge sharing among vocational schoolteachers presents both opportunities and challenges. This study addresses a critical gap in the understanding of how psychological factors (such as self-efficacy) and contextual factors (such as trust environments) influence knowledge-sharing behaviours in information systems. This study includes 332 vocational schoolteachers and employs structural equation modelling to examine how psychological and contextual factors enhance or inhibit sharing intentions. The results revealed that psychological factors significantly impact sharing intentions, whereas contextual factors bolster self-efficacy and behaviours. The findings offer valuable insights into the optimisation of information systems environments to facilitate effective knowledge sharing, thereby contributing to enhanced collaborative practices and technology adoption in educational settings. This research underscores the need to integrate social cognitive career theory into information systems to better understand the complex dynamics of knowledge sharing on mobile platforms."
A web-based information intervention for family caregivers of patients with Dementia: A randomized controlled trial,"This study aimed to evaluate the efficacy of a web-based health information intervention on knowledge, care burden and attitudes of family caregivers of patients with dementia. This study is a unblinded randomised controlled trial. The study population consisted of family caregivers of patients with dementia (n = 50) which were randomly allocated to the intervention group (access to the web-based health information) or control group (access to information as usual). The participants completed knowledge, care burden and attitude questionnaire at baseline and at two months follow-up. A total of 50 caregivers participated in this study. Before the intervention, there was no statistically significant difference between the knowledge, care burden and attitude score between the two groups. In comparison to the control group after the intervention, participants in the intervention group showed significant improvements in all outcomes. These findings provide further evidence that web-based information interventions helped caregivers feel more confident, empathetic and concerned about dementia care with less care burden. Â© The Author(s) 2022.","This study aimed to evaluate the efficacy of a web-based health information intervention on knowledge, care burden and attitudes of family caregivers of patients with dementia. This study is a unblinded randomised controlled trial. The study population consisted of family caregivers of patients with dementia (n = 50) which were randomly allocated to the intervention group (access to the web-based health information) or control group (access to information as usual). The participants completed knowledge, care burden and attitude questionnaire at baseline and at two months follow-up. A total of 50 caregivers participated in this study. Before the intervention, there was no statistically significant difference between the knowledge, care burden and attitude score between the two groups. In comparison to the control group after the intervention, participants in the intervention group showed significant improvements in all outcomes. These findings provide further evidence that web-based information interventions helped caregivers feel more confident, empathetic and concerned about dementia care with less care burden."
Revisiting delayed recognition in science: A large-scale and comprehensive study,"Delayed recognition, exemplified by the phenomenon of sleeping beauties, presents a compelling narrative within the dynamics of scientific impact and innovation. Our investigation delves into the nuanced facets of delayed acknowledgement, uncovering its profound implications and innovation pathways. Through the analysis of extensive datasets and advanced methodologies, we elucidate the intricate connections between delayed recognition and the realms of scientific and technological influence. Our study not only reveals correlations between atypical combinations of knowledge and the emergence of sleeping beauties but also sheds light on the relationship between delayed recognition and disruptive paradigm shifts in scientific evolution, suggesting their potential role in shaping scientific breakthroughs. Furthermore, our analysis highlights the journey of delayed recognition, often culminating in significant contributions across diverse fields, including notable achievements, such as Nobel-worthy milestones. This article advances our understanding of scientific evolution and the complex landscape of acknowledging pioneering research. Â© The Author(s) 2024.","Delayed recognition, exemplified by the phenomenon of sleeping beauties, presents a compelling narrative within the dynamics of scientific impact and innovation. Our investigation delves into the nuanced facets of delayed acknowledgement, uncovering its profound implications and innovation pathways. Through the analysis of extensive datasets and advanced methodologies, we elucidate the intricate connections between delayed recognition and the realms of scientific and technological influence. Our study not only reveals correlations between atypical combinations of knowledge and the emergence of sleeping beauties but also sheds light on the relationship between delayed recognition and disruptive paradigm shifts in scientific evolution, suggesting their potential role in shaping scientific breakthroughs. Furthermore, our analysis highlights the journey of delayed recognition, often culminating in significant contributions across diverse fields, including notable achievements, such as Nobel-worthy milestones. This article advances our understanding of scientific evolution and the complex landscape of acknowledging pioneering research."
Study on the use and perception of Sci-Hub among Korean researchers,"This study analysed the use and perception of Sci-Hub, a representative Black Open Access website, to understand the behaviours of Korean researchers facing limited access to the information required for research activities. Debates on the role and future of Sci-Hub vary widely, raising questions of illegality and morality. While many studies have examined Sci-Hub, few focused on users and their motivations. This study employed a mixed-methods approach, conducting quantitative and qualitative research on Korean researchers to address this gap. The findings reveal that Sci-Hubâs copyright infringement has not discouraged its use. Researchers who prioritise quick access to information over copyright concerns deemed the unsegmented subscription environment and cumbersome, time-consuming library services inadequate, thus justifying Sci-Hubâs use. Researchers are expected to continue utilising Sci-Hub, notwithstanding concerns about illegality, morality and ethics. This study underscores the challenges researchers encounter in obtaining academic information, and advocates for more efficient access options. Â© The Author(s) 2024.","This study analysed the use and perception of Sci-Hub, a representative Black Open Access website, to understand the behaviours of Korean researchers facing limited access to the information required for research activities. Debates on the role and future of Sci-Hub vary widely, raising questions of illegality and morality. While many studies have examined Sci-Hub, few focused on users and their motivations. This study employed a mixed-methods approach, conducting quantitative and qualitative research on Korean researchers to address this gap. The findings reveal that Sci-Hubs copyright infringement has not discouraged its use. Researchers who prioritise quick access to information over copyright concerns deemed the unsegmented subscription environment and cumbersome, time-consuming library services inadequate, thus justifying Sci-Hubs use. Researchers are expected to continue utilising Sci-Hub, notwithstanding concerns about illegality, morality and ethics. This study underscores the challenges researchers encounter in obtaining academic information, and advocates for more efficient access options."
Recommending physicians with multimodal data and medical knowledge graph on healthcare platforms,"Healthcare platforms have attracted many physicians and provided convenient medical services to patients. However, the large number of physicians brings the difficulty of finding suitable physicians for the patients. Despite attempts to develop recommendation methods to address this challenge, they fail to leverage multimodal medical data, which contain numerical, categorical, textual and visual data valuable for inferring patientsâ preferences for physicians. Besides, previous methods ignore the semantic gap between patientsâ health conditions and physiciansâ specialties. The conditions describe the patientsâ symptoms, while the specialties indicate the diseases the physicians can treat. They have different vocabularies and cannot be directly compared for generating recommendations. We put forward an innovative physician recommendation approach to effectively address the above research gaps. Our approach entails merging multimodal data with multiple network modules and employing a medical knowledge graph to fill the semantic gap. To assess the validity of our suggested approach, we perform comprehensive trials on real-world data. The trial outcomes indicate that our approach surpasses its variants and existing methods in the aspects of HR@k, MRR@k and NDCG@k. Â© The Author(s) 2024.","Healthcare platforms have attracted many physicians and provided convenient medical services to patients. However, the large number of physicians brings the difficulty of finding suitable physicians for the patients. Despite attempts to develop recommendation methods to address this challenge, they fail to leverage multimodal medical data, which contain numerical, categorical, textual and visual data valuable for inferring patients preferences for physicians. Besides, previous methods ignore the semantic gap between patients health conditions and physicians specialties. The conditions describe the patients symptoms, while the specialties indicate the diseases the physicians can treat. They have different vocabularies and cannot be directly compared for generating recommendations. We put forward an innovative physician recommendation approach to effectively address the above research gaps. Our approach entails merging multimodal data with multiple network modules and employing a medical knowledge graph to fill the semantic gap. To assess the validity of our suggested approach, we perform comprehensive trials on real-world data. The trial outcomes indicate that our approach surpasses its variants and existing methods in the aspects of HR@k, MRR@k and NDCG@"
Connecting information literacy and social capital to better utilise knowledge resources in the workplace,"Human resources and intellectual capital are best utilised through an ongoing interaction between individual and social processes. Still there is a research gap of empirical multilevel studies, focusing both on individual and organisational aspects of knowledge processes. To fill this gap, this article reports on a quantitative study, where the relationship between information literacy and social capital, representing the individual and social contexts affecting organisational knowledge processes, is explored. Structural equation modelling-based analysis of 378 employees working in different companies in Finland demonstrated that information literacy supports all three dimensions of social capital at workplace. Strong information handling skills enable better access to knowledge beyond the resources of an individual, that is, social capital. The results of the study contribute to a better understanding of how to manage human resources and the information and knowledge processes that employees are expected to be involved in. Â© The Author(s) 2021.","Human resources and intellectual capital are best utilised through an ongoing interaction between individual and social processes. Still there is a research gap of empirical multilevel studies, focusing both on individual and organisational aspects of knowledge processes. To fill this gap, this article reports on a quantitative study, where the relationship between information literacy and social capital, representing the individual and social contexts affecting organisational knowledge processes, is explored. Structural equation modelling-based analysis of 378 employees working in different companies in Finland demonstrated that information literacy supports all three dimensions of social capital at workplace. Strong information handling skills enable better access to knowledge beyond the resources of an individual, that is, social capital. The results of the study contribute to a better understanding of how to manage human resources and the information and knowledge processes that employees are expected to be involved in."
A spatio-temporal evolution analysis framework based on sentiment recognition for temple murals,"Murals are important resources for carrying cultural heritage, historical evidence and artistic memory. The sentiment of a mural is the transmission of its inner thoughts, closely related to the region and dynasty to which the mural belongs. To explore the sentiment evolution patterns of temple murals, we construct a spatio-temporal evolution analysis framework based on sentiment recognition. This framework mainly includes feature extraction, sentiment recognition and sentiment evolution analysis. First, we extract the colour features, local features, global semantic features, patch features and structure relations to represent the visual features of temple murals. Second, the semantics of spatio-temporal attributes and titles of murals are extracted through the fine-tuned BERT (Bidirectional Encoder Representations from Transformers) to enhance the feature discrimination for sentiment recognition. Third, we introduce the SMOTE (Synthetic Minority Oversampling Technique) to reduce the influence of imbalanced data and select RF (random forest) as the optimal classifier. The F1 score of the fine-grained sentiment recognition model is up to 81.37%. Finally, we collect the temple murals and reveal the characteristics and patterns of sentiment evolution from the spatial, temporal and spatio-temporal perspectives. Â© The Author(s) 2024.","Murals are important resources for carrying cultural heritage, historical evidence and artistic memory. The sentiment of a mural is the transmission of its inner thoughts, closely related to the region and dynasty to which the mural belongs. To explore the sentiment evolution patterns of temple murals, we construct a spatio-temporal evolution analysis framework based on sentiment recognition. This framework mainly includes feature extraction, sentiment recognition and sentiment evolution analysis. First, we extract the colour features, local features, global semantic features, patch features and structure relations to represent the visual features of temple murals. Second, the semantics of spatio-temporal attributes and titles of murals are extracted through the fine-tuned BERT (Bidirectional Encoder Representations from Transformers) to enhance the feature discrimination for sentiment recognition. Third, we introduce the SMOTE (Synthetic Minority Oversampling Technique) to reduce the influence of imbalanced data and select RF (random forest) as the optimal classifier. The F1 score of the fine-grained sentiment recognition model is up to 81.37%. Finally, we collect the temple murals and reveal the characteristics and patterns of sentiment evolution from the spatial, temporal and spatio-temporal perspectives."
Use of mind mapping in search process to clarify information needs and improve search satisfaction,"A mind map is an approach to the organisation of the human mind that prepares the ground for thinking. Inspired by the function of the mind in handling a situation, this article reports on an empirical study that evaluated the efficiency of mind map techniques and tools in formulating and refining information needs. The study examined graduate studentsâ Internet information searching. Two simulated search tasks were completed by participants in two search sessions. The results revealed no statistically significant difference between searching with a mind map and without a mind map, and therefore, no advantage could be found for using a mind map in the search process. Participants were happier with their search session when not using mind maps; mind map might help information need clarification, but it is a barrier to interaction and serendipity retrieval. However, this could be due to the search setting where the mind map had to be used as a separate tool and not an integrated component of the search system. The article also discusses some potential benefits of mind mapping for searching. Â© The Author(s) 2021.","A mind map is an approach to the organisation of the human mind that prepares the ground for thinking. Inspired by the function of the mind in handling a situation, this article reports on an empirical study that evaluated the efficiency of mind map techniques and tools in formulating and refining information needs. The study examined graduate students Internet information searching. Two simulated search tasks were completed by participants in two search sessions. The results revealed no statistically significant difference between searching with a mind map and without a mind map, and therefore, no advantage could be found for using a mind map in the search process. Participants were happier with their search session when not using mind maps; mind map might help information need clarification, but it is a barrier to interaction and serendipity retrieval. However, this could be due to the search setting where the mind map had to be used as a separate tool and not an integrated component of the search system. The article also discusses some potential benefits of mind mapping for searching."
Evaluating and ranking the digital content generation components for marketing the libraries and information centresâ goods and services using fuzzy TOPSIS technique,"Since content audiences, including libraries and information centres, are increasingly geared to digital environments and virtual networks, the production and delivery of high-quality digital content are becoming continuously important. So far, several components have been introduced by researchers for evaluating the quality of digital content generation. However, due to the uncertainty of the importance rate and value of each of these components, it has not yet been possible to use them effectively to evaluate the content produced. This study aimed to rank the components of content generation to allow accurate evaluation of them for users as well as content providers and distributors including libraries and marketers. The ranked content can motivate digital content producers and distributors to better evaluate the quality of digital content, better attract customers and make more effective decisions about the quality of digital content use based on their specific goals. Initially, 42 of the most important components were identified from the literature. Then, the next steps were taken to rank these components, and based on three rounds of Delphi interviews, the expertsâ views on the importance rate of each of the components were obtained, analysed and ranked. Since in this ranking, the importance of a wide range of components should be highlighted towards each other, the fuzzy TOPSIS technique was emphasised for analysing the views of 16 experts in the field of content generation in Iran. This ranking indicated that components such as âfindable and accessâ, ânon-disturbing and helpfulâ, âclearâ and âremarkableâ are the main pillars of content generation and are of the utmost importance. The results can be used as an effective tool to improve the quality of content. Moreover, it increases audience engagement in digital environments and social networks, and encourages them to make more use of the digital content of libraries. Â© The Author(s) 2021.","Since content audiences, including libraries and information centres, are increasingly geared to digital environments and virtual networks, the production and delivery of high-quality digital content are becoming continuously important. So far, several components have been introduced by researchers for evaluating the quality of digital content generation. However, due to the uncertainty of the importance rate and value of each of these components, it has not yet been possible to use them effectively to evaluate the content produced. This study aimed to rank the components of content generation to allow accurate evaluation of them for users as well as content providers and distributors including libraries and marketers. The ranked content can motivate digital content producers and distributors to better evaluate the quality of digital content, better attract customers and make more effective decisions about the quality of digital content use based on their specific goals. Initially, 42 of the most important components were identified from the literature. Then, the next steps were taken to rank these components, and based on three rounds of Delphi interviews, the experts views on the importance rate of each of the components were obtained, analysed and ranked. Since in this ranking, the importance of a wide range of components should be highlighted towards each other, the fuzzy TOPSIS technique was emphasised for analysing the views of 16 experts in the field of content generation in Iran. This ranking indicated that components such as findable and access, non-disturbing and helpful, clear and remarkable are the main pillars of content generation and are of the utmost importance. The results can be used as an effective tool to improve the quality of content. Moreover, it increases audience engagement in digital environments and social networks, and encourages them to make more use of the digital content of libraries."
A typology of research discovery tools,"There has been a proliferation of new research discovery tools that aid scientists in finding relevant publications. To obtain a general overview of this development, this article generates a conceptual typology of all possible research discovery tools by drawing from the information-theoretical concepts of redundancy/variety. Bibliometric links between scholarly publications can thus exhibit âredundancyâ (i.e. expectable linkages between academic works) or âvarietyâ (i.e. original co-occurrence patterns). On the redundancy-reproducing end of the typology are machines that harness extant co-citations or keyword queries, such as academic search engines and paper recommender systems. The variety end of the spectrum harbours services that enable categorial browsing or that suggest publications randomly, such as journalsâ tables of contents or random paper bots. The typology has implications for understanding how the design of research discovery platforms may ultimately shape aggregate citational networks of science. Â© The Author(s) 2021.","There has been a proliferation of new research discovery tools that aid scientists in finding relevant publications. To obtain a general overview of this development, this article generates a conceptual typology of all possible research discovery tools by drawing from the information-theoretical concepts of redundancy/variety. Bibliometric links between scholarly publications can thus exhibit redundancy ( expectable linkages between academic works) or variety ( original co-occurrence patterns). On the redundancy-reproducing end of the typology are machines that harness extant co-citations or keyword queries, such as academic search engines and paper recommender systems. The variety end of the spectrum harbours services that enable categorial browsing or that suggest publications randomly, such as journals tables of contents or random paper bots. The typology has implications for understanding how the design of research discovery platforms may ultimately shape aggregate citational networks of science."
Important citations identification by exploiting generative model into discriminative model,"Although the citations between scientific documents are deemed as a vehicle for dissemination, inheritance and development of scientific knowledge, not all citations are well-positioned to be equal. A plethora of taxonomies and machine-learning models have been implemented to tackle the task of citation function and importance classification from qualitative aspect. Inspired by the success of kernel functions from resulting general models to promote the performance of the support vector machine (SVM) model, this work exploits the potential of combining generative and discriminative models for the task of citation importance classification. In more detail, generative features are generated from a topic model, citation influence model (CIM) and then fed to two discriminative traditional machine-learning models, SVM and RF (random forest), and a deep learning model, convolutional neural network (CNN), with other 13 traditional features to identify important citations. The extensive experiments are performed on two data sets with different characteristics. These three models perform better on the data set from one discipline. It is very possible that the patterns for important citations may vary by the fields, which disable machine-learning models to learn effectively the discriminative patterns from publications from multiple domains. The RF classifier outperforms the SVM classifier, which accords with many prior studies. However, the CNN model does not achieve the desired performance due to small-scaled data set. Furthermore, our CIM modelâbased features improve further the performance for identifying important citations. Â© The Author(s) 2021.","Although the citations between scientific documents are deemed as a vehicle for dissemination, inheritance and development of scientific knowledge, not all citations are well-positioned to be equal. A plethora of taxonomies and machine-learning models have been implemented to tackle the task of citation function and importance classification from qualitative aspect. Inspired by the success of kernel functions from resulting general models to promote the performance of the support vector machine (SVM) model, this work exploits the potential of combining generative and discriminative models for the task of citation importance classification. In more detail, generative features are generated from a topic model, citation influence model and then fed to two discriminative traditional machine-learning models, SVM and RF (random forest), and a deep learning model, convolutional neural network (CNN), with other 13 traditional features to identify important citations. The extensive experiments are performed on two data sets with different characteristics. These three models perform better on the data set from one discipline. It is very possible that the patterns for important citations may vary by the fields, which disable machine-learning models to learn effectively the discriminative patterns from publications from multiple domains. The RF classifier outperforms the SVM classifier, which accords with many prior studies. However, the CNN model does not achieve the desired performance due to small-scaled data set. Furthermore, our CIM modelbased features improve further the performance for identifying important citations."
Wildfire risk weighting and behaviour prediction using open geospatial data and ontologies,"This article presents a novel approach to wildfire risk assessment and behaviour prediction by leveraging open geospatial data and ontologies. The proposed methodology includes a spatially weighted index model and multicriteria analysis to represent the risk of forest fires in the affected area. It bridges gaps in theory and practice, offering a comprehensive solution for evaluating potential forest fire risk in near real time, predicting fire behaviour and elucidating the semantics of fire management. During dry and hot conditions, forest fires tend to escalate. Hence, we propose an algorithm that combines expertsâ empirical criteria and open-source data to identify dangerous fires in near real time, aiding authorities in directing attention to the riskiest areas. The objective is to predict forest fire behaviour, a complex and nonlinear system influenced by dynamic factors such as weather conditions, topography and land use. Our methodology enables real-time assessment of potential forest fire risks, complemented by predictive fire behaviour scenarios and a descriptive ontology of fire management semantics. We examine existing fire-related ontologies and propose a comprehensive one encompassing incident descriptions, firefighting resources, actor interrelations and knowledge for effective action. By classifying fire sources, our algorithm enables strategic decision-making to prevent uncontrolled fires. This solution significantly enhances data using semantic and spatial relationships among wildfire resources. Furthermore, we demonstrate how ontologies improve data integration and interoperability among diverse systems and organisations involved in forest fire risk management, fostering better coordination and faster responses to critical situations. To facilitate decision-making, we create decision-making scenarios linked to analysed hot spots, drawing from open hot spot data such as National Aeronautics and Space Administration (NASA) Fire Information for Resource Management System (FIRMS), OpenStreetMap (OSM), OpenWeatherMap (OWM) and OpenTopoData (OTD). We propose an ordinal and linguistic classification system (F1âF5) denoting risk levels as low, moderate, high, very high and extreme. These values are obtained through factor aggregation and fuzzy logic. A publicly accessible, interactive web map displays the results derived from this model. Overall, our contributions to wildfire risk management provide authorities with a valuable tool to make informed decisions and mitigate the damaging effects of wildfires. Â© The Author(s) 2023.","This article presents a novel approach to wildfire risk assessment and behaviour prediction by leveraging open geospatial data and ontologies. The proposed methodology includes a spatially weighted index model and multicriteria analysis to represent the risk of forest fires in the affected area. It bridges gaps in theory and practice, offering a comprehensive solution for evaluating potential forest fire risk in near real time, predicting fire behaviour and elucidating the semantics of fire management. During dry and hot conditions, forest fires tend to escalate. Hence, we propose an algorithm that combines experts empirical criteria and open-source data to identify dangerous fires in near real time, aiding authorities in directing attention to the riskiest areas. The objective is to predict forest fire behaviour, a complex and nonlinear system influenced by dynamic factors such as weather conditions, topography and land use. Our methodology enables real-time assessment of potential forest fire risks, complemented by predictive fire behaviour scenarios and a descriptive ontology of fire management semantics. We examine existing fire-related ontologies and propose a comprehensive one encompassing incident descriptions, firefighting resources, actor interrelations and knowledge for effective action. By classifying fire sources, our algorithm enables strategic decision-making to prevent uncontrolled fires. This solution significantly enhances data using semantic and spatial relationships among wildfire resources. Furthermore, we demonstrate how ontologies improve data integration and interoperability among diverse systems and organisations involved in forest fire risk management, fostering better coordination and faster responses to critical situations. To facilitate decision-making, we create decision-making scenarios linked to analysed hot spots, drawing from open hot spot data such as National Aeronautics and Space Administration (NASA) Fire Information for Resource Management System (FIRMS), OpenStreetMap (OSM), OpenWeatherMap (OWM) and OpenTopoData (OTD). We propose an ordinal and linguistic classification system (F1F5) denoting risk levels as low, moderate, high, very high and extreme. These values are obtained through factor aggregation and fuzzy logic. A publicly accessible, interactive web map displays the results derived from this model. Overall, our contributions to wildfire risk management provide authorities with a valuable tool to make informed decisions and mitigate the damaging effects of wildfires."
Infectious epidemics and the research output of nations: A data-driven analysis,"During the last years, several infectious diseases have caused widespread nationwide epidemics that affected information seeking behaviours, people mobility, economics and research trends. Examples of these epidemics are 2003 severe acute respiratory syndrome (SARS) epidemic in mainland China and Hong Kong, 2014â2016 Ebola epidemic in Guinea and Sierra Leone, 2015â2016 Zika epidemic in Brazil, Colombia and Puerto Rico and the recent COVID-19 epidemic in China and other countries. In this research article, we investigate the effect of large-scale outbreaks of infectious diseases on the research productivity and landscape of nations through the analysis of the research outputs of main countries affected by SARS, Zika and Ebola epidemics as returned by Web of Science Core Collection. Despite the mobility restrictions and the limitations of work conditions due to the epidemics, we surprisingly found that the research characteristics and productivity of the countries that have excellent or moderate research traditions and communities are not affected by infectious epidemics due to their robust long-term research structures and policy. Similarly, large-scale infectious outbreaks can even boost the research productivity of countries with limited research traditions thanks to international capacity building collaborations provided by organisations and associations from leading research countries. Â© The Author(s) 2021.","During the last years, several infectious diseases have caused widespread nationwide epidemics that affected information seeking behaviours, people mobility, economics and research trends. Examples of these epidemics are 2003 severe acute respiratory syndrome (SARS) epidemic in mainland China and Hong Kong, 20142016 Ebola epidemic in Guinea and Sierra Leone, 20152016 Zika epidemic in Brazil, Colombia and Puerto Rico and the recent COVID-19 epidemic in China and other countries. In this research article, we investigate the effect of large-scale outbreaks of infectious diseases on the research productivity and landscape of nations through the analysis of the research outputs of main countries affected by SARS, Zika and Ebola epidemics as returned by Web of Science Core Collection. Despite the mobility restrictions and the limitations of work conditions due to the epidemics, we surprisingly found that the research characteristics and productivity of the countries that have excellent or moderate research traditions and communities are not affected by infectious epidemics due to their robust long-term research structures and policy. Similarly, large-scale infectious outbreaks can even boost the research productivity of countries with limited research traditions thanks to international capacity building collaborations provided by organisations and associations from leading research countries."
Social media and online safety practices of young parents,"Studies of parentsâ online safety concerns typically centre on information privacy and on worries over unknown third parties preying on children, whereas investigations into youth perspectives on online safety have found young people to focus on threats to safety or reputation by known individuals. The case of youth who are themselves parents raises questions regarding how these differing perspectives are negotiated by individuals who are in dual roles as youth and parents. Using interview and ethnographic observation data from the longitudinal Young Parent Study in British Columbia, Canada, this analysis investigates social media and online safety practices of 113 young parents. Online safety concerns of young parents in this study focused on personal safety, their childrenâs online privacy and image management. These concerns reflect their dual roles, integrating youth image and information management concerns with parental concerns over the safety and information privacy of their own children. Â© The Author(s) 2021.","Studies of parents online safety concerns typically centre on information privacy and on worries over unknown third parties preying on children, whereas investigations into youth perspectives on online safety have found young people to focus on threats to safety or reputation by known individuals. The case of youth who are themselves parents raises questions regarding how these differing perspectives are negotiated by individuals who are in dual roles as youth and parents. Using interview and ethnographic observation data from the longitudinal Young Parent Study in British Columbia, Canada, this analysis investigates social media and online safety practices of 113 young parents. Online safety concerns of young parents in this study focused on personal safety, their childrens online privacy and image management. These concerns reflect their dual roles, integrating youth image and information management concerns with parental concerns over the safety and information privacy of their own children."
Word-embedding-based query expansion: Incorporating Deep Averaging Networks in Arabic document retrieval,"One of the main issues associated with search engines is the queryâdocument vocabulary mismatch problem, a long-standing problem in Information Retrieval (IR). This problem occurs when a user query does not match the content of stored documents, and it affects most search tasks. Automatic query expansion (AQE) is one of the most common approaches used to address this problem. Various AQE techniques have been proposed; these mainly involve finding synonyms or related words for the query terms. Word embedding (WE) is one of the methods that are currently receiving significant attention. Most of the existing AQE techniques focus on expanding the individual query terms rather the entire query during the expansion process, and this can lead to query drift if poor expansion terms are selected. In this article, we introduce Deep Averaging Networks (DANs), an architecture that feeds the average of the WE vectors produced by the Word2Vec toolkit for the terms in a query through several linear neural network layers. This average vector is assumed to represent the meaning of the query as a whole and can be used to find expansion terms that are relevant to the complete query. We explore the potential of DANs for AQE in Arabic document retrieval. We experiment with using DANs for AQE in the classic probabilistic BM25 model as well as for two recent expansion strategies: Embedding-Based Query Expansion approach (EQE1) and Prospect-Guided Query Expansion Strategy (V2Q). Although DANs did not improve all outcomes when used in the BM25 model, it outperformed all baselines when incorporated into the EQE1 and V2Q expansion strategies. Â© The Author(s) 2021.","One of the main issues associated with search engines is the querydocument vocabulary mismatch problem, a long-standing problem in Information Retrieval (IR). This problem occurs when a user query does not match the content of stored documents, and it affects most search tasks. Automatic query expansion (AQE) is one of the most common approaches used to address this problem. Various AQE techniques have been proposed; these mainly involve finding synonyms or related words for the query terms. Word embedding (WE) is one of the methods that are currently receiving significant attention. Most of the existing AQE techniques focus on expanding the individual query terms rather the entire query during the expansion process, and this can lead to query drift if poor expansion terms are selected. In this article, we introduce Deep Averaging Networks (DANs), an architecture that feeds the average of the WE vectors produced by the Word2Vec toolkit for the terms in a query through several linear neural network layers. This average vector is assumed to represent the meaning of the query as a whole and can be used to find expansion terms that are relevant to the complete query. We explore the potential of DANs for AQE in Arabic document retrieval. We experiment with using DANs for AQE in the classic probabilistic BM25 model as well as for two recent expansion strategies: Embedding-Based Query Expansion approach (EQE1) and Prospect-Guided Query Expansion Strategy (V2Q). Although DANs did not improve all outcomes when used in the BM25 model, it outperformed all baselines when incorporated into the EQE1 and V2Q expansion strategies."
Delphi study of risk to individuals who disclose personal information online,"A two-round Delphi study was conducted to explore priorities for addressing online risk to individuals. A corpus of literature was created based on 69 peer-reviewed articles about privacy risk and the privacy calculus published between 2014 and 2019. A cluster analysis of the resulting text-base using Pearsonâs correlation coefficient resulted in seven broad topics. After two rounds of the Delphi survey with experts in information security and information literacy, the following topics were identified as priorities for further investigation: personalisation versus privacy, responsibility for privacy on social networks, measuring privacy risk, and perceptions of powerlessness and the resulting apathy. The Delphi approach provided clear conclusions about research topics and has potential as a tool for prioritising future research areas. Â© The Author(s) 2021.","A two-round Delphi study was conducted to explore priorities for addressing online risk to individuals. A corpus of literature was created based on 69 peer-reviewed articles about privacy risk and the privacy calculus published between 2014 and 2019. A cluster analysis of the resulting text-base using Pearsons correlation coefficient resulted in seven broad topics. After two rounds of the Delphi survey with experts in information security and information literacy, the following topics were identified as priorities for further investigation: personalisation versus privacy, responsibility for privacy on social networks, measuring privacy risk, and perceptions of powerlessness and the resulting apathy. The Delphi approach provided clear conclusions about research topics and has potential as a tool for prioritising future research areas."
Scientistsâ response to global public health emergencies: A bibliometrics perspective,"The unprecedented COVID-19 outbreak at the end of 2019 has produced a worldwide health crisis. Scientific research, especially international research collaboration, is crucial to deal successfully with the epidemic. This article aims to review the response modes, and especially the international collaboration characteristic, of the academic community to similar public health events in the past. Based on relevant studies of four major public health emergencies in the past, the major public health emergencies were regarded as ânew knowledgeâ in the academic field. By using knowledge diffusion indicators, such as the breadth and speed of diffusion, and combined with the development characteristics of the event, this article explores the diffusion characteristics of the four major public health emergencies in the academic exchange system and then identifies the academic communityâs response mode to the outbreaks. In addition, the characteristics of international collaboration in response to the public health events and the impact of international collaboration on the academic communityâs response are analysed. Through the analysis of the international collaboration network, the cooperative groups and core countries in the research collaboration network related to the major public health emergencies are obtained. In terms of COVID-19, it is found that the response speed and intensity of scientists have been significantly improved, but more focus should be given to international collaboration. Our findings could be beneficial to both decision-makers and researchers in policy formulation and conducting research, respectively, to optimally deal with COVID-19 and possible outbreaks in the future. Â© The Author(s) 2021.","The unprecedented COVID-19 outbreak at the end of 2019 has produced a worldwide health crisis. Scientific research, especially international research collaboration, is crucial to deal successfully with the epidemic. This article aims to review the response modes, and especially the international collaboration characteristic, of the academic community to similar public health events in the past. Based on relevant studies of four major public health emergencies in the past, the major public health emergencies were regarded as new knowledge in the academic field. By using knowledge diffusion indicators, such as the breadth and speed of diffusion, and combined with the development characteristics of the event, this article explores the diffusion characteristics of the four major public health emergencies in the academic exchange system and then identifies the academic communitys response mode to the outbreaks. In addition, the characteristics of international collaboration in response to the public health events and the impact of international collaboration on the academic communitys response are analysed. Through the analysis of the international collaboration network, the cooperative groups and core countries in the research collaboration network related to the major public health emergencies are obtained. In terms of COVID-19, it is found that the response speed and intensity of scientists have been significantly improved, but more focus should be given to international collaboration. Our findings could be beneficial to both decision-makers and researchers in policy formulation and conducting research, respectively, to optimally deal with COVID-19 and possible outbreaks in the future."
Unsupervised extractive multi-document summarization method based on transfer learning from BERT multi-task fine-tuning,"Text representation is a fundamental cornerstone that impacts the effectiveness of several text summarization methods. Transfer learning using pre-trained word embedding models has shown promising results. However, most of these representations do not consider the order and the semantic relationships between words in a sentence, and thus they do not carry the meaning of a full sentence. To overcome this issue, the current study proposes an unsupervised method for extractive multi-document summarization based on transfer learning from BERT sentence embedding model. Moreover, to improve sentence representation learning, we fine-tune BERT model on supervised intermediate tasks from GLUE benchmark datasets using single-task and multi-task fine-tuning methods. Experiments are performed on the standard DUCâ2002â2004 datasets. The obtained results show that our method has significantly outperformed several baseline methods and achieves a comparable and sometimes better performance than the recent state-of-the-art deep learningâbased methods. Furthermore, the results show that fine-tuning BERT using multi-task learning has considerably improved the performance. Â© The Author(s) 2021.","Text representation is a fundamental cornerstone that impacts the effectiveness of several text summarization methods. Transfer learning using pre-trained word embedding models has shown promising results. However, most of these representations do not consider the order and the semantic relationships between words in a sentence, and thus they do not carry the meaning of a full sentence. To overcome this issue, the current study proposes an unsupervised method for extractive multi-document summarization based on transfer learning from BERT sentence embedding model. Moreover, to improve sentence representation learning, we fine-tune BERT model on supervised intermediate tasks from GLUE benchmark datasets using single-task and multi-task fine-tuning methods. Experiments are performed on the standard DUC20022004 datasets. The obtained results show that our method has significantly outperformed several baseline methods and achieves a comparable and sometimes better performance than the recent state-of-the-art deep learningbased methods. Furthermore, the results show that fine-tuning BERT using multi-task learning has considerably improved the performance."
âAccess necessitates being seenâ: Queer visibility and intersectional embodiment within the health information practices of queer community leaders,"Navigating healthcare infrastructures is particularly challenging for queer-identifying individuals, with significant barriers emerging around stigma and practitioner ignorance. Further intersecting, historically marginalised identities such as oneâs race, age or ability exacerbate such engagement with healthcare, particularly the access to and use of reliable and appropriate health information. We explore the salience of oneâs queer identity relative to other embodied identities when navigating health information and care for themselves and their communities. Thirty semi-structured interviews with queer community leaders from South Carolina inform our discussion of the role oneâs queer visibility plays relational to the visibility of other identities. We find that leaders and their communities navigate these intersectional visibilities through unique and iterative approaches to health information seeking, sharing and use predicated upon anti-queer, racist, ableist and misogynistic sentiments. Findings can inform queer-inclusive, intersectionally informed interventions by health and information professionals such as non-profit advocacy organisations and medical librarians. Â© The Author(s) 2021.","Navigating healthcare infrastructures is particularly challenging for queer-identifying individuals, with significant barriers emerging around stigma and practitioner ignorance. Further intersecting, historically marginalised identities such as ones race, age or ability exacerbate such engagement with healthcare, particularly the access to and use of reliable and appropriate health information. We explore the salience of ones queer identity relative to other embodied identities when navigating health information and care for themselves and their communities. Thirty semi-structured interviews with queer community leaders from South Carolina inform our discussion of the role ones queer visibility plays relational to the visibility of other identities. We find that leaders and their communities navigate these intersectional visibilities through unique and iterative approaches to health information seeking, sharing and use predicated upon anti-queer, racist, ableist and misogynistic sentiments. Findings can inform queer-inclusive, intersectionally informed interventions by health and information professionals such as non-profit advocacy organisations and medical librarians."
Identifying effective cognitive biases in information retrieval,"The purpose of this study is to identify the types of cognitive biases in the process of information retrieval. This research used a mixed-method approach for data collection. The research population consisted of 25 information retrieval specialists and 30 post-graduate students. We employed three tools for collecting data, including a checklist, log files and semi-structured interviews. The findings showed that from the perspective of information retrieval specialists, the cognitive biases such as âFamiliarityâ, âAnchoringâ, âRush to solveâ and âCurse of knowledgeâ could be of the greatest importance in the field of information retrieval. Also, in terms of usersâ searching, the âRush to solve problemsâ and âMere exposure effectsâ biases have the highest frequency, and the âOutcomeâ and âCurse of knowledgeâ biases have the lowest frequency in the process of user retrieval information. It can be concluded that, because cognitive biases occurring in information retrieval, designers of information retrieval systems and librarians should pay attention to this issue in designing and evaluating information systems. Â© The Author(s) 2021.","The purpose of this study is to identify the types of cognitive biases in the process of information retrieval. This research used a mixed-method approach for data collection. The research population consisted of 25 information retrieval specialists and 30 post-graduate students. We employed three tools for collecting data, including a checklist, log files and semi-structured interviews. The findings showed that from the perspective of information retrieval specialists, the cognitive biases such as Familiarity, Anchoring, Rush to solve and Curse of knowledge could be of the greatest importance in the field of information retrieval. Also, in terms of users searching, the Rush to solve problems and Mere exposure effects biases have the highest frequency, and the Outcome and Curse of knowledge biases have the lowest frequency in the process of user retrieval information. It can be concluded that, because cognitive biases occurring in information retrieval, designers of information retrieval systems and librarians should pay attention to this issue in designing and evaluating information systems."
Query-focused summarisation in research articles based on semantic function of sentences,"Query-focused summarisation (QFS) in research articles is usually used to help researchers to sum up content related to specific aspects of research articles. However, most QFS approaches failed to consider the inherent structure of research articles to speculate the semantic functions of sentences to make summarisations relate to the given aspect more precisely. Systematic functional linguistic studies suggested that research articles contain inherent structures in which sections and sentences have their specific functions. We suppose these structures can be used as auxiliary information for scientific summarisation. In this article, we seek to improve existing extractive QFS methods by using the macrostructure and discourse segment structure of research articles. We categorise sentences in research articles into different types according to their semantic functions and assign different weights to each type of sentence using an action-based relevance calculation method. We show that our system outperforms the baseline system on a benchmark dataset. Our findings suggest that using the inherent structure of research articles as assistance is practical for scientific summarisation. Â© The Author(s) 2022.","Query-focused summarisation (QFS) in research articles is usually used to help researchers to sum up content related to specific aspects of research articles. However, most QFS approaches failed to consider the inherent structure of research articles to speculate the semantic functions of sentences to make summarisations relate to the given aspect more precisely. Systematic functional linguistic studies suggested that research articles contain inherent structures in which sections and sentences have their specific functions. We suppose these structures can be used as auxiliary information for scientific summarisation. In this article, we seek to improve existing extractive QFS methods by using the macrostructure and discourse segment structure of research articles. We categorise sentences in research articles into different types according to their semantic functions and assign different weights to each type of sentence using an action-based relevance calculation method. We show that our system outperforms the baseline system on a benchmark dataset. Our findings suggest that using the inherent structure of research articles as assistance is practical for scientific summarisation."
"RETRACTED: Government regulation of the Internet as instrument of digital protectionism in case of developing countries (Journal of Information Science, (2023), 49, 3, (595-608), 10.1177/01655515211014142)",corrected-proof ts1 Â© The Author(s) 2023.,
A novel filter feature selection method for text classification: Extensive Feature Selector,"As the huge dimensionality of textual data restrains the classification accuracy, it is essential to apply feature selection (FS) methods as dimension reduction step in text classification (TC) domain. Most of the FS methods for TC contain several number of probabilities. In this study, we proposed a new FS method named as Extensive Feature Selector (EFS), which benefits from corpus-based and class-based probabilities in its calculations. The performance of EFS is compared with nine well-known FS methods, namely, Chi-Squared (CHI2), Class Discriminating Measure (CDM), Discriminative Power Measure (DPM), Odds Ratio (OR), Distinguishing Feature Selector (DFS), Comprehensively Measure Feature Selection (CMFS), Discriminative Feature Selection (DFSS), Normalised Difference Measure (NDM) and MaxâMin Ratio (MMR) using Multinomial Naive Bayes (MNB), Support-Vector Machines (SVMs) and k-Nearest Neighbour (KNN) classifiers on four benchmark data sets. These data sets are Reuters-21578, 20-Newsgroup, Mini 20-Newsgroup and Polarity. The experiments were carried out for six different feature sizes which are 10, 30, 50, 100, 300 and 500. Experimental results show that the performance of EFS method is more successful than the other nine methods in most cases according to micro-F1 and macro-F1 scores. Â© The Author(s) 2021.","As the huge dimensionality of textual data restrains the classification accuracy, it is essential to apply feature selection (FS) methods as dimension reduction step in text classification (TC) domain. Most of the FS methods for TC contain several number of probabilities. In this study, we proposed a new FS method named as Extensive Feature Selector (EFS), which benefits from corpus-based and class-based probabilities in its calculations. The performance of EFS is compared with nine well-known FS methods, namely, Chi-Squared (CHI2), Class Discriminating Measure , Discriminative Power Measure (DPM), Odds Ratio (OR), Distinguishing Feature Selector (DFS), Comprehensively Measure Feature Selection (CMFS), Discriminative Feature Selection (DFSS), Normalised Difference Measure (NDM) and MaxMin Ratio (MMR) using Multinomial Naive Bayes (MNB), Support-Vector Machines (SVMs) and k-Nearest Neighbour (KNN) classifiers on four benchmark data sets. These data sets are Reuters-21578, 20-Newsgroup, Mini 20-Newsgroup and Polarity. The experiments were carried out for six different feature sizes which are 10, 30, 50, 100, 300 and 500. Experimental results show that the performance of EFS method is more successful than the other nine methods in most cases according to micro-F1 and macro-F1 scores."
ClickbaitTR: Dataset for clickbait detection from Turkish news sites and social media with a comparative analysis via machine learning algorithms,"Clickbait is a strategy that aims to attract peopleâs attention and direct them to specific content. Clickbait titles, created by the information that is not included in the main content or using intriguing expressions with various text-related features, have become very popular, especially in social media. This study expands the Turkish clickbait dataset that we had constructed for clickbait detection in our proof-of-concept study, written in Turkish. We achieve a 48,060 sample size by adding 8859 tweets and release a publicly available dataset â ClickbaitTR â with its open-source data analysis library. We apply machine learning algorithms such as Artificial Neural Network (ANN), Logistic Regression, Random Forest, Long Short-Term Memory Network (LSTM), Bidirectional Long Short-Term Memory (BiLSTM) and Ensemble Classifier on 48,060 news headlines extracted from Twitter. The results show that the Logistic Regression algorithm has 85% accuracy; the Random Forest algorithm has a performance of 86% accuracy; the LSTM has 93% accuracy; the ANN has 93% accuracy; the Ensemble Classifier has 93% accuracy; and finally, the BiLSTM has 97% accuracy. A thorough discussion is provided for the psychological aspects of clickbait strategy focusing on curiosity and interest arousal. In addition to a successful clickbait detection performance and the detailed analysis of clickbait sentences in terms of language and psychological aspects, this study also contributes to clickbait detection studies with the largest clickbait dataset in Turkish. Â© The Author(s) 2021.","Clickbait is a strategy that aims to attract peoples attention and direct them to specific content. Clickbait titles, created by the information that is not included in the main content or using intriguing expressions with various text-related features, have become very popular, especially in social media. This study expands the Turkish clickbait dataset that we had constructed for clickbait detection in our proof-of-concept study, written in Turkish. We achieve a 48,060 sample size by adding 8859 tweets and release a publicly available dataset ClickbaitTR with its open-source data analysis library. We apply machine learning algorithms such as Artificial Neural Network (ANN), Logistic Regression, Random Forest, Long Short-Term Memory Network (LSTM), Bidirectional Long Short-Term Memory (BiLSTM) and Ensemble Classifier on 48,060 news headlines extracted from Twitter. The results show that the Logistic Regression algorithm has 85% accuracy; the Random Forest algorithm has a performance of 86% accuracy; the LSTM has 93% accuracy; the ANN has 93% accuracy; the Ensemble Classifier has 93% accuracy; and finally, the BiLSTM has 97% accuracy. A thorough discussion is provided for the psychological aspects of clickbait strategy focusing on curiosity and interest arousal. In addition to a successful clickbait detection performance and the detailed analysis of clickbait sentences in terms of language and psychological aspects, this study also contributes to clickbait detection studies with the largest clickbait dataset in Turkish."
Sequential patent trading recommendation using knowledge-aware attentional bidirectional long short-term memory network (KBiLSTM),"With the rapid development of the patent marketplace, patent trading recommendation is required to mitigate the technology searching cost of patent buyers. Current research focuses on the recommendation based on existing patents of a company; a few studies take into account the sequential pattern of patent acquisition activities and the possible diversity of a companyâs business interests. Moreover, the profiling of patents based on solely patent documents fails to capture the high-order information of patents. To bridge the gap, we propose a knowledge-aware attentional bidirectional long short-term memory network (KBiLSTM) method for patent trading recommendation. KBiLSTM uses knowledge graph embeddings to profile patents with rich patent information. It introduces bidirectional long short-term memory network (BiLSTM) to capture the sequential pattern in a companyâs historical records. In addition, to address a companyâs diverse technology interests, we design an attention mechanism to aggregate the companyâs historical patents given a candidate patent. Experimental results on the United States Patent and Trademark Office (USPTO) data set show that KBiLSTM outperforms state-of-the-art baselines for patent trading recommendation in terms of F1 and normalised discounted cumulative gain (nDCG). The attention visualisation of randomly selected company intuitively demonstrates the recommendation effectiveness. Â© The Author(s) 2021.","With the rapid development of the patent marketplace, patent trading recommendation is required to mitigate the technology searching cost of patent buyers. Current research focuses on the recommendation based on existing patents of a company; a few studies take into account the sequential pattern of patent acquisition activities and the possible diversity of a companys business interests. Moreover, the profiling of patents based on solely patent documents fails to capture the high-order information of patents. To bridge the gap, we propose a knowledge-aware attentional bidirectional long short-term memory network (KBiLSTM) method for patent trading recommendation. KBiLSTM uses knowledge graph embeddings to profile patents with rich patent information. It introduces bidirectional long short-term memory network (BiLSTM) to capture the sequential pattern in a companys historical records. In addition, to address a companys diverse technology interests, we design an attention mechanism to aggregate the companys historical patents given a candidate patent. Experimental results on the United States Patent and Trademark Office (USPTO) data set show that KBiLSTM outperforms state-of-the-art baselines for patent trading recommendation in terms of F1 and normalised discounted cumulative gain (nDCG). The attention visualisation of randomly selected company intuitively demonstrates the recommendation effectiveness."
Article promotion on Twitter and Facebook: A case study of Cell journal,"Social media has become an increasingly important channel of scholarly communication, especially for promoting the latest research outputs, so its role in facilitating access to academic texts is worth exploring. Based on 324 posts containing scholarly articles shared by journal Cell on Twitter and Facebook, this study compared the user engagement performance of articles posted on both platforms and examined the effect of such social media promotion and user engagement on article visiting. The user engagement performance of the articles was measured by retweets, shares, reactions, and likes, while click data tracked through bitly.com were used to indicate article visits. Statistical analysis, correlation analysis, and regression analysis were applied to explore and understand these data. For Cell, Facebook posts have a more significant influence than similar tweets in terms of volume. The user engagement on Facebook is 2.5~4 times as much as on Twitter. Moreover, the click metric of short links shows that Cellâs posts on Facebook directed twice as many visitors to the papers as posts on Twitter. However, the efficiency of the two platforms is approximate when the difference in the volume of followers is eliminated. The correlation and regression analysis suggested that user engagement positively affects the visiting of Cellâs papers. Both reactions and shares would affect the clicks of the short links to paper text. The results shed light on the implications of sharing scholarly articles on social media platforms for the promotion of article visits. Â© The Author(s) 2021.","Social media has become an increasingly important channel of scholarly communication, especially for promoting the latest research outputs, so its role in facilitating access to academic texts is worth exploring. Based on 324 posts containing scholarly articles shared by journal Cell on Twitter and Facebook, this study compared the user engagement performance of articles posted on both platforms and examined the effect of such social media promotion and user engagement on article visiting. The user engagement performance of the articles was measured by retweets, shares, reactions, and likes, while click data tracked through bitly.com were used to indicate article visits. Statistical analysis, correlation analysis, and regression analysis were applied to explore and understand these data. For Cell, Facebook posts have a more significant influence than similar tweets in terms of volume. The user engagement on Facebook is 2.5~4 times as much as on Twitter. Moreover, the click metric of short links shows that Cells posts on Facebook directed twice as many visitors to the papers as posts on Twitter. However, the efficiency of the two platforms is approximate when the difference in the volume of followers is eliminated. The correlation and regression analysis suggested that user engagement positively affects the visiting of Cells papers. Both reactions and shares would affect the clicks of the short links to paper text. The results shed light on the implications of sharing scholarly articles on social media platforms for the promotion of article visits."
Effect of Chinese characters on machine learning for Chinese author name disambiguation: A counterfactual evaluation,"Chinese author names are known to be more difficult to disambiguate than other ethnic names because they tend to share surnames and forenames, thus creating many homonyms. In this study, we demonstrate how using Chinese characters can affect machine learning for author name disambiguation. For analysis, 15K author names recorded in Chinese are transliterated into English and simplified by initialising their forenames to create counterfactual scenarios, reflecting real-world indexing practices in which Chinese characters are usually unavailable. The results show that Chinese author names that are highly ambiguous in English or with initialised forenames tend to become less confusing if their Chinese characters are included in the processing. Our findings indicate that recording Chinese author names in native script can help researchers and digital libraries enhance authority control of Chinese author names that continue to increase in size in bibliographic data. Â© The Author(s) 2021.","Chinese author names are known to be more difficult to disambiguate than other ethnic names because they tend to share surnames and forenames, thus creating many homonyms. In this study, we demonstrate how using Chinese characters can affect machine learning for author name disambiguation. For analysis, 15K author names recorded in Chinese are transliterated into English and simplified by initialising their forenames to create counterfactual scenarios, reflecting real-world indexing practices in which Chinese characters are usually unavailable. The results show that Chinese author names that are highly ambiguous in English or with initialised forenames tend to become less confusing if their Chinese characters are included in the processing. Our findings indicate that recording Chinese author names in native script can help researchers and digital libraries enhance authority control of Chinese author names that continue to increase in size in bibliographic data."
"Correlation between universitiesâ Altmetric Attention Scores and their performance scores in Nature Index, Leiden, Times Higher Education and Quacquarelli Symonds ranking systems","Objective: Altmetrics are claimed to measure the scientific, societal, educational, technological and economic impacts of science. They have some of these dimensions in common with university ranking and evaluating systems. Their results are, therefore, expected to be partially convergent with the systemsâ. Given the importance of the scientific and non-scientific impacts of science, this study investigated the correlations of universitiesâ altmetrics with their total and dimensional scores in Nature Index, Leiden, Times Higher Education (THE) and Quacquarelli Symonds (QS). Methodology: Following a correlational design, it explored an available sample of the universities commonly ranked in the systems in 2017. The data were collected from online documents using checklists and analysed by the Spearman correlation. As Altmetric Attention Score (ASS) is efficient in that it integrates several indicators into a single one, it was used as the proxy of the universitiesâ social performance. Findings: The universities showed significant positive correlations between their ASSs and their performance scores on the total and dimensional levels, except for industry income in THE, with an insignificant correlation, and proportion of collaborative publication less than 100 km. in Leiden, with an inverse correlation. The correlations ranged from weak to marginally strong. Conclusion: The positive relationships between the universitiesâ performance and ASSs signified that there existed some similarities in what they measured. However, they were of weak-to-marginally strong powers, implying that the metrics differed in what they measured. The findings contribute to the existing knowledge by providing some evidence of convergence between university-level altmetrics and university performances in various dimensions. Â© The Author(s) 2021.","Objective: Altmetrics are claimed to measure the scientific, societal, educational, technological and economic impacts of science. They have some of these dimensions in common with university ranking and evaluating systems. Their results are, therefore, expected to be partially convergent with the systems. Given the importance of the scientific and non-scientific impacts of science, this study investigated the correlations of universities altmetrics with their total and dimensional scores in Nature Index, Leiden, Times Higher Education (THE) and Quacquarelli Symonds (QS). Methodology: Following a correlational design, it explored an available sample of the universities commonly ranked in the systems in 2017. The data were collected from online documents using checklists and analysed by the Spearman correlation. As Altmetric Attention Score (ASS) is efficient in that it integrates several indicators into a single one, it was used as the proxy of the universities social performance. The universities showed significant positive correlations between their ASSs and their performance scores on the total and dimensional levels, except for industry income in THE, with an insignificant correlation, and proportion of collaborative publication less than 100 km. in Leiden, with an inverse correlation. The correlations ranged from weak to marginally strong. Conclusion: The positive relationships between the universities performance and ASSs signified that there existed some similarities in what they measured. However, they were of weak-to-marginally strong powers, implying that the metrics differed in what they measured. The findings contribute to the existing knowledge by providing some evidence of convergence between university-level altmetrics and university performances in various dimensions."
Influence of personality traits on usersâ viewing behaviour,"Different views on the role of personal factors in moderating individual viewing behaviour exist. This study examined the impact of personality traits on individual viewing behaviour of facial stimulus. A total of 96 students (46 males and 50 females, age 23â28 years) were participated in this study. The Big-Five personality traits of all the participants together with data related to their eye-movements were collected and analysed. The results showed three groups of users who scored high on the personality traits of neuroticism, agreeableness and conscientiousness. Individuals who scored high in a specific personality trait were more probably to interpret the visual image differently from individuals with other personality traits. To determine the extent to which a specific personality trait is associated with usersâ viewing behaviour of visual stimulus, a predictive model was developed and validated. The prediction results showed that 96.73% of the identified personality traits can potentially be predicted by the viewing behaviour of users. The findings of this study can expand the current understanding of human personality and choice behaviour. The study also contributes to the perceptual encoding process of faces and the perceptual mechanism in the holistic face processing theory. Â© The Author(s) 2021.","Different views on the role of personal factors in moderating individual viewing behaviour exist. This study examined the impact of personality traits on individual viewing behaviour of facial stimulus. A total of 96 students (46 males and 50 females, age 2328 years) were participated in this study. The Big-Five personality traits of all the participants together with data related to their eye-movements were collected and analysed. The results showed three groups of users who scored high on the personality traits of neuroticism, agreeableness and conscientiousness. Individuals who scored high in a specific personality trait were more probably to interpret the visual image differently from individuals with other personality traits. To determine the extent to which a specific personality trait is associated with users viewing behaviour of visual stimulus, a predictive model was developed and validated. The prediction results showed that 96.73% of the identified personality traits can potentially be predicted by the viewing behaviour of users. The findings of this study can expand the current understanding of human personality and choice behaviour. The study also contributes to the perceptual encoding process of faces and the perceptual mechanism in the holistic face processing theory."
Mass aesthetic changes in the context of the development of world museums,"This article is relevant, as in the process when the world community is experiencing crisis phenomena in the public consciousness and social forms of existence, the change of the museum as an accumulator of works of art and cultural centres acquires historical significance. The novelty of the study is determined by the fact that exhibitions can be held not only online or during the period when museums act as cultural centres. The purpose of the study is to research the aesthetic changes in the context of global art communication through exhibition areas in the world of museums. The leading research method was comparative analysis, thanks to which mass aesthetic changes in the process of changing the global socio-economic environment were studied. The basis for the work of UNESCO as a global repository and management centre in the museum community was shown. The authors note that the formation of museum competence and a change in the aesthetics of mass consciousness on this basis is possible only if the structural content of the coordination of museum art. The authors see the creation of a single-world museum centre as the basis for such a change. Â© The Author(s) 2021.","This article is relevant, as in the process when the world community is experiencing crisis phenomena in the public consciousness and social forms of existence, the change of the museum as an accumulator of works of art and cultural centres acquires historical significance. The novelty of the study is determined by the fact that exhibitions can be held not only online or during the period when museums act as cultural centres. The purpose of the study is to research the aesthetic changes in the context of global art communication through exhibition areas in the world of museums. The leading research method was comparative analysis, thanks to which mass aesthetic changes in the process of changing the global socio-economic environment were studied. The basis for the work of UNESCO as a global repository and management centre in the museum community was shown. The authors note that the formation of museum competence and a change in the aesthetics of mass consciousness on this basis is possible only if the structural content of the coordination of museum art. The authors see the creation of a single-world museum centre as the basis for such a change."
Who seeks and shares misinformation about politicians? Focusing on the roles of party- and politician-level social identities,"Although numerous studies have explained the flow of misinformation, finding studies that theoretically examine the psychological factors related to individualsâ information behaviours is difficult. Social media data or meta-level analyses have limitations in providing an understanding of behaviours and processes at the individual level. Accordingly, this study aims to construct a predictive model for biased information seeking and sharing as a response to misinformation, which is information without the certainty of its truth, through a survey (N = 602). Applying social psychological concepts (i.e. social identity theory), two types of social identities were proposed as key factors of biased information seeking and sharing in the research model. Our model allows forecasting of what types of individuals are more likely to skip the fact-checking process and share misinformation. Â© The Author(s) 2023.","Although numerous studies have explained the flow of misinformation, finding studies that theoretically examine the psychological factors related to individuals information behaviours is difficult. Social media data or meta-level analyses have limitations in providing an understanding of behaviours and processes at the individual level. Accordingly, this study aims to construct a predictive model for biased information seeking and sharing as a response to misinformation, which is information without the certainty of its truth, through a survey (N = 602). Applying social psychological concepts ( social identity theory), two types of social identities were proposed as key factors of biased information seeking and sharing in the research model. Our model allows forecasting of what types of individuals are more likely to skip the fact-checking process and share misinformation."
Exploring the visual distinguishability and topic autocorrelation of murals unearthed in China from the spatial perspective,"Murals unearthed in China have outstanding regional characteristics and one of the largest period spans in scale and variety. To explore the visual distinguishability and topic autocorrelation of murals unearthed in China from the spatial perspective, multiple classification models are employed to classify murals unearthed in China through visual features. Then, the k-means is employed to mine topics, and they are analysed through topic intensities (TIs), Moranâs Index (MI) and spatial topic concentration degrees (STCDs). In addition, the characteristics of topic distribution and evolution are summarised and revealed in the spatial dimension. From a spatial perspective, it verifies the distinguishability of visual features of murals through ViT_BOW_GNB, and the precision of this model is 98.17%. Thirteen topics are clustered through k-means, and the distribution of mural topics is spatial autocorrelation according to MI. Besides, the topic evolves from the political centre to the surrounding area, and the topics with high intensities are highly concentrated in spatial. This study reveals the spatial characteristics of the mural at the level of visual features and semantics, which facilitates the digital management, conservation and knowledge discovery of cultural heritage resources. Â© The Author(s) 2023.","Murals unearthed in China have outstanding regional characteristics and one of the largest period spans in scale and variety. To explore the visual distinguishability and topic autocorrelation of murals unearthed in China from the spatial perspective, multiple classification models are employed to classify murals unearthed in China through visual features. Then, the k-means is employed to mine topics, and they are analysed through topic intensities (TIs), Morans Index and spatial topic concentration degrees (STCDs). In addition, the characteristics of topic distribution and evolution are summarised and revealed in the spatial dimension. From a spatial perspective, it verifies the distinguishability of visual features of murals through ViT_BOW_GNB, and the precision of this model is 98.17%. Thirteen topics are clustered through k-means, and the distribution of mural topics is spatial autocorrelation according to Besides, the topic evolves from the political centre to the surrounding area, and the topics with high intensities are highly concentrated in spatial. This study reveals the spatial characteristics of the mural at the level of visual features and semantics, which facilitates the digital management, conservation and knowledge discovery of cultural heritage resources."
A domain categorisation of vocabularies based on a deep learning classifier,"The publication of large amounts of open data is an increasing trend. This is a consequence of initiatives like Linked Open Data (LOD) that aims at publishing and linking data sets published in the World Wide Web. Linked Data publishers should follow a set of principles for their task. This information is described in a 2011 document that includes the consideration of reusing vocabularies as key. The Linked Open Vocabularies (LOV) project attempts to collect the vocabularies and ontologies commonly used in LOD. These ontologies have been classified by domain following the criteria of LOV members, thus having the disadvantage of introducing personal biases. This article presents an automatic classifier of ontologies based on the main categories appearing in Wikipedia. For that purpose, word-embedding models are used in combination with deep learning techniques. Results show that with a hybrid model of regular Deep Neural Networks (DNNs), Recurrent Neural Network (RNN) and Convolutional Neural Network (CNN), classification could be made with an accuracy of 93.57%. A further evaluation of the domain matchings between LOV and the classifier brings possible matchings in 79.8% of the cases. Â© The Author(s) 2021.","The publication of large amounts of open data is an increasing trend. This is a consequence of initiatives like Linked Open Data (LOD) that aims at publishing and linking data sets published in the World Wide Web. Linked Data publishers should follow a set of principles for their task. This information is described in a 2011 document that includes the consideration of reusing vocabularies as key. The Linked Open Vocabularies (LOV) project attempts to collect the vocabularies and ontologies commonly used in LOD. These ontologies have been classified by domain following the criteria of LOV members, thus having the disadvantage of introducing personal biases. This article presents an automatic classifier of ontologies based on the main categories appearing in Wikipedia. For that purpose, word-embedding models are used in combination with deep learning techniques. Results show that with a hybrid model of regular Deep Neural Networks (DNNs), Recurrent Neural Network (RNN) and Convolutional Neural Network (CNN), classification could be made with an accuracy of 93.57%. A further evaluation of the domain matchings between LOV and the classifier brings possible matchings in 79.8% of the cases."
"Embodying algorithms, enactive artificial intelligence and the extended cognition: You can see as much as you know about algorithm","The recent proliferation of artificial intelligence (AI) gives rise to questions on how users interact with AI services and how algorithms embody the values of users. Despite the surging popularity of AI, how users evaluate algorithms, how people perceive algorithmic decisions, and how they relate to algorithmic functions remain largely unexplored. Invoking the idea of embodied cognition, we characterize core constructs of algorithms that drive the value of embodiment and conceptualizes these factors in reference to trust by examining how they influence the user experience of personalized recommendation algorithms. The findings elucidate the embodied cognitive processes involved in reasoning algorithmic characteristics â fairness, accountability, transparency, and explainability â with regard to their fundamental linkages with trust and ensuing behaviors. Users use a dual-process model, whereby a sense of trust built on a combination of normative values and performance-related qualities of algorithms. Embodied algorithmic characteristics are significantly linked to trust and performance expectancy. Heuristic and systematic processes through embodied cognition provide a concise guide to its conceptualization of AI experiences and interaction. The identified user cognitive processes provide information on a userâs cognitive functioning and patterns of behavior as well as a basis for subsequent metacognitive processes. Â© The Author(s) 2021.","The recent proliferation of artificial intelligence (AI) gives rise to questions on how users interact with AI services and how algorithms embody the values of users. Despite the surging popularity of AI, how users evaluate algorithms, how people perceive algorithmic decisions, and how they relate to algorithmic functions remain largely unexplored. Invoking the idea of embodied cognition, we characterize core constructs of algorithms that drive the value of embodiment and conceptualizes these factors in reference to trust by examining how they influence the user experience of personalized recommendation algorithms. The findings elucidate the embodied cognitive processes involved in reasoning algorithmic characteristics fairness, accountability, transparency, and explainability with regard to their fundamental linkages with trust and ensuing behaviors. Users use a dual-process model, whereby a sense of trust built on a combination of normative values and performance-related qualities of algorithms. Embodied algorithmic characteristics are significantly linked to trust and performance expectancy. Heuristic and systematic processes through embodied cognition provide a concise guide to its conceptualization of AI experiences and interaction. The identified user cognitive processes provide information on a users cognitive functioning and patterns of behavior as well as a basis for subsequent metacognitive processes."
Detecting the publicâs information behaviour preferences in multiple emergency events,"Due to the frequent occurrence of public emergency events and the extensive use of social media in recent years, the public is more and more involved in the communication of public emergencies. As important members of the public, the rapid and massive information sharing of social media users makes them play an increasingly crucial role in the emergency processing. Hence, it is necessary to analyse information behaviours of social media users in emergency events. This article mined the information behaviours of users in multiple events to reflect the publicâs behaviour preferences, aiming to provide information support for emergency handling. Specifically, we collected the user-generated contents related to emergency events, and then analysed the user-generated contents from multiple dimensions to obtain the corresponding information behaviours. Finally, based on the comparative analysis of four events, the information behaviour preferences of the public during emergencies were obtained. The experimental results indicate that the publicâs behaviours in emergencies are related to their own interests and economic status, and curiosity about the details of events is the consistent appeal of the public. Â© The Author(s) 2021.","Due to the frequent occurrence of public emergency events and the extensive use of social media in recent years, the public is more and more involved in the communication of public emergencies. As important members of the public, the rapid and massive information sharing of social media users makes them play an increasingly crucial role in the emergency processing. Hence, it is necessary to analyse information behaviours of social media users in emergency events. This article mined the information behaviours of users in multiple events to reflect the publics behaviour preferences, aiming to provide information support for emergency handling. Specifically, we collected the user-generated contents related to emergency events, and then analysed the user-generated contents from multiple dimensions to obtain the corresponding information behaviours. Finally, based on the comparative analysis of four events, the information behaviour preferences of the public during emergencies were obtained. The experimental results indicate that the publics behaviours in emergencies are related to their own interests and economic status, and curiosity about the details of events is the consistent appeal of the public."
Open access movement in the scholarly world: Pathways for libraries in developing countries,"Open access is a scholarly publishing model that has emerged as an alternative to traditional subscription-based journal publishing. This study explores the adoption of the open access movement worldwide and the role that libraries can play in addressing those factors which are slowing its progress within developing countries. The study has drawn upon both qualitative data from a focused literature review and quantitative data from major open access platforms. The results indicate that while the open access movement is steadily gaining acceptance worldwide, the progress in developing countries within geographical areas such as Africa, Asia and Oceania is quite a bit slower. Two significant factors are the cost of publishing fees and the lack of institutional open access mandates and policies to encourage uptake. The study provides suggested strategies for academic libraries to help overcome current challenges. Â© The Author(s) 2023.","Open access is a scholarly publishing model that has emerged as an alternative to traditional subscription-based journal publishing. This study explores the adoption of the open access movement worldwide and the role that libraries can play in addressing those factors which are slowing its progress within developing countries. The study has drawn upon both qualitative data from a focused literature review and quantitative data from major open access platforms. The results indicate that while the open access movement is steadily gaining acceptance worldwide, the progress in developing countries within geographical areas such as Africa, Asia and Oceania is quite a bit slower. Two significant factors are the cost of publishing fees and the lack of institutional open access mandates and policies to encourage uptake. The study provides suggested strategies for academic libraries to help overcome current challenges."
The expansion of team size in library and information science (LIS): Is bigger always better?,"The increasing prevalence of large research teams in contemporary science has prompted an investigation into the trends of team size in library and information science (LIS). In this study, we analysed 103,299 LIS publications written by 129,560 unique authors between 2000 and 2020 sourced from the Microsoft Academic Graph (MAG) data sets. We conducted a temporal analysis from multiple dimensions, including journal quartiles, research topics and researchers with varying impact levels. In addition, we employed two multivariate linear regression models â with and without author fixed effects â to scrutinise the relationship between team size and publication impact. Our findings reveal continuous growth in LIS team size. Notably, publications in higher-quartile journals tend to have larger teams; the team size of technical topic publications is generally larger than that of theoretical topic publications; and researchers with a higher h-index are able to assemble larger teams. Although we observed that co-authored papers have a higher average citation impact than single-authored papers, the overall positive impact of team expansion on citation growth is not always significant within the common LIS team size range (three to six authors). Our research suggests that indiscriminately increasing the size of a team may not be a prudent decision. Â© The Author(s) 2023.","The increasing prevalence of large research teams in contemporary science has prompted an investigation into the trends of team size in library and information science (LIS). In this study, we analysed 103,299 LIS publications written by 129,560 unique authors between 2000 and 2020 sourced from the Microsoft Academic Graph (MAG) data sets. We conducted a temporal analysis from multiple dimensions, including journal quartiles, research topics and researchers with varying impact levels. In addition, we employed two multivariate linear regression models with and without author fixed effects to scrutinise the relationship between team size and publication impact. Our findings reveal continuous growth in LIS team size. Notably, publications in higher-quartile journals tend to have larger teams; the team size of technical topic publications is generally larger than that of theoretical topic publications; and researchers with a higher h-index are able to assemble larger teams. Although we observed that co-authored papers have a higher average citation impact than single-authored papers, the overall positive impact of team expansion on citation growth is not always significant within the common LIS team size range (three to six authors). Our research suggests that indiscriminately increasing the size of a team may not be a prudent decision."
ALIS: A novel metric in lineage-independent evaluation of scholars,"Evaluative bibliometrics often attempts to explore various methods to measure individual scholarly influence. Scholarly independence (SI) is a unique indicator that can be used to understand and assess the research performances of individual scholars. The SI is a rare quality that most funding agencies and universities seek during funding decisions or hiring processes. We propose author lineage independent score (ALIS), a unique model to measure SI of a scholar by using his or her academic genealogy tree as the underlying graph structure. The analysis is performed on real data of 100 authors, collected from the Web of Science (WoS) and the Mathematics Genealogy Project. The analysis is further validated on a larger scale, on a simulated sample of 10,000 authors. The simulation exercise is the proof-of-concept for scalability of the metric and the proposed optimisation model. ALIS exploits genealogical relationships between scholars and their mentors and collaborating communities and constructs an influence scoring model based on the Genealogy tree structure of the respective scholars. The implications from the theoretical model are found to be profound in tracing known and recursive citation patterns among peers. The genealogy tree is used to investigate the advisorâadvisee relationship and lays the foundation for defining metrics used to calculate the various indicators such as non-genealogy citations (NGCs), non-community citations (NCCs) and other citation quotient (OCQ). As these indicators/parameters are novel and thus not readily accessible, algorithms are written to compute these indicator values for the scholars under study. Â© The Author(s) 2021.","Evaluative bibliometrics often attempts to explore various methods to measure individual scholarly influence. Scholarly independence (SI) is a unique indicator that can be used to understand and assess the research performances of individual scholars. The SI is a rare quality that most funding agencies and universities seek during funding decisions or hiring processes. We propose author lineage independent score (ALIS), a unique model to measure SI of a scholar by using his or her academic genealogy tree as the underlying graph structure. The analysis is performed on real data of 100 authors, collected from the Web of Science (WoS) and the Mathematics Genealogy Project. The analysis is further validated on a larger scale, on a simulated sample of 10,000 authors. The simulation exercise is the proof-of-concept for scalability of the metric and the proposed optimisation model. ALIS exploits genealogical relationships between scholars and their mentors and collaborating communities and constructs an influence scoring model based on the Genealogy tree structure of the respective scholars. The implications from the theoretical model are found to be profound in tracing known and recursive citation patterns among peers. The genealogy tree is used to investigate the advisoradvisee relationship and lays the foundation for defining metrics used to calculate the various indicators such as non-genealogy citations (NGCs), non-community citations (NCCs) and other citation quotient (OCQ). As these indicators/parameters are novel and thus not readily accessible, algorithms are written to compute these indicator values for the scholars under study."
Searchable Turkish OCRed historical newspaper collection 1928â1942,"The newspaper emerged as a distinct cultural form in early 17th-century Europe. It is bound up with the early modern period of history. Historical newspapers are of utmost importance to nations and its people, and researchers from different disciplines rely on these papers to improve our understanding of the past. In pursuit of satisfying this need, Istanbul University Head Office of Library and Documentation provides access to a big database of scanned historical newspapers. To take it another step further and make the documents more accessible, we need to run optical character recognition (OCR) and named entity recognition (NER) tasks on the whole database and index the results to allow for full-text search mechanism. We design and implement a system encompassing the whole pipeline starting from scrapping the dataset from the original website to providing a graphical user interface to run search queries, and it manages to do that successfully. Proposed system provides to search people, culture and security-related keywords and to visualise them. Â© The Author(s) 2021.","The newspaper emerged as a distinct cultural form in early 17th-century Europe. It is bound up with the early modern period of history. Historical newspapers are of utmost importance to nations and its people, and researchers from different disciplines rely on these papers to improve our understanding of the past. In pursuit of satisfying this need, Istanbul University Head Office of Library and Documentation provides access to a big database of scanned historical newspapers. To take it another step further and make the documents more accessible, we need to run optical character recognition (OCR) and named entity recognition (NER) tasks on the whole database and index the results to allow for full-text search mechanism. We design and implement a system encompassing the whole pipeline starting from scrapping the dataset from the original website to providing a graphical user interface to run search queries, and it manages to do that successfully. Proposed system provides to search people, culture and security-related keywords and to visualise them."
Context-based understanding of food-related queries using a culinary knowledge model,"Dietary practices are governed by a mix of ethnographic aspects, such as social, cultural and environmental factors. These aspects need to be taken into consideration during an analysis of food-related queries. Queries are usually ambiguous. It is essential to understand, analyse and refine the queries for better search and retrieval. The work is focused on identifying the explicit, implicit and hidden facets of a query, taking into consideration the context â culinary domain. This article proposes a technique for query understanding, analysis and refinement based on a domain specific knowledge model. Queries are conceptualised by mapping the query term to concepts defined in the model. This allows an understanding of the semantic point of view of a query and an ability to determine the meaning of its terms and their interrelatedness. The knowledge model acts as a backbone providing the context for query understanding, analysis and refinement and outperforms other models, such as Schema.org, BBC Food Ontology and Recipe Ontology. Â© The Author(s) 2021.","Dietary practices are governed by a mix of ethnographic aspects, such as social, cultural and environmental factors. These aspects need to be taken into consideration during an analysis of food-related queries. Queries are usually ambiguous. It is essential to understand, analyse and refine the queries for better search and retrieval. The work is focused on identifying the explicit, implicit and hidden facets of a query, taking into consideration the context culinary domain. This article proposes a technique for query understanding, analysis and refinement based on a domain specific knowledge model. Queries are conceptualised by mapping the query term to concepts defined in the model. This allows an understanding of the semantic point of view of a query and an ability to determine the meaning of its terms and their interrelatedness. The knowledge model acts as a backbone providing the context for query understanding, analysis and refinement and outperforms other models, such as Schema.org, BBC Food Ontology and Recipe Ontology."
Multi-agent-based hybrid peer-to-peer system for distributed information retrieval,"With the increasingly huge amount of data located in various databases and the need for users to access them, distributed information retrieval (DIR) has been at the core of the preoccupations of a number of researchers. Indeed, numerous DIR systems and architectures have been proposed including the broker-based architecture. Moreover, providing DIR with more flexibility and adaptability has led researchers thinking to build DIR with software agents. Thus, this research proposes a design and an implementation of a novel system based on the broker-based architecture and the peer-to-peer (P2P) network called broker-based P2P network. The proposed architecture is implemented with a multi-agent system (MAS) where the main agent playing the role of the broker, receives query from a peer agent and forwards them to other peer agents each with their index and resources. Upon completing retrieval process at each peer agent, results are directly sent to the peer agent that initiated the query without using the broker agent. Java Agent DEvelopment framework (JADE) is used to implement the agents and, for experiments, TERRIER (TERabyte RetRIEveR) is extended and used as the search engine to retrieve the Text Retrieval Conference (TREC) collections dataset notably TREC-6. The peer agent that originated the query progressively collects results coming from other peer agents, normalises and merges them and then proceeds with re-ranking. For normalisation, MinMax and Sum that are unsupervised normalisation methods are used. Â© The Author(s) 2021.","With the increasingly huge amount of data located in various databases and the need for users to access them, distributed information retrieval (DIR) has been at the core of the preoccupations of a number of researchers. Indeed, numerous DIR systems and architectures have been proposed including the broker-based architecture. Moreover, providing DIR with more flexibility and adaptability has led researchers thinking to build DIR with software agents. Thus, this research proposes a design and an implementation of a novel system based on the broker-based architecture and the peer-to-peer (P2P) network called broker-based P2P network. The proposed architecture is implemented with a multi-agent system (MAS) where the main agent playing the role of the broker, receives query from a peer agent and forwards them to other peer agents each with their index and resources. Upon completing retrieval process at each peer agent, results are directly sent to the peer agent that initiated the query without using the broker agent. Java Agent DEvelopment framework (JADE) is used to implement the agents and, for experiments, TERRIER (TERabyte RetRIEveR) is extended and used as the search engine to retrieve the Text Retrieval Conference (TREC) collections dataset notably TREC-6. The peer agent that originated the query progressively collects results coming from other peer agents, normalises and merges them and then proceeds with re-ranking. For normalisation, MinMax and Sum that are unsupervised normalisation methods are used."
Social capital or non-human sources? A cross-context study on information source selection of migrant farmer workers,"Insufficient examination of social factors obscures the reason why non-human information sources are under-utilised by social groups with lower information literacy. This study explores the mechanism of information source selection (ISS) of Chinese migrant farmer workers (MFWs) in different industries by conducting a cross-context analysis. After iterative analyses of multiple cases, a theoretical model of information source selection within an individualâs information world is constructed. It explains why MFWs make more use of social capitals than non-human information sources in information seeking. Besides, the information needs are examined form both the needed information and the need itself. A classification of social capital as human information source is created and the roles that social capitals and non-human information sources play in ISS are identified. This study provides novel theoretical insights into the âoldâ issue of ISS, and thus has practical implications for public information service providers and MFW-related policy makers. Â© The Author(s) 2021.","Insufficient examination of social factors obscures the reason why non-human information sources are under-utilised by social groups with lower information literacy. This study explores the mechanism of information source selection (ISS) of Chinese migrant farmer workers (MFWs) in different industries by conducting a cross-context analysis. After iterative analyses of multiple cases, a theoretical model of information source selection within an individuals information world is constructed. It explains why MFWs make more use of social capitals than non-human information sources in information seeking. Besides, the information needs are examined form both the needed information and the need itself. A classification of social capital as human information source is created and the roles that social capitals and non-human information sources play in ISS are identified. This study provides novel theoretical insights into the old issue of ISS, and thus has practical implications for public information service providers and MFW-related policy makers."
Research on differential and interactive impact of China-led and US-led open-access articles,"With the development of Web 2.0, social media dialogue has been increasingly important within the world of open access (OA), striving for more user-generated content and ease of use. In this article, we analysed the impact of OA articles published by both Chinese and the American researchers using PLOS ONE. Papers published in the same year, using citation and social media metrics, were all used to analyse the correlation between the level of social media metrics and citation. Overall, the impact of OA articles published within the United States is higher than OA articles published in China. The results showed that citations and number of Mendeley readers have a significant correlation, which reflect the similar impact in evaluation of OA articles. However, most social media metrics did not have an obvious correlation with impact evaluation, which indicates the social media metrics are useful when paired with citations, but not irreplaceable to citations. Social media metrics appear to be a useful alternative metrics to accurately reflecting the impact of OA articles within the scientific community. Â© The Author(s) 2021.","With the development of Web 2.0, social media dialogue has been increasingly important within the world of open access (OA), striving for more user-generated content and ease of use. In this article, we analysed the impact of OA articles published by both Chinese and the American researchers using PLOS ONE. Papers published in the same year, using citation and social media metrics, were all used to analyse the correlation between the level of social media metrics and citation. Overall, the impact of OA articles published within the United States is higher than OA articles published in China. The results showed that citations and number of Mendeley readers have a significant correlation, which reflect the similar impact in evaluation of OA articles. However, most social media metrics did not have an obvious correlation with impact evaluation, which indicates the social media metrics are useful when paired with citations, but not irreplaceable to citations. Social media metrics appear to be a useful alternative metrics to accurately reflecting the impact of OA articles within the scientific community."
Incorporating heterogeneous information in deep learning with informative meta-paths for community recommendations,"Communities of interest promote knowledge sharing and discovery in social network platforms. However, platform users face difficulties of finding suitable communities, given their increasing number. Although recommendations have been proposed to help users find communities of interest, these methods ignore or exclude heterogeneous interactions between users and communities. In addition, widely used meta-paths help capture the complex semantic relation among entities but heavily rely on domain knowledge. In this study, we propose a novel recommendation model based on informative meta-path discovery in heterogeneous information networks and deep learning. Users, communities, relevant items and their relations are considered as entities in a heterogeneous information network, from where informative meta-paths are extracted on the basis of information theory to measure user-community similarities. Finally, similarities are incorporated in a deep learning model to predict whether target users join candidate communities. The proposed recommendation model is evaluated and compared against baseline methods using two data sets. Results demonstrate the superior performance of the present model in terms of precision, recall and F score. Â© The Author(s) 2021.","Communities of interest promote knowledge sharing and discovery in social network platforms. However, platform users face difficulties of finding suitable communities, given their increasing number. Although recommendations have been proposed to help users find communities of interest, these methods ignore or exclude heterogeneous interactions between users and communities. In addition, widely used meta-paths help capture the complex semantic relation among entities but heavily rely on domain knowledge. In this study, we propose a novel recommendation model based on informative meta-path discovery in heterogeneous information networks and deep learning. Users, communities, relevant items and their relations are considered as entities in a heterogeneous information network, from where informative meta-paths are extracted on the basis of information theory to measure user-community similarities. Finally, similarities are incorporated in a deep learning model to predict whether target users join candidate communities. The proposed recommendation model is evaluated and compared against baseline methods using two data sets. Results demonstrate the superior performance of the present model in terms of precision, recall and F score."
Informational features of WhatsApp in everyday life in Madrid: An exploratory study,"WhatsApp is one of the most used social media tools, but little is known about its use for everyday purposes. In this study, the informational features of WhatsApp in everyday life in Madrid are analysed through 30 semi-structured interviews, resulting in an informational typology of the messages, a description of the informational purposes of WhatsApp use and descriptions of the social use of WhatsApp. We conclude that WhatsApp allows us to deepen our understanding of the informational habits of people in everyday life. Â© The Author(s) 2021.","WhatsApp is one of the most used social media tools, but little is known about its use for everyday purposes. In this study, the informational features of WhatsApp in everyday life in Madrid are analysed through 30 semi-structured interviews, resulting in an informational typology of the messages, a description of the informational purposes of WhatsApp use and descriptions of the social use of WhatsApp. We conclude that WhatsApp allows us to deepen our understanding of the informational habits of people in everyday life."
The ripple effect of dataset reuse: Contextualising the data lifecycle for machine learning data sets and social impact,"Although there exists a rich literature on data lifecycle, a common framework for data lifecycle depicts reuse as the last stage. However, this framework fails to explain the complex lifecycle of machine learning (ML) data sets, which can have many different afterlives. Data sets for ML can be expanded to supplement previous research, and researchers can concatenate multiple data sets to develop new models. This study discusses ML dataset reuse through the lens of the dataâinformationâknowledgeâwisdom pyramid. In social science research, researchers might reuse data to analyse a new research question that is still in the context of the data domain. By contrast, research practices in ML, where researchers layer multiple data sets for training purposes, require us to ask whether the existing data lifecycle model, ending with âreuseâ, is appropriate for explaining such an iterative and layered lifecycle. This study introduces one case of merging computer vision data set and natural language processing data set and two cases of applying ML models from outside of the ML community (hate speech detection and politeness detection) to justify a framework for a ML dataset lifecycle. Last but not least, this study proposes a ML dataset lifecycle and provides case examples to describe each stage. Â© The Author(s) 2023.","Although there exists a rich literature on data lifecycle, a common framework for data lifecycle depicts reuse as the last stage. However, this framework fails to explain the complex lifecycle of machine learning data sets, which can have many different afterlives. Data sets for ML can be expanded to supplement previous research, and researchers can concatenate multiple data sets to develop new models. This study discusses ML dataset reuse through the lens of the datainformationknowledgewisdom pyramid. In social science research, researchers might reuse data to analyse a new research question that is still in the context of the data domain. By contrast, research practices in ML, where researchers layer multiple data sets for training purposes, require us to ask whether the existing data lifecycle model, ending with reuse, is appropriate for explaining such an iterative and layered lifecycle. This study introduces one case of merging computer vision data set and natural language processing data set and two cases of applying ML models from outside of the ML community (hate speech detection and politeness detection) to justify a framework for a ML dataset lifecycle. Last but not least, this study proposes a ML dataset lifecycle and provides case examples to describe each stage."
A review on h-index and its alternative indices,"In recent years, several scientometrics and bibliometrics indicators were proposed to evaluate the scientific impact of individuals, institutions, colleges, universities and research teams. The h-index gives a breakthrough in the research community for assessing the scientific impact of an individual. It got a lot of attention due to its simplicity, and several other indicators were proposed to extend the properties of the h-index and to overcome its shortcomings. In this literature review, we have discussed the advantages and limitations of almost all scientometrics and bibliometrics indicators, which have been categorised into seven categories based on their properties: (1) complement of h-index, (2) based on total number of authors, (3) based on publication age, (4) combination of two indices, (5) based on excess citation count, (6) based on total publication count and (7) based on other variants. The primary objective of this article is to study all those indicators which have been proposed to evaluate the scientific impact of an individual researcher or a group of researchers. Â© The Author(s) 2021.","In recent years, several scientometrics and bibliometrics indicators were proposed to evaluate the scientific impact of individuals, institutions, colleges, universities and research teams. The h-index gives a breakthrough in the research community for assessing the scientific impact of an individual. It got a lot of attention due to its simplicity, and several other indicators were proposed to extend the properties of the h-index and to overcome its shortcomings. In this literature review, we have discussed the advantages and limitations of almost all scientometrics and bibliometrics indicators, which have been categorised into seven categories based on their properties: complement of h-index, based on total number of authors, based on publication age, combination of two indices, based on excess citation count, based on total publication count and based on other variants. The primary objective of this article is to study all those indicators which have been proposed to evaluate the scientific impact of an individual researcher or a group of researchers."
Usability of data-oriented user interfaces for cultural heritage: A systematic mapping study,"This study surveys the state of the art in usability and user experience strategies applied to applications that deal with large amounts of data in the field of cultural heritage, highlighting the most prominent aspects and underlining the under-explored. In these applications, large amounts of data need to be wisely presented to help final users at drawing conclusions and making decisions. While sophisticated technology may be used to improve the user experience, it should not be applied to the detriment of usability, which is critical for the success of these applications. We performed a systematic mapping study to classify the literature retrieved in the four largest scientific databases by a structured search string. We classify applications according to purpose, intended users, the way they address and evaluate user experience and usability, among others, and include the analysis of combined results through maps. Findings reveal the contradiction that while most articles are intended for the education and tourism of the general public, only half of the studies evaluate usability. Moreover, there is a significant research gap in user interfaces for systems in the context of preventive conservation, for research, assessment and decision assistance. This is the first systematic mapping study combining usability and cultural heritage, especially for data-oriented applications. It shows that more research is necessary to assist conservators and researchers and to address usability from early stages of development. Â© The Author(s) 2021.","This study surveys the state of the art in usability and user experience strategies applied to applications that deal with large amounts of data in the field of cultural heritage, highlighting the most prominent aspects and underlining the under-explored. In these applications, large amounts of data need to be wisely presented to help final users at drawing conclusions and making decisions. While sophisticated technology may be used to improve the user experience, it should not be applied to the detriment of usability, which is critical for the success of these applications. We performed a systematic mapping study to classify the literature retrieved in the four largest scientific databases by a structured search string. We classify applications according to purpose, intended users, the way they address and evaluate user experience and usability, among others, and include the analysis of combined results through maps. Findings reveal the contradiction that while most articles are intended for the education and tourism of the general public, only half of the studies evaluate usability. Moreover, there is a significant research gap in user interfaces for systems in the context of preventive conservation, for research, assessment and decision assistance. This is the first systematic mapping study combining usability and cultural heritage, especially for data-oriented applications. It shows that more research is necessary to assist conservators and researchers and to address usability from early stages of development."
Between trust and mistrust: Source evaluation behaviours of Brazilian and French adolescents searching for scientific information on the Web,"In this qualitative study, 41 adolescents from Brazil and France were asked to conduct web-based searches for science-related information. Different search contexts and current socio-scientific issues were used in order to represent a variety of situations that adolescents are confronted with in their everyday life. Subsequently, they were interviewed about their individual searches. We sought to answer three questions: (a) What types of sources do Brazilian and French adolescents access in their searches? (b) Which evaluation criteria do they use to select the sources they access? (c) Do their source choices differ according to the search context? The results reveal that, regardless of search context, the participants demonstrated trust in news sources, and mistrust towards official sources, mostly as a function of the topic. Receiver-related criteria were particularly relevant for the Brazilian adolescents when selecting the sources, while the French participants based their choices more on parameters related to the content. Results suggest how previous knowledge and interest on the topic can influence teenagersâ choices. Â© The Author(s) 2023.","In this qualitative study, 41 adolescents from Brazil and France were asked to conduct web-based searches for science-related information. Different search contexts and current socio-scientific issues were used in order to represent a variety of situations that adolescents are confronted with in their everyday life. Subsequently, they were interviewed about their individual searches. We sought to answer three questions: (a) What types of sources do Brazilian and French adolescents access in their searches? (b) Which evaluation criteria do they use to select the sources they access? Do their source choices differ according to the search context? The results reveal that, regardless of search context, the participants demonstrated trust in news sources, and mistrust towards official sources, mostly as a function of the topic. Receiver-related criteria were particularly relevant for the Brazilian adolescents when selecting the sources, while the French participants based their choices more on parameters related to the content. Results suggest how previous knowledge and interest on the topic can influence teenagers choices."
Women authorship in library and information science journals from 1981 to 2020: Is equitable representation being attained?,"This study examines the proportion of women as first authors in major library and information science (LIS) journals over the years 1981â2020. Author name and year data were collected for 10 LIS journals â five that are associated more with library topics and five with information science topics â and analysed using the genderize.io tool. Both general trends over time and comparisons of information science versus library science journals are presented. The findings indicate significant growth in the proportion of women authors among the LIS journals, but primarily concentrated only among the library science journals, with information science journals falling well behind. Representation of women authors (~60%) still lags well below the overall representation of women in librarianship (~80%). These findings suggest that there is still considerable growth needed to decrease the gender gap among authorship in top LIS journals. Â© The Author(s) 2021.","This study examines the proportion of women as first authors in major library and information science (LIS) journals over the years 19812020. Author name and year data were collected for 10 LIS journals five that are associated more with library topics and five with information science topics and analysed using the genderize.io tool. Both general trends over time and comparisons of information science versus library science journals are presented. The findings indicate significant growth in the proportion of women authors among the LIS journals, but primarily concentrated only among the library science journals, with information science journals falling well behind. Representation of women authors (~60%) still lags well below the overall representation of women in librarianship (~80%). These findings suggest that there is still considerable growth needed to decrease the gender gap among authorship in top LIS journals."
Open access editorial policies of SciELO health sciences journals,"SciELO promotes open access and cooperative publication of scholarly journals, based mainly in Latin America, the Caribbean, Spain and Portugal. SciELO was created to offer solutions to increase the visibility of participating journals and facilitate free access to their full texts. This work aims to analyse the open access editorial policies implemented by the health sciences journals of the SciELO network (411 journals at the time of this study) in terms of authorsâ rights, copyright issues, self-archiving policies and openness. From SciELO health sciences journals network, 92% of the 411 journals use a Creative Commons licence, 89% require transfer of author copyright and 14% apply author processing charges. According to the past SHERPA/RoMEO taxonomy of self-archiving policies, 8.5% of the journals were classified as white, 81.5% blue and 10% green. The openness of journals calculated through the Open Access Spectrum approach was higher than 60% in more than 80% of the total journals. Out of the 411 journals in SciELO portals, 380 have their own website. Discrepancies were found between licences stated in SciELO compared with the ones used in their websites, mainly due to the lack of declared licences in either of the two sources or because the licences did not match. The licences used on the websites and in SciELO were also compared with their corresponding records in the Directory of Open Access Journals and Crossref, and again the differences were narrowly related to the data supplier. Â© The Author(s) 2021.","SciELO promotes open access and cooperative publication of scholarly journals, based mainly in Latin America, the Caribbean, Spain and Portugal. SciELO was created to offer solutions to increase the visibility of participating journals and facilitate free access to their full texts. This work aims to analyse the open access editorial policies implemented by the health sciences journals of the SciELO network (411 journals at the time of this study) in terms of authors rights, copyright issues, self-archiving policies and openness. From SciELO health sciences journals network, 92% of the 411 journals use a Creative Commons licence, 89% require transfer of author copyright and 14% apply author processing charges. According to the past SHERPA/RoMEO taxonomy of self-archiving policies, 8.5% of the journals were classified as white, 81.5% blue and 10% green. The openness of journals calculated through the Open Access Spectrum approach was higher than 60% in more than 80% of the total journals. Out of the 411 journals in SciELO portals, 380 have their own website. Discrepancies were found between licences stated in SciELO compared with the ones used in their websites, mainly due to the lack of declared licences in either of the two sources or because the licences did not match. The licences used on the websites and in SciELO were also compared with their corresponding records in the Directory of Open Access Journals and Crossref, and again the differences were narrowly related to the data supplier."
A Social Network Analysisâbased approach to investigate user behaviour during a cryptocurrency speculative bubble,"In this article, we present a Social Network Analysisâbased approach to investigate user behaviour during a cryptocurrency speculative bubble in order to extract knowledge patterns about it. Our approach is general and can be applied to any past, present and future cryptocurrency speculative bubble. To verify its potential, we apply it to investigate the Ethereum speculative bubble happened in the years 2017 and 2018. We also describe several interesting knowledge patterns about the behaviour of specific categories of users that we obtained from this investigation. Furthermore, we describe how our approach can support the construction of an identikit of the speculators who maneuvered behind the Ethereum bubble analysed. Finally, we show that this capability of supporting the hunting for speculators is intrinsic of our approach and can cover past, present and future bubbles. Â© The Author(s) 2021.","In this article, we present a Social Network Analysisbased approach to investigate user behaviour during a cryptocurrency speculative bubble in order to extract knowledge patterns about it. Our approach is general and can be applied to any past, present and future cryptocurrency speculative bubble. To verify its potential, we apply it to investigate the Ethereum speculative bubble happened in the years 2017 and 2018. We also describe several interesting knowledge patterns about the behaviour of specific categories of users that we obtained from this investigation. Furthermore, we describe how our approach can support the construction of an identikit of the speculators who maneuvered behind the Ethereum bubble analysed. Finally, we show that this capability of supporting the hunting for speculators is intrinsic of our approach and can cover past, present and future bubbles."
Rumour detection based on deep hybrid structural and sequential representation networks,"The rapid development of online social networks significantly facilitates the interaction of people and dramatically expands the diffusion sphere of information. Rumours, however, are not excluded from the list of beneficiaries. The widespread of rumours has lengthened the psychological distance and caused tremendous economic losses, and rumour detection has become an inescapable and challenging task of great practical importance. In this work, we propose a novel neural network architecture for rumour classification and early rumour detection of fine-grained categories. Unlike using tree-like modules for structural feature extraction, we build an information stream network and employ graph convolutional networks to explore the relations among the hierarchical nodes in the network. To enhance the sequential representation learning, the module of deep bilateral gated recurrent units is further incorporated to reveal the crucial features hiding behind the information flow. Moreover, to organically fuse the learned high-dimensionally structural and sequential features, attention mechanism is applied to automatically adjust the trainable weights. Comparative experiments on two real-world datasets demonstrate that our proposed model outperforms the state-of-the-art methods in the task of fine-grained rumour classification and is capable of identifying rumours at an early stage. Â© The Author(s) 2021.","The rapid development of online social networks significantly facilitates the interaction of people and dramatically expands the diffusion sphere of information. Rumours, however, are not excluded from the list of beneficiaries. The widespread of rumours has lengthened the psychological distance and caused tremendous economic losses, and rumour detection has become an inescapable and challenging task of great practical importance. In this work, we propose a novel neural network architecture for rumour classification and early rumour detection of fine-grained categories. Unlike using tree-like modules for structural feature extraction, we build an information stream network and employ graph convolutional networks to explore the relations among the hierarchical nodes in the network. To enhance the sequential representation learning, the module of deep bilateral gated recurrent units is further incorporated to reveal the crucial features hiding behind the information flow. Moreover, to organically fuse the learned high-dimensionally structural and sequential features, attention mechanism is applied to automatically adjust the trainable weights. Comparative experiments on two real-world datasets demonstrate that our proposed model outperforms the state-of-the-art methods in the task of fine-grained rumour classification and is capable of identifying rumours at an early stage."
Modelling the factors that determine online news-sharing behaviour of social media users: The role of perceived message and online environmental characteristics,"This study modelled the online environment characteristics and message characteristics that predict news sharing among social media users. This studyâs data were obtained from a cross-sectional national survey conducted in Nigeria. Qualtrics was used to recruit data from 1320 participants in Nigeria. The participants were recruited via a stratified quota sampling which reflected the countryâs census statistics for gender and age. We found the message characteristics to predict news-sharing behaviour among social media users in Nigeria. By implication, the characteristics of a message encountered online influence the news-sharing behaviour of social media users. We also found that the online environment characteristics predict news-sharing behaviour, which implies that the external factor, that is, the relationship a user has with his network members, predicts sharing behaviour. Some theoretical and practical implications were provided to conclude the study. Â© The Author(s) 2023.","This study modelled the online environment characteristics and message characteristics that predict news sharing among social media users. This studys data were obtained from a cross-sectional national survey conducted in Nigeria. Qualtrics was used to recruit data from 1320 participants in Nigeria. The participants were recruited via a stratified quota sampling which reflected the countrys census statistics for gender and age. We found the message characteristics to predict news-sharing behaviour among social media users in Nigeria. By implication, the characteristics of a message encountered online influence the news-sharing behaviour of social media users. We also found that the online environment characteristics predict news-sharing behaviour, which implies that the external factor, that is, the relationship a user has with his network members, predicts sharing behaviour. Some theoretical and practical implications were provided to conclude the study."
An integrative framework of information as both objective and subjective,"We present a model of information that integrates two competing perspectives of information by emulating the Chinese philosophy of yin-yang. The model embraces the two key dimensions of information that exist harmoniously: information as (1) objective and veridical representations in the world (information as object) and (2) socially constructed interpretations that are a result of contextual influences (information as subject). We argue that these two facets of information cocreate information as a unified system and complement one another through two processes, which we denote as forming and informing. While the information literature has historically treated these objective and subjective identities of information as incompatible, we argue that they are mutually relevant and that our understanding of one actually enhances our understanding of the other. Â© The Author(s) 2021.","We present a model of information that integrates two competing perspectives of information by emulating the Chinese philosophy of yin-yang. The model embraces the two key dimensions of information that exist harmoniously: information as objective and veridical representations in the world (information as object) and socially constructed interpretations that are a result of contextual influences (information as subject). We argue that these two facets of information cocreate information as a unified system and complement one another through two processes, which we denote as forming and informing. While the information literature has historically treated these objective and subjective identities of information as incompatible, we argue that they are mutually relevant and that our understanding of one actually enhances our understanding of the other."
Binary background model with geometric mean for author-independent authorship verification,"Authorship verification (AV) is one of the main problems of authorship analysis and digital text forensics. The classical AV problem is to decide whether or not a particular author wrote the document in question. However, if there is one and relatively short document as the authorâs known document, the verification problem becomes more difficult than the classical AV and needs a generalised solution. Regarding to decide AV of the given two unlabeled documents (2D-AV), we proposed a system that provides an author-independent solution with the help of a Binary Background Model (BBM). The BBM is a supervised model that provides an informative background to distinguish document pairs written by the same or different authors. To evaluate the document pairs in one representation, we also proposed a new, simple and efficient document combination method based on the geometric mean of the stylometric features. We tested the performance of the proposed system for both author-dependent and author-independent AV cases. In addition, we introduced a new, well-defined, manually labelled Turkish blog corpus to be used in subsequent studies about authorship analysis. Using a publicly available English blog corpus for generating the BBM, the proposed system demonstrated an accuracy of over 90% from both trained and unseen authorsâ test sets. Furthermore, the proposed combination method and the system using the BBM with the English blog corpus were also evaluated with other genres, which were used in the international PAN AV competitions, and achieved promising results. Â© The Author(s) 2021.","Authorship verification (AV) is one of the main problems of authorship analysis and digital text forensics. The classical AV problem is to decide whether or not a particular author wrote the document in question. However, if there is one and relatively short document as the authors known document, the verification problem becomes more difficult than the classical AV and needs a generalised solution. Regarding to decide AV of the given two unlabeled documents (2D-AV), we proposed a system that provides an author-independent solution with the help of a Binary Background Model (BBM). The BBM is a supervised model that provides an informative background to distinguish document pairs written by the same or different authors. To evaluate the document pairs in one representation, we also proposed a new, simple and efficient document combination method based on the geometric mean of the stylometric features. We tested the performance of the proposed system for both author-dependent and author-independent AV cases. In addition, we introduced a new, well-defined, manually labelled Turkish blog corpus to be used in subsequent studies about authorship analysis. Using a publicly available English blog corpus for generating the BBM, the proposed system demonstrated an accuracy of over 90% from both trained and unseen authors test sets. Furthermore, the proposed combination method and the system using the BBM with the English blog corpus were also evaluated with other genres, which were used in the international PAN AV competitions, and achieved promising results."
Real time anomalies detection in crowd using convolutional long short-term memory network,"Violence is a critical social problem and demands to evaluate through computer vision approaches. At present, the incidences of violent actions get grown in the community, particularly in public places due to several economic and social causes. Moreover, our societyâs populations are increasing day by day and it is challenging to keep citizens within limits as well as monitoring human activities in crowd is too hard. Thus, government organizations including local bodies, require examining such occurrences through smart surveillance. In this research, a lightweight computational architecture has been presented to classify non-violent and violent activities. A model has been proposed to extract time-based features using smart devices, high-speed wireless networks and cloud servers to classify real-time human activities. For this purpose, a deep learning-based model is employed to detect violent activities and assist the stakeholders in exposing such activities in real-time. Convolutional long short-term memory (Conv-LSTM) is employed to extend fully connected LSTM (FC-LSTM) to capture the frame and detect violent actions. The proposed model accomplished 95.16% validation accuracy using a standard crowd anomaly dataset. Â© The Author(s) 2021.","Violence is a critical social problem and demands to evaluate through computer vision approaches. At present, the incidences of violent actions get grown in the community, particularly in public places due to several economic and social causes. Moreover, our societys populations are increasing day by day and it is challenging to keep citizens within limits as well as monitoring human activities in crowd is too hard. Thus, government organizations including local bodies, require examining such occurrences through smart surveillance. In this research, a lightweight computational architecture has been presented to classify non-violent and violent activities. A model has been proposed to extract time-based features using smart devices, high-speed wireless networks and cloud servers to classify real-time human activities. For this purpose, a deep learning-based model is employed to detect violent activities and assist the stakeholders in exposing such activities in real-time. Convolutional long short-term memory (Conv-LSTM) is employed to extend fully connected LSTM (FC-LSTM) to capture the frame and detect violent actions. The proposed model accomplished 95.16% validation accuracy using a standard crowd anomaly dataset."
A novel scheme of domain transfer in document-level cross-domain sentiment classification,"The sentiment classification aims to learn sentiment features from the annotated corpus and automatically predict the sentiment polarity of new sentiment text. However, people have different ways of expressing feelings in different domains. Thus, there are important differences in the characteristics of sentimental distribution across different domains. At the same time, in certain specific domains, due to the high cost of corpus collection, there is no annotated corpus available for the classification of sentiment. Therefore, it is necessary to leverage or reuse existing annotated corpus for training. In this article, we proposed a new algorithm for extracting central sentiment sentences in product reviews, and improved the pre-trained language model Bidirectional Encoder Representations from Transformers (BERT) to achieve the domain transfer for cross-domain sentiment classification. We used various pre-training language models to prove the effectiveness of the newly proposed joint algorithm for text-ranking and emotional words extraction, and utilised Amazon product reviews data set to demonstrate the effectiveness of our proposed domain-transfer framework. The experimental results of 12 different cross-domain pairs showed that the new cross-domain classification method was significantly better than several popular cross-domain sentiment classification methods. Â© The Author(s) 2021.","The sentiment classification aims to learn sentiment features from the annotated corpus and automatically predict the sentiment polarity of new sentiment text. However, people have different ways of expressing feelings in different domains. Thus, there are important differences in the characteristics of sentimental distribution across different domains. At the same time, in certain specific domains, due to the high cost of corpus collection, there is no annotated corpus available for the classification of sentiment. Therefore, it is necessary to leverage or reuse existing annotated corpus for training. In this article, we proposed a new algorithm for extracting central sentiment sentences in product reviews, and improved the pre-trained language model Bidirectional Encoder Representations from Transformers (BERT) to achieve the domain transfer for cross-domain sentiment classification. We used various pre-training language models to prove the effectiveness of the newly proposed joint algorithm for text-ranking and emotional words extraction, and utilised Amazon product reviews data set to demonstrate the effectiveness of our proposed domain-transfer framework. The experimental results of 12 different cross-domain pairs showed that the new cross-domain classification method was significantly better than several popular cross-domain sentiment classification methods."
The hybridised indexing method for research-based information retrieval,"An information retrieval system (IRS) is used to retrieve documents based on an information need. The IRS makes relevance judgements by attempting to match a query to a document. As IRS capabilities are indexing design dependent, the hybrid indexing method (IRS-H) is introduced. The objectives of this article are to examine IRS-H (as an alternative indexing method that performs exact phrase matching) and IRS-I, regarding retrieval usefulness, identification of relevant documents, and the quality of rejecting irrelevant documents by conducting three experiments and by analysing the related data. Three experiments took place where a collection of 100 research documents and 75 queries were presented to: (1) five participants answering a questionnaire, (2) IRS-I to generate data and (3) IRS-H to generate data. The data generated during the experiments were statistically analysed using the performance measurements of Precision, Recall and Specificity, and one-tailed Studentâs t-tests. The results reveal that IRS-H (1) increased the retrieval of relevant documents, (2) reduced incorrect identification of relevant documents and (3) increased the quality of rejecting irrelevant documents. The research found that the hybrid indexing method, using a small closed document collection of a hundred documents, produced the required outputs and that it may be used as an alternative IRS indexing method. Â© The Author(s) 2021.","An information retrieval system (IRS) is used to retrieve documents based on an information need. The IRS makes relevance judgements by attempting to match a query to a document. As IRS capabilities are indexing design dependent, the hybrid indexing method (IRS-H) is introduced. The objectives of this article are to examine IRS-H (as an alternative indexing method that performs exact phrase matching) and IRS-I, regarding retrieval usefulness, identification of relevant documents, and the quality of rejecting irrelevant documents by conducting three experiments and by analysing the related data. Three experiments took place where a collection of 100 research documents and 75 queries were presented to: five participants answering a questionnaire, IRS-I to generate data and IRS-H to generate data. The data generated during the experiments were statistically analysed using the performance measurements of Precision, Recall and Specificity, and one-tailed Students t-tests. The results reveal that IRS-H increased the retrieval of relevant documents, reduced incorrect identification of relevant documents and increased the quality of rejecting irrelevant documents. The research found that the hybrid indexing method, using a small closed document collection of a hundred documents, produced the required outputs and that it may be used as an alternative IRS indexing method."
A novel focal-loss and class-weight-aware convolutional neural network for the classification of in-text citations,"We argue that citations, as they have different reasons and functions, should not all be treated in the same way. Using the large, annotated dataset of about 10K citation contexts annotated by human experts, extracted from the Association for Computational Linguistics repository, we present a deep learningâbased citation context classification architecture. Unlike all existing state-of-the-art feature-based citation classification models, our proposed convolutional neural network (CNN) with fastText-based pre-trained embedding vectors uses only the citation context as its input to outperform them in both binary- (important and non-important) and multi-class (Use, Extends, CompareOrContrast, Motivation, Background, Other) citation classification tasks. Furthermore, we propose using focal-loss and class-weight functions in the CNN model to overcome the inherited class imbalance issues in citation classification datasets. We show that using the focal-loss function with CNN adds a factor of (Formula presented.) to the cross-entropy function. Our model improves on the baseline results by achieving an encouraging 90.6 F1 score with 90.7% accuracy and a 72.3 F1 score with a 72.1% accuracy score, respectively, for binary- and multi-class citation classification tasks. Â© The Author(s) 2021.","We argue that citations, as they have different reasons and functions, should not all be treated in the same way. Using the large, annotated dataset of about 10K citation contexts annotated by human experts, extracted from the Association for Computational Linguistics repository, we present a deep learningbased citation context classification architecture. Unlike all existing state-of-the-art feature-based citation classification models, our proposed convolutional neural network (CNN) with fastText-based pre-trained embedding vectors uses only the citation context as its input to outperform them in both binary- (important and non-important) and multi-class (Use, Extends, CompareOrContrast, Motivation, Background, Other) citation classification tasks. Furthermore, we propose using focal-loss and class-weight functions in the CNN model to overcome the inherited class imbalance issues in citation classification datasets. We show that using the focal-loss function with CNN adds a factor of (Formula presented.) to the cross-entropy function. Our model improves on the baseline results by achieving an encouraging 90.6 F1 score with 90.7% accuracy and a 72.3 F1 score with a 72.1% accuracy score, respectively, for binary- and multi-class citation classification tasks."
Using ISO and Semantic Web standard for building a multilingual terminology e-Dictionary: A use case of Chinese ceramic vases,"Cultural heritage is the legacy of physical artefacts and intangible attributes of a group or society that is inherited from past generations. Terminology is a tool for the dissemination and communication of cultural heritage. The lack of clearly identified terminologies is an obstacle to communication and knowledge sharing. Especially, for experts with different languages, it is difficult to understand what the term refers to only through terms. Our work aims to respond to this issue by implementing practices drawn from the Semantic Web and ISO Terminology standards (ISO 704 and ISO 1087-1) and more particularly, by building in a W3C format ontology as knowledge infrastructure to construct a multilingual terminology e-Dictionary. The Chinese ceramic vases of the Ming and Qing dynasties are the application cases of our work. The method of building ontology is the âterm-and-characteristic guided methodâ, which follows the ISO principles of Terminology. The main result of this work is an online terminology e-Dictionary. The terminology e-Dictionary could help archaeologists communicate and understand the concepts denoted by terms in different languages and provide a new perspective based on ontology for the digital protection of cultural heritage. The e-Dictionary was published at http://www.dh.ketrc.com/e-dictionary.html. Â© The Author(s) 2021.","Cultural heritage is the legacy of physical artefacts and intangible attributes of a group or society that is inherited from past generations. Terminology is a tool for the dissemination and communication of cultural heritage. The lack of clearly identified terminologies is an obstacle to communication and knowledge sharing. Especially, for experts with different languages, it is difficult to understand what the term refers to only through terms. Our work aims to respond to this issue by implementing practices drawn from the Semantic Web and ISO Terminology standards (ISO 704 and ISO 1087-1) and more particularly, by building in a W3C format ontology as knowledge infrastructure to construct a multilingual terminology e-Dictionary. The Chinese ceramic vases of the Ming and Qing dynasties are the application cases of our work. The method of building ontology is the term-and-characteristic guided method, which follows the ISO principles of Terminology. The main result of this work is an online terminology e-Dictionary. The terminology e-Dictionary could help archaeologists communicate and understand the concepts denoted by terms in different languages and provide a new perspective based on ontology for the digital protection of cultural heritage. The e-Dictionary was published at http://www.dh.ketrc.com/e-dictionary.html."
Understanding the costs and challenges of the digital divide through UK council services,"This study investigates the issue of digital exclusion resulting from the digitisation of government and council services within the United Kingdom. An initial analysis of customer support log data from a council in a large UK city helped identify the most commonly queried services and modes of support. The main findings are based on qualitative analysis of 10 interviews, structured around the results from the log analysis, conducted with front-line staff members at the central library of the same council. The study identifies a range of issues associated with the provision of e-government services and the subsequent under-utilisation by the public, including poor design, issues with effective access and the level of digital literacy among end users. The study also proposes the concept of the âdigital carerâ, a friend or family member who is relied upon by users unable to interact with e-government services themselves. The findings of this study have implications for the way in which these services are designed and delivered and point to the need for further work that can contribute to the UK digital economy by facilitating better access to e-government services and reduce digital exclusion, especially for elderly and marginalised users. Â© The Author(s) 2021.","This study investigates the issue of digital exclusion resulting from the digitisation of government and council services within the United Kingdom. An initial analysis of customer support log data from a council in a large UK city helped identify the most commonly queried services and modes of support. The main findings are based on qualitative analysis of 10 interviews, structured around the results from the log analysis, conducted with front-line staff members at the central library of the same council. The study identifies a range of issues associated with the provision of e-government services and the subsequent under-utilisation by the public, including poor design, issues with effective access and the level of digital literacy among end users. The study also proposes the concept of the digital carer, a friend or family member who is relied upon by users unable to interact with e-government services themselves. The findings of this study have implications for the way in which these services are designed and delivered and point to the need for further work that can contribute to the UK digital economy by facilitating better access to e-government services and reduce digital exclusion, especially for elderly and marginalised users."
Determinants of societal and academic recognition: Evidence from randomised controlled trials,"Given the increasing importance of recognition in academia and the vital role of randomised controlled trials (RCTs) in medical research and clinical decisions, this study verifies how RCTsâ academic and societal impacts are affected by visibility factors, subjects and methodological validity. This study concentrated on a sample of 446 RCTs indexed in Scopus and evaluated by Cochrane reviewers in terms of their methodological validity. The altmetrics, bibliometric and bibliographical information were extracted from Altmetric.com and Scopus, and the contributing countriesâ development ranks were obtained from the United Nations Development report. The linear regression analyses revealed that citations and altmetrics depend on some subjects. They are also affected by publication year and journalsâ previous reputation. Citations are also affected by keyword counts and reference counts. Keyword counts and contributing countriesâ developmental rank also predict the tweet counts. While none of the methodological validity dimensions were found to predict citations, âIncomplete Outcome Dataâ and âRandom Sequence Generationâ significantly, though slightly, affect Mendeley Readership and tweets, respectively. By confirming the dependence of RCTsâ recognition on some methodological validity features and attention-inducing characteristics, the study provides further evidence on the interaction of quality and visibility dynamisms in the recognition network and the complementary role of societal mentions for academic citation. Â© The Author(s) 2021.","Given the increasing importance of recognition in academia and the vital role of randomised controlled trials (RCTs) in medical research and clinical decisions, this study verifies how RCTs academic and societal impacts are affected by visibility factors, subjects and methodological validity. This study concentrated on a sample of 446 RCTs indexed in Scopus and evaluated by Cochrane reviewers in terms of their methodological validity. The altmetrics, bibliometric and bibliographical information were extracted from Altmetric.com and Scopus, and the contributing countries development ranks were obtained from the United Nations Development report. The linear regression analyses revealed that citations and altmetrics depend on some subjects. They are also affected by publication year and journals previous reputation. Citations are also affected by keyword counts and reference counts. Keyword counts and contributing countries developmental rank also predict the tweet counts. While none of the methodological validity dimensions were found to predict citations, Incomplete Outcome Data and Random Sequence Generation significantly, though slightly, affect Mendeley Readership and tweets, respectively. By confirming the dependence of RCTs recognition on some methodological validity features and attention-inducing characteristics, the study provides further evidence on the interaction of quality and visibility dynamisms in the recognition network and the complementary role of societal mentions for academic citation."
Disambiguating the definitions of the concept âtransformative innovationâ,"The definition of âtransformative innovationâ is still ambiguous, making it difficult to develop more targeted strategies for steering scientific and technological innovation. In this study, taking extant academic publications as our research object, we used topic extraction and visualisation tools to explore the intersections and differences among transformative and other innovative concepts. The correlation degrees among the concepts related to âtransformative innovationâ were used to distinguish the relationships among concepts related to âbreakthrough innovationâ and âdisruptive innovationâ. We further analysed the definitional differences among âtransformative innovationâ, âemerging technologyâ and âgroundbreaking researchâ. The results showed that the concepts of âbreakthrough innovationâ and âdisruptive innovationâ can be integrated into the scope of âtransformative innovationâ. However, the definitions of âtransformative innovationâ, âemerging technologyâ and âresearch frontierâ had their distinct characteristics. âTransformative innovationâ focused on major impactful technological changes. âEmerging technologyâ focused on novel technology and its promoting. âResearch frontierâ focused on research activities taking place at the frontiers of knowledge. Therefore, distinctive science and technology (S&T) policies are needed for the different types of innovation. This article introduces a novel multidimensional method to place the different concepts associated with âtransformative innovationâ into a unified framework. This framework is expected to support policymakers in their S&T policymaking, and it would aid the work of S&T researchers. Â© The Author(s) 2021.","The definition of transformative innovation is still ambiguous, making it difficult to develop more targeted strategies for steering scientific and technological innovation. In this study, taking extant academic publications as our research object, we used topic extraction and visualisation tools to explore the intersections and differences among transformative and other innovative concepts. The correlation degrees among the concepts related to transformative innovation were used to distinguish the relationships among concepts related to breakthrough innovation and disruptive innovation. We further analysed the definitional differences among transformative innovation, emerging technology and groundbreaking research. The results showed that the concepts of breakthrough innovation and disruptive innovation can be integrated into the scope of transformative innovation. However, the definitions of transformative innovation, emerging technology and research frontier had their distinct characteristics. Transformative innovation focused on major impactful technological changes. Emerging technology focused on novel technology and its promoting. Research frontier focused on research activities taking place at the frontiers of knowledge. Therefore, distinctive science and technology (S&T) policies are needed for the different types of innovation. This article introduces a novel multidimensional method to place the different concepts associated with transformative innovation into a unified framework. This framework is expected to support policymakers in their S&T policymaking, and it would aid the work of S&T researchers."
Obsolescence of the literature: A study of included studies in Cochrane reviews,"Ageing or obsolescence describes the process of declining use of a particular publication over time and can affect the results of a citation analyses as the length of citation window can change rankings. Obsolescence may not only vary across fields but also across subfields or sub-disciplines. The aim of this study is to determine the sub-disciplinary differences of obsolescence on a larger scale allowing for differences over time as well. The study presents the results of an analysis of 82,759 references across 53 healthcare and health policy topics. The references in this study are extracted from systematic reviews published from 2012 to 2016. The analyses of obsolescence include median citation age and mean citation age. This study finds that the median citation age and the mean citation age differ considerably across groups. For the latter indicator, an analysis of the confidence intervals confirms these differences. Using the subfield categorisation from Cochrane review groups, we found larger differences across subfields than in the citing half-lives published by Journal Citation Reports. Obsolescence is important to consider when setting the length of the citation windows. This study emphasises the vast differences across health sciences subfields. The length of the citation period is thus highly important for the results of a bibliometric evaluation or study covering fields with very varying obsolescence rates. Â© The Author(s) 2021.","Ageing or obsolescence describes the process of declining use of a particular publication over time and can affect the results of a citation analyses as the length of citation window can change rankings. Obsolescence may not only vary across fields but also across subfields or sub-disciplines. The aim of this study is to determine the sub-disciplinary differences of obsolescence on a larger scale allowing for differences over time as well. The study presents the results of an analysis of 82,759 references across 53 healthcare and health policy topics. The references in this study are extracted from systematic reviews published from 2012 to 2016. The analyses of obsolescence include median citation age and mean citation age. This study finds that the median citation age and the mean citation age differ considerably across groups. For the latter indicator, an analysis of the confidence intervals confirms these differences. Using the subfield categorisation from Cochrane review groups, we found larger differences across subfields than in the citing half-lives published by Journal Citation Reports. Obsolescence is important to consider when setting the length of the citation windows. This study emphasises the vast differences across health sciences subfields. The length of the citation period is thus highly important for the results of a bibliometric evaluation or study covering fields with very varying obsolescence rates."
"Journal titles and mission statements: Lexical structure, diversity, and readability in business, management and accounting research","There is an established research agenda on dissecting an articleâs components and their association with a journalâs prestige. However, journalsâ titles and their overview, aim and scope (i.e. journalâs mission statement â JMS(s)) have not been investigated with the same diligence. This study aims to conduct a comprehensive outlook of titles and JMSsâ lexical structure and identify significant differences between journals prestige and type of access and their JMS content in the field of business, management and accounting (BMA), considering the fieldâs experience in developing and applying mission statements. Titles and JMSsâ structural analysis reflected current and critical discussion in BMA: a predilection for counterintuitive findings and information and communication technology (ICT) tools. JMSs expressed primarily target customers and markets. JMSs from reputable journals showed a higher betweenness for key-terms related to rigorous features. In contrast, JMSs of lower reputable journals highlighted indexing attributes. The Wilcoxon rank-sum and the KruskalâWallis tests showed significant differences in the JMSsâ median diversity regarding the journalâs type of access and best quartiles. Â© The Author(s) 2021.","There is an established research agenda on dissecting an articles components and their association with a journals prestige. However, journals titles and their overview, aim and scope ( journals mission statement JMS(s)) have not been investigated with the same diligence. This study aims to conduct a comprehensive outlook of titles and JMSs lexical structure and identify significant differences between journals prestige and type of access and their JMS content in the field of business, management and accounting (BMA), considering the fields experience in developing and applying mission statements. Titles and JMSs structural analysis reflected current and critical discussion in BMA: a predilection for counterintuitive findings and information and communication technology (ICT) tools. JMSs expressed primarily target customers and markets. JMSs from reputable journals showed a higher betweenness for key-terms related to rigorous features. In contrast, JMSs of lower reputable journals highlighted indexing attributes. The Wilcoxon rank-sum and the KruskalWallis tests showed significant differences in the JMSs median diversity regarding the journals type of access and best quartiles."
A realistic evaluation of the dark side of data in the digital ecosystem,"This article examines the negative consequences that can arise from the utilisation of innovative data practices implemented by organisations. While these technologies offer significant value, their improper implementation can lead to harmful practices that undermine the rights of individuals within societies. Through a systematic literature review of 383 articles employing the realistic evaluation theory, this study synthesises key findings to identify the contextual factors that contribute to these harmful practices. The results highlight the challenges posed by the characteristics of Big Data, often resulting in haphazard data implementation scenarios. Three critical mechanisms, namely data transparency, biases, and breaches, interact with these implementation contexts, leading to adverse outcomes that compromise individual empowerment, societal fairness, and personal privacy. In addition, this article identifies important areas for future research and provides recommendations for policymakers to effectively manage the negative aspects of data practices, ensuring sustainability within the digital ecosystem. Â© The Author(s) 2023.","This article examines the negative consequences that can arise from the utilisation of innovative data practices implemented by organisations. While these technologies offer significant value, their improper implementation can lead to harmful practices that undermine the rights of individuals within societies. Through a systematic literature review of 383 articles employing the realistic evaluation theory, this study synthesises key findings to identify the contextual factors that contribute to these harmful practices. The results highlight the challenges posed by the characteristics of Big Data, often resulting in haphazard data implementation scenarios. Three critical mechanisms, namely data transparency, biases, and breaches, interact with these implementation contexts, leading to adverse outcomes that compromise individual empowerment, societal fairness, and personal privacy. In addition, this article identifies important areas for future research and provides recommendations for policymakers to effectively manage the negative aspects of data practices, ensuring sustainability within the digital ecosystem."
Boosting innovation performance through big data analytics:An empirical investigationon the role of firm agility,"While past studies proposed the role of big data analytics (BDA) as one of the primary pathways to business value creation, current knowledge on the link between BDA and innovation performance remains limited. In this regard, this study intends to fill this research gap by developing a theoretical framework for understanding how and under which mechanisms BDA influences innovation performance. Firm agility (conceptualised as sensing agility, decision-making agility and acting agility) is used in this research as the mediator between BDA and innovation performance. Besides, this research conceptualises two moderating variables: data-driven culture and BDA team sophistication. This study employs partial least squares (PLS) to test and validate the proposed hypotheses using survey data of 185 firms. The results show that firm agility significantly mediates the link between BDA use and innovation performance. Besides, the results suggest that data-driven culture moderates the relation between sensing agility and decision-making agility. This research also supports the moderating role of BDA team sophistication on the link between BDA use and sensing agility. Â© The Author(s) 2021.","While past studies proposed the role of big data analytics (BDA) as one of the primary pathways to business value creation, current knowledge on the link between BDA and innovation performance remains limited. In this regard, this study intends to fill this research gap by developing a theoretical framework for understanding how and under which mechanisms BDA influences innovation performance. Firm agility (conceptualised as sensing agility, decision-making agility and acting agility) is used in this research as the mediator between BDA and innovation performance. Besides, this research conceptualises two moderating variables: data-driven culture and BDA team sophistication. This study employs partial least squares (PLS) to test and validate the proposed hypotheses using survey data of 185 firms. The results show that firm agility significantly mediates the link between BDA use and innovation performance. Besides, the results suggest that data-driven culture moderates the relation between sensing agility and decision-making agility. This research also supports the moderating role of BDA team sophistication on the link between BDA use and sensing agility."
"RETRACTED: Information security: Legal regulations in Azerbaijan and abroad (Journal of Information Science, (2022), 48, 6, (811-824), 10.1177/0165551520981813)",corrected-proof ts1 Â© The Author(s) 2023.,
"RETRACTED: Information skills and literacy in investigative journalism in the social media era (Journal of Information Science, (2022), (01655515221094442), 10.1177/01655515221094442)",corrected-proof ts1 Â© The Author(s) 2023.,
Not just for the money? An examination of the motives behind physiciansâ sharing of paid health information,"Online platforms make it possible for physicians to share online information with the public, however, few studies have explored the underlying mechanism of physiciansâ sharing of paid health information. Drawing on motivation theory, this study developed a theoretical framework to explore the effects of extrinsic motivation, enjoyment, and professional motivation on the sharing of paid information, as well as the contingent role of income ratio (online to offline) and online reputation. The model was tested with both objective and subjective data, which contain responses from 298 physicians. The results show that extrinsic motivation, enjoyment, and professional motivation play significant roles in inducing physicians to share paid information. Furthermore, income ratio can moderate the effects of motives on paid information sharing. Besides, the effect of professional motivation can be more effective in certain situations (low-level income ratio or high online reputation). This study contributes to the literature on knowledge sharing, online health behaviour, and motivation theory, and provides implications for practitioners. Â© The Author(s) 2021.","Online platforms make it possible for physicians to share online information with the public, however, few studies have explored the underlying mechanism of physicians sharing of paid health information. Drawing on motivation theory, this study developed a theoretical framework to explore the effects of extrinsic motivation, enjoyment, and professional motivation on the sharing of paid information, as well as the contingent role of income ratio (online to offline) and online reputation. The model was tested with both objective and subjective data, which contain responses from 298 physicians. The results show that extrinsic motivation, enjoyment, and professional motivation play significant roles in inducing physicians to share paid information. Furthermore, income ratio can moderate the effects of motives on paid information sharing. Besides, the effect of professional motivation can be more effective in certain situations (low-level income ratio or high online reputation). This study contributes to the literature on knowledge sharing, online health behaviour, and motivation theory, and provides implications for practitioners."
Global scientific collaboration: A social network analysis and data mining of the co-authorship networks,"Co-authorship networks consist of nodes and numerous links indicating scientific collaboration of researchers. These networks could be studied through social networks analysis and data mining techniques. The focus of the article is twofold: the first objective is the analysis of the co-authorship networks of the top 60 countries that had the highest number of scientific publications in the world, and the second one is the discovery of collaboration patterns of highly cited papers of these countries. To do so, all scientific publications of the top 60 countries in all fields as well as their highly cited papers were included in the study period between 2011 and 2015. The research samples in the first part included 10,460,999 documents and in the second part encompassed 711,025 highly cited papers. Required data were extracted from web of science database. To analyse co-authorship networks, centrality indices and clustering coefficient were used. UCINET, Pajek, VOSviewer and BibExcel software were used to map co-authorship networks of the countries and to calculate indices. Finally, the discovery of collaboration patterns in highly cited papers is studied through association rules. The research data indicated that over 95% of documents has been produced by the top 60 countries. In addition, the USA, Germany, England, France and Spain launched the most co-authorship. Quantitatively, there have been the most powerful collaboration links between China and the USA, the USA and England, the USA and Germany, and the USA and Canada. The clustering data indicated that collaborations of the top countries of the world were in three main clusters. The Friedman test showed that there was a significant difference in the priorities of the countries for collaboration; and the USA, China, England, Germany, France, Japan and Italy are in the top priority for collaboration, respectively. The results of collaboration pattern in highly cited papers indicated that the USA participates in more than half of collaboration patterns for producing highly cited papers. Â© The Author(s) 2021.","Co-authorship networks consist of nodes and numerous links indicating scientific collaboration of researchers. These networks could be studied through social networks analysis and data mining techniques. The focus of the article is twofold: the first objective is the analysis of the co-authorship networks of the top 60 countries that had the highest number of scientific publications in the world, and the second one is the discovery of collaboration patterns of highly cited papers of these countries. To do so, all scientific publications of the top 60 countries in all fields as well as their highly cited papers were included in the study period between 2011 and 2015. The research samples in the first part included 10,460,999 documents and in the second part encompassed 711,025 highly cited papers. Required data were extracted from web of science database. To analyse co-authorship networks, centrality indices and clustering coefficient were used. UCINET, Pajek, VOSviewer and BibExcel software were used to map co-authorship networks of the countries and to calculate indices. Finally, the discovery of collaboration patterns in highly cited papers is studied through association rules. The research data indicated that over 95% of documents has been produced by the top 60 countries. In addition, the USA, Germany, England, France and Spain launched the most co-authorship. Quantitatively, there have been the most powerful collaboration links between China and the USA, the USA and England, the USA and Germany, and the USA and Canada. The clustering data indicated that collaborations of the top countries of the world were in three main clusters. The Friedman test showed that there was a significant difference in the priorities of the countries for collaboration; and the USA, China, England, Germany, France, Japan and Italy are in the top priority for collaboration, respectively. The results of collaboration pattern in highly cited papers indicated that the USA participates in more than half of collaboration patterns for producing highly cited papers."
"Steady ship: Digital, online, and e-libraries (1971â2020)","This paper attempts to examine the e-libraries, digital libraries, electronic libraries, and online libraries employing bibliometric study techniques from 1971 to 2020. It focuses to consolidate the published documents on the library indexed in Web of Science. It has been observed that we found 4266 published documents employing bibliometric analysis. The study findings show that digital libraries are the top topic, proceeding paper is the top type of document, and most are published in the English language. Similarly, the year 2006 to 2010 has the highest number of published documents, top author Fox EA, Dept. Comp Sci top organisation, United States top country, and digital libraries as a top keyword has been found. Further, the name of Liu et al. has been at the top of the authorâs list. Moreover, the results are presented in tables and figures to show the trend of data. Â© The Author(s) 2021.","This paper attempts to examine the e-libraries, digital libraries, electronic libraries, and online libraries employing bibliometric study techniques from 1971 to 2020. It focuses to consolidate the published documents on the library indexed in Web of Science. It has been observed that we found 4266 published documents employing bibliometric analysis. The study findings show that digital libraries are the top topic, proceeding paper is the top type of document, and most are published in the English language. Similarly, the year 2006 to 2010 has the highest number of published documents, top author Fox EA, Dept. Comp Sci top organisation, United States top country, and digital libraries as a top keyword has been found. Further, the name of Liu et al. has been at the top of the authors list. Moreover, the results are presented in tables and figures to show the trend of data."
Enhancing data quality to mine credible patterns,"The importance of big data is widely accepted in various fields. Organisations spend a lot of money to collect, process and mine the data to identify patterns. These patterns facilitate their future decision-making process to improve the organisational performance and profitability. However, among discovered patterns, there are some meaningless and misleading patterns which restrict the effectiveness of decision-making process. The presence of data discrepancies, noise and outliers also impacts the quality of discovered patterns and leads towards missing strategic goals and objectives. Quality inception of these discovered patterns is vital before utilising them in making predictions, decision-making process or strategic planning. Mining useful and credible patterns over social media is a challenging task. Often, people spread targeted content for character assassination or defamation of brands. Recently, some studies have evaluated the credibility of information over social media based on usersâ surveys, expertsâ judgement and manually annotating Twitter tweets to predict credibility. Unfortunately, due to the large volume and exponential growth of data, these surveys and annotation-based information credibility techniques are not efficiently applicable. This article presents a data quality and credibility evaluation framework to determine the quality of individual data instances. This framework provides a way to discover useful and credible patterns using credibility indicators. Moreover, a new Twitter bot detection algorithm is proposed to classify tweets generated by Twitter bots and real users. The results of conducted experiments showed that the proposed model generates a positive impact on improving classification accuracy and quality of discovered patterns. Â© The Author(s) 2021.","The importance of big data is widely accepted in various fields. Organisations spend a lot of money to collect, process and mine the data to identify patterns. These patterns facilitate their future decision-making process to improve the organisational performance and profitability. However, among discovered patterns, there are some meaningless and misleading patterns which restrict the effectiveness of decision-making process. The presence of data discrepancies, noise and outliers also impacts the quality of discovered patterns and leads towards missing strategic goals and objectives. Quality inception of these discovered patterns is vital before utilising them in making predictions, decision-making process or strategic planning. Mining useful and credible patterns over social media is a challenging task. Often, people spread targeted content for character assassination or defamation of brands. Recently, some studies have evaluated the credibility of information over social media based on users surveys, experts judgement and manually annotating Twitter tweets to predict credibility. Unfortunately, due to the large volume and exponential growth of data, these surveys and annotation-based information credibility techniques are not efficiently applicable. This article presents a data quality and credibility evaluation framework to determine the quality of individual data instances. This framework provides a way to discover useful and credible patterns using credibility indicators. Moreover, a new Twitter bot detection algorithm is proposed to classify tweets generated by Twitter bots and real users. The results of conducted experiments showed that the proposed model generates a positive impact on improving classification accuracy and quality of discovered patterns."
Detection of conspiracy propagators using psycho-linguistic characteristics,"The rise of social media has offered a fast and easy way for the propagation of conspiracy theories and other types of disinformation. Despite the research attention that has received, fake news detection remains an open problem and users keep sharing articles that contain false statements but which they consider real. In this article, we focus on the role of users in the propagation of conspiracy theories that is a specific type of disinformation. First, we compare profile and psycho-linguistic patterns of online users that tend to propagate posts that support conspiracy theories and of those who propagate posts that refute them. To this end, we perform a comparative analysis over various profile, psychological and linguistic characteristics using social media texts of users that share posts about conspiracy theories. Then, we compare the effectiveness of those characteristics for predicting whether a user is a conspiracy propagator or not. In addition, we propose ConspiDetector, a model that is based on a convolutional neural network (CNN) and which combines word embeddings with psycho-linguistic characteristics extracted from the tweets of users to detect conspiracy propagators. The results show that ConspiDetector can improve the performance in detecting conspiracy propagators by 8.82% compared with the CNN baseline with regard to F1-metric. Â© The Author(s) 2021.","The rise of social media has offered a fast and easy way for the propagation of conspiracy theories and other types of disinformation. Despite the research attention that has received, fake news detection remains an open problem and users keep sharing articles that contain false statements but which they consider real. In this article, we focus on the role of users in the propagation of conspiracy theories that is a specific type of disinformation. First, we compare profile and psycho-linguistic patterns of online users that tend to propagate posts that support conspiracy theories and of those who propagate posts that refute them. To this end, we perform a comparative analysis over various profile, psychological and linguistic characteristics using social media texts of users that share posts about conspiracy theories. Then, we compare the effectiveness of those characteristics for predicting whether a user is a conspiracy propagator or not. In addition, we propose ConspiDetector, a model that is based on a convolutional neural network (CNN) and which combines word embeddings with psycho-linguistic characteristics extracted from the tweets of users to detect conspiracy propagators. The results show that ConspiDetector can improve the performance in detecting conspiracy propagators by 8.82% compared with the CNN baseline with regard to F1-metric."
Automatic construction of academic profile: A case of information science domain,"To provide junior researchers with domain-specific concepts efficiently, an automatic approach for academic profiling is needed. First, to obtain personal records of a given scholar, typical supervised approaches often utilise structured data like infobox in Wikipedia as training dataset, but it may lead to a severe mis-labelling problem when they are utilised to train a model directly. To address this problem, a new relation embedding method is proposed for fine-grained entity typing, in which the initial vector of entities and a new penalty scheme are considered, based on the semantic distance of entities and relations. Also, to highlight critical concepts relevant to renowned scholars, scholarsâ selective bibliographies which contain massive academic terms are analysed by a newly proposed extraction method based on logistic regression, AdaBoost algorithm and learning-to-rank techniques. It bridges the gap that conventional supervised methods only return binary classification results and fail to help researchers understand the relative importance of selected concepts. Categories of experiments on academic profiling and corresponding benchmark datasets demonstrate that proposed approaches outperform existing methods notably. The proposed techniques provide an automatic way for junior researchers to obtain organised knowledge in a specific domain, including scholarsâ background information and domain-specific concepts. Â© The Author(s) 2021.","To provide junior researchers with domain-specific concepts efficiently, an automatic approach for academic profiling is needed. First, to obtain personal records of a given scholar, typical supervised approaches often utilise structured data like infobox in Wikipedia as training dataset, but it may lead to a severe mis-labelling problem when they are utilised to train a model directly. To address this problem, a new relation embedding method is proposed for fine-grained entity typing, in which the initial vector of entities and a new penalty scheme are considered, based on the semantic distance of entities and relations. Also, to highlight critical concepts relevant to renowned scholars, scholars selective bibliographies which contain massive academic terms are analysed by a newly proposed extraction method based on logistic regression, AdaBoost algorithm and learning-to-rank techniques. It bridges the gap that conventional supervised methods only return binary classification results and fail to help researchers understand the relative importance of selected concepts. Categories of experiments on academic profiling and corresponding benchmark datasets demonstrate that proposed approaches outperform existing methods notably. The proposed techniques provide an automatic way for junior researchers to obtain organised knowledge in a specific domain, including scholars background information and domain-specific concepts."
Impact of COVID-19 on search in an organisation,"COVID-19 has created unprecedented organisational challenges, yet no study has examined the impact on information search. A case study in a knowledge-intensive organisation was undertaken on 2.5 million search queries during the pandemic. A surge of unique users and COVID-19 search queries in March 2020 may equate to âpeak uncertainty and activityâ, demonstrating the importance of corporate search engines in times of crisis. Search volumes dropped 24% after lockdowns; an âL-shapedâ recovery may be a surrogate for business activity. COVID-19 search queries transitioned from awareness, to impact, strategy, response and ways of working that may influence future search design. Low click through rates imply some information needs were not met and searches on mental health increased. In extreme situations (i.e. a pandemic), companies may need to move faster, monitoring and exploiting their enterprise search logs in real time as these reflect uncertainty and anxiety that may exist in the enterprise. Â© The Author(s) 2021.","COVID-19 has created unprecedented organisational challenges, yet no study has examined the impact on information search. A case study in a knowledge-intensive organisation was undertaken on 2.5 million search queries during the pandemic. A surge of unique users and COVID-19 search queries in March 2020 may equate to peak uncertainty and activity, demonstrating the importance of corporate search engines in times of crisis. Search volumes dropped 24% after lockdowns; an L-shaped recovery may be a surrogate for business activity. COVID-19 search queries transitioned from awareness, to impact, strategy, response and ways of working that may influence future search design. Low click through rates imply some information needs were not met and searches on mental health increased. In extreme situations ( a pandemic), companies may need to move faster, monitoring and exploiting their enterprise search logs in real time as these reflect uncertainty and anxiety that may exist in the enterprise."
"Misplaced trust? The relationship between trust, ability to identify commercially influenced results and search engine preference","People have a high level of trust in search engines, especially Google, but only limited knowledge of them, as numerous studies have shown. This leads to the question: To what extent is this trust justified considering the lack of familiarity among users with how search engines work and the business models they are founded on? We assume that trust in Google, search engine preferences and knowledge of result types are interrelated. To examine this assumption, we conducted a representative online survey with n = 2012 German Internet users. We show that users with little search engine knowledge are more likely to trust and use Google than users with more knowledge. A contradiction revealed itself â users strongly trust Google, yet they are unable to adequately evaluate search results. For those users, this may be problematic since it can potentially affect knowledge acquisition. Consequently, there is a need to promote user information literacy to create a more solid foundation for user trust in search engines. The impact of our study lies in emphasising the need for creating appropriate training formats to promote information literacy. Â© The Author(s) 2021.","People have a high level of trust in search engines, especially Google, but only limited knowledge of them, as numerous studies have shown. This leads to the question: To what extent is this trust justified considering the lack of familiarity among users with how search engines work and the business models they are founded on? We assume that trust in Google, search engine preferences and knowledge of result types are interrelated. To examine this assumption, we conducted a representative online survey with n = 2012 German Internet users. We show that users with little search engine knowledge are more likely to trust and use Google than users with more knowledge. A contradiction revealed itself users strongly trust Google, yet they are unable to adequately evaluate search results. For those users, this may be problematic since it can potentially affect knowledge acquisition. Consequently, there is a need to promote user information literacy to create a more solid foundation for user trust in search engines. The impact of our study lies in emphasising the need for creating appropriate training formats to promote information literacy."
Enterprise resource planning adoption model for well-informed decision in higher learning institutions,"Enterprise resource planning (ERP) has been found to have a key role in the management of higher learning institutions (HLIs) and schools. However, the literature shows no universal model to support and shed light on the adoption of ERP, which lessens the chances for an effective ERP adoption and usage. Therefore, a new model is needed for successful adoption and the eventual enhanced decision-making, and as such, there is a need to investigate the factors that can bring about ERP system adoption. Models for ERP adoption in literature are few and far between, and what few exist are not applicable as they do not cover all the major factors that can contribute to adoption success. Hence, in this article, an ERP adoption model was brought forward for HLIs for the promotion of their decision-making process. The model was developed through the integration of DeLone and McLeanâs information success model and the technology, organisation and environment (TOE) theory. The study distributed 500 survey questionnaire copies online and collected 364 from HLIs respondents, after which they were retrieved, and data were analysed through partial least squares structural equation modeling (PLS-SEM) 3 statistical software. On the basis of the obtained analysis findings, technological, organisational and environmental factors had significant and positive effects on ERP adoption, and ERP adoption had a positive and significant effect on the decision-making of HLIs. The entire factors were found to be significant in their effects, and ERP adoption sufficiently explained variance extracted from decision-making. The study contributes to the literature through the pioneering measurement of factors categorised under technological, organisational and environmental dimensions, with ERP adoption and decision-making encapsulated in a single model. Â© The Author(s) 2021.","Enterprise resource planning (ERP) has been found to have a key role in the management of higher learning institutions (HLIs) and schools. However, the literature shows no universal model to support and shed light on the adoption of ERP, which lessens the chances for an effective ERP adoption and usage. Therefore, a new model is needed for successful adoption and the eventual enhanced decision-making, and as such, there is a need to investigate the factors that can bring about ERP system adoption. Models for ERP adoption in literature are few and far between, and what few exist are not applicable as they do not cover all the major factors that can contribute to adoption success. Hence, in this article, an ERP adoption model was brought forward for HLIs for the promotion of their decision-making process. The model was developed through the integration of DeLone and McLeans information success model and the technology, organisation and environment (TOE) theory. The study distributed 500 survey questionnaire copies online and collected 364 from HLIs respondents, after which they were retrieved, and data were analysed through partial least squares structural equation modeling (PLS-SEM) 3 statistical software. On the basis of the obtained analysis findings, technological, organisational and environmental factors had significant and positive effects on ERP adoption, and ERP adoption had a positive and significant effect on the decision-making of HLIs. The entire factors were found to be significant in their effects, and ERP adoption sufficiently explained variance extracted from decision-making. The study contributes to the literature through the pioneering measurement of factors categorised under technological, organisational and environmental dimensions, with ERP adoption and decision-making encapsulated in a single model."
A recommendation-based reading list system prototype for learning and resource management,"A reading list is a list of reading items recommended by an academic to assist studentsâ acquisition of knowledge for a specific subject. Subsequently, the libraries of higher education institutions collect and assemble reading lists according to specific courses and offer the students the reading list service. However, the reading list is created based on localised intelligence, restricted to the academicâs knowledge of their field, semantics, experience and awareness of developments. This investigation aims to present the views and comments of academics, and library staff, on an envisaged aggregated reading list service, which aggregates recommended reading items from various higher education institutions. This being the aim, we build a prototype, which aggregates reading lists from different universities and showcase it to 19 academics and library staff in various higher education institutions to capture their views, comments and any recommendations. In the process, we also showcase the feasibility of collecting and aggregating reading lists, in addition to understanding the process of reading lists creation at their respective higher education institutions. The prototype successfully showcases the creation of ranked lists of reading items, authors, topics, modules and courses. Academics and library staff indicated that aggregated lists would collectively benefit the academic community. Consequently, recommendations in the form of process implementations and technological applications are made to overcome and successfully implement the proposed aggregated reading list service. This proof-of-concept demonstrates potential benefits for the academic community and identifies further challenges to overcome in order to scale it up to the implementation stage. Â© The Author(s) 2021.","A reading list is a list of reading items recommended by an academic to assist students acquisition of knowledge for a specific subject. Subsequently, the libraries of higher education institutions collect and assemble reading lists according to specific courses and offer the students the reading list service. However, the reading list is created based on localised intelligence, restricted to the academics knowledge of their field, semantics, experience and awareness of developments. This investigation aims to present the views and comments of academics, and library staff, on an envisaged aggregated reading list service, which aggregates recommended reading items from various higher education institutions. This being the aim, we build a prototype, which aggregates reading lists from different universities and showcase it to 19 academics and library staff in various higher education institutions to capture their views, comments and any recommendations. In the process, we also showcase the feasibility of collecting and aggregating reading lists, in addition to understanding the process of reading lists creation at their respective higher education institutions. The prototype successfully showcases the creation of ranked lists of reading items, authors, topics, modules and courses. Academics and library staff indicated that aggregated lists would collectively benefit the academic community. Consequently, recommendations in the form of process implementations and technological applications are made to overcome and successfully implement the proposed aggregated reading list service. This proof-of-concept demonstrates potential benefits for the academic community and identifies further challenges to overcome in order to scale it up to the implementation stage."
How to improve the university library intelligent knowledge service: A system dynamics model,"Based on the theory of system dynamics, this article analyses the system composition of a university library intelligent knowledge service (IKS) and its relevant functions. First, we designed a system dynamics model involved the steps and flow of system modelling, the analysis and frame diagram, the system boundaries and their settings, and the system causality model. Furthermore, the system simulation of the IKS is carried out with result analysis, validity test and application analysis. The system simulation results show that (1) the model can simulate the system operation processes of a university library IKS, (2) the model reveals the relationships and the operation trends among the elements of the IKS and (3) the effective implementation of a library IKS can be improved by following the essential law of service development, paying attention to the coordination and interaction among constituent elements, and influencing measures around key variables. Â© The Author(s) 2021.","Based on the theory of system dynamics, this article analyses the system composition of a university library intelligent knowledge service (IKS) and its relevant functions. First, we designed a system dynamics model involved the steps and flow of system modelling, the analysis and frame diagram, the system boundaries and their settings, and the system causality model. Furthermore, the system simulation of the IKS is carried out with result analysis, validity test and application analysis. The system simulation results show that the model can simulate the system operation processes of a university library IKS, the model reveals the relationships and the operation trends among the elements of the IKS and the effective implementation of a library IKS can be improved by following the essential law of service development, paying attention to the coordination and interaction among constituent elements, and influencing measures around key variables."
From data mining to wisdom mining,"The knowledge gained from data mining is highly dependent on the experience of an expert for further analysis to increase effectiveness and wise decision-making. This mined knowledge requires actionability enhancement before it can be applied to real-world problems. The literature highlights the reasons that emerged the need to incorporate human wisdom in decision-making for complex problems. To solve this problem, a domain called âWisdom Miningâ is recommended, proposing a set of algorithms parallel to the algorithms proposed by the data mining. In wisdom mining, a process to extract wisdom needs to be defined with less influence from an expert. This review proposed improvements to data mining techniques and their applications in the real world and emphasised the need to seek ways to harness wisdom from data. This study covers the diverse definitions and different perspectives of wisdom within philosophy, psychology, management and computer science. This comprehensive literature review served as a foundation for constructing a wise decision framework that aided in identifying the wisdom factors like context, utility, location and time. The inclusion of these wisdom factors in existing data mining algorithms makes the transition from data mining to wisdom mining possible. This research includes the relationship between these two mining process that facilitated further elucidation of the wisdom mining process. Potential research trends in the domain are also seen as a potential endeavour to improve the analysis and use of data. Â© The Author(s) 2021.","The knowledge gained from data mining is highly dependent on the experience of an expert for further analysis to increase effectiveness and wise decision-making. This mined knowledge requires actionability enhancement before it can be applied to real-world problems. The literature highlights the reasons that emerged the need to incorporate human wisdom in decision-making for complex problems. To solve this problem, a domain called Wisdom Mining is recommended, proposing a set of algorithms parallel to the algorithms proposed by the data mining. In wisdom mining, a process to extract wisdom needs to be defined with less influence from an expert. This review proposed improvements to data mining techniques and their applications in the real world and emphasised the need to seek ways to harness wisdom from data. This study covers the diverse definitions and different perspectives of wisdom within philosophy, psychology, management and computer science. This comprehensive literature review served as a foundation for constructing a wise decision framework that aided in identifying the wisdom factors like context, utility, location and time. The inclusion of these wisdom factors in existing data mining algorithms makes the transition from data mining to wisdom mining possible. This research includes the relationship between these two mining process that facilitated further elucidation of the wisdom mining process. Potential research trends in the domain are also seen as a potential endeavour to improve the analysis and use of data."
Multi-thread hierarchical deep model for context-aware sentiment analysis,"Real-time messaging and opinion sharing in social media websites have made them valuable sources of different kinds of information. This source provides the opportunity for doing different kinds of analysis. Sentiment analysis as one of the most important of these analyses gains increasing interests. However, the research in this field is still facing challenges. The mainstream of the sentiment analysis research on social media websites and microblogs just exploits the textual content of the posts. This makes the analysis hard because microblog posts are short and noisy. However, they have lots of contexts which can be exploited for sentiment analysis. In order to use the context as an auxiliary source, some recent papers use reply/retweet to model the context of the target post. We claim that multiple sequential contexts can be used jointly in a unified model. In this article, we propose a context-aware multi-thread hierarchical long short-term memory (MHLSTM) that jointly models different kinds of contexts, such as tweep, hashtag and reply besides the content of the target post. Experimental evaluations on a real-world Twitter data set demonstrate that our proposed model can outperform some strong baseline models by 28.39% in terms of relative error reduction. Â© The Author(s) 2021.","Real-time messaging and opinion sharing in social media websites have made them valuable sources of different kinds of information. This source provides the opportunity for doing different kinds of analysis. Sentiment analysis as one of the most important of these analyses gains increasing interests. However, the research in this field is still facing challenges. The mainstream of the sentiment analysis research on social media websites and microblogs just exploits the textual content of the posts. This makes the analysis hard because microblog posts are short and noisy. However, they have lots of contexts which can be exploited for sentiment analysis. In order to use the context as an auxiliary source, some recent papers use reply/retweet to model the context of the target post. We claim that multiple sequential contexts can be used jointly in a unified model. In this article, we propose a context-aware multi-thread hierarchical long short-term memory (MHLSTM) that jointly models different kinds of contexts, such as tweep, hashtag and reply besides the content of the target post. Experimental evaluations on a real-world Twitter data set demonstrate that our proposed model can outperform some strong baseline models by 28.39% in terms of relative error reduction."
Effect of data environment and cognitive ability on participantsâ attitude towards data governance,"Data governance has received research attention, but its effect on public attitude has not been sufficiently explored. To analyse the attitude towards public participation in data governance in the context of a tourism platform, we conduct an empirical model that aims to understand the impact of data governance environment and participantsâ cognitive ability on attitude. Taking tourism sharing platforms as an example, we collected 339 questionnaires for data analysis. Results show that data quality and website design have a positive effect on usersâ attitude towards data governance through data literacy self-efficacy. Data literacy self-efficacy has a suppression effect between data quality and attitude towards data governance and has the same effect between website design and attitude towards data governance. Data quality and website design have a positive effect on usersâ attitude towards data governance through platform interaction. Platform interactivity plays a mediating role between data quality and attitude towards data governance and has the same effect between website design and attitude towards data governance. Data policy has a positive effect on usersâ data literacy self-efficacy but no significant effect on platform interactivity. Moreover, this study provides theoretical and practical implications that can guide the government in policy implementation and platform managers in data governance. Â© The Author(s) 2021.","Data governance has received research attention, but its effect on public attitude has not been sufficiently explored. To analyse the attitude towards public participation in data governance in the context of a tourism platform, we conduct an empirical model that aims to understand the impact of data governance environment and participants cognitive ability on attitude. Taking tourism sharing platforms as an example, we collected 339 questionnaires for data analysis. Results show that data quality and website design have a positive effect on users attitude towards data governance through data literacy self-efficacy. Data literacy self-efficacy has a suppression effect between data quality and attitude towards data governance and has the same effect between website design and attitude towards data governance. Data quality and website design have a positive effect on users attitude towards data governance through platform interaction. Platform interactivity plays a mediating role between data quality and attitude towards data governance and has the same effect between website design and attitude towards data governance. Data policy has a positive effect on users data literacy self-efficacy but no significant effect on platform interactivity. Moreover, this study provides theoretical and practical implications that can guide the government in policy implementation and platform managers in data governance."
Government regulation of the Internet as instrument of digital protectionism in case of developing countries,"The research is devoted to the study of digital protectionism technologies, in particular, Internet censorship as a non-tariff barrier to digital trade and the determination of the strategic motives of states to use them. The reports âFreedom on the Netâ and âThe network readiness index 2020â acted as a basic data source for the study of modern instruments of government regulation of interactions in the digital environment. Internet censorship technologies have been considered in six countries with varying levels of Internet freedom: Russia, Belarus, Kazakhstan, Georgia, Armenia and Estonia. The key instruments of digital protectionism as a non-tariff barrier of the digital economy have been identified, such as: localisation requirements; restrictions on cross-border data flow; system of national protection of intellectual property rights; discriminatory, unique standards or burdensome testing; filtering or blocking; restrictions on electronic payment systems or the use of encryption; cybersecurity threats and forced technology transfer. Internet censorship technologies have been demonstrated and their influence on the strategic development of trade relations between economies in cyberspace has been determined. The scientific value of the article lies in substantiating the understanding of Internet censorship as a natural tool for regulating the development of a digital society and international trade relations. Each state at one time goes through a technological stage of development, which leads to the emergence of different levels of digital isolation and integration; and Internet censorship is a natural element in the system of building a national platform economy and consolidating the countryâs internal technological and innovative advantages in digital realities. Â© The Author(s) 2021.","The research is devoted to the study of digital protectionism technologies, in particular, Internet censorship as a non-tariff barrier to digital trade and the determination of the strategic motives of states to use them. The reports Freedom on the Net and The network readiness index 2020 acted as a basic data source for the study of modern instruments of government regulation of interactions in the digital environment. Internet censorship technologies have been considered in six countries with varying levels of Internet freedom: Russia, Belarus, Kazakhstan, Georgia, Armenia and Estonia. The key instruments of digital protectionism as a non-tariff barrier of the digital economy have been identified, such as: localisation requirements; restrictions on cross-border data flow; system of national protection of intellectual property rights; discriminatory, unique standards or burdensome testing; filtering or blocking; restrictions on electronic payment systems or the use of encryption; cybersecurity threats and forced technology transfer. Internet censorship technologies have been demonstrated and their influence on the strategic development of trade relations between economies in cyberspace has been determined. The scientific value of the article lies in substantiating the understanding of Internet censorship as a natural tool for regulating the development of a digital society and international trade relations. Each state at one time goes through a technological stage of development, which leads to the emergence of different levels of digital isolation and integration; and Internet censorship is a natural element in the system of building a national platform economy and consolidating the countrys internal technological and innovative advantages in digital realities."
Improvements for research data repositories: The case of text spam,"Current research has evolved in such a way scientists must not only adequately describe the algorithms they introduce and the results of their application, but also ensure the possibility of reproducing the results and comparing them with those obtained through other approximations. In this context, public data sets (sometimes shared through repositories) are one of the most important elements for the development of experimental protocols and test benches. This study has analysed a significant number of CS/ML (Computer Science/Machine Learning) research data repositories and data sets and detected some limitations that hamper their utility. Particularly, we identify and discuss the following demanding functionalities for repositories: (1) building customised data sets for specific research tasks, (2) facilitating the comparison of different techniques using dissimilar pre-processing methods, (3) ensuring the availability of software applications to reproduce the pre-processing steps without using the repository functionalities and (4) providing protection mechanisms for licencing issues and user rights. To show the introduced functionality, we created STRep (Spam Text Repository) web application which implements our recommendations adapted to the field of spam text repositories. In addition, we launched an instance of STRep in the URL https://rdata.4spam.group to facilitate understanding of this study. Â© The Author(s) 2021.","Current research has evolved in such a way scientists must not only adequately describe the algorithms they introduce and the results of their application, but also ensure the possibility of reproducing the results and comparing them with those obtained through other approximations. In this context, public data sets (sometimes shared through repositories) are one of the most important elements for the development of experimental protocols and test benches. This study has analysed a significant number of CS/ML (Computer Science/Machine Learning) research data repositories and data sets and detected some limitations that hamper their utility. Particularly, we identify and discuss the following demanding functionalities for repositories: building customised data sets for specific research tasks, facilitating the comparison of different techniques using dissimilar pre-processing methods, ensuring the availability of software applications to reproduce the pre-processing steps without using the repository functionalities and providing protection mechanisms for licencing issues and user rights. To show the introduced functionality, we created STRep (Spam Text Repository) web application which implements our recommendations adapted to the field of spam text repositories. In addition, we launched an instance of STRep in the URL https://rdata.4spam.group to facilitate understanding of this study."
Discovering the role model of authors by embedding research history,"A role model that supports career planning is important for authors in the academic area to improve research abilities. In this study, we discovered a role model in bibliographic networks based on two perspectives: (1) high research performance to be exemplary and (2) a similar research history that can be easily followed by authors. We assume that the year-wise subgraphs in the dynamic bibliographic network signify the âresearch historyâ. We discovered role models of authors in three steps: (1) learning vector representations of research history in dynamic bibliographic networks, (2) measuring the similarity of authors according to the research history and (3) visualising role models. With this process, we can recommend a reasonable role model whose research path the authors can easily follow. In addition, we verified the effectiveness of the research history embeddings and the accuracy of the recommended role model in a real data set. Â© The Author(s) 2021.","A role model that supports career planning is important for authors in the academic area to improve research abilities. In this study, we discovered a role model in bibliographic networks based on two perspectives: high research performance to be exemplary and a similar research history that can be easily followed by authors. We assume that the year-wise subgraphs in the dynamic bibliographic network signify the research history. We discovered role models of authors in three steps: learning vector representations of research history in dynamic bibliographic networks, measuring the similarity of authors according to the research history and visualising role models. With this process, we can recommend a reasonable role model whose research path the authors can easily follow. In addition, we verified the effectiveness of the research history embeddings and the accuracy of the recommended role model in a real data set."
Copy-move image forged information detection and localisation in digital images using deep convolutional network,"Image tempering is one of the significant issues in the modern era. The use of powerful tools for image editing with advanced technology and its widespread on social media raised questions on data integrity. Currently, the protection of images is uncertain and a severe concern, mainly when it transfers over the Internet. Thus, it is essential to detect an anomaly in images through artificial intelligence techniques. The simple way of image forgery is called copy-move, where a part of an image is replicated in the same image to hide unwanted content of the image. However, image processing through handcrafted features usually looks for pattern concerns with duplicate content, limiting their employment for huge data classification. On the other side, deep learning approaches achieve promising results, but their performance depends on training data with fine-tuning of hyperparameters. Thus, we proposed a custom convolutional neural network (CNN) architecture with a pre-trained model ResNet101 through a transfer learning approach. For this purpose, both models are trained on five different datasets. In both cases, the impact of the model is evaluated through accuracy, precision, recall, F-score and achieved the highest 98.4% accuracy using the Coverage dataset. Â© The Author(s) 2021.","Image tempering is one of the significant issues in the modern era. The use of powerful tools for image editing with advanced technology and its widespread on social media raised questions on data integrity. Currently, the protection of images is uncertain and a severe concern, mainly when it transfers over the Internet. Thus, it is essential to detect an anomaly in images through artificial intelligence techniques. The simple way of image forgery is called copy-move, where a part of an image is replicated in the same image to hide unwanted content of the image. However, image processing through handcrafted features usually looks for pattern concerns with duplicate content, limiting their employment for huge data classification. On the other side, deep learning approaches achieve promising results, but their performance depends on training data with fine-tuning of hyperparameters. Thus, we proposed a custom convolutional neural network (CNN) architecture with a pre-trained model ResNet101 through a transfer learning approach. For this purpose, both models are trained on five different datasets. In both cases, the impact of the model is evaluated through accuracy, precision, recall, F-score and achieved the highest 98.4% accuracy using the Coverage dataset."
Communicating knowledge-focus through websites of higher education institutions,"Although higher education institutions (HEIs) are expected to be the leaders in knowledge generation and dissemination, it is often not clear whether they are knowledge-aware, that is, have a knowledge focus. In this article, the communication with stakeholders through HEI websites is examined to determine to what extent these institutions communicate about their knowledge initiatives and projects. This is done through an investigative study involving all HEIs and their faculties in Slovakia. Using content analysis, the study examines whether the publicly available resources on HEIsâ websites contain knowledge-related keywords, indicating the existence of a knowledge-focus. The results reveal that the websites of some HEIs contain hundreds of these resources, whereas others have none. Statistical evidence confirms that the intensity of communication about knowledge terms increases with the age and size of the HEI and is also dependent on the type of HEI (public, private state, foreign). Other dependencies between the examined factors have also been revealed, for example, HEIs that rank higher in Webometrics indicators are more intensive in their knowledge communications. Â© The Author(s) 2021.","Although higher education institutions (HEIs) are expected to be the leaders in knowledge generation and dissemination, it is often not clear whether they are knowledge-aware, that is, have a knowledge focus. In this article, the communication with stakeholders through HEI websites is examined to determine to what extent these institutions communicate about their knowledge initiatives and projects. This is done through an investigative study involving all HEIs and their faculties in Slovakia. Using content analysis, the study examines whether the publicly available resources on HEIs websites contain knowledge-related keywords, indicating the existence of a knowledge-focus. The results reveal that the websites of some HEIs contain hundreds of these resources, whereas others have none. Statistical evidence confirms that the intensity of communication about knowledge terms increases with the age and size of the HEI and is also dependent on the type of HEI (public, private state, foreign). Other dependencies between the examined factors have also been revealed, for example, HEIs that rank higher in Webometrics indicators are more intensive in their knowledge communications."
A semiautomatic annotation approach for sentiment analysis,"Sentiment analysis (SA) aims to extract usersâ opinions automatically from their posts and comments. Almost all prior works have used machine learning algorithms. Recently, SA research has shown promising performance in using the deep learning approach. However, deep learning is greedy and requires large datasets to learn, so it takes more time for data annotation. In this research, we proposed a semiautomatic approach using NaÃ¯ve Bayes (NB) to annotate a new dataset in order to reduce the human effort and time spent on the annotation process. We created a dataset for the purpose of training and testing the classifier by collecting Saudi dialect tweets. The dataset produced from the semiautomatic model was then used to train and test deep learning classifiers to perform Saudi dialect SA. The accuracy achieved by the NB classifier was 83%. The trained semiautomatic model was used to annotate the new dataset before it was fed into the deep learning classifiers. The three deep learning classifiers tested in this research were convolutional neural network (CNN), long short-term memory (LSTM) and bidirectional long short-term memory (Bi-LSTM). Support vector machine (SVM) was used as the baseline for comparison. Overall, the performance of the deep learning classifiers exceeded that of SVM. The results showed that CNN reported the highest performance. On one hand, the performance of Bi-LSTM was higher than that of LSTM and SVM, and, on the other hand, the performance of LSTM was higher than that of SVM. The proposed semiautomatic annotation approach is usable and promising to increase speed and save time and effort in the annotation process. Â© The Author(s) 2021.","Sentiment analysis (SA) aims to extract users opinions automatically from their posts and comments. Almost all prior works have used machine learning algorithms. Recently, SA research has shown promising performance in using the deep learning approach. However, deep learning is greedy and requires large datasets to learn, so it takes more time for data annotation. In this research, we proposed a semiautomatic approach using Nave Bayes (NB) to annotate a new dataset in order to reduce the human effort and time spent on the annotation process. We created a dataset for the purpose of training and testing the classifier by collecting Saudi dialect tweets. The dataset produced from the semiautomatic model was then used to train and test deep learning classifiers to perform Saudi dialect SA. The accuracy achieved by the NB classifier was 83%. The trained semiautomatic model was used to annotate the new dataset before it was fed into the deep learning classifiers. The three deep learning classifiers tested in this research were convolutional neural network (CNN), long short-term memory (LSTM) and bidirectional long short-term memory (Bi-LSTM). Support vector machine (SVM) was used as the baseline for comparison. Overall, the performance of the deep learning classifiers exceeded that of SVM. The results showed that CNN reported the highest performance. On one hand, the performance of Bi-LSTM was higher than that of LSTM and SVM, and, on the other hand, the performance of LSTM was higher than that of SVM. The proposed semiautomatic annotation approach is usable and promising to increase speed and save time and effort in the annotation process."
Personalised attraction recommendation for enhancing topic diversity and accuracy,"Attraction recommendation plays an important role in tourism, such as solving information overload problems and recommending proper attractions to users. Currently, most recommendation methods are dedicated to improving the accuracy of recommendations. However, recommendation methods only focusing on accuracy tend to recommend popular items that are often purchased by users, which results in a lack of diversity and low visibility of non-popular items. Hence, many studies have suggested the importance of recommendation diversity and proposed improved methods, but there is room for improvement. First, the definition of diversity for different items requires consideration for domain characteristics. Second, the existing algorithms for improving diversity sacrifice the accuracy of recommendations. Therefore, the article utilises the topic âfeatures of attractionsâ to define the calculation method of recommendation diversity. We developed a two-stage optimisation model to enhance recommendation diversity while maintaining the accuracy of recommendations. In the first stage, an optimisation model considering topic diversity is proposed to increase recommendation diversity and generate candidate attractions. In the second stage, we propose a minimisation misclassification cost optimisation model to balance recommendation diversity and accuracy. To assess the performance of the proposed method, experiments are conducted with real-world travel data. The results indicate that the proposed two-stage optimisation model can significantly improve the diversity and accuracy of recommendations. Â© The Author(s) 2021.","Attraction recommendation plays an important role in tourism, such as solving information overload problems and recommending proper attractions to users. Currently, most recommendation methods are dedicated to improving the accuracy of recommendations. However, recommendation methods only focusing on accuracy tend to recommend popular items that are often purchased by users, which results in a lack of diversity and low visibility of non-popular items. Hence, many studies have suggested the importance of recommendation diversity and proposed improved methods, but there is room for improvement. First, the definition of diversity for different items requires consideration for domain characteristics. Second, the existing algorithms for improving diversity sacrifice the accuracy of recommendations. Therefore, the article utilises the topic features of attractions to define the calculation method of recommendation diversity. We developed a two-stage optimisation model to enhance recommendation diversity while maintaining the accuracy of recommendations. In the first stage, an optimisation model considering topic diversity is proposed to increase recommendation diversity and generate candidate attractions. In the second stage, we propose a minimisation misclassification cost optimisation model to balance recommendation diversity and accuracy. To assess the performance of the proposed method, experiments are conducted with real-world travel data. The results indicate that the proposed two-stage optimisation model can significantly improve the diversity and accuracy of recommendations."
Assessing the credibility of COVID-19 vaccine mis/disinformation in online discussion,"This study examines how the credibility of the content of mis- or disinformation, as well as the believability of authors creating such information is assessed in online discussion. More specifically, the investigation was focused on the credibility of mis- or disinformation about COVID-19 vaccines. To this end, a sample of 1887 messages posted to a Reddit discussion group was scrutinised by means of qualitative content analysis. The findings indicate that in the assessment of the authorâs credibility, the most important criteria are his or her reputation, expertise and honesty in argumentation. In the judgement of the credibility of the content of mis/disinformation, objectivity of information and plausibility of arguments are highly important. The findings highlight that in the assessment of the credibility of mis/disinformation, the authorâs qualities such as poor reputation, incompetency and dishonesty are particularly significant because they trigger expectancies about how the information content created by the author is judged. Â© The Author(s) 2021.","This study examines how the credibility of the content of mis- or disinformation, as well as the believability of authors creating such information is assessed in online discussion. More specifically, the investigation was focused on the credibility of mis- or disinformation about COVID-19 vaccines. To this end, a sample of 1887 messages posted to a Reddit discussion group was scrutinised by means of qualitative content analysis. The findings indicate that in the assessment of the authors credibility, the most important criteria are his or her reputation, expertise and honesty in argumentation. In the judgement of the credibility of the content of mis/disinformation, objectivity of information and plausibility of arguments are highly important. The findings highlight that in the assessment of the credibility of mis/disinformation, the authors qualities such as poor reputation, incompetency and dishonesty are particularly significant because they trigger expectancies about how the information content created by the author is judged."
Identifying key factors and actions: Initial steps in the Open Science Policy Design and Implementation Process,"The coronavirus pandemic has illustrated the lack of a holistic approach in implementing Open Science (OS), leading to an inability to fully utilise its potential to inform prompt, evidence-based policy responses. In this view, this study aims to identify and categorise the factors influencing the adoption of OS and proposes possible actions for decision-makers to develop relevant policies. To achieve this, semi-structured interviews were conducted with 36 experts from Australia, France, the Netherlands, South Korea, the United Kingdom, and the United States as well as eminent international entities. During the interviews, they were asked to answer a range of questions that emerged from a systematic literature review. The responses were coded and analysed using a grounded theory approach. This led to the identification of four thematic clusters, containing a total of 24 factors that can either enable or inhibit OS practices, namely, (a) external; (b) institutional and regulatory; (c) resource-related; and (d) individual and motivational. Drawing upon Ostromâs Institutional Analysis and Development framework, we also propose a conceptual model that integrates these factors, accompanied with corresponding actions, into a tangible process of OS policy design and implementation. Â© The Author(s) 2023.","The coronavirus pandemic has illustrated the lack of a holistic approach in implementing Open Science (OS), leading to an inability to fully utilise its potential to inform prompt, evidence-based policy responses. In this view, this study aims to identify and categorise the factors influencing the adoption of OS and proposes possible actions for decision-makers to develop relevant policies. To achieve this, semi-structured interviews were conducted with 36 experts from Australia, France, the Netherlands, South Korea, the United Kingdom, and the United States as well as eminent international entities. During the interviews, they were asked to answer a range of questions that emerged from a systematic literature review. The responses were coded and analysed using a grounded theory approach. This led to the identification of four thematic clusters, containing a total of 24 factors that can either enable or inhibit OS practices, namely, (a) external; (b) institutional and regulatory; resource-related; and individual and motivational. Drawing upon Ostroms Institutional Analysis and Development framework, we also propose a conceptual model that integrates these factors, accompanied with corresponding actions, into a tangible process of OS policy design and implementation."
"RETRACTED: E-Learning as a basis for the development of media competences in students (Journal of Information Science, (2023), 49, 4, (1111-1125), 10.1177/01655515211040656)",corrected-proof ts1 Â© The Author(s) 2023.,
Using text mining to glean insights from COVID-19 literature,"The purpose of this study is to develop a text clusteringâbased analysis of COVID-19 research articles. Owing to the proliferation of published COVID-19 research articles, researchers need a method for reducing the number of articles they have to search through to find material relevant to their expertise. The study analyzes 83,264 abstracts from research articles related to COVID-19. The textual data are analysed using singular value decomposition (SVD) and the expectationâmaximisation (EM) algorithm. Results suggest that text clustering can both reveal hidden research themes in the published literature related to COVID-19, and reduce the number of articles that researchers need to search through to find material relevant to their field of interest. Â© The Author(s) 2021.","The purpose of this study is to develop a text clusteringbased analysis of COVID-19 research articles. Owing to the proliferation of published COVID-19 research articles, researchers need a method for reducing the number of articles they have to search through to find material relevant to their expertise. The study analyzes 83,264 abstracts from research articles related to COVID-19. The textual data are analysed using singular value decomposition (SVD) and the expectationmaximisation (EM) algorithm. Results suggest that text clustering can both reveal hidden research themes in the published literature related to COVID-19, and reduce the number of articles that researchers need to search through to find material relevant to their field of interest."
A discriminative method for global query expansion and term reweighting using co-occurrence graphs,"This article presents a new query expansion (QE) method aiming to tackle term mismatch in information retrieval (IR). Previous research showed that selecting good expansion terms which do not hurt retrieval effectiveness remains an open and challenging research question. Our method investigates how global statistics of term co-occurrence can be used effectively to enhance expansion term selection and reweighting. Indeed, we build a co-occurrence graph using a context window approach over the entire collection, thus adopting a global QE approach. Then, we employ a semantic similarity measure inspired by the Okapi BM25 model, which allows to evaluate the discriminative power of words and to select relevant expansion terms based on their similarity to the query as a whole. The proposed method includes a reweighting step where selected terms are assigned weights according to their relevance to the query. Whatâs more, our method does not require matrix factorisation or complex text mining processes. It only requires simple co-occurrence statistics about terms, which reduces complexity and insures scalability. Finally, it has two free parameters that may be tuned to adapt the model to the context of a given collection and control co-occurrence normalisation. Extensive experiments on four standard datasets of English (TREC Robust04 and Washington Post) and French (CLEF2000 and CLEF2003) show that our method improves both retrieval effectiveness and robustness in terms of various evaluation metrics and outperforms competitive state-of-the-art baselines with significantly better results. We also investigate the impact of varying the number of expansion terms on retrieval results. Â© The Author(s) 2021.","This article presents a new query expansion (QE) method aiming to tackle term mismatch in information retrieval (IR). Previous research showed that selecting good expansion terms which do not hurt retrieval effectiveness remains an open and challenging research question. Our method investigates how global statistics of term co-occurrence can be used effectively to enhance expansion term selection and reweighting. Indeed, we build a co-occurrence graph using a context window approach over the entire collection, thus adopting a global QE approach. Then, we employ a semantic similarity measure inspired by the Okapi BM25 model, which allows to evaluate the discriminative power of words and to select relevant expansion terms based on their similarity to the query as a whole. The proposed method includes a reweighting step where selected terms are assigned weights according to their relevance to the query. Whats more, our method does not require matrix factorisation or complex text mining processes. It only requires simple co-occurrence statistics about terms, which reduces complexity and insures scalability. Finally, it has two free parameters that may be tuned to adapt the model to the context of a given collection and control co-occurrence normalisation. Extensive experiments on four standard datasets of English (TREC Robust04 and Washington Post) and French (CLEF2000 and CLEF2003) show that our method improves both retrieval effectiveness and robustness in terms of various evaluation metrics and outperforms competitive state-of-the-art baselines with significantly better results. We also investigate the impact of varying the number of expansion terms on retrieval results."
Human-centred design on crowdsourcing annotation towards improving active learning model performance,"Active learning in machine learning is an effective approach to reducing the cost of human efforts for generating labels. The iterative process of active learning involves a human annotation step, during which crowdsourcing could be leveraged. It is essential for organisations adopting the active learning method to obtain a high model performance. This study aims to identify effective crowdsourcing interaction designs to promote the quality of human annotations and therefore the natural language processing (NLP)-based machine learning model performance. Specifically, the study experimented with four human-centred design techniques: highlight, guidelines, validation and text amount. Based on different combinations of the four design elements, the study developed 15 different annotation interfaces and recruited crowd workers to annotate texts with these interfaces. Annotated data under different designs were used separately to iteratively train a machine learning model. The results show that the design techniques of highlight and guideline play an essential role in improving the quality of human labels and therefore the performance of active learning models, while the impact of validation and text amount on model performance can be either positive in some cases or negative in other cases. The âsimpleâ designs (i.e. D1, D2, D7 and D14) with a few design techniques contribute to the top performance of models. The results provide practical implications to inspire the design of a crowdsourcing labelling system used for active learning. Â© The Author(s) 2023.","Active learning in machine learning is an effective approach to reducing the cost of human efforts for generating labels. The iterative process of active learning involves a human annotation step, during which crowdsourcing could be leveraged. It is essential for organisations adopting the active learning method to obtain a high model performance. This study aims to identify effective crowdsourcing interaction designs to promote the quality of human annotations and therefore the natural language processing (NLP)-based machine learning model performance. Specifically, the study experimented with four human-centred design techniques: highlight, guidelines, validation and text amount. Based on different combinations of the four design elements, the study developed 15 different annotation interfaces and recruited crowd workers to annotate texts with these interfaces. Annotated data under different designs were used separately to iteratively train a machine learning model. The results show that the design techniques of highlight and guideline play an essential role in improving the quality of human labels and therefore the performance of active learning models, while the impact of validation and text amount on model performance can be either positive in some cases or negative in other cases. The simple designs ( D1, D2, D7 and D14) with a few design techniques contribute to the top performance of models. The results provide practical implications to inspire the design of a crowdsourcing labelling system used for active learning."
A polyphony of characteristics: An analysis of the categorisation of musicâs subgenres,"We examine how music subgenres are differentiated from each other within seven parent genres â classical, folk, reggae, country, blues, electronic and jazz â according to two different sources, AllMusic and the Library of Congress Genre/Form Terms. Medium was by far the most common differentiator, but there were many others, with most subgenres defined according to multiple characteristic types, the use of which varied greatly across genres. Overall, differentiation was based more on characteristics intrinsic to the music, but prominent extrinsic characteristic types included culture and period. Also prominent was the identification of characteristics associated with other subgenres and genres, representing hybridisation. The resulting codebook of characteristics only partly overlaps with the major facets of music identified in the knowledge organisation literature. Our research conceptualises the musical subgenre, suggesting that music subgenres are differentiated from and connected to other subgenres, and to higher-level genres, in complex, familial ways â horizontally, vertically and obliquely. Â© The Author(s) 2023.","We examine how music subgenres are differentiated from each other within seven parent genres classical, folk, reggae, country, blues, electronic and jazz according to two different sources, AllMusic and the Library of Congress Genre/Form Terms. Medium was by far the most common differentiator, but there were many others, with most subgenres defined according to multiple characteristic types, the use of which varied greatly across genres. Overall, differentiation was based more on characteristics intrinsic to the music, but prominent extrinsic characteristic types included culture and period. Also prominent was the identification of characteristics associated with other subgenres and genres, representing hybridisation. The resulting codebook of characteristics only partly overlaps with the major facets of music identified in the knowledge organisation literature. Our research conceptualises the musical subgenre, suggesting that music subgenres are differentiated from and connected to other subgenres, and to higher-level genres, in complex, familial ways horizontally, vertically and obliquely."
Open Government Data: Usage trends and metadata quality,"Open Government Data (OGD) have the potential to support social and economic progress. However, this potential can be frustrated if these data remain unused. Although the literature suggests that OGD data setsâ metadata quality is one of the main factors affecting their use, to the best of our knowledge, no quantitative study provided evidence of this relationship. Considering about 400,000 data sets of 28 national, municipal and international OGD portals, we have programmatically analysed their usage, their metadata quality and the relationship between the two. Our analysis has highlighted three main findings. First, regardless of their size, the software platform adopted, and their administrative and territorial coverage, most OGD data sets are underutilised. Second, OGD portals pay varying attention to the quality of their data setsâ metadata. Third, we did not find clear evidence that data setsâ usage is positively correlated to better metadata publishing practices. Finally, we have considered other factors, such as data setsâ category, and some demographic characteristics of the OGD portals, and analysed their relationship with data setsâ usage, obtaining partially affirmative answers. Â© The Author(s) 2021.","Open Government Data (OGD) have the potential to support social and economic progress. However, this potential can be frustrated if these data remain unused. Although the literature suggests that OGD data sets metadata quality is one of the main factors affecting their use, to the best of our knowledge, no quantitative study provided evidence of this relationship. Considering about 400,000 data sets of 28 national, municipal and international OGD portals, we have programmatically analysed their usage, their metadata quality and the relationship between the two. Our analysis has highlighted three main findings. First, regardless of their size, the software platform adopted, and their administrative and territorial coverage, most OGD data sets are underutilised. Second, OGD portals pay varying attention to the quality of their data sets metadata. Third, we did not find clear evidence that data sets usage is positively correlated to better metadata publishing practices. Finally, we have considered other factors, such as data sets category, and some demographic characteristics of the OGD portals, and analysed their relationship with data sets usage, obtaining partially affirmative answers."
Hybrid approach for text categorization: A case study with Bangla news article,"The incredible expansion of online texts due to the Internet has intensified and revived the interest of sorting, managing and categorising the documents into their respective domains. This shows the pressing need for automatic text categorization system to assign a document into its appropriate domain. In this article, the focus is on showcasing the effectiveness of a hybrid approach that works elegantly by combining text-based and graph-based features. The hybrid approach was applied on 14,373 Bangla articles with 57,22,569 tokens collected from various online news corpora covering nine categories. This article also presents the individual application of both the features to explicate how they generally work. For classification purposes, the feature sets were passed through the Bayesian classification methods which yield satisfactory results with 98.73% accuracy for NaÃ¯ve Bayes Multinomial (NBM). Also, to test the robustness and language independency of the system, the experiments were performed on two popular English datasets as well. Â© The Author(s) 2021.","The incredible expansion of online texts due to the Internet has intensified and revived the interest of sorting, managing and categorising the documents into their respective domains. This shows the pressing need for automatic text categorization system to assign a document into its appropriate domain. In this article, the focus is on showcasing the effectiveness of a hybrid approach that works elegantly by combining text-based and graph-based features. The hybrid approach was applied on 14,373 Bangla articles with 57,22,569 tokens collected from various online news corpora covering nine categories. This article also presents the individual application of both the features to explicate how they generally work. For classification purposes, the feature sets were passed through the Bayesian classification methods which yield satisfactory results with 98.73% accuracy for Nave Bayes Multinomial (NBM). Also, to test the robustness and language independency of the system, the experiments were performed on two popular English datasets as well."
An approach for document retrieval using cluster-based inverted indexing,"Document retrieval plays an important role in knowledge management as it facilitates us to discover the relevant information from the existing data. This article proposes a cluster-based inverted indexing algorithm for document retrieval. First, the pre-processing is done to remove the unnecessary and redundant words from the documents. Then, the indexing of documents is done by the cluster-based inverted indexing algorithm, which is developed by integrating the piecewise fuzzy C-means (piFCM) clustering algorithm and inverted indexing. After providing the index to the documents, the query matching is performed for the user queries using the Bhattacharyya distance. Finally, the query optimisation is done by the Pearson correlation coefficient, and the relevant documents are retrieved. The performance of the proposed algorithm is analysed by the WebKB data set and Twenty Newsgroups data set. The analysis exposes that the proposed algorithm offers high performance with a precision of 1, recall of 0.70 and F-measure of 0.8235. The proposed document retrieval system retrieves the most relevant documents and speeds up the storing and retrieval of information. Â© The Author(s) 2021.","Document retrieval plays an important role in knowledge management as it facilitates us to discover the relevant information from the existing data. This article proposes a cluster-based inverted indexing algorithm for document retrieval. First, the pre-processing is done to remove the unnecessary and redundant words from the documents. Then, the indexing of documents is done by the cluster-based inverted indexing algorithm, which is developed by integrating the piecewise fuzzy C-means (piFCM) clustering algorithm and inverted indexing. After providing the index to the documents, the query matching is performed for the user queries using the Bhattacharyya distance. Finally, the query optimisation is done by the Pearson correlation coefficient, and the relevant documents are retrieved. The performance of the proposed algorithm is analysed by the WebKB data set and Twenty Newsgroups data set. The analysis exposes that the proposed algorithm offers high performance with a precision of 1, recall of 0.70 and F-measure of 0.8235. The proposed document retrieval system retrieves the most relevant documents and speeds up the storing and retrieval of information."
Does interdisciplinarity attract more social media attention?,"The impact of interdisciplinarity is a popular research topic, but most studies on this subject focus mainly on scientific impact. In recent years, social media platforms have become useful tools for assessing broader effects beyond scientific or academic impacts. Thus, exploring the effect of interdisciplinarity on social media platforms may yield interesting results. In order to examine the relationship between interdisciplinarity and social media attention, we analysed all publications in the Scopus database for the years 2018 and 2019. Three social media platforms with the highest coverage of research articles, Mendeley, Twitter and Facebook, and three common interdisciplinarity indicators, Leinster-Cobbold diversity indices (LCDiv), Rao-Stirling diversity (RS) and DIV, were applied to cross-validate our findings. It appears that interdisciplinarity does increase social media attention on Mendeley, Twitter and Facebook alike. The results indicate that interdisciplinary research indeed can attract more social media attention. Â© The Author(s) 2023.","The impact of interdisciplinarity is a popular research topic, but most studies on this subject focus mainly on scientific impact. In recent years, social media platforms have become useful tools for assessing broader effects beyond scientific or academic impacts. Thus, exploring the effect of interdisciplinarity on social media platforms may yield interesting results. In order to examine the relationship between interdisciplinarity and social media attention, we analysed all publications in the Scopus database for the years 2018 and 2019. Three social media platforms with the highest coverage of research articles, Mendeley, Twitter and Facebook, and three common interdisciplinarity indicators, Leinster-Cobbold diversity indices , Rao-Stirling diversity (RS) and DIV, were applied to cross-validate our findings. It appears that interdisciplinarity does increase social media attention on Mendeley, Twitter and Facebook alike. The results indicate that interdisciplinary research indeed can attract more social media attention."
A guided latent Dirichlet allocation approach to investigate real-time latent topics of Twitter data during Hurricane Laura,"Natural disasters cause significant damage, casualties and economical losses. Twitter has been used to support prompt disaster response and management because people tend to communicate and spread information on public social media platforms during disaster events. To retrieve real-time situational awareness (SA) information from tweets, the most effective way to mine text is using natural language processing (NLP). Among the advanced NLP models, the supervised approach can classify tweets into different categories to gain insight and leverage useful SA information from social media data. However, high-performing supervised models require domain knowledge to specify categories and involve costly labelling tasks. This research proposes a guided latent Dirichlet allocation (LDA) workflow to investigate temporal latent topics from tweets during a recent disaster event, the 2020 Hurricane Laura. With integration of prior knowledge, a coherence model, LDA topics visualisation and validation from official reports, our guided approach reveals that most tweets contain several latent topics during the 10-day period of Hurricane Laura. This result indicates that state-of-the-art supervised models have not fully utilised tweet information because they only assign each tweet a single label. In contrast, our model can not only identify emerging topics during different disaster events but also provides multilabel references to the classification schema. In addition, our results can help to quickly identify and extract SA information to responders, stakeholders and the general public so that they can adopt timely responsive strategies and wisely allocate resource during Hurricane events. Â© The Author(s) 2021.","Natural disasters cause significant damage, casualties and economical losses. Twitter has been used to support prompt disaster response and management because people tend to communicate and spread information on public social media platforms during disaster events. To retrieve real-time situational awareness (SA) information from tweets, the most effective way to mine text is using natural language processing (NLP). Among the advanced NLP models, the supervised approach can classify tweets into different categories to gain insight and leverage useful SA information from social media data. However, high-performing supervised models require domain knowledge to specify categories and involve costly labelling tasks. This research proposes a guided latent Dirichlet allocation (LDA) workflow to investigate temporal latent topics from tweets during a recent disaster event, the 2020 Hurricane Laura. With integration of prior knowledge, a coherence model, LDA topics visualisation and validation from official reports, our guided approach reveals that most tweets contain several latent topics during the 10-day period of Hurricane Laura. This result indicates that state-of-the-art supervised models have not fully utilised tweet information because they only assign each tweet a single label. In contrast, our model can not only identify emerging topics during different disaster events but also provides multilabel references to the classification schema. In addition, our results can help to quickly identify and extract SA information to responders, stakeholders and the general public so that they can adopt timely responsive strategies and wisely allocate resource during Hurricane events."
A semantic metric for concepts similarity in knowledge graphs,"Semantic similarity between concepts concerns expressing the degree of similarity in meaning between two concepts in a computational model. This problem has recently attracted considerable attention from researchers in attempting to automate the understanding of word meanings to expedite the classification of usersâ opinions and attitudes embedded in text. In this article, a semantic similarity metric is presented. The proposed metric, namely, weighted information-content (wic), exploits the information content of the least common subsumer of two compared concepts and the depth information in knowledge graphs such as DBPedia and YAGO. The two similarity components were combined using calibrated cooperative contributions from both similarity components. A statistical test using the Spearman correlations on well-known human judgement word-similarity data sets showed that the wic metric produced more highly correlated similarities compared with state-of-the-art metrics. In addition, a real-world aspect category classification was evaluated, which exhibited further increased accuracy and recall. Â© The Author(s) 2021.","Semantic similarity between concepts concerns expressing the degree of similarity in meaning between two concepts in a computational model. This problem has recently attracted considerable attention from researchers in attempting to automate the understanding of word meanings to expedite the classification of users opinions and attitudes embedded in text. In this article, a semantic similarity metric is presented. The proposed metric, namely, weighted information-content (wic), exploits the information content of the least common subsumer of two compared concepts and the depth information in knowledge graphs such as DBPedia and YAGO. The two similarity components were combined using calibrated cooperative contributions from both similarity components. A statistical test using the Spearman correlations on well-known human judgement word-similarity data sets showed that the wic metric produced more highly correlated similarities compared with state-of-the-art metrics. In addition, a real-world aspect category classification was evaluated, which exhibited further increased accuracy and recall."
Adapting the influences of publishers to perform news event detection,"Online news outlets have the power to influence public policy issues. To understand the opinions of the people, many government departments check online news outlets to manually detect events that interest people. This process is time-consuming. To promptly respond to public expectations, this research proposes a framework for detecting news events that may interest government departments. This article proposes a method for finding event trigger words used to represent an event. The news media can be a critical participant in âagenda-settingâ, which means that more widely discussed news is more attractive and critical than news that is less discussed. However, few studies have considered the influence of news media publishers from the âagenda settingâ perspective. Therefore, this study proposes an âagenda settingâ-based filter to establish a high-impact news event detection model. The proposed framework identifies trigger words and utilises word embedding to find news eventârelated words. After that, an event detection model is designed to determine the events that are attractive to government departments. The experimental results show that purity increases from 0.666 when no extraction method is used to 0.809 when the extraction method in this study is used. The overall improvement trend shows significant improvement in event detection performance. Â© The Author(s) 2021.","Online news outlets have the power to influence public policy issues. To understand the opinions of the people, many government departments check online news outlets to manually detect events that interest people. This process is time-consuming. To promptly respond to public expectations, this research proposes a framework for detecting news events that may interest government departments. This article proposes a method for finding event trigger words used to represent an event. The news media can be a critical participant in agenda-setting, which means that more widely discussed news is more attractive and critical than news that is less discussed. However, few studies have considered the influence of news media publishers from the agenda setting perspective. Therefore, this study proposes an agenda setting-based filter to establish a high-impact news event detection model. The proposed framework identifies trigger words and utilises word embedding to find news eventrelated words. After that, an event detection model is designed to determine the events that are attractive to government departments. The experimental results show that purity increases from 0.666 when no extraction method is used to 0.809 when the extraction method in this study is used. The overall improvement trend shows significant improvement in event detection performance."
E-Learning as a basis for the development of media competences in students,"The study describes the structure of media literacy and its key aspects: digital literacy, content interpretation, content generation and digital awareness. The results of the survey at the end of the training course confirmed the effectiveness of educational practices on digital media platforms in developing media competencies. According to self-assessment results, learning activities have improved digital skills. The study made it possible to determine the key success factors for the development and consolidation of modern media competencies in the context of online learning. The involvement of each student in the generation of educational content has formed the teaching tactics for the stable development of target media competencies and skills. Group interactions of students associated with the course met student demand for socialisation and contributed to educational progress. It was concluded that media literacy is the result of well-planned and well-organised practical activities of students in the digital media space. Â© The Author(s) 2021.","The study describes the structure of media literacy and its key aspects: digital literacy, content interpretation, content generation and digital awareness. The results of the survey at the end of the training course confirmed the effectiveness of educational practices on digital media platforms in developing media competencies. According to self-assessment results, learning activities have improved digital skills. The study made it possible to determine the key success factors for the development and consolidation of modern media competencies in the context of online learning. The involvement of each student in the generation of educational content has formed the teaching tactics for the stable development of target media competencies and skills. Group interactions of students associated with the course met student demand for socialisation and contributed to educational progress. It was concluded that media literacy is the result of well-planned and well-organised practical activities of students in the digital media space."
Efficient indexing and retrieval of patient information from the big data using MapReduce framework and optimisation,"Large and complex data becomes a valuable resource in biomedical discovery, which is highly facilitated to increase the scientific resources for retrieving the helpful information. However, indexing and retrieving the patient information from the disparate source of big data is challenging in biomedical research. Indexing and retrieving the patient information from big data is performed using the MapReduce framework. In this research, the indexing and retrieval of information are performed using the proposed Jaya-Sine Cosine Algorithm (JayaâSCA)-based MapReduce framework. Initially, the input big data is forwarded to the mapper randomly. The average of each mapper data is calculated, and these data are forwarded to the reducer, where the representative data are stored. For each user query, the input query is matched with the reducer, and thereby, it switches over to the mapper for retrieving the matched best result. The bilevel matching is performed while retrieving the data from the mapper based on the distance between the query. The similarity measure is computed based on the parametric-enabled similarity measure (PESM), cosine similarity and the proposed JayaâSCA, which is the integration of the Jaya algorithm and the SCA. Moreover, the proposed JayaâSCA algorithm attained the maximum value of F-measure, recall and precision of 0.5323, 0.4400 and 0.6867, respectively, using the StatLog Heart Disease dataset. Â© The Author(s) 2021.","Large and complex data becomes a valuable resource in biomedical discovery, which is highly facilitated to increase the scientific resources for retrieving the helpful information. However, indexing and retrieving the patient information from the disparate source of big data is challenging in biomedical research. Indexing and retrieving the patient information from big data is performed using the MapReduce framework. In this research, the indexing and retrieval of information are performed using the proposed Jaya-Sine Cosine Algorithm (JayaSCA)-based MapReduce framework. Initially, the input big data is forwarded to the mapper randomly. The average of each mapper data is calculated, and these data are forwarded to the reducer, where the representative data are stored. For each user query, the input query is matched with the reducer, and thereby, it switches over to the mapper for retrieving the matched best result. The bilevel matching is performed while retrieving the data from the mapper based on the distance between the query. The similarity measure is computed based on the parametric-enabled similarity measure (PESM), cosine similarity and the proposed JayaSCA, which is the integration of the Jaya algorithm and the SCA. Moreover, the proposed JayaSCA algorithm attained the maximum value of F-measure, recall and precision of 0.5323, 0.4400 and 0.6867, respectively, using the StatLog Heart Disease dataset."
Competitive intelligence empirical validation and application: Foundations for knowledge advancement and relevance to practice,"The competitive intelligence (CI) construct must be scientifically defined, characterised, empirically validated and accurately measured to grow in science and business. This study aims at elevating the accuracy of the empirical validation of the CI construct suggested and confirmed by Madureira, Popovic and Castelli to serve as the scientific foundation for CI praxis. This construct is selected due to its unmatched recency, thoroughness, universality identified limitations of its empirical validation. We relied on a multistrand design of fully sequential with equivalent status qualitative and quantitative mix-methods followed by the triangulation of the findings and the development of the meta-inferences. Validity, reliability and applicability were tested using computer-aided text analysis and artificial intelligence methods based on 61 in-depth interviews with CI subject matter experts. Contributions to knowledge advancement and relevance to practice derive from the scientific-grade empirical construct validation, providing undisputed levels of accuracy, consistency, applicability, and triangulation of results. This study highlights three critical implications. First, the delimitation of the body of knowledge and recognition of the CI domain serve as the baseline for theory development. Second, the validated construct guarantees reproducibility, replicability and generalisability, laying the foundations for establishing CI science, practice and education. Third, creating a common language and shared understanding will drive the much-claimed definitional consensus. This study thus stands as a foundational pillar in supporting CI praxis in improving decision-making quality and the performance of organisations. Â© The Author(s) 2023.","The competitive intelligence construct must be scientifically defined, characterised, empirically validated and accurately measured to grow in science and business. This study aims at elevating the accuracy of the empirical validation of the CI construct suggested and confirmed by Madureira, Popovic and Castelli to serve as the scientific foundation for CI praxis. This construct is selected due to its unmatched recency, thoroughness, universality identified limitations of its empirical validation. We relied on a multistrand design of fully sequential with equivalent status qualitative and quantitative mix-methods followed by the triangulation of the findings and the development of the meta-inferences. Validity, reliability and applicability were tested using computer-aided text analysis and artificial intelligence methods based on 61 in-depth interviews with CI subject matter experts. Contributions to knowledge advancement and relevance to practice derive from the scientific-grade empirical construct validation, providing undisputed levels of accuracy, consistency, applicability, and triangulation of results. This study highlights three critical implications. First, the delimitation of the body of knowledge and recognition of the CI domain serve as the baseline for theory development. Second, the validated construct guarantees reproducibility, replicability and generalisability, laying the foundations for establishing CI science, practice and education. Third, creating a common language and shared understanding will drive the much-claimed definitional consensus. This study thus stands as a foundational pillar in supporting CI praxis in improving decision-making quality and the performance of organisations."
Implementation of a conversational virtual assistant for open government data portal: Effects on citizens,"Insufficient support services are one of the barriers for citizenâs open government data (OGD) utilisation. Considering the strengths and effectiveness of virtual assistants (VAs) in providing instant and user-friendly support services, this study focuses primarily on its implementation for OGD portals. A conversational VA prototype is specifically developed. A between-subjects experiment with one factor (VA/text-based support services) was carried out. Results show citizensâ positive attitudes towards VAs providing support services on OGD portals. No significant difference was found for citizensâ acceptance, trustworthiness and rapport for OGD portals with and without a VA. Accuracy rate of completing tasks with VAs is slightly higher. A correlation lies between trustworthiness and rapport with OGD portals. This study benefits the improvement of OGD portalsâ online services to enhance citizensâ experiences and provides a research model both for investigating VAsâ effects and for comparing different designs of a same kind of service for an interface. Â© The Author(s) 2023.","Insufficient support services are one of the barriers for citizens open government data (OGD) utilisation. Considering the strengths and effectiveness of virtual assistants (VAs) in providing instant and user-friendly support services, this study focuses primarily on its implementation for OGD portals. A conversational VA prototype is specifically developed. A between-subjects experiment with one factor (VA/text-based support services) was carried out. Results show citizens positive attitudes towards VAs providing support services on OGD portals. No significant difference was found for citizens acceptance, trustworthiness and rapport for OGD portals with and without a VA. Accuracy rate of completing tasks with VAs is slightly higher. A correlation lies between trustworthiness and rapport with OGD portals. This study benefits the improvement of OGD portals online services to enhance citizens experiences and provides a research model both for investigating VAs effects and for comparing different designs of a same kind of service for an interface."
Trends and challenges of digital divide and digital inclusion: A bibliometric analysis,"Information and Communication Technologies (ICTs) are of great importance in todayâs society and have permeated different aspects of human life. In fact, access to them is now considered a fundamental right. There exists, however, a gap between individuals and populations who have access to these technologies and those who do not, which has led to social exclusion. In addition, the COVID-19 pandemic has exacerbated the effects of this disparity. In this regard, digital inclusion, through ICTs, becomes a strategy to close not only technical but also social gaps, thereby bringing well-being to vulnerable groups and favouring compliance with the Sustainable Development Goals (SDGs). Given the importance and topicality of this matter, we conducted a bibliometric analysis, which aims to answer what are the main trends in digital inclusion and digital divide studies and what are the challenges facing digital inclusion initiatives in the social context? For this purpose, we applied a search equation in Scopus and used VOSviewer. With this analysis, we were able to identify the evolution of publications over time and the main authors, countries and topics in the field, and the trends and challenges in digital inclusion initiatives. Finally, we conclude that this study can be used to address other research topics, such as the role of ICTs in the promotion of the SDGs through digital inclusion initiatives, the psychosocial aspects of technology adoption and the need for public policies that serve as a platform for digital and social inclusion. Â© The Author(s) 2023.","Information and Communication Technologies (ICTs) are of great importance in todays society and have permeated different aspects of human life. In fact, access to them is now considered a fundamental right. There exists, however, a gap between individuals and populations who have access to these technologies and those who do not, which has led to social exclusion. In addition, the COVID-19 pandemic has exacerbated the effects of this disparity. In this regard, digital inclusion, through ICTs, becomes a strategy to close not only technical but also social gaps, thereby bringing well-being to vulnerable groups and favouring compliance with the Sustainable Development Goals (SDGs). Given the importance and topicality of this matter, we conducted a bibliometric analysis, which aims to answer what are the main trends in digital inclusion and digital divide studies and what are the challenges facing digital inclusion initiatives in the social context? For this purpose, we applied a search equation in Scopus and used VOSviewer. With this analysis, we were able to identify the evolution of publications over time and the main authors, countries and topics in the field, and the trends and challenges in digital inclusion initiatives. Finally, we conclude that this study can be used to address other research topics, such as the role of ICTs in the promotion of the SDGs through digital inclusion initiatives, the psychosocial aspects of technology adoption and the need for public policies that serve as a platform for digital and social inclusion."
Effect of the topic-combination novelty on the disruption and impact of scientific articles: Evidence from PubMed,"Novelty, disruption and impact are essential concepts for understanding the originality and importance of scientific discoveries. By drawing on a large-scale corpus consisting of nearly 0.9 million PubMed papers published between 1970 and 2009 and their citations before 2018 in the Web of Science, we found that the topic-combination novelty has different effects on the impact and disruption of scientific papers, that is, an inverted U-shaped effect on the impact and a positive effect on disruption. One of our contributions is that we have significantly improved the reliability of topic-combination novelty by applying MeSH terms of PubMed to the measurement of novelty. Another contribution is that we have explained how a novel combination of MeSH terms of an article contributes to citations and citation networks, that is, the middle-level novelty is more likely to achieve large citation counts. In contrast, high topic-combination novelty relates to the discontinuity in the focal paperâs citation network. Â© The Author(s) 2023.","Novelty, disruption and impact are essential concepts for understanding the originality and importance of scientific discoveries. By drawing on a large-scale corpus consisting of nearly 0.9 million PubMed papers published between 1970 and 2009 and their citations before 2018 in the Web of Science, we found that the topic-combination novelty has different effects on the impact and disruption of scientific papers, that is, an inverted U-shaped effect on the impact and a positive effect on disruption. One of our contributions is that we have significantly improved the reliability of topic-combination novelty by applying MeSH terms of PubMed to the measurement of novelty. Another contribution is that we have explained how a novel combination of MeSH terms of an article contributes to citations and citation networks, that is, the middle-level novelty is more likely to achieve large citation counts. In contrast, high topic-combination novelty relates to the discontinuity in the focal papers citation network."
Relevance feedback for building pooled test collections,"Offline evaluation of information retrieval systems depends on test collections. These datasets provide the researchers with a corpus of documents, topics and relevance judgements indicating which documents are relevant for each topic. Gathering the latter is costly, requiring human assessors to judge the documents. Therefore, experts usually judge only a portion of the corpus. The most common approach for selecting that subset is pooling. By intelligently choosing which documents to assess, it is possible to optimise the number of positive labels for a given budget. For this reason, much work has focused on developing techniques to better select which documents from the corpus merit human assessments. In this article, we propose using relevance feedback to prioritise the documents when building new pooled test collections. We explore several state-of-the-art statistical feedback methods for prioritising the documents the algorithm presents to the assessors. A thorough comparison on eight Text Retrieval Conference (TREC) datasets against strong baselines shows that, among other results, our proposals improve in retrieving relevant documents with lower assessment effort than other state-of-the-art adjudicating methods without harming the reliability, fairness and reusability. Â© The Author(s) 2023.","Offline evaluation of information retrieval systems depends on test collections. These datasets provide the researchers with a corpus of documents, topics and relevance judgements indicating which documents are relevant for each topic. Gathering the latter is costly, requiring human assessors to judge the documents. Therefore, experts usually judge only a portion of the corpus. The most common approach for selecting that subset is pooling. By intelligently choosing which documents to assess, it is possible to optimise the number of positive labels for a given budget. For this reason, much work has focused on developing techniques to better select which documents from the corpus merit human assessments. In this article, we propose using relevance feedback to prioritise the documents when building new pooled test collections. We explore several state-of-the-art statistical feedback methods for prioritising the documents the algorithm presents to the assessors. A thorough comparison on eight Text Retrieval Conference (TREC) datasets against strong baselines shows that, among other results, our proposals improve in retrieving relevant documents with lower assessment effort than other state-of-the-art adjudicating methods without harming the reliability, fairness and reusability."
Scientometric review of Web 3.0,"Web 3.0 is a next-generation web architecture that envisions a more decentralised, secure and intelligent Internet. Its implications are vast and could potentially impact various research areas, such as e-commerce, social networking, finance, healthcare and education. It can be seen as a confluence of various technological advancements, including blockchain, artificial intelligence, semantic web and decentralised web technologies, which continue to attract substantial research interest in several dimensions and categories throughout the last few decades. A detailed scientometric analysis was undertaken to obtain concise understanding on development and publication trends of this multi-dimensional field. Corpus of 1154 articles, extracted from Web of Science from 2002 to 2022, were used to identify networks of co-authorship, keywords, subject categories, institutions and countries engaged in publishing on Web 3.0 along with co-citation and cluster analysis. Networks and interactive visualisations created using CiteSpace revealed new research areas where Web 3.0 may be beneficial and potential directions of development for Web 3.0 discipline. We identify Journalism 3.0, Personal Data Stores (PDS), Decentralised File Storages (DFS) and Metaverse as emerging domains Web 3.0 research, seeking overwhelming research attention globally. Â© The Author(s) 2023.","Web 3.0 is a next-generation web architecture that envisions a more decentralised, secure and intelligent Internet. Its implications are vast and could potentially impact various research areas, such as e-commerce, social networking, finance, healthcare and education. It can be seen as a confluence of various technological advancements, including blockchain, artificial intelligence, semantic web and decentralised web technologies, which continue to attract substantial research interest in several dimensions and categories throughout the last few decades. A detailed scientometric analysis was undertaken to obtain concise understanding on development and publication trends of this multi-dimensional field. Corpus of 1154 articles, extracted from Web of Science from 2002 to 2022, were used to identify networks of co-authorship, keywords, subject categories, institutions and countries engaged in publishing on Web 3.0 along with co-citation and cluster analysis. Networks and interactive visualisations created using CiteSpace revealed new research areas where Web 3.0 may be beneficial and potential directions of development for Web 3.0 discipline. We identify Journalism 3.0, Personal Data Stores (PDS), Decentralised File Storages (DFS) and Metaverse as emerging domains Web 3.0 research, seeking overwhelming research attention globally."
Improved multi-lingual sentiment analysis and recognition using deep learning,"Speech emotion recognition (SER) is still a fresh in natural language processing domain since the accuracy is beyond targeted. Mainly due to real-time applications such as humanârobot interaction, human behaviour evaluation and virtual reality rely heavily on SER. Moreover, cross-lingual SER plays a significant role in practical applications, especially when users of different cultural and linguistic backgrounds interact with the system. However, the existing conventional approaches of SER cannot be employed for real-world applications because it uses the same corpus for training and testing, which cannot be used for multi-lingual environments to detect or classify real emotions. In such a situation, the performance of SER is degraded. Therefore, the proposed work develops cross-lingual emotion recognition through Urdu, Italian, English and German. The features are extracted through the most employed audio feature known as MFCCs (Mel Frequency Cepstral Coefficients). Experimental results exhibited that the proposed deep learning model comes out with promising results on the URDU data set with 91.25% accuracy using random forest (RF) and XGBoost classifier. Â© The Author(s) 2023.","Speech emotion recognition (SER) is still a fresh in natural language processing domain since the accuracy is beyond targeted. Mainly due to real-time applications such as humanrobot interaction, human behaviour evaluation and virtual reality rely heavily on SER. Moreover, cross-lingual SER plays a significant role in practical applications, especially when users of different cultural and linguistic backgrounds interact with the system. However, the existing conventional approaches of SER cannot be employed for real-world applications because it uses the same corpus for training and testing, which cannot be used for multi-lingual environments to detect or classify real emotions. In such a situation, the performance of SER is degraded. Therefore, the proposed work develops cross-lingual emotion recognition through Urdu, Italian, English and German. The features are extracted through the most employed audio feature known as MFCCs (Mel Frequency Cepstral Coefficients). Experimental results exhibited that the proposed deep learning model comes out with promising results on the URDU data set with 91.25% accuracy using random forest (RF) and XGBoost classifier."
Modelling and analysis of misinformation diffusion based on the double intervention mechanism,"Although official departments attempt to intervene against misinformation, the personal field often conflicts with the goals of these departments. Thus, when rumours spread widely on social media, decision-makers often use a combination of rigid and soft control measures, such as blocking keywords, deleting misinformation, suspending accounts or refuting misinformation, to decrease the diffusion of misinformation. However, existing methods rarely consider the interplay of blocking and rebuttal measures, resulting in an unclear effect of the double intervention mechanism. To address these issues, we propose a novel misinformation diffusion model called SEIRI (susceptible, exposed, infective, removed, and infective) that considers the double intervention mechanism and secondary diffusion characteristics. We analyse the stability of the proposed model, obtain rumour-free and rumour-spread equilibriums, and calculate the basic reproduction number. Furthermore, we conduct numerical simulations to analyse the influence of key parameters through comparative experiments. Finally, we validate the effectiveness of the proposed approach by crawling a real-world data set of COVID-19-related misinformation tweets from Sina Weibo. Our comparison experiments with other similar works show that the SEIRI model provides superior performance in characterising the actual spread of misinformation. Our findings lead to several practical implications for public health policymaking. Â© The Author(s) 2023.","Although official departments attempt to intervene against misinformation, the personal field often conflicts with the goals of these departments. Thus, when rumours spread widely on social media, decision-makers often use a combination of rigid and soft control measures, such as blocking keywords, deleting misinformation, suspending accounts or refuting misinformation, to decrease the diffusion of misinformation. However, existing methods rarely consider the interplay of blocking and rebuttal measures, resulting in an unclear effect of the double intervention mechanism. To address these issues, we propose a novel misinformation diffusion model called SEIRI (susceptible, exposed, infective, removed, and infective) that considers the double intervention mechanism and secondary diffusion characteristics. We analyse the stability of the proposed model, obtain rumour-free and rumour-spread equilibriums, and calculate the basic reproduction number. Furthermore, we conduct numerical simulations to analyse the influence of key parameters through comparative experiments. Finally, we validate the effectiveness of the proposed approach by crawling a real-world data set of COVID-19-related misinformation tweets from Sina Weibo. Our comparison experiments with other similar works show that the SEIRI model provides superior performance in characterising the actual spread of misinformation. Our findings lead to several practical implications for public health policymaking."
Recognising formula entailment using long short-term memory network,"The article presents an approach to recognise formula entailment, which concerns finding entailment relationships between pairs of math formulae. As the current formula-similarity-detection approaches fail to account for broader relationships between pairs of math formulae, recognising formula entailment becomes paramount. To this end, a long short-term memory (LSTM) neural network using symbol-by-symbol attention for recognising formula entailment is implemented. However, owing to the unavailability of relevant training and validation corpora, the first and foremost step is to create a sufficiently large-sized symbol-level MATHENTAIL data set in an automated fashion. Depending on the extent of similarity between the corresponding symbol embeddings, the symbol pairs in the MATHENTAIL data set are assigned âentailmentâ or âneutralâ labels. An improved symbol-to-vector (isymbol2vec) method generates mathematical symbols (in LATEX) and their embeddings using the Wikipedia corpus of scientific documents and Continuous Bag of Words (CBOW) architecture. Eventually, the LSTM network, trained and validated using the MATHENTAIL data set, predicts formulae entailment for test formulae pairs with a reasonable accuracy of 62.2%. Â© The Author(s) 2023.","The article presents an approach to recognise formula entailment, which concerns finding entailment relationships between pairs of math formulae. As the current formula-similarity-detection approaches fail to account for broader relationships between pairs of math formulae, recognising formula entailment becomes paramount. To this end, a long short-term memory (LSTM) neural network using symbol-by-symbol attention for recognising formula entailment is implemented. However, owing to the unavailability of relevant training and validation corpora, the first and foremost step is to create a sufficiently large-sized symbol-level MATHENTAIL data set in an automated fashion. Depending on the extent of similarity between the corresponding symbol embeddings, the symbol pairs in the MATHENTAIL data set are assigned entailment or neutral labels. An improved symbol-to-vector (isymbol2vec) method generates mathematical symbols (in LATEX) and their embeddings using the Wikipedia corpus of scientific documents and Continuous Bag of Words (CBOW) architecture. Eventually, the LSTM network, trained and validated using the MATHENTAIL data set, predicts formulae entailment for test formulae pairs with a reasonable accuracy of 62.2%."
The impact of social media marketing on the dissemination of mini program in social network with different community structure,"Mini program has become an important infrastructure for the Internetisation of traditional brands. Previous studies have mostly explored the design and development of mini program, but little research has been conducted on the mini program dissemination process. Studying the impact of social media marketing on the mini program dissemination process has important theoretical and applicable value for the in-depth discovery of the mini program dissemination mechanism and the marketing strategies improvement for mini program operators. Based on the classical SusceptibleâInfectedâRecovered (SIR) epidemic dissemination dynamic model and combined with the Sense, Interest and Interactive, Connect and Diffusion, Action and Share user behaviour consumption model, we propose a mini program propagation model considering social media marketing and community structure, and establish a set of differential equations reflecting the mini program propagation rules. The validity of the model and experimental results are verified through the comparison among the simulation, actual data and numerical experimental results. The experimental results show that the improvement of social media marketing can considerably promote the dissemination of mini program. Surprisingly, with the same social media marketing investment, the dissemination effect of mini program in a weak community is better than that in a strong community. The reason for this is that the higher clustering coefficient fails to offset the negative impact of high modularity on marketing information dissemination. Consequently, compared with the marketing needs of the weak community structure, more social media marketing efforts need to be invested in the strong community structure. Improving social media marketing in the early stages of dissemination can maximise the marketing effect. Moreover, the effect of combining of advertising marketing and interactive marketing in social media marketing is better than that of a single marketing strategy. Â© The Author(s) 2023.","Mini program has become an important infrastructure for the Internetisation of traditional brands. Previous studies have mostly explored the design and development of mini program, but little research has been conducted on the mini program dissemination process. Studying the impact of social media marketing on the mini program dissemination process has important theoretical and applicable value for the in-depth discovery of the mini program dissemination mechanism and the marketing strategies improvement for mini program operators. Based on the classical SusceptibleInfectedRecovered (SIR) epidemic dissemination dynamic model and combined with the Sense, Interest and Interactive, Connect and Diffusion, Action and Share user behaviour consumption model, we propose a mini program propagation model considering social media marketing and community structure, and establish a set of differential equations reflecting the mini program propagation rules. The validity of the model and experimental results are verified through the comparison among the simulation, actual data and numerical experimental results. The experimental results show that the improvement of social media marketing can considerably promote the dissemination of mini program. Surprisingly, with the same social media marketing investment, the dissemination effect of mini program in a weak community is better than that in a strong community. The reason for this is that the higher clustering coefficient fails to offset the negative impact of high modularity on marketing information dissemination. Consequently, compared with the marketing needs of the weak community structure, more social media marketing efforts need to be invested in the strong community structure. Improving social media marketing in the early stages of dissemination can maximise the marketing effect. Moreover, the effect of combining of advertising marketing and interactive marketing in social media marketing is better than that of a single marketing strategy."
Assessing causality among topics and sentiments: The case of the G20 discussion on Twitter,"Although the identification of topics and sentiments from social media content has attracted substantial research, little work has been carried out on the extraction of causal relationships among those topics and sentiments. This article proposes a methodology aimed at building a causal graph where nodes represent topics and emotions extracted from social media usersâ posts. To illustrate the proposed methodology, we collected a large multi-year dataset of tweets related to different editions of the G20 summit, which was locally indexed for further analysis. Topic-relevant queries are crafted from phrases extracted by experts from G20 output documents on four main recurring topics, namely government, society, environment and health and economics. Subsequently, sentiments are identified on the retrieved tweets using a lexicon based on Plutchikâs wheel of emotions. Finally, a causality test that uses stochastic dominance is applied to build a causal graph among topics and emotions by exploiting the asymmetries of explaining a variable from other variables. The applied causality discovery process relies on observational data only and does not require any assumptions of linearity, parametric definitions or temporal precedence. In our analysis, we observe that although the time series of topics and emotions always show high correlation coefficients, stochastic causality provides a means to tell apart causal relationships from other forms of associations. The proposed methodology can be applied to better understand social behaviour on social media, offering support to decision and policy making and their communication by government leaders. Â© The Author(s) 2023.","Although the identification of topics and sentiments from social media content has attracted substantial research, little work has been carried out on the extraction of causal relationships among those topics and sentiments. This article proposes a methodology aimed at building a causal graph where nodes represent topics and emotions extracted from social media users posts. To illustrate the proposed methodology, we collected a large multi-year dataset of tweets related to different editions of the G20 summit, which was locally indexed for further analysis. Topic-relevant queries are crafted from phrases extracted by experts from G20 output documents on four main recurring topics, namely government, society, environment and health and economics. Subsequently, sentiments are identified on the retrieved tweets using a lexicon based on Plutchiks wheel of emotions. Finally, a causality test that uses stochastic dominance is applied to build a causal graph among topics and emotions by exploiting the asymmetries of explaining a variable from other variables. The applied causality discovery process relies on observational data only and does not require any assumptions of linearity, parametric definitions or temporal precedence. In our analysis, we observe that although the time series of topics and emotions always show high correlation coefficients, stochastic causality provides a means to tell apart causal relationships from other forms of associations. The proposed methodology can be applied to better understand social behaviour on social media, offering support to decision and policy making and their communication by government leaders."
Ontology is what makes data interesting: Interestingness framework for COVID-19 corpora,"The COVID-19 pandemic has already shown to be a worldwide threat, demonstrating how susceptible humans may be. It has also inspired experts from a range of aspects and countries to find the potential solution to control the widespread. In line with this, our research proposes a novel framework for finding interesting facts from COVID-19 corpora using domain ontology. Since data mining with domain knowledge provides semantically rich facts, we use ontology in our proposed approaches. Most of the state-of-the-art methods rely on instance level or user intervention. These methods do not entirely exploit the richness of ontology. In this work, we demonstrate how to extract exciting rules from data at ontologyâs schema and instance levels. Our experiments were carried out on two COVID-19 corpora that depict COVID-19 patientsâ symptoms and drug information. The proposed framework outperformed the traditional methods by reducing the number of rules by 70% and generating semantic-rich rules that are more user-readable and quickly adopted by decision-makers. Furthermore, to support our claims, we compared the outcomes of the proposed framework with the most recent approach in the field. Also, statistically significant tests and domain expert evaluations are conducted to validate our framework. Â© The Author(s) 2023.","The COVID-19 pandemic has already shown to be a worldwide threat, demonstrating how susceptible humans may be. It has also inspired experts from a range of aspects and countries to find the potential solution to control the widespread. In line with this, our research proposes a novel framework for finding interesting facts from COVID-19 corpora using domain ontology. Since data mining with domain knowledge provides semantically rich facts, we use ontology in our proposed approaches. Most of the state-of-the-art methods rely on instance level or user intervention. These methods do not entirely exploit the richness of ontology. In this work, we demonstrate how to extract exciting rules from data at ontologys schema and instance levels. Our experiments were carried out on two COVID-19 corpora that depict COVID-19 patients symptoms and drug information. The proposed framework outperformed the traditional methods by reducing the number of rules by 70% and generating semantic-rich rules that are more user-readable and quickly adopted by decision-makers. Furthermore, to support our claims, we compared the outcomes of the proposed framework with the most recent approach in the field. Also, statistically significant tests and domain expert evaluations are conducted to validate our framework."
Use of positive terms and certainty language in retracted and non-retracted articles: The case of biochemistry,"This study aimed to compare retracted (due to misconduct) and non-retracted articles in biochemistry, in terms of proportion of positive terms, certainty score and different certainty aspects. The data set of this study composed of 662 retracted and non-retracted articles published in the time period of 2018â2020 and indexed in Scopus. These 662 articles accounted for 331 non-retracted and 331 retracted articles, which were matched using matching and covariate balancing analysis. The analysis in this article was done using several regression models. Regarding the use of positive terms, the findings showed that retracted articles were 16% less probable to use positive terms in abstracts, titles and findings presented in conclusion and discussion compared with non-retracted articles. In addition, the results regarding the analysis of certainty language, showed that retracted articles were 15% less probable to use certain language, measured by certainty score, in presenting their scientific findings. Finally, regarding the certainty aspects, the results of regression models showed that retracted articles had 11% less likelihood to present their research findings using certain probability aspect. Â© The Author(s) 2023.","This study aimed to compare retracted (due to misconduct) and non-retracted articles in biochemistry, in terms of proportion of positive terms, certainty score and different certainty aspects. The data set of this study composed of 662 retracted and non-retracted articles published in the time period of 20182020 and indexed in Scopus. These 662 articles accounted for 331 non-retracted and 331 retracted articles, which were matched using matching and covariate balancing analysis. The analysis in this article was done using several regression models. Regarding the use of positive terms, the findings showed that retracted articles were 16% less probable to use positive terms in abstracts, titles and findings presented in conclusion and discussion compared with non-retracted articles. In addition, the results regarding the analysis of certainty language, showed that retracted articles were 15% less probable to use certain language, measured by certainty score, in presenting their scientific findings. Finally, regarding the certainty aspects, the results of regression models showed that retracted articles had 11% less likelihood to present their research findings using certain probability aspect."
Ranking coherence in topic models using statistically validated networks,"Probabilistic topic models have become one of the most widespread machine learning techniques in textual analysis. Topic discovering is an unsupervised process that does not guarantee the interpretability of its output. Hence, the automatic evaluation of topic coherence has attracted the interest of many researchers over the last decade, and it is an open research area. This article offers a new quality evaluation method based on statistically validated networks (SVNs). The proposed probabilistic approach consists of representing each topic as a weighted network of its most probable words. The presence of a link between each pair of words is assessed by statistically validating their co-occurrence in sentences against the null hypothesis of random co-occurrence. The proposed method allows one to distinguish between high-quality and low-quality topics, by making use of a battery of statistical tests. The statistically significant pairwise associations of words represented by the links in the SVN might reasonably be expected to be strictly related to the semantic coherence and interpretability of a topic. Therefore, the more connected the network, the more coherent the topic in question. We demonstrate the effectiveness of the method through an analysis of a real text corpus, which shows that the proposed measure is more correlated with human judgement than the state-of-the-art coherence measures. Â© The Author(s) 2023.","Probabilistic topic models have become one of the most widespread machine learning techniques in textual analysis. Topic discovering is an unsupervised process that does not guarantee the interpretability of its output. Hence, the automatic evaluation of topic coherence has attracted the interest of many researchers over the last decade, and it is an open research area. This article offers a new quality evaluation method based on statistically validated networks (SVNs). The proposed probabilistic approach consists of representing each topic as a weighted network of its most probable words. The presence of a link between each pair of words is assessed by statistically validating their co-occurrence in sentences against the null hypothesis of random co-occurrence. The proposed method allows one to distinguish between high-quality and low-quality topics, by making use of a battery of statistical tests. The statistically significant pairwise associations of words represented by the links in the SVN might reasonably be expected to be strictly related to the semantic coherence and interpretability of a topic. Therefore, the more connected the network, the more coherent the topic in question. We demonstrate the effectiveness of the method through an analysis of a real text corpus, which shows that the proposed measure is more correlated with human judgement than the state-of-the-art coherence measures."
LEGO: Linked electronic government ontology,"E-government services are subject to a growing level of complexity, which requires a disruptive approach that better support the citizen needs concerning the government administration. Nowadays, available information technologies facilitate the description and online execution of administrative tasks, saving time and reducing possible errors. These technologies reduce administrative costs but require a complex electronic government system. We propose using semantic technologies to describe the e-government organisational units and services in the Open Government Data and Services context. The use of semantics improves government management, service delivery and decision-making processes. This article presents an extension of related work, introducing the evolution of the Ontology for Electronic Government (EGO): integrating other existing ontologies, supporting new features to describe e-government services and widening the usage scenarios. This extension enables the use in a real scenario with four use cases: the electronic government in the Province of Misiones (Argentina). However, the use in the domain of electronic government in a provincial context is also a proof of concept that this approach is general enough to expand into superior domains of countries that adopt the republican system of government with the division of government into the executive, legislative and judicial branches. Â© The Author(s) 2023.","E-government services are subject to a growing level of complexity, which requires a disruptive approach that better support the citizen needs concerning the government administration. Nowadays, available information technologies facilitate the description and online execution of administrative tasks, saving time and reducing possible errors. These technologies reduce administrative costs but require a complex electronic government system. We propose using semantic technologies to describe the e-government organisational units and services in the Open Government Data and Services context. The use of semantics improves government management, service delivery and decision-making processes. This article presents an extension of related work, introducing the evolution of the Ontology for Electronic Government (EGO): integrating other existing ontologies, supporting new features to describe e-government services and widening the usage scenarios. This extension enables the use in a real scenario with four use cases: the electronic government in the Province of Misiones (Argentina). However, the use in the domain of electronic government in a provincial context is also a proof of concept that this approach is general enough to expand into superior domains of countries that adopt the republican system of government with the division of government into the executive, legislative and judicial branches."
"Tracing the evolution of digitalisation research in business and management fields: Bibliometric analysis, topic modelling and deep learning trend forecasting","Research on digitalisation trends and digital topics has become one of the most prolific streams of research within the fields of business and management during the course of the past few years. The purpose of this study is to provide a general picture of the intellectual structure and the conceptual space of this research realm. To this purpose, 6067 publications related to digital topics, indexed in the business and management categories of Web of Science (WoS), and dated from 1990 to 2020 are explored based on the approaches of bibliometric analysis, topic modelling and trend forecasting. The results of the bibliometric analysis comprise insights into the publication and citation structure, the most productive authors, the most productive universities, the most productive countries, the most productive journals, the most cited studies and the most prevalent themes and sub-themes on digitalisation in business and management. In addition, the outcomes of the topic modelling give new knowledge on the latent topical structure along with the rising, falling and fluctuating trends of this literature. In addition, the results of the trend forecasting enable readers to have a glimpse of how the underlying trends of the literature will probably change within the next years until 2025. These results provide guidance and orientation for both academics and practitioners who are initiating or currently developing their efforts in this discipline. Â© The Author(s) 2023.","Research on digitalisation trends and digital topics has become one of the most prolific streams of research within the fields of business and management during the course of the past few years. The purpose of this study is to provide a general picture of the intellectual structure and the conceptual space of this research realm. To this purpose, 6067 publications related to digital topics, indexed in the business and management categories of Web of Science (WoS), and dated from 1990 to 2020 are explored based on the approaches of bibliometric analysis, topic modelling and trend forecasting. The results of the bibliometric analysis comprise insights into the publication and citation structure, the most productive authors, the most productive universities, the most productive countries, the most productive journals, the most cited studies and the most prevalent themes and sub-themes on digitalisation in business and management. In addition, the outcomes of the topic modelling give new knowledge on the latent topical structure along with the rising, falling and fluctuating trends of this literature. In addition, the results of the trend forecasting enable readers to have a glimpse of how the underlying trends of the literature will probably change within the next years until 2025. These results provide guidance and orientation for both academics and practitioners who are initiating or currently developing their efforts in this discipline."
"Who shares scholarly output on Facebook? A systematic investigation into their location, productivity and identities","Facebook mentions to scholarly outputs are important data source for scholarly communication and altmetrics. However, little is known about who are producing these mentions. With statistical analysis of over 1.5 million records, this study has revealed scientific Facebook usersâ geographic distribution and productivity distribution. Furthermore, via coding analysis based on random sampling strategy and stratified sampling strategy, their identities are recognised and classified in a systematic way. Results show the following. (1) Scientific Facebook users are widely distributed around the world but highly concentrated, presenting a different pattern compared with general Facebook users. (2) Productivity distribution is highly skewed towards the lowly productive scientific Facebook users. (3) Identities of scientific Facebook users are very diversified, and are proposed to be classified into 6 basic types and 30 specific types, verifying that various stakeholders other than scholars are engaged with scholarly outputs on Facebook. (4) Organisational scientific Facebook users (percentage > 80%) are dominant. Meanwhile, public users (percentage = 55%) have surpassed researcher users (percentage = 32%) as the major type. Among public users, various business organisations (percentage = 26%) and social organisations (percentage = 19%) are playing an important role. (5) Level of activeness has strong connection with usersâ identities. There is clear increasing pattern regarding percentage of public users with decreasing level of activeness. These results indicate that Facebook mentions can measure interactions with scholarly outputs from much broader categories of users other than scholars and userâs context information needs to be considered when using Facebook mention counts for evaluative purpose. Â© The Author(s) 2023.","Facebook mentions to scholarly outputs are important data source for scholarly communication and altmetrics. However, little is known about who are producing these mentions. With statistical analysis of over 1.5 million records, this study has revealed scientific Facebook users geographic distribution and productivity distribution. Furthermore, via coding analysis based on random sampling strategy and stratified sampling strategy, their identities are recognised and classified in a systematic way. Results show the following. Scientific Facebook users are widely distributed around the world but highly concentrated, presenting a different pattern compared with general Facebook users. Productivity distribution is highly skewed towards the lowly productive scientific Facebook users. Identities of scientific Facebook users are very diversified, and are proposed to be classified into 6 basic types and 30 specific types, verifying that various stakeholders other than scholars are engaged with scholarly outputs on Facebook. Organisational scientific Facebook users (percentage > 80%) are dominant. Meanwhile, public users (percentage = 55%) have surpassed researcher users (percentage = 32%) as the major type. Among public users, various business organisations (percentage = 26%) and social organisations (percentage = 19%) are playing an important role. Level of activeness has strong connection with users identities. There is clear increasing pattern regarding percentage of public users with decreasing level of activeness. These results indicate that Facebook mentions can measure interactions with scholarly outputs from much broader categories of users other than scholars and users context information needs to be considered when using Facebook mention counts for evaluative purpose."
Factors affecting university studentsâ information literacy education: An empirical study using fuzzy-set qualitative comparative analysis,"With the continuous development of information technology, information literacy education is becoming more and more important for university students. However, information literacy education in Chinese universities is still in the development stage and the cultivation effect needs to be improved. Therefore, this study takes the teaching of Information Retrieval as an example, based on constructivist learning theory and social exchange theory, reveals the influential factors related to the cultivation effect and explores the causal rela-tionship between each variable and the cultivation effect through a qualitative comparative analysis research method with a configura-tion perspective. This study finds that university students in information literacy education can be classified into four categories: context-driven, class-guidance, meaning-realised and motivation satisfaction. For these four types of university students, this study proposes corresponding countermeasures for information literacy cultivation in order to enhance the effectiveness of cultivation. Â© The Author(s) 2023.","With the continuous development of information technology, information literacy education is becoming more and more important for university students. However, information literacy education in Chinese universities is still in the development stage and the cultivation effect needs to be improved. Therefore, this study takes the teaching of Information Retrieval as an example, based on constructivist learning theory and social exchange theory, reveals the influential factors related to the cultivation effect and explores the causal rela-tionship between each variable and the cultivation effect through a qualitative comparative analysis research method with a configura-tion perspective. This study finds that university students in information literacy education can be classified into four categories: context-driven, class-guidance, meaning-realised and motivation satisfaction. For these four types of university students, this study proposes corresponding countermeasures for information literacy cultivation in order to enhance the effectiveness of cultivation."
How authors evaluate the novelty of their articles: A comparative analysis of highlights in research articles,"Due to the growing capacities of research articles and journals, the tasks of selecting interesting articles and quickly identifying interesting sections of articles have become primary challenges faced by researchers. Therefore, the notion of Highlights, a novel introductory section included in academic publications, has been proposed to directly emphasise the novelty and value of research articles to improve article retrieval and knowledge dissemination. In this article, we developed a classification schema featuring five categories to analyse the content and explore the features of sentences contained in the Highlights sections of articles. Subsequently, we conducted an experiment by using the fields of Library and Information Science (LIS) and Computer Science (CS) as examples to statistically analyse domain differences in the arrangement of Highlights sections. The experiment focused on both the sentence level and the article level and emphasised differences in research paradigms and principles of evaluation. In particular, we discovered that LIS is relatively âresult-heavyâ, while CS is âmethod-heavyâ; furthermore, in self-evaluated contributions, LIS authors concentrated more on academic contributions and applications, while CS authors preferred to demonstrate the value of their articles by comparing their research with previous studies. Â© The Author(s) 2023.","Due to the growing capacities of research articles and journals, the tasks of selecting interesting articles and quickly identifying interesting sections of articles have become primary challenges faced by researchers. Therefore, the notion of Highlights, a novel introductory section included in academic publications, has been proposed to directly emphasise the novelty and value of research articles to improve article retrieval and knowledge dissemination. In this article, we developed a classification schema featuring five categories to analyse the content and explore the features of sentences contained in the Highlights sections of articles. Subsequently, we conducted an experiment by using the fields of Library and Information Science (LIS) and Computer Science (CS) as examples to statistically analyse domain differences in the arrangement of Highlights sections. The experiment focused on both the sentence level and the article level and emphasised differences in research paradigms and principles of evaluation. In particular, we discovered that LIS is relatively result-heavy, while CS is method-heavy; furthermore, in self-evaluated contributions, LIS authors concentrated more on academic contributions and applications, while CS authors preferred to demonstrate the value of their articles by comparing their research with previous studies."
A model of planned and unplanned information-seeking behaviour,"The main purpose of this article is to present a model for information-seeking behaviour with an emphasis on unplanned and planned behaviour of users in using library resources and services. The working method was that, reviewing the literature and previous information behaviour models, such as Wilson, Ellis, Kuhlthau, and Dervin models, this article proposes a novel model of information-seeking behaviour for library users. Our model of information-seeking behaviour was developed by combining the existing models of planned information-seeking behaviour with the focus on the factors affecting unplanned rather than planned behaviour of users in accessing resources or services. Our proposed model for information-seeking behaviour of clients has two main parts. The first part planned behaviour resulting from a problem or a certain information need according to which the user seeks to find information in a planned manner. The second part deals with unplanned behaviour shaped by a hidden or uncertain information need. Finally, both types of behaviour can result in the discovery, extraction, collection and use of information. Â© The Author(s) 2023.","The main purpose of this article is to present a model for information-seeking behaviour with an emphasis on unplanned and planned behaviour of users in using library resources and services. The working method was that, reviewing the literature and previous information behaviour models, such as Wilson, Ellis, Kuhlthau, and Dervin models, this article proposes a novel model of information-seeking behaviour for library users. Our model of information-seeking behaviour was developed by combining the existing models of planned information-seeking behaviour with the focus on the factors affecting unplanned rather than planned behaviour of users in accessing resources or services. Our proposed model for information-seeking behaviour of clients has two main parts. The first part planned behaviour resulting from a problem or a certain information need according to which the user seeks to find information in a planned manner. The second part deals with unplanned behaviour shaped by a hidden or uncertain information need. Finally, both types of behaviour can result in the discovery, extraction, collection and use of information."
A pipeline for medical literature search and its evaluation,"One database commonly used by clinicians for searching the medical literature and practicing evidence-based medicine is PubMed. As the literature grows, it has become challenging for users to find the relevant material quickly because most of the time the relevant results are not on the top. In this article, we propose a search and ranking pipeline to improve the search results based on relevancy. We first propose an ensemble model consisting of three classifiers: bidirectional long-short-term memory conditional random field (bi-LSTM-CRF), support vector machine and naive Bayes to recognise PICO (patient, intervention, comparison, outcome) elements from abstracts. The ensemble was trained on an annotated corpus consisting of 5000 abstracts split into 4000 and 1000 training and testing data, respectively. The ensemble recorded an accuracy of 93%. We then retrieved around 927,000 articles from PubMed for the years 2017â2021 (access date 16 April 2021). For every abstract, we extracted and grouped all P, I and O terms, and stored these groups along with the article ID in a separate database. During the search, every P, I and O term of the query is searched only in its corresponding group of every abstract. The scoring method simply counts the number of matches between the queryâs P, I and O elements and the words in P, I and O groups, respectively. The abstracts are sorted by the number of matches and the top five abstracts are listed using their pre-stored abstract IDs. A comprehensive user study was conducted where 60 different queries were formulated and used to generate ranked search results using both PubMed and our proposed model. Five medical professionals assessed the ranked search results and marked every item as relevant or non-relevant. Both models were compared using precision@K and mean-average-precision@K metrics where K is 5. For most of the queries, our model produced higher precision@K values than PubMed. The mean-average-precision@K value of our model is also higher than PubMed (0.83 versus 0.67). Â© The Author(s) 2023.","One database commonly used by clinicians for searching the medical literature and practicing evidence-based medicine is PubMed. As the literature grows, it has become challenging for users to find the relevant material quickly because most of the time the relevant results are not on the top. In this article, we propose a search and ranking pipeline to improve the search results based on relevancy. We first propose an ensemble model consisting of three classifiers: bidirectional long-short-term memory conditional random field (bi-LSTM-CRF), support vector machine and naive Bayes to recognise PICO (patient, intervention, comparison, outcome) elements from abstracts. The ensemble was trained on an annotated corpus consisting of 5000 abstracts split into 4000 and 1000 training and testing data, respectively. The ensemble recorded an accuracy of 93%. We then retrieved around 927,000 articles from PubMed for the years 20172021 (access date 16 April 2021). For every abstract, we extracted and grouped all P, I and O terms, and stored these groups along with the article ID in a separate database. During the search, every P, I and O term of the query is searched only in its corresponding group of every abstract. The scoring method simply counts the number of matches between the querys P, I and O elements and the words in P, I and O groups, respectively. The abstracts are sorted by the number of matches and the top five abstracts are listed using their pre-stored abstract IDs. A comprehensive user study was conducted where 60 different queries were formulated and used to generate ranked search results using both PubMed and our proposed model. Five medical professionals assessed the ranked search results and marked every item as relevant or non-relevant. Both models were compared using precision@K and mean-average-precision@K metrics where K is 5. For most of the queries, our model produced higher precision@K values than PubMed. The mean-average-precision@K value of our model is also higher than PubMed (0.83 versus 0.67)."
Research on the co-evolution of competitive public opinion and intervention strategy based on Markov process,"Under Omni-media environment, Online Social Networks (OSN) have gradually become the most momentous platform for information propagation. Considering the interaction and coexistence of both positive and negative public opinion information (referred to as public opinion), it is of great significance for social development and economic stability to understand the co-evolution process of competitive public opinion and compress the spreading space of negative public opinion. Allowing for this point, this paper constructed a two-stage spreading model of competitive public opinion combing with the actual case of public opinion propagation, analysed the main factors influencing the co-evolution process, such as netizensâ intimacy, network literacy, and so on, and redefined netizensâ state transition probability matrix with the help of Markov process. Then, the effectiveness of the spreading model was verified and the propagation rule of public opinion was discussed in open and closed OSN through simulation experiments. Finally, the intervention strategies were proposed and optimised with the limitation of cost. The results show that the propagation of public opinion mainly depends on netizensâ behaviour with low literacy and presents difference characteristics in two types of OSN. During the intervention process of public opinion propagation, there exists an effective intervention interval and the best intervention strategy varies with the change of network topology. Our research provided a cornerstone for further understanding of the co-evolution process of competitive public opinion and the research conclusions also provided a certain decision-making reference for enterprises, governments and other regulators to reasonably respond to the propagation of public opinion. Â© The Author(s) 2023.","Under Omni-media environment, Online Social Networks (OSN) have gradually become the most momentous platform for information propagation. Considering the interaction and coexistence of both positive and negative public opinion information (referred to as public opinion), it is of great significance for social development and economic stability to understand the co-evolution process of competitive public opinion and compress the spreading space of negative public opinion. Allowing for this point, this paper constructed a two-stage spreading model of competitive public opinion combing with the actual case of public opinion propagation, analysed the main factors influencing the co-evolution process, such as netizens intimacy, network literacy, and so on, and redefined netizens state transition probability matrix with the help of Markov process. Then, the effectiveness of the spreading model was verified and the propagation rule of public opinion was discussed in open and closed OSN through simulation experiments. Finally, the intervention strategies were proposed and optimised with the limitation of cost. The results show that the propagation of public opinion mainly depends on netizens behaviour with low literacy and presents difference characteristics in two types of OSN. During the intervention process of public opinion propagation, there exists an effective intervention interval and the best intervention strategy varies with the change of network topology. Our research provided a cornerstone for further understanding of the co-evolution process of competitive public opinion and the research conclusions also provided a certain decision-making reference for enterprises, governments and other regulators to reasonably respond to the propagation of public opinion."
Internet of things adoption and use in academic libraries: A review and directions for future research,"This study aims to synthesise the findings of research on Internet of Things (IoTs) adoption and use in libraries. This systematic literature review is based on Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) method and comprises publications in the five world-renowned databases. The libraries adopted IoTs for saving time, enhance performance and efficiency, improve the quality of services, and ease in collection accessibility. This study identified various IoTs-based practices including auto-notification of circulation tasks, inventory management, tracing usersâ data from virtual/physical card, user tracking and self-guided virtual tour of library. To adopt and use IoTs, libraries faced several challenges such as security and privacy, cost, lack of standards and policy, require highly integrated environment, and lack of management interest. The critical IoTs adoption and usage factors as well as various challenges identified would provide valuable insights to library professionals to design state-of-the-art smart technologies drive services. Â© The Author(s) 2023.","This study aims to synthesise the findings of research on Internet of Things (IoTs) adoption and use in libraries. This systematic literature review is based on Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) method and comprises publications in the five world-renowned databases. The libraries adopted IoTs for saving time, enhance performance and efficiency, improve the quality of services, and ease in collection accessibility. This study identified various IoTs-based practices including auto-notification of circulation tasks, inventory management, tracing users data from virtual/physical card, user tracking and self-guided virtual tour of library. To adopt and use IoTs, libraries faced several challenges such as security and privacy, cost, lack of standards and policy, require highly integrated environment, and lack of management interest. The critical IoTs adoption and usage factors as well as various challenges identified would provide valuable insights to library professionals to design state-of-the-art smart technologies drive services."
Information architecture on the websites of major American political parties: Qualitative-heuristic assessment and comparative analysis,"The Internet has become a very important tool of modern political communication. Political parties use it for a variety of purposes, including relationships with its supporters and potential voters. Therefore, the website of any political party is an important channel of communication. Such a website should be user-friendly to various audiences. The purpose of the article is to assess the quality of information architecture (IA) on the websites of the two major American political parties: the Republican Party and the Democratic Party. Triangulation of methods was used in the research: a qualitative-heuristic assessment of the IA on the mentioned websites, expert assessment and a comparative analysis. Within the examined websites, there is a noticeable lack of some important components of the IA. The website of the Republican Party is slightly better in terms of quality than the website of Democratic Party by 14.09%. The results of the conducted research may have an impact not only on the assessment of the technological advancement of parties but also on the image and public perception, and therefore also the effectiveness in reaching voters of a given party. This approach should contribute to the development of websites research in order to improve the quality of user experience and information processes. Â© The Author(s) 2023.","The Internet has become a very important tool of modern political communication. Political parties use it for a variety of purposes, including relationships with its supporters and potential voters. Therefore, the website of any political party is an important channel of communication. Such a website should be user-friendly to various audiences. The purpose of the article is to assess the quality of information architecture (IA) on the websites of the two major American political parties: the Republican Party and the Democratic Party. Triangulation of methods was used in the research: a qualitative-heuristic assessment of the IA on the mentioned websites, expert assessment and a comparative analysis. Within the examined websites, there is a noticeable lack of some important components of the IA. The website of the Republican Party is slightly better in terms of quality than the website of Democratic Party by 14.09%. The results of the conducted research may have an impact not only on the assessment of the technological advancement of parties but also on the image and public perception, and therefore also the effectiveness in reaching voters of a given party. This approach should contribute to the development of websites research in order to improve the quality of user experience and information processes."
Sci-Hub use among Spanish researchers: Enemy or a learning opportunity for libraries?,"Access to scientific literature is the cornerstone of scientific knowledge and numerous scientific-technical and social advances. However, in 2018, major difficulties in accessing the scientific literature have been reported. Despite Open Access has made more than 50% of scientific literature accessible without paywalls, during this yearâs access to pirate scientific resources, such as Sci-Hub, has increased. It is one of the most popular resources among researchers and university students. The key aspect is to differentiate between these kinds of resources (black open access or shadow library) and the Open Access movement. Black open access violates copyright regulations, and Open Access wants to give authors control over the integrity of their work and the right to be properly acknowledged and cited. We conducted a questionnaire with 17 items about the use of Sci-Hub among Spanish-speaking sciences and social sciences researchers. Libraries must learn from these kinds of resources how to improve the access to their scientific resources, and propose to the editors a different way of business, more similar to Spotify or Netflix than the journal bundles offered nowadays, usually with numerous journals with an embargo period or no relevance to the scientific community. Â© The Author(s) 2023.","Access to scientific literature is the cornerstone of scientific knowledge and numerous scientific-technical and social advances. However, in 2018, major difficulties in accessing the scientific literature have been reported. Despite Open Access has made more than 50% of scientific literature accessible without paywalls, during this years access to pirate scientific resources, such as Sci-Hub, has increased. It is one of the most popular resources among researchers and university students. The key aspect is to differentiate between these kinds of resources (black open access or shadow library) and the Open Access movement. Black open access violates copyright regulations, and Open Access wants to give authors control over the integrity of their work and the right to be properly acknowledged and cited. We conducted a questionnaire with 17 items about the use of Sci-Hub among Spanish-speaking sciences and social sciences researchers. Libraries must learn from these kinds of resources how to improve the access to their scientific resources, and propose to the editors a different way of business, more similar to Spotify or Netflix than the journal bundles offered nowadays, usually with numerous journals with an embargo period or no relevance to the scientific community."
Using R to develop a corpus of full-text journal articles,"Over the past two decades, databases and the tools to access them in a simple manner have become increasingly available, allowing historical and modern-day topics to be merged and studied. Throughout the recent COVID-19 pandemic, for example, many researchers have reflected on whether any lessons learned from the Spanish flu pandemic of 1918 could have been helpful in the present pandemic. Most studies using text-mining applications rarely use full-text journal articles. This article provides a methodology used to develop a full-text journal article corpus using the R fulltext package. Using the proposed methodology, 2743 full-text journal articles were obtained. The aim of this article is to provide a methodology and supplementary codes for researchers to use the R fulltext package to curate a full-text journal corpus. Â© The Author(s) 2023.","Over the past two decades, databases and the tools to access them in a simple manner have become increasingly available, allowing historical and modern-day topics to be merged and studied. Throughout the recent COVID-19 pandemic, for example, many researchers have reflected on whether any lessons learned from the Spanish flu pandemic of 1918 could have been helpful in the present pandemic. Most studies using text-mining applications rarely use full-text journal articles. This article provides a methodology used to develop a full-text journal article corpus using the R fulltext package. Using the proposed methodology, 2743 full-text journal articles were obtained. The aim of this article is to provide a methodology and supplementary codes for researchers to use the R fulltext package to curate a full-text journal corpus."
Tracing policy diffusion: Identifying main paths in policy citation networks,"Citation-based main path analysis (MPA) has been widely applied to identify developmental trajectories of science and technology, while rarely used to detect paths of policy diffusion. Compared with scientific publications and patents, policy documents show some distinct characteristics, such as citation relationships with different legal validity, which could be considered to improve the policy citation analysis. To this end, this study formally constructs a policy citation network based on a plethora of citing/cited links embedded in the textual content of policy documents and proposes a preference-adjusted main path analysis (PMPA) approach to track historical routes of policy diffusion. PMPA incorporates two kinds of policy citation preferences, including validity bias and time bias. An evidence analysis from Chinaâs new energy policies (NEPs) is implemented to show the efficacy of the proposed approach. The results unveil that the preference-adjusted main path approach can capture more important policies and more informative main paths of policy diffusion than the original MPA. Moreover, our research can yield in-depth insight into the evolutionary process of policy diffusion and provide guidance for policy-makers and industry decision-makers to formulate practical policy-making. Â© The Author(s) 2023.","Citation-based main path analysis (MPA) has been widely applied to identify developmental trajectories of science and technology, while rarely used to detect paths of policy diffusion. Compared with scientific publications and patents, policy documents show some distinct characteristics, such as citation relationships with different legal validity, which could be considered to improve the policy citation analysis. To this end, this study formally constructs a policy citation network based on a plethora of citing/cited links embedded in the textual content of policy documents and proposes a preference-adjusted main path analysis (PMPA) approach to track historical routes of policy diffusion. PMPA incorporates two kinds of policy citation preferences, including validity bias and time bias. An evidence analysis from Chinas new energy policies (NEPs) is implemented to show the efficacy of the proposed approach. The results unveil that the preference-adjusted main path approach can capture more important policies and more informative main paths of policy diffusion than the original MPA. Moreover, our research can yield in-depth insight into the evolutionary process of policy diffusion and provide guidance for policy-makers and industry decision-makers to formulate practical policy-making."
CHV.br: Exploratory study for the development of a consumer health vocabulary (CHV) supported by a network model for Brazilian Portuguese language,"Successful consumer health vocabulary (CHV) models have been engineered and updated by using automatic term extraction techniques from online content. However, the relationship between terms has yet to be mapped. This study aims to describe a CHV model for the Brazilian Portuguese language that is supported by a complex network. The method was split up into three distinct stages: (1) collect and automatically extract terms from structured data sources on the web, such as Unified Medical Language System (UMLS) vocabularies and DBpedia; (2) construct a complex network; and (3) select the terms supported by clustering techniques. A model called CHV.br was developed and supported by a complex network structure which makes connections between the controlled vocabulary and consumer vocabulary and maps semantic relationships as categories, synonyms and related terms. CHV.br contains 146,956 terms, of which 31,439 are UMLS preferred terms and 83,279 are synonyms. The CHV.br is available and powered by Simple Knowledge Organization System and Resource Description Framework standards. The method used in this study showed to be valid for the selection of the candidate terms by connecting the terms from different reliable resources, in addition to expanding the number of terms and their semantic relationships. The content and structure of CHV.br could play a vital role in enhancing the development of consumer-oriented health applications. Â© The Author(s) 2023.","Successful consumer health vocabulary (CHV) models have been engineered and updated by using automatic term extraction techniques from online content. However, the relationship between terms has yet to be mapped. This study aims to describe a CHV model for the Brazilian Portuguese language that is supported by a complex network. The method was split up into three distinct stages: collect and automatically extract terms from structured data sources on the web, such as Unified Medical Language System (UMLS) vocabularies and DBpedia; construct a complex network; and select the terms supported by clustering techniques. A model called CHV.br was developed and supported by a complex network structure which makes connections between the controlled vocabulary and consumer vocabulary and maps semantic relationships as categories, synonyms and related terms. CHV.br contains 146,956 terms, of which 31,439 are UMLS preferred terms and 83,279 are synonyms. The CHV.br is available and powered by Simple Knowledge Organization System and Resource Description Framework standards. The method used in this study showed to be valid for the selection of the candidate terms by connecting the terms from different reliable resources, in addition to expanding the number of terms and their semantic relationships. The content and structure of CHV.br could play a vital role in enhancing the development of consumer-oriented health applications."
SKIFF: Spherical K-means with iterative feature filtering for text document clustering,"Text clustering has been an overlooked field of text mining that requires more attention. Several applications require automatic text organisation which relies on an information retrieval system based on organised search results. Spherical k-means is a successful adaptation of the classic k-means algorithm for text clustering. However, conventional methods to accelerate k-means may not apply to spherical k-means due to the different nature of text document data. The proposed work introduces an iterative feature filtering technique that reduces the data size during the process of clustering which further produces more feature-relevant clusters in less time compared to classic spherical k-means. The novelty of the proposed method is that feature assessment is distinct from the objective function of clustering and derived from the cluster structure. Experimental results show that the proposed scheme achieves computation speed without sacrificing cluster quality over popular text corpora. The demonstrated results are satisfactory and outperform compared to recent works in this domain. Â© The Author(s) 2023.","Text clustering has been an overlooked field of text mining that requires more attention. Several applications require automatic text organisation which relies on an information retrieval system based on organised search results. Spherical k-means is a successful adaptation of the classic k-means algorithm for text clustering. However, conventional methods to accelerate k-means may not apply to spherical k-means due to the different nature of text document data. The proposed work introduces an iterative feature filtering technique that reduces the data size during the process of clustering which further produces more feature-relevant clusters in less time compared to classic spherical k-means. The novelty of the proposed method is that feature assessment is distinct from the objective function of clustering and derived from the cluster structure. Experimental results show that the proposed scheme achieves computation speed without sacrificing cluster quality over popular text corpora. The demonstrated results are satisfactory and outperform compared to recent works in this domain."
Towards a semantic approach in GLAM Labs: The case of the Data Foundry at the National Library of Scotland,"GLAM organisations have been exploring the benefits of publishing their digital collections in a wide variety of forms since the 2000s. Many institutions, and in particular libraries, have applied the Semantic Web and Linked Data to their main catalogues. Recent advances in technology and innovative approaches concerning the reuse of the digital collections by means of computational access have paved the way for the creation of laboratories within GLAM organisations. In this work, we present a framework that works in several steps to transform into linked open data (LOD), assess and enrich with external repositories the data sets made available by GLAM organisations under open licences. The framework has been applied to three metadata data sets made available by the Data Foundry at the National Library of Scotland obtaining as a result a collection of LOD repositories, a data quality assessment and examples of exploration. The results of this work are publicly available and can be applied to other domains such as digital humanities and data science. Â© The Author(s) 2023.","GLAM organisations have been exploring the benefits of publishing their digital collections in a wide variety of forms since the 2000s. Many institutions, and in particular libraries, have applied the Semantic Web and Linked Data to their main catalogues. Recent advances in technology and innovative approaches concerning the reuse of the digital collections by means of computational access have paved the way for the creation of laboratories within GLAM organisations. In this work, we present a framework that works in several steps to transform into linked open data (LOD), assess and enrich with external repositories the data sets made available by GLAM organisations under open licences. The framework has been applied to three metadata data sets made available by the Data Foundry at the National Library of Scotland obtaining as a result a collection of LOD repositories, a data quality assessment and examples of exploration. The results of this work are publicly available and can be applied to other domains such as digital humanities and data science."
The effects of international research collaboration on the policy impact of research: A causal inference drawing on the journal Lancet,"Research findings have been widely used as evidence for policy-making. The internationalisation of research activities has been increasing in recent decades, particularly during the COVID-19 pandemic. Previous studies have revealed that international research collaboration can enhance the academic impact of research. However, the effects that international research collaboration exerts on the policy impact of research are still unknown. This study aims to examine the effects of international research collaboration on the policy impact of research (as measured by the number of citations in policy documents) using a causal inference approach. Research articles published by the journal Lancet between 2000 and 2019 were selected as the study sample (n = 6098). The number of policy citations of each article was obtained from Overton, the largest database of policy citations. Propensity score matching analysis, which takes a causal inference approach, was used to examine the dataset. Four other matching methods and alternative datasets of different sizes were used to test the robustness of the results. The results of this study reveal that international research collaboration has significant and positive effects on the policy impact of research (coefficient = 4.323, p < 0.001). This study can provide insight to researchers, research institutions and grant funders for improving the policy impact of research. Â© The Author(s) 2023.","Research findings have been widely used as evidence for policy-making. The internationalisation of research activities has been increasing in recent decades, particularly during the COVID-19 pandemic. Previous studies have revealed that international research collaboration can enhance the academic impact of research. However, the effects that international research collaboration exerts on the policy impact of research are still unknown. This study aims to examine the effects of international research collaboration on the policy impact of research (as measured by the number of citations in policy documents) using a causal inference approach. Research articles published by the journal Lancet between 2000 and 2019 were selected as the study sample (n = 6098). The number of policy citations of each article was obtained from Overton, the largest database of policy citations. Propensity score matching analysis, which takes a causal inference approach, was used to examine the dataset. Four other matching methods and alternative datasets of different sizes were used to test the robustness of the results. The results of this study reveal that international research collaboration has significant and positive effects on the policy impact of research (coefficient = 4.323, p < 0.001). This study can provide insight to researchers, research institutions and grant funders for improving the policy impact of research."
"Improving semantic information retrieval by combining possibilistic networks, vector space model and pseudo-relevance feedback","To improve the performance of information retrieval systems (IRSs), we propose in this article a novel approach that enriches the userâs queries with new concepts. Indeed, query expansion is one of the best methods that plays an important role in improving searches for a better semantic information retrieval. The proposed approach in this study combines possibilistic networks (PNs), the vector space model (VSM) and pseudo-relevance feedback (PRF) to evaluate and add relevant concepts to the initial index of the userâs query. First, query expansion is performed using PN, VSM and domain knowledge. PRF is then exploited to enrich, in a second round, the userâs query by applying the same approach used in the first expansion step. To evaluate the performance of the developed system, denoted conceptual information retrieval model (CIRM), several experiments of query expansion are performed. The experiments carried out on the OHSUMED and Clinical Trials corpora showed that using the two measures of possibility and necessity combined the cosinus similarity and PRF improves the query expansion process. Indeed, the improvement rate of our approach compared with the baseline is +28, 49% in terms of P@5. Â© The Author(s) 2023.","To improve the performance of information retrieval systems (IRSs), we propose in this article a novel approach that enriches the users queries with new concepts. Indeed, query expansion is one of the best methods that plays an important role in improving searches for a better semantic information retrieval. The proposed approach in this study combines possibilistic networks (PNs), the vector space model (VSM) and pseudo-relevance feedback (PRF) to evaluate and add relevant concepts to the initial index of the users query. First, query expansion is performed using PN, VSM and domain knowledge. PRF is then exploited to enrich, in a second round, the users query by applying the same approach used in the first expansion step. To evaluate the performance of the developed system, denoted conceptual information retrieval model (CIRM), several experiments of query expansion are performed. The experiments carried out on the OHSUMED and Clinical Trials corpora showed that using the two measures of possibility and necessity combined the cosinus similarity and PRF improves the query expansion process. Indeed, the improvement rate of our approach compared with the baseline is +28, 49% in terms of P@5."
A social diagnosis mechanism for healthcare knowledge sharing,"In recent years, social networks have grown rapidly, and their applications in the healthcare domain are increasingly proposed. Using the crowd wisdom generated from social networks, we can find similar and reliable people sharing helpful experiences. The existing dedicated social networking services for health mainly focus on sharing, but not categorising and extracting. In this research, we construct an environment for social knowledge sharing and expert referring. Analysing queries from online public health databases and the factors of health similarity, social reliability and social intimacy, we extract health knowledge to recommend relevant social knowledge (also called threads) and helpful experts providing consulting. Specifically, the proposed social diagnosis mechanism helps the health seeker to identify relevant threads and recommends enthusiastic experts for healthcare support. Experimental results reveal that the proposed mechanism can effectively improve healthcare knowledge sharing and realise diagnosis support from the crowd. Â© The Author(s) 2023.","In recent years, social networks have grown rapidly, and their applications in the healthcare domain are increasingly proposed. Using the crowd wisdom generated from social networks, we can find similar and reliable people sharing helpful experiences. The existing dedicated social networking services for health mainly focus on sharing, but not categorising and extracting. In this research, we construct an environment for social knowledge sharing and expert referring. Analysing queries from online public health databases and the factors of health similarity, social reliability and social intimacy, we extract health knowledge to recommend relevant social knowledge (also called threads) and helpful experts providing consulting. Specifically, the proposed social diagnosis mechanism helps the health seeker to identify relevant threads and recommends enthusiastic experts for healthcare support. Experimental results reveal that the proposed mechanism can effectively improve healthcare knowledge sharing and realise diagnosis support from the crowd."
Digital information security management policy in academic libraries: A systematic review (2010â2022),"Digital information security management (DISM) is considered an important tool to ensure the privacy and protection of data and resources in an electronic environment. The purpose of this research is to look into the applications of DISM policies in terms of practices and implementation in academic libraries. It also identifies the challenges faced by academic libraries in applying these DISM practices regarding policy. A systematic literature review was conducted to achieve the objectives of the study. The data were collected from well-known different databases, that is, Library Information Science and Technology Abstracts (LISTA), Library and Information Science Abstracts (LISA), IEEE Xplore, Emerald Insight, ACM Digital Library, Scopus, Sage journals, Taylor & Francis, ProQuest, Science Direct, Wiley Online Library, and Google Scholar. It followed the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines to choose relevant articles with keyword searching. Some academic libraries have developed DISM policies on data protection, data backup, information security (IS) systems, the development of hardware and software, the training of library staff, data protection from malware and social engineering, and data security and privacy. A few libraries have developed a mechanism to protect and secure usersâ sensitive data from hackers, viruses, malware and social engineering. Findings indicated that both organisations and users trust libraries due to their strict privacy and data security policies. However, some academic libraries did not adopt and implement DISM policies in their organisations, even though they had written DISM policies. Libraries have been facing issues with the DISM policy on data security and privacy, data backup, IS systems, hardware and software upgrades and technical support of library staff. They also face budgetary challenges and a lack of readiness among librarians to adopt emerging tools and technology such as DISM. Library professionals faced challenges in developing and implementing the DISM policy. For data security and privacy, stakeholders, administrators and library professionals must promote the DISM policy culture in academics. This study is beneficial for library professionals, policymakers, administrators and management to make DISM policies and implement them in their organisation or libraries to secure sensitive, personal data and resources. This article is the first review of DISM policy in academic libraries of its kind and would be useful for information professionals, stakeholders and administrators. Â© The Author(s) 2023.","Digital information security management (DISM) is considered an important tool to ensure the privacy and protection of data and resources in an electronic environment. The purpose of this research is to look into the applications of DISM policies in terms of practices and implementation in academic libraries. It also identifies the challenges faced by academic libraries in applying these DISM practices regarding policy. A systematic literature review was conducted to achieve the objectives of the study. The data were collected from well-known different databases, that is, Library Information Science and Technology Abstracts (LISTA), Library and Information Science Abstracts (LISA), IEEE Xplore, Emerald Insight, ACM Digital Library, Scopus, Sage journals, Taylor & Francis, ProQuest, Science Direct, Wiley Online Library, and Google Scholar. It followed the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines to choose relevant articles with keyword searching. Some academic libraries have developed DISM policies on data protection, data backup, information security (IS) systems, the development of hardware and software, the training of library staff, data protection from malware and social engineering, and data security and privacy. A few libraries have developed a mechanism to protect and secure users sensitive data from hackers, viruses, malware and social engineering. Findings indicated that both organisations and users trust libraries due to their strict privacy and data security policies. However, some academic libraries did not adopt and implement DISM policies in their organisations, even though they had written DISM policies. Libraries have been facing issues with the DISM policy on data security and privacy, data backup, IS systems, hardware and software upgrades and technical support of library staff. They also face budgetary challenges and a lack of readiness among librarians to adopt emerging tools and technology such as DISM. Library professionals faced challenges in developing and implementing the DISM policy. For data security and privacy, stakeholders, administrators and library professionals must promote the DISM policy culture in academics. This study is beneficial for library professionals, policymakers, administrators and management to make DISM policies and implement them in their organisation or libraries to secure sensitive, personal data and resources. This article is the first review of DISM policy in academic libraries of its kind and would be useful for information professionals, stakeholders and administrators."
Is peer review duration shorter for attractive manuscripts?,"Authors often ask how long the peer review process takes. Peer review duration has attracted much attention in academia in recent years. Existing research focuses primarily on the statistical characteristics of peer review duration, with few studies considering the potential influence of manuscriptsâ attractiveness. This study aims to fill this research gap by employing attention economy theory. By analysing the peer review history of articles published in The British Medical Journal and 16 information and library science journals, we find a significant negative relationship between peer review duration and the Altmetric Attention Score. Overall, our study offers a new perspective on peer review behaviour and bridges the divide between peer reviews and altmetrics. Â© The Author(s) 2023.","Authors often ask how long the peer review process takes. Peer review duration has attracted much attention in academia in recent years. Existing research focuses primarily on the statistical characteristics of peer review duration, with few studies considering the potential influence of manuscripts attractiveness. This study aims to fill this research gap by employing attention economy theory. By analysing the peer review history of articles published in The British Medical Journal and 16 information and library science journals, we find a significant negative relationship between peer review duration and the Altmetric Attention Score. Overall, our study offers a new perspective on peer review behaviour and bridges the divide between peer reviews and altmetrics."
Knowledge visualisation and strategic decision-making outcomes in small and medium-sized enterprises,"This article examines the direct and indirect (through strategic decision-making processes) impacts of knowledge visualisation on strategic decision quality and strategic decision commitment in small- and medium-sized enterprises (SMEs). Structural equation modelling (SEM) methodology was used to test the research model based on data collected from 209 senior managers of manufacturing SMEs that operate in food production in Egypt. The results suggest that knowledge visualisation has a significant positive effect on strategic decision-making processes, strategic decision quality and strategic decision commitment. In addition, strategic decision quality is dependent on strategic decision-making processes, but decision-making processes do not significantly impact strategic decision commitment. Knowledge visualisation through strategic decision-making processes could help organisations in making high-quality decisions. The research adds to the theories of knowledge-based view (KBV) and rational decision-making by demonstrating that knowledge visualisation could be a catalyst for effective strategic decision-making. Â© The Author(s) 2023.","This article examines the direct and indirect (through strategic decision-making processes) impacts of knowledge visualisation on strategic decision quality and strategic decision commitment in small- and medium-sized enterprises (SMEs). Structural equation modelling (SEM) methodology was used to test the research model based on data collected from 209 senior managers of manufacturing SMEs that operate in food production in Egypt. The results suggest that knowledge visualisation has a significant positive effect on strategic decision-making processes, strategic decision quality and strategic decision commitment. In addition, strategic decision quality is dependent on strategic decision-making processes, but decision-making processes do not significantly impact strategic decision commitment. Knowledge visualisation through strategic decision-making processes could help organisations in making high-quality decisions. The research adds to the theories of knowledge-based view (KBV) and rational decision-making by demonstrating that knowledge visualisation could be a catalyst for effective strategic decision-making."
MNN4Rec: A relation-aware approach based on multi-view news network for news recommendation,"Personalised news recommendation comprises two crucial components: news understanding and user modelling. Previous studies have attempted to model news understanding and user interests using various internal news information and external knowledge graphs (KG). However, they have overlooked the collaborative function of the external KG and the internal information among diverse news and user behaviours, resulting in serious news cold-start problems and poor interpretability of user interests. To address these issues, this article proposes a novel approach called Relation-Aware Approach based on Multi-view News Network for News Recommendation (MNN4Rec). Specifically, MNN4Rec first constructs a Multi-view News Network (MNN), which includes candidate news and user-clicked news, and represents their exclusive multi-view information as heterogeneous nodes. Furthermore, we develop explicit and implicit news relationships and design a special sampling algorithm to search for news co-neighbours. We then use a novel dual-channel graph attention mechanism to obtain the fine-grained news understanding representation. Moreover, we construct explainable user interests by modelling the interaction of user-clicked news through the multi-headed self-attention mechanism in both semantic and relation levels. Finally, we match candidate news understanding with user interests to generate a prediction score for recommendation. Experimental results on Microsoftâs news data set MIND demonstrate that MNN4Rec outperforms existing news-recommendation methods while also mitigating the cold-start problem and enhancing the interpretability of user interests. Our code is available at https://github.com/JiangHaoPG11/MNN4Rec_code. Â© The Author(s) 2023.","Personalised news recommendation comprises two crucial components: news understanding and user modelling. Previous studies have attempted to model news understanding and user interests using various internal news information and external knowledge graphs (KG). However, they have overlooked the collaborative function of the external KG and the internal information among diverse news and user behaviours, resulting in serious news cold-start problems and poor interpretability of user interests. To address these issues, this article proposes a novel approach called Relation-Aware Approach based on Multi-view News Network for News Recommendation (MNN4Rec). Specifically, MNN4Rec first constructs a Multi-view News Network (MNN), which includes candidate news and user-clicked news, and represents their exclusive multi-view information as heterogeneous nodes. Furthermore, we develop explicit and implicit news relationships and design a special sampling algorithm to search for news co-neighbours. We then use a novel dual-channel graph attention mechanism to obtain the fine-grained news understanding representation. Moreover, we construct explainable user interests by modelling the interaction of user-clicked news through the multi-headed self-attention mechanism in both semantic and relation levels. Finally, we match candidate news understanding with user interests to generate a prediction score for recommendation. Experimental results on Microsofts news data set MIND demonstrate that MNN4Rec outperforms existing news-recommendation methods while also mitigating the cold-start problem and enhancing the interpretability of user interests. Our code is available at https://github.com/JiangHaoPG11/MNN4Rec_code."
"Image retrieval effectiveness of Bing Images, Google Images and Yahoo Image Search in the scientific field of tourism and COVID-19","The year 2020 brought a big concern for the global community because of COVID-19, which affected every sector of society, and tourism is no exception. Researchers across the globe are publishing their studies related to different dimensions of tourism in the context of COVID-19, and images have formed an essential component of their research. In tourism, images related to COVID-19 can open new dimensions for scholars. The main aim of the research is to measure the retrieval effectiveness of three image search engines (ISEs), that is, Bing Images, Google Images and Yahoo Image Search, concerning images related to COVID-19 and tourism. The study attempts to identify the capability of the ISEs to retrieve the desired and actual images related to COVID-19 and tourism. The PubMed Central (PMC) Database was consulted to retrieve the desired images and develop a testbed. The advanced search feature of PMC Database was explored by typing the search terms âCOVID-19â and âTourismâ using âANDâ operator to make the search more comprehensive. Both the terms were searched against the âFigure/Tableâ caption to retrieve papers carrying images related to COVID-19 and tourism. Queries were executed across the select ISEs, that is, Bing Images, Google Images and Yahoo Image Search. Retrieved images were individually analysed against the original image from the articles to determine the Precision, Relative Recall, F-Measure and Fallout Ratio. The format of the images in JPG/JPEG, besides checking the original image rank in the retrieved lot, was also ascertained. Bing Images scores more in terms of Mean Precision, followed by Google Images and Yahoo Image Search. For the Relative Recall measure, Google Images scores high, followed by Bing Images and Yahoo Image Search, respectively. Regarding F-Measure and Fallout Ratio, Bing Images outperforms Google Images and Yahoo Image Search. In retrieving the sought format of JPG/JPEG, Google Images performs best, followed by Yahoo Image Search and Bing Images. Google Images produces the original image at the first rank on more than one occasion. In contrast, Bing Images retrieves the original image at the first rank in two instances. Yahoo Images performs poorly over this metric as it does not retrieve any original image at the first rank on any other instance. The study cannot be generalised as the scope is only limited to the images indexed by PMC. Furthermore, the retrieval effectiveness of only three ISEs is measured. The study is the first to measure the retrieval effectiveness of ISEs in retrieving images related to the COVID-19 pandemic and tourism. The study can be extended across other image-indexing databases pertinent to tourism studies, and the retrieval effectiveness of other ISEs can also be considered. Â© The Author(s) 2023.","The year 2020 brought a big concern for the global community because of COVID-19, which affected every sector of society, and tourism is no exception. Researchers across the globe are publishing their studies related to different dimensions of tourism in the context of COVID-19, and images have formed an essential component of their research. In tourism, images related to COVID-19 can open new dimensions for scholars. The main aim of the research is to measure the retrieval effectiveness of three image search engines (ISEs), that is, Bing Images, Google Images and Yahoo Image Search, concerning images related to COVID-19 and tourism. The study attempts to identify the capability of the ISEs to retrieve the desired and actual images related to COVID-19 and tourism. The PubMed Central (PMC) Database was consulted to retrieve the desired images and develop a testbed. The advanced search feature of PMC Database was explored by typing the search terms COVID-19 and Tourism using AND operator to make the search more comprehensive. Both the terms were searched against the Figure/Table caption to retrieve papers carrying images related to COVID-19 and tourism. Queries were executed across the select ISEs, that is, Bing Images, Google Images and Yahoo Image Search. Retrieved images were individually analysed against the original image from the articles to determine the Precision, Relative Recall, F-Measure and Fallout Ratio. The format of the images in JPG/JPEG, besides checking the original image rank in the retrieved lot, was also ascertained. Bing Images scores more in terms of Mean Precision, followed by Google Images and Yahoo Image Search. For the Relative Recall measure, Google Images scores high, followed by Bing Images and Yahoo Image Search, respectively. Regarding F-Measure and Fallout Ratio, Bing Images outperforms Google Images and Yahoo Image Search. In retrieving the sought format of JPG/JPEG, Google Images performs best, followed by Yahoo Image Search and Bing Images. Google Images produces the original image at the first rank on more than one occasion. In contrast, Bing Images retrieves the original image at the first rank in two instances. Yahoo Images performs poorly over this metric as it does not retrieve any original image at the first rank on any other instance. The study cannot be generalised as the scope is only limited to the images indexed by PMC. Furthermore, the retrieval effectiveness of only three ISEs is measured. The study is the first to measure the retrieval effectiveness of ISEs in retrieving images related to the COVID-19 pandemic and tourism. The study can be extended across other image-indexing databases pertinent to tourism studies, and the retrieval effectiveness of other ISEs can also be considered."
Investigating the components of bibliotherapy implementation process in digital hospital libraries of Iran: Survey analysis,"The study aimed to identify the components of bibliotherapy implementation in digital hospital libraries of Iran. The research method regarding purpose is applied and regarding methodology is survey. This showed the desired level of reliability was human resources, information resources, technology and technology infrastructure, services, copyright and financial resource. The most important basic requirements for the implementation of this plan in hospital digital libraries were as follows: Method of providing information resources: âCollection of various digital resources to launch bibliotherapy (audiovisual resources, audio, video, etc.)â; the most desirable people to provide information resources: âLibrarians of digital libraries in consultation with physicians and consultantsâ; digital resource prioritisation: âMultimedia Resourcesâ; services for users: âProviding digital information resources, suitable for any age group in the field of bibliotherapyâ; legal barriers: âThe unwillingness of authors and publishers to support free access and at the same time protect the copyright of digital resources in the field of bibliotherapyâ; and economic barriers: âImpossibility to provide the necessary funding to provide bibliotherapy related services in hospital digital librariesâ. Its conceptual and structural model, design and research results indicate a proper fit of the components. Based on the findings of the present study, it can be concluded that the components of bibliotherapy implementation in hospital digital libraries can be classified into six sections in terms of importance and effectiveness: technical and technological infrastructure, services, financial resources, information resources, copyright and human resources. Â© The Author(s) 2023.","The study aimed to identify the components of bibliotherapy implementation in digital hospital libraries of Iran. The research method regarding purpose is applied and regarding methodology is survey. This showed the desired level of reliability was human resources, information resources, technology and technology infrastructure, services, copyright and financial resource. The most important basic requirements for the implementation of this plan in hospital digital libraries were as follows: Method of providing information resources: Collection of various digital resources to launch bibliotherapy (audiovisual resources, audio, video, etc.); the most desirable people to provide information resources: Librarians of digital libraries in consultation with physicians and consultants; digital resource prioritisation: Multimedia Resources; services for users: Providing digital information resources, suitable for any age group in the field of bibliotherapy; legal barriers: The unwillingness of authors and publishers to support free access and at the same time protect the copyright of digital resources in the field of bibliotherapy; and economic barriers: Impossibility to provide the necessary funding to provide bibliotherapy related services in hospital digital libraries. Its conceptual and structural model, design and research results indicate a proper fit of the components. Based on the findings of the present study, it can be concluded that the components of bibliotherapy implementation in hospital digital libraries can be classified into six sections in terms of importance and effectiveness: technical and technological infrastructure, services, financial resources, information resources, copyright and human resources."
"A fieldwork manual as a regulatory device: Instructing, prescribing and describing documentation work","Research on how archaeological fieldwork manuals, a sub-category of methods handbooks, regulate research documentation is limited. Qualitative content analysis of 25 English-language archaeological field manuals from the early 1900s to 2010s showed that they instruct how to describe the documentation work, prescribe practices and workflows, and function as often pre-coordinated descriptions of work. A manual forms a âworking spaceâ that is sometimes adopted as such by following the detailed advice given in some of the texts but likely more often used as a more general point of reference. The fact that many manuals do not provide exact recipes for the fieldwork as a whole means that they function as comprehensive representations and documentation (paradata) of actual fieldwork practices only when read in parallel with field documentation. Â© The Author(s) 2023.","Research on how archaeological fieldwork manuals, a sub-category of methods handbooks, regulate research documentation is limited. Qualitative content analysis of 25 English-language archaeological field manuals from the early 1900s to 2010s showed that they instruct how to describe the documentation work, prescribe practices and workflows, and function as often pre-coordinated descriptions of work. A manual forms a working space that is sometimes adopted as such by following the detailed advice given in some of the texts but likely more often used as a more general point of reference. The fact that many manuals do not provide exact recipes for the fieldwork as a whole means that they function as comprehensive representations and documentation (paradata) of actual fieldwork practices only when read in parallel with field documentation."
The effect of digital literacy on technology acceptance: An evaluation on administrative staff in higher education,"Digital literacy is a critical skill that administrative staff must acquire to continue their business activities at higher education. However, digital literacy studies for administrative staff seem to be neglected in the current literature. This article examines the impact of higher education administrative staffâs digital literacy on their intention to use digital technologies while performing their tasks. For this purpose, a conceptual model consisting of effort expectancy and performance expectancy structures based on the unified theory of acceptance and use of technology and expanded with the digital literacy dimension has been created. Data were collected from 158 participants who were administrative staff of two higher education institutions in TÃ¼rkiye to evaluate the theoretical model. The data were analysed using the structural equation modelling technique. The findings revealed the relationship between the digital literacy skills of higher education administrative staff and their intention to use digital technology. According to the results, digital literacy has a direct effect on effort expectancy but not on performance expectancy. Also, contrary to our expectations, digital literacy does not directly affect the intention to use. However, digital literacy affects the intention to use digital technology through effort expectancy and performance expectancy. In higher education, personnel with low digital literacy skills should be identified and in-service training should be provided. This is one of the first studies to address the impact of digital literacy on technology acceptance by administrative staff working in higher education. Â© The Author(s) 2023.","Digital literacy is a critical skill that administrative staff must acquire to continue their business activities at higher education. However, digital literacy studies for administrative staff seem to be neglected in the current literature. This article examines the impact of higher education administrative staffs digital literacy on their intention to use digital technologies while performing their tasks. For this purpose, a conceptual model consisting of effort expectancy and performance expectancy structures based on the unified theory of acceptance and use of technology and expanded with the digital literacy dimension has been created. Data were collected from 158 participants who were administrative staff of two higher education institutions in Trkiye to evaluate the theoretical model. The data were analysed using the structural equation modelling technique. The findings revealed the relationship between the digital literacy skills of higher education administrative staff and their intention to use digital technology. According to the results, digital literacy has a direct effect on effort expectancy but not on performance expectancy. Also, contrary to our expectations, digital literacy does not directly affect the intention to use. However, digital literacy affects the intention to use digital technology through effort expectancy and performance expectancy. In higher education, personnel with low digital literacy skills should be identified and in-service training should be provided. This is one of the first studies to address the impact of digital literacy on technology acceptance by administrative staff working in higher education."
Is adopting artificial intelligence in libraries urgency or a buzzword? A systematic literature review,"This study aims to investigate the implementation of artificial intelligence (AI) in libraries from 2011 to 2020. This study uses PRISMA guidelines to perform a systematic literature review (SLR). The articles were obtained mainly from the SCOPUS database, with Google Scholar as the supporting database. AI can easily be adopted in libraries, especially for technical services such as classification and cataloguing, library management such as staffing and decision-making, library services such as referencing and information service, and for information literacy. Successful AI adoption is, however, still debatable, because there are many requirements that need to be met, so that it can be inclusively adopted in libraries. There is a lack of research on the application of AI in libraries, especially in the context of its actual implementation. The results of this study offer insights on the implementation of AI in library support services. Â© The Author(s) 2023.","This study aims to investigate the implementation of artificial intelligence (AI) in libraries from 2011 to 2020. This study uses PRISMA guidelines to perform a systematic literature review (SLR). The articles were obtained mainly from the SCOPUS database, with Google Scholar as the supporting database. AI can easily be adopted in libraries, especially for technical services such as classification and cataloguing, library management such as staffing and decision-making, library services such as referencing and information service, and for information literacy. Successful AI adoption is, however, still debatable, because there are many requirements that need to be met, so that it can be inclusively adopted in libraries. There is a lack of research on the application of AI in libraries, especially in the context of its actual implementation. The results of this study offer insights on the implementation of AI in library support services."
Discovering research data management trends from job advertisements using a text-mining approach,"In todayâs data-driven culture, research data management (RDM) is essential for the research community. The demand for reusing research datasets is a challenging and diverse process for the scientific community. Despite this, it is essential in RDM to discover trends and themes using text mining, which is scarce. The purpose of this study is to employ text mining to discover insights from job advertisements associated with RDM profiles, which collected 810 advertisements. We found RDM-related patterns using latent Dirichlet allocation (LDA) and identified three key contexts. The first is âresearch services in librariesâ, with the topics of research services, research information, research universities, collection processes and library services. The second context is âresearch dataâ, which includes RDM, business data, university data, research data, health research, science research, social science research, data centres, data services, statistical software, digital scholarship and digital preservation. The third context is âworkplace environmentâ, and the topics are leadership, work development and scientific position. Job title normalisation reveals names such as âdata librarianâ, âlibrarianâ, âdirectorâ, âdata curatorâ, âdata managerâ, âresearch data librarianâ, âdata specialistâ and âdata officerâ are frequently employed. Focusing on titles with a single or double occurrence is new and interesting for developing nations. Reputable institutions such as Harvard, Stanford and the Massachusetts Institute of Technology, as well as countries such as the United States, the United Kingdom, Canada and Germany, are the major participants in RDM practises and services. This discovery will assist higher education institutions, RDM stakeholders, which aid in the formulation of curriculum, and job seekers to familiarise themselves with the themes. Â© The Author(s) 2023.","In todays data-driven culture, research data management (RDM) is essential for the research community. The demand for reusing research datasets is a challenging and diverse process for the scientific community. Despite this, it is essential in RDM to discover trends and themes using text mining, which is scarce. The purpose of this study is to employ text mining to discover insights from job advertisements associated with RDM profiles, which collected 810 advertisements. We found RDM-related patterns using latent Dirichlet allocation (LDA) and identified three key contexts. The first is research services in libraries, with the topics of research services, research information, research universities, collection processes and library services. The second context is research data, which includes RDM, business data, university data, research data, health research, science research, social science research, data centres, data services, statistical software, digital scholarship and digital preservation. The third context is workplace environment, and the topics are leadership, work development and scientific position. Job title normalisation reveals names such as data librarian, librarian, director, data curator, data manager, research data librarian, data specialist and data officer are frequently employed. Focusing on titles with a single or double occurrence is new and interesting for developing nations. Reputable institutions such as Harvard, Stanford and the Massachusetts Institute of Technology, as well as countries such as the United States, the United Kingdom, Canada and Germany, are the major participants in RDM practises and services. This discovery will assist higher education institutions, RDM stakeholders, which aid in the formulation of curriculum, and job seekers to familiarise themselves with the themes."
Quantifying scientistsâ research ability by taking institutionsâ scientific impact as priori information,"Scholar performance evaluation is extremely important in research assessment decisions, such as funding allocation, academic rankings, and academic promotion. In this article, we propose the institution Q model (IQ) and its two variants (IQ-2 and IQ-3), which aim to evaluate the individual-level research ability to publish high-quality scientific papers. Specifically, our models integrate scientistsâ institutions, countries and collaborators as valuable prior information and jointly evaluate the research ability of scientists from different institutions. To estimate model parameters and hidden variables defined in our models, we propose a generic BBVI-EM algorithm. To test the effectiveness of our models, we examine their performance on the synthetic data and the empirical data (17,750/26,992 scientists in the computer science/physics field). We find that our models can more accurately quantify the research ability of scientists and institutions and more effectively predict scientistsâ scientific impact (the h-index and total citations) than the Q model and common machine learning models. In conclusion, our models are effective evaluation and prediction tools for quantifying research ability and predicting the scientific impact, and the BBVI-EM algorithm is an effective variational inference algorithm. This study makes a theoretical contribution to broaden the idea of incorporating the academic environment into scientific evaluation. Â© The Author(s) 2023.","Scholar performance evaluation is extremely important in research assessment decisions, such as funding allocation, academic rankings, and academic promotion. In this article, we propose the institution Q model (IQ) and its two variants (IQ-2 and IQ-3), which aim to evaluate the individual-level research ability to publish high-quality scientific papers. Specifically, our models integrate scientists institutions, countries and collaborators as valuable prior information and jointly evaluate the research ability of scientists from different institutions. To estimate model parameters and hidden variables defined in our models, we propose a generic BBVI-EM algorithm. To test the effectiveness of our models, we examine their performance on the synthetic data and the empirical data (17,750/26,992 scientists in the computer science/physics field). We find that our models can more accurately quantify the research ability of scientists and institutions and more effectively predict scientists scientific impact (the h-index and total citations) than the Q model and common machine learning models. In conclusion, our models are effective evaluation and prediction tools for quantifying research ability and predicting the scientific impact, and the BBVI-EM algorithm is an effective variational inference algorithm. This study makes a theoretical contribution to broaden the idea of incorporating the academic environment into scientific evaluation."
A deep CNN architecture with novel pooling layer applied to two Sudanese Arabic sentiment data sets,"Arabic sentiment analysis has become an important research field in recent years. Initially, work focused on Modern Standard Arabic (MSA), which is the most widely used form. Since then, work has been carried out on several different dialects, including Egyptian, Levantine and Moroccan. Moreover, a number of data sets have been created to support such work. However, up until now, no work has been carried out on Sudanese Arabic, a dialect which has 32 million speakers. In this article, two new public data sets are introduced, the two-class Sudanese Sentiment Data set (SudSenti2) and the three-class Sudanese Sentiment Data set (SudSenti3). In the preparation phase, we establish a Sudanese stopword list. Furthermore, a convolutional neural network (CNN) architecture, Sentiment Convolutional MMA (SCM), is proposed, comprising five CNN layers together with a novel Mean Max Average (MMA) pooling layer, to extract the best features. This SCM model is applied to SudSenti2 and SudSenti3 and shown to be superior to the baseline models, with accuracies of 92.25% and 85.23% (Experiments 1 and 2). The performance of MMA is compared with Max, Avg and Min and shown to be better on SudSenti2, the Saudi Sentiment Data set and the MSA Hotel Arabic Review Data set by 1.00%, 0.83% and 0.74%, respectively (Experiment 3). Next, we conduct an ablation study to determine the contribution to performance of text normalisation and the Sudanese stopword list (Experiment 4). For normalisation, this makes a difference of 0.43% on two-class and 0.45% on three-class. For the custom stoplist, the differences are 0.82% and 0.72%, respectively. Finally, the model is compared with other deep learning classifiers, including transformer-based language models for Arabic, and shown to be comparable for SudSenti2 (Experiment 5). Â© The Author(s) 2023.","Arabic sentiment analysis has become an important research field in recent years. Initially, work focused on Modern Standard Arabic (MSA), which is the most widely used form. Since then, work has been carried out on several different dialects, including Egyptian, Levantine and Moroccan. Moreover, a number of data sets have been created to support such work. However, up until now, no work has been carried out on Sudanese Arabic, a dialect which has 32 million speakers. In this article, two new public data sets are introduced, the two-class Sudanese Sentiment Data set (SudSenti2) and the three-class Sudanese Sentiment Data set (SudSenti3). In the preparation phase, we establish a Sudanese stopword list. Furthermore, a convolutional neural network (CNN) architecture, Sentiment Convolutional MMA (SCM), is proposed, comprising five CNN layers together with a novel Mean Max Average (MMA) pooling layer, to extract the best features. This SCM model is applied to SudSenti2 and SudSenti3 and shown to be superior to the baseline models, with accuracies of 92.25% and 85.23% (Experiments 1 and 2). The performance of MMA is compared with Max, Avg and Min and shown to be better on SudSenti2, the Saudi Sentiment Data set and the MSA Hotel Arabic Review Data set by 1.00%, 0.83% and 0.74%, respectively (Experiment 3). Next, we conduct an ablation study to determine the contribution to performance of text normalisation and the Sudanese stopword list (Experiment 4). For normalisation, this makes a difference of 0.43% on two-class and 0.45% on three-class. For the custom stoplist, the differences are 0.82% and 0.72%, respectively. Finally, the model is compared with other deep learning classifiers, including transformer-based language models for Arabic, and shown to be comparable for SudSenti2 (Experiment 5)."
Transforming metadata content guidelines and instructions to linked data,"Among metadata-related standards, data content standards like metadata guidelines and instructions for creating metadata still remain in legacy forms. This study investigates a way to transform data content standards to linked data (LD) through conversion from other formats, while referring to the proposed layered framework. Under the basic policies for making LD on which this study is based, several principal matters were examined: (a) defining units to be assigned with Universal Resource Identifiers (URIs), (b) defining relationships among the instructions with URIs and (c) expressing instructed content in instructions properly with certain Resource Description Framework (RDF) properties. With the proper choice(s) for each matter, some actual standards were converted to LD: Resource Description and Access (RDA) and Dublin Core User Guide. The results showed that the adopted way of transforming data content standards to LD is valid and proper, and the resultant LD would be expected to be utilised in various manners. Â© The Author(s) 2023.","Among metadata-related standards, data content standards like metadata guidelines and instructions for creating metadata still remain in legacy forms. This study investigates a way to transform data content standards to linked data through conversion from other formats, while referring to the proposed layered framework. Under the basic policies for making LD on which this study is based, several principal matters were examined: (a) defining units to be assigned with Universal Resource Identifiers (URIs), (b) defining relationships among the instructions with URIs and expressing instructed content in instructions properly with certain Resource Description Framework (RDF) properties. With the proper choice(s) for each matter, some actual standards were converted to LD: Resource Description and Access (RDA) and Dublin Core User Guide. The results showed that the adopted way of transforming data content standards to LD is valid and proper, and the resultant LD would be expected to be utilised in various manners."
AIRank: An algorithm on evaluating the academic influence of papers based on heterogeneous academic network,"Evaluation of papersâ academic influence is a hot issue in the field of scientific research management. Academic big data provides a data treasure with the coexistence of different types of academic entities, which can be used to evaluate academic influence from a more macro and comprehensive perspective. Based on academic big data, a heterogeneous academic network composed of links within and between three types of academic entities (authors, papers and venues) is constructed. In addition, a new academic influence ranking algorithm, AIRank, is proposed to evaluate papersâ academic influence. Different from the existing academic influence ranking algorithms, AIRank has made innovations in the following two aspects. (1) AIRank distinguishes the influence transmission intensity between different node pairs. Different from the strategy of evenly distributing influence among different node pairs, AIRank quantifies the intensity of influence transmission between node pairs based on investigating the citation emotional attribute, semantic similarity and academic quality differences between node pairs. Based on the intensity characteristics, AIRank realises the distribution and transmission of influence among different node pairs. (2) AIRank incorporates the influence transmission from heterogeneous neighbours in evaluating papersâ influence. According to the academic influence of author nodes and venue nodes, AIRank fine-tunes the iteration formula of paper influence to obtain the ranking of papers under the joint influence of homogeneous and heterogeneous neighbours. Experimental results show that, compared with the ranking results based on citation frequency and PageRank algorithm, AIRank algorithm can produce more differentiated and reasonable academic influence ranking results. Â© The Author(s) 2023.","Evaluation of papers academic influence is a hot issue in the field of scientific research management. Academic big data provides a data treasure with the coexistence of different types of academic entities, which can be used to evaluate academic influence from a more macro and comprehensive perspective. Based on academic big data, a heterogeneous academic network composed of links within and between three types of academic entities (authors, papers and venues) is constructed. In addition, a new academic influence ranking algorithm, AIRank, is proposed to evaluate papers academic influence. Different from the existing academic influence ranking algorithms, AIRank has made innovations in the following two aspects. AIRank distinguishes the influence transmission intensity between different node pairs. Different from the strategy of evenly distributing influence among different node pairs, AIRank quantifies the intensity of influence transmission between node pairs based on investigating the citation emotional attribute, semantic similarity and academic quality differences between node pairs. Based on the intensity characteristics, AIRank realises the distribution and transmission of influence among different node pairs. AIRank incorporates the influence transmission from heterogeneous neighbours in evaluating papers influence. According to the academic influence of author nodes and venue nodes, AIRank fine-tunes the iteration formula of paper influence to obtain the ranking of papers under the joint influence of homogeneous and heterogeneous neighbours. Experimental results show that, compared with the ranking results based on citation frequency and PageRank algorithm, AIRank algorithm can produce more differentiated and reasonable academic influence ranking results."
Strategies for information source selection: A focus group study on young people in Europe,"The article presents findings from a study that examined how young people select, consult and evaluate multiple information sources to validate the information they seek. It contributes to the field of information behaviour and helps in designing better information services. Eight focus group interviews were carried out in four different locations across Europe. A total of 37 young people participated through purposive sampling. The study illustrates participantsâ complex information pathways through which they consult multiple sources to reach the most trusted information source. The content analysis of the data showed that ascribed cognitive authority and affective factors such as confidentiality, privacy and empathy strongly determine the selection of an information source. The study observes the young participantsâ dependency on networked and human sources for ease of access and reluctance to rely on mainstream media and textual information. The study has strong practical implications for designing information services and developing communication materials targeted at young people. Â© The Author(s) 2023.","The article presents findings from a study that examined how young people select, consult and evaluate multiple information sources to validate the information they seek. It contributes to the field of information behaviour and helps in designing better information services. Eight focus group interviews were carried out in four different locations across Europe. A total of 37 young people participated through purposive sampling. The study illustrates participants complex information pathways through which they consult multiple sources to reach the most trusted information source. The content analysis of the data showed that ascribed cognitive authority and affective factors such as confidentiality, privacy and empathy strongly determine the selection of an information source. The study observes the young participants dependency on networked and human sources for ease of access and reluctance to rely on mainstream media and textual information. The study has strong practical implications for designing information services and developing communication materials targeted at young people."
Empowering the middle management: Incorporating data summarisation and visualisation techniques in management information systems,"Humanâcomputer interaction (HCI) researchers and practitioners have explored how computers and software can aid users in decision-making. Accurate information is crucial in decision-making; therefore, when designing a management information system (MIS), it is necessary to understand the information needs of the end-users and to incorporate this understanding in a way which allows the users to obtain an accurate and intuitive snapshot of the data. Although most of the day-to-day decisions in various organisations are made by middle management employees (as opposed to executives and senior managers), previous literature has scarcely examined how middle management employeesâ decision-making process can be improved through the use of MIS. In this study, we examine the impact of different data summarisation and visualisation layouts on different aspects of the decision-making process taking into consideration the decision-makersâ personal characteristics, and propose to include the results of this examination in the MIS design. To this end, we recruited participants from a crowdsourcing platform. Participants were required to complete a task which simulates middle management employeesâ day-to-day operational decision-making scenario. To better understand the effects of participantsâ personal characteristics and their visual abilities, participants were asked to answer a personality questionnaire and a questionnaire testing for visual abilities. The results indicate that when looking at the population as a single group, the effect of the data summarisation and visualisation layouts on the decision-making process cannot be discerned. However, when taking into consideration additional user characteristics, the results indicate that the data summarisation and visualisation layouts affect the decisions made. Â© The Author(s) 2023.","Humancomputer interaction (HCI) researchers and practitioners have explored how computers and software can aid users in decision-making. Accurate information is crucial in decision-making; therefore, when designing a management information system (MIS), it is necessary to understand the information needs of the end-users and to incorporate this understanding in a way which allows the users to obtain an accurate and intuitive snapshot of the data. Although most of the day-to-day decisions in various organisations are made by middle management employees (as opposed to executives and senior managers), previous literature has scarcely examined how middle management employees decision-making process can be improved through the use of MIS. In this study, we examine the impact of different data summarisation and visualisation layouts on different aspects of the decision-making process taking into consideration the decision-makers personal characteristics, and propose to include the results of this examination in the MIS design. To this end, we recruited participants from a crowdsourcing platform. Participants were required to complete a task which simulates middle management employees day-to-day operational decision-making scenario. To better understand the effects of participants personal characteristics and their visual abilities, participants were asked to answer a personality questionnaire and a questionnaire testing for visual abilities. The results indicate that when looking at the population as a single group, the effect of the data summarisation and visualisation layouts on the decision-making process cannot be discerned. However, when taking into consideration additional user characteristics, the results indicate that the data summarisation and visualisation layouts affect the decisions made."
TsEDM: A two-steps event detection model for Chinese event detection,"Event detection (ED) consists of two phases â trigger identification (TI) and trigger classification (TC). Traditional ED adopts a unified model to process the above two-stage tasks at once. We argue that there are certain differences in the contextual semantics required and the goals of these two phases in ED. In which, TI remains suffers from the word-trigger mismatch problems in languages without natural word delimiters such as Chinese. And the TC is facing challenging problems of trigger ambiguity and multiple triggers in a sentence. In this article, we propose a brand-new two-steps event detection model (TsEDM), which attempts to alleviate above-mentioned problems. Specifically, a novel âhead-tail dual-pointerâ (HT-DP) labelling strategy is developed to obtain more candidate triggers to overcome the problems of continuous labelling, nested labelling and independent labelling in the first step (TI). Besides, an âentityâtopicâcandidateâtriggerâ interaction graph (E2T-IG) is constructed in the second step (TC) to consider the interaction relationship between candidate triggers and core information inter or in all event sentences, which enhance the representation of each candidate trigger. Last but not least, a shake-gated and residual-based atrous convolution neural network (SGR-ACNN) is proposed as the common framework of these two steps, which dynamically integrates various representations as model inputs. Experiments on the ACE2005-CN show that TsEDM significantly outperforms state-of-the-art (SOTA) methods. Â© The Author(s) 2023.","Event detection (ED) consists of two phases trigger identification (TI) and trigger classification (TC). Traditional ED adopts a unified model to process the above two-stage tasks at once. We argue that there are certain differences in the contextual semantics required and the goals of these two phases in ED. In which, TI remains suffers from the word-trigger mismatch problems in languages without natural word delimiters such as Chinese. And the TC is facing challenging problems of trigger ambiguity and multiple triggers in a sentence. In this article, we propose a brand-new two-steps event detection model (TsEDM), which attempts to alleviate above-mentioned problems. Specifically, a novel head-tail dual-pointer (HT-DP) labelling strategy is developed to obtain more candidate triggers to overcome the problems of continuous labelling, nested labelling and independent labelling in the first step (TI). Besides, an entitytopiccandidatetrigger interaction graph (E2T-IG) is constructed in the second step (TC) to consider the interaction relationship between candidate triggers and core information inter or in all event sentences, which enhance the representation of each candidate trigger. Last but not least, a shake-gated and residual-based atrous convolution neural network (SGR-ACNN) is proposed as the common framework of these two steps, which dynamically integrates various representations as model inputs. Experiments on the ACE2005-CN show that TsEDM significantly outperforms state-of-the-art (SOTA) methods."
Modelling in engineering: A citation context analysis,"Purely quantitative citation measures are widely used to evaluate research grants, to compare the output of researcher or to benchmark universities. The intuition that not all citations are the same, however, can be illustrated by two examples. First, studies have shown that erroneous or controversial papers have higher citation counts. Second, does a high-level citation in an introduction have the same impact as a reference to a paper that serves as a conceptual starting point? Companions to purely quantitative measures are the so-called citation context analyses which aim to obtain a better understanding of the link between citing and cited work. In this article, we propose a classification scheme for citation context analysis in the field of modelling in engineering. The categories were defined based on an extensive literature review and input from experts in the field of modelling. We propose a detailed scheme with six categories (Perfunctory, Background Information, Comparing/Confirming, Critique/Refutation, Inspiring, Using/Expanding) and a simplified scheme with three categories (High-level, Critical Analysis, Extending) that can be used within automatic classification approaches. The results of manually classifying 129 randomly selected citations show that 87% of citations fall into the high-level category. This study confirms that critical citations are not common in written academic discourse, even though criticism is essential for scientific progress and knowledge construction. Â© The Author(s) 2023.","Purely quantitative citation measures are widely used to evaluate research grants, to compare the output of researcher or to benchmark universities. The intuition that not all citations are the same, however, can be illustrated by two examples. First, studies have shown that erroneous or controversial papers have higher citation counts. Second, does a high-level citation in an introduction have the same impact as a reference to a paper that serves as a conceptual starting point? Companions to purely quantitative measures are the so-called citation context analyses which aim to obtain a better understanding of the link between citing and cited work. In this article, we propose a classification scheme for citation context analysis in the field of modelling in engineering. The categories were defined based on an extensive literature review and input from experts in the field of modelling. We propose a detailed scheme with six categories (Perfunctory, Background Information, Comparing/Confirming, Critique/Refutation, Inspiring, Using/Expanding) and a simplified scheme with three categories (High-level, Critical Analysis, Extending) that can be used within automatic classification approaches. The results of manually classifying 129 randomly selected citations show that 87% of citations fall into the high-level category. This study confirms that critical citations are not common in written academic discourse, even though criticism is essential for scientific progress and knowledge construction."
"Knowledge diffusion for individual literature from the perspective of Altmetrics: Models, measurement and features","This article studies the effect of online social media on knowledge diffusion. In this article, an SIS (SusceptibleâInfectiousâSusceptible) model was established to chase the knowledge diffusion and identify the proper model for knowledge diffusion of individual literature from the perspective of Altmetrics. Based on the data collected from the PLoS ONE database, the transition probabilities of each paper were calculated, and the papers were divided into six groups according to the transition probabilities using a K-means algorithm. This article explores the impacts of transition probabilities and summarises the similarities and differences of the patterns of knowledge diffusion of each group from the perspective of Altmetrics. Research results showed that the SIS model could be used to describe the knowledge diffusion of individual literature from the perspective of Altmetrics. Besides, the classification method proposed in this article could also be applied in future informetric research. In addition, this article also contributes to the practitioners of knowledge diffusion and online platforms. Â© The Author(s) 2023.","This article studies the effect of online social media on knowledge diffusion. In this article, an SIS (SusceptibleInfectiousSusceptible) model was established to chase the knowledge diffusion and identify the proper model for knowledge diffusion of individual literature from the perspective of Altmetrics. Based on the data collected from the PLoS ONE database, the transition probabilities of each paper were calculated, and the papers were divided into six groups according to the transition probabilities using a K-means algorithm. This article explores the impacts of transition probabilities and summarises the similarities and differences of the patterns of knowledge diffusion of each group from the perspective of Altmetrics. Research results showed that the SIS model could be used to describe the knowledge diffusion of individual literature from the perspective of Altmetrics. Besides, the classification method proposed in this article could also be applied in future informetric research. In addition, this article also contributes to the practitioners of knowledge diffusion and online platforms."
Discovering fans and anti-fans among social media users based on their emotional reactions and comments,"While audience profiling has been a critical concern in social media marketing, little research has used a systematic methodology to identify fans and anti-fans in social media communities. This study aimed to develop a fan and anti-fan detection model by analysing social media usersâ mood responses and comments on fan page posts. The sentiment analysis of comments was conducted using a bidirectional long short-term memory (LSTM) model. A total of 83 posts, 849,820 emoticons and 216,688 comments were collected from two different fan pages over 14 days. Results showed that the proposed model, combining emotional reaction analysis and sentiment analysis of their comments, exhibited better fan and anti-fan identification capability than the single-dimensional behaviour model. It exhibited 96% accuracy, 100% precision and 93% recall in terms of community management. This study provides a novel, accurate and efficient way to identify fans and anti-fans that can help form more targeted marketing strategies for social media managers in a cost-effective manner. Â© The Author(s) 2023.","While audience profiling has been a critical concern in social media marketing, little research has used a systematic methodology to identify fans and anti-fans in social media communities. This study aimed to develop a fan and anti-fan detection model by analysing social media users mood responses and comments on fan page posts. The sentiment analysis of comments was conducted using a bidirectional long short-term memory (LSTM) model. A total of 83 posts, 849,820 emoticons and 216,688 comments were collected from two different fan pages over 14 days. Results showed that the proposed model, combining emotional reaction analysis and sentiment analysis of their comments, exhibited better fan and anti-fan identification capability than the single-dimensional behaviour model. It exhibited 96% accuracy, 100% precision and 93% recall in terms of community management. This study provides a novel, accurate and efficient way to identify fans and anti-fans that can help form more targeted marketing strategies for social media managers in a cost-effective manner."
Joint entityârelation extraction for natural disaster based on table filling,"In this article, we propose a joint extraction of entityârelation triplets in natural disaster cases based on word pair relation table filling. For computational accuracy concerns or other reasons, traditional works often do entity recognition and relation extraction separately; it might put less attention over the task connections and triplet global association. We propose the Global Table Attention GRU (GL-TGRU) model as a joint approach that uses sequence information encoding and table information encoding to jointly learn the representation and enhance the global association of entity and relation in table filling. We evaluated the proposed model on the public data set SciERC and the natural disaster data set SSD-HDS, respectively. The F1 scores of experimental results of the GL-TGRU model for entity identification and relation extraction achieved 65.81% and 37.30% on SciERC and achieved 93.89% and 84.06% on SSD-HDS. The results show our model helps to capture more global association relations of entity and relation, which can better identify the entityârelation triplet information. Â© The Author(s) 2023.","In this article, we propose a joint extraction of entityrelation triplets in natural disaster cases based on word pair relation table filling. For computational accuracy concerns or other reasons, traditional works often do entity recognition and relation extraction separately; it might put less attention over the task connections and triplet global association. We propose the Global Table Attention GRU (GL-TGRU) model as a joint approach that uses sequence information encoding and table information encoding to jointly learn the representation and enhance the global association of entity and relation in table filling. We evaluated the proposed model on the public data set SciERC and the natural disaster data set SSD-HDS, respectively. The F1 scores of experimental results of the GL-TGRU model for entity identification and relation extraction achieved 65.81% and 37.30% on SciERC and achieved 93.89% and 84.06% on SSD-HDS. The results show our model helps to capture more global association relations of entity and relation, which can better identify the entityrelation triplet information."
ICT literacy and community connectedness in a Hispanic-majority community,"While many studies have examined information and communication technology (ICT) among digital natives, there has been little investigation of the ICT literacy skills among digital immigrants, individuals who were raised in a time before the widespread use of digital technology. This study examined the ICT literacy skills of Hispanic-majority communities relative to measures of community connectedness, Internet use and demographics of individuals. The researchers made use of a survey method in collecting the data from participants involved in the study. Data collected were later analyzed using regression and correlation analysis techniques with the help of the SPSS tool, to find the relationship and effect among variables under study. The findings suggest that it is exposure to technology, not age, gender, ethnic, or educational factors, that is responsible for the development of greater ICT literacy skills. As discovered in the study, factors like age, ethnicity, educational attainment and political beliefs have no significant effect on ICT literacy. Â© The Author(s) 2023.","While many studies have examined information and communication technology (ICT) among digital natives, there has been little investigation of the ICT literacy skills among digital immigrants, individuals who were raised in a time before the widespread use of digital technology. This study examined the ICT literacy skills of Hispanic-majority communities relative to measures of community connectedness, Internet use and demographics of individuals. The researchers made use of a survey method in collecting the data from participants involved in the study. Data collected were later analyzed using regression and correlation analysis techniques with the help of the SPSS tool, to find the relationship and effect among variables under study. The findings suggest that it is exposure to technology, not age, gender, ethnic, or educational factors, that is responsible for the development of greater ICT literacy skills. As discovered in the study, factors like age, ethnicity, educational attainment and political beliefs have no significant effect on ICT literacy."
Technology acceptance research: Meta-analysis,"Rapid digitalisation has resulted in a literature about technology acceptance that is ever increasing in size, naturally creating debates about the developments in the field and their implications. Given the size of the literature and the range of factors, theories and applications considered, this article reviewed the relevant literature using a meta-analytical approach. The objective of this review was twofold: (a) to provide a comprehensive analysis of the factors contributing to technology acceptance and investigate their effects, depending on theoretical underpinnings, and (b) to explore the conditions explaining the variance in the effects of predictors time-, application- and journal-wise. This review analysed data from 693 papers. A total of 21 independent predictors having differential effects on attitude, intention and use behaviour were found. The effects of the predictors were different depending on the theoretical frameworks they were related to. The analysis of the consistency of the role of the predictors suggested that there was no longitudinal change in their effect sizes. However, a significant variance was found when comparing predictors across research applications and the journals in which the papers were published. The analysis of publication bias demonstrated a tendency to publish studies with significant results, although no evidence was found of p-value manipulation. Â© The Author(s) 2023.","Rapid digitalisation has resulted in a literature about technology acceptance that is ever increasing in size, naturally creating debates about the developments in the field and their implications. Given the size of the literature and the range of factors, theories and applications considered, this article reviewed the relevant literature using a meta-analytical approach. The objective of this review was twofold: (a) to provide a comprehensive analysis of the factors contributing to technology acceptance and investigate their effects, depending on theoretical underpinnings, and (b) to explore the conditions explaining the variance in the effects of predictors time-, application- and journal-wise. This review analysed data from 693 papers. A total of 21 independent predictors having differential effects on attitude, intention and use behaviour were found. The effects of the predictors were different depending on the theoretical frameworks they were related to. The analysis of the consistency of the role of the predictors suggested that there was no longitudinal change in their effect sizes. However, a significant variance was found when comparing predictors across research applications and the journals in which the papers were published. The analysis of publication bias demonstrated a tendency to publish studies with significant results, although no evidence was found of p-value manipulation."
Online health information consumersâ learning across health-related search tasks from the perspective of retrieval platform switching,"This study aims to investigate consumersâ modification of retrieval platform switch paths across health-related search tasks and learning characteristics via such a switch. Mixed research methods were used in this study. A lab user experiment was designed to obtain data on consumersâ health information search behaviour. Screen recordings and interview data were both coded and analysed. Research results show that health consumers acquired different kinds of health knowledge units from different retrieval platforms, and there are five change patterns of retrieval platform switch paths which reveal three types of learning. The results suggest that health consumers learn not only task-related knowledge but also retrieval skills during the switch of retrieval platforms. The research findings further develop the search as learning process research framework from the dimension of retrieval platform switch patterns and contribute to the enhancement of consumersâ health information retrieval abilities. Â© The Author(s) 2023.","This study aims to investigate consumers modification of retrieval platform switch paths across health-related search tasks and learning characteristics via such a switch. Mixed research methods were used in this study. A lab user experiment was designed to obtain data on consumers health information search behaviour. Screen recordings and interview data were both coded and analysed. Research results show that health consumers acquired different kinds of health knowledge units from different retrieval platforms, and there are five change patterns of retrieval platform switch paths which reveal three types of learning. The results suggest that health consumers learn not only task-related knowledge but also retrieval skills during the switch of retrieval platforms. The research findings further develop the search as learning process research framework from the dimension of retrieval platform switch patterns and contribute to the enhancement of consumers health information retrieval abilities."
SA-MAIS: Hybrid automatic sentiment analyser for stock market,"Sentiment analysis of stock-related tweets is a challenging task, not only due to the specificity of the domain but also because of the short nature of the texts. This work proposes SA-MAIS, a two-step lightweight methodology, specially adapted to perform sentiment analysis in domain-constrained short-text messages. To tackle the issue of domain specificity, based on word frequency, the most relevant words are automatically extracted from the new domain and then manually tagged to update an existing domain-specific sentiment lexicon. The sentiment classification is then performed by combining the updated domain-specific lexicon with VADER sentiment analysis, a well-known and widely used sentiment analysis tool. The proposed method is compared with other well-known and widely used sentiment analysis tools, including transformer-based models, such as BERTweet, Twitter-roBERTa and FinBERT, on a domain-specific corpus of stock market-related tweets comprising 1 million messages. The experimental results show that the proposed approach largely surpasses the performance of the other sentiment analysis tools, reaching an overall accuracy of 72.0%. The achieved results highlight the advantage of using a hybrid method that combines domain-specific lexicons with existing generalist tools for the inference of textual sentiment in domain-specific short-text messages. Â© The Author(s) 2023.","Sentiment analysis of stock-related tweets is a challenging task, not only due to the specificity of the domain but also because of the short nature of the texts. This work proposes SA-MAIS, a two-step lightweight methodology, specially adapted to perform sentiment analysis in domain-constrained short-text messages. To tackle the issue of domain specificity, based on word frequency, the most relevant words are automatically extracted from the new domain and then manually tagged to update an existing domain-specific sentiment lexicon. The sentiment classification is then performed by combining the updated domain-specific lexicon with VADER sentiment analysis, a well-known and widely used sentiment analysis tool. The proposed method is compared with other well-known and widely used sentiment analysis tools, including transformer-based models, such as BERTweet, Twitter-roBERTa and FinBERT, on a domain-specific corpus of stock market-related tweets comprising 1 million messages. The experimental results show that the proposed approach largely surpasses the performance of the other sentiment analysis tools, reaching an overall accuracy of 72.0%. The achieved results highlight the advantage of using a hybrid method that combines domain-specific lexicons with existing generalist tools for the inference of textual sentiment in domain-specific short-text messages."
Industry-research fronts â Private sector collaboration with research institutions in Latin America and the Caribbean,"In which research fields is industry involved with research institutions in Latin America and the Caribbean (LAC)? To shed light on this question, we applied bibliographic coupling to 13,000+ research articles and 500,000+ references indexed in Scopus for 1996â2021 as a means of determining the research fronts in which LAC-based research institutions collaborated with knowledge-intensive companies. Fields with higher betweenness centrality were those multidisciplinary, followed by physical (e.g. computer science applications), life (e.g. genetics), health (e.g. public health, environmental and occupational health) and social sciences and humanities (e.g. strategy and management). Furthermore, the period-by-period analysis unveiled a focused venturing into the physical sciences from 1996 to 2002. However, from the 2003â2021 periods, the new fields explored were mainly in the social sciences and humanities. Finally, we identified several unexplored research fronts, particularly in health (e.g. care planning) and the social sciences and humanities (e.g. demography). Â© The Author(s) 2023.","In which research fields is industry involved with research institutions in Latin America and the Caribbean (LAC)? To shed light on this question, we applied bibliographic coupling to 13,000+ research articles and 500,000+ references indexed in Scopus for 19962021 as a means of determining the research fronts in which LAC-based research institutions collaborated with knowledge-intensive companies. Fields with higher betweenness centrality were those multidisciplinary, followed by physical ( computer science applications), life ( genetics), health ( public health, environmental and occupational health) and social sciences and humanities ( strategy and management). Furthermore, the period-by-period analysis unveiled a focused venturing into the physical sciences from 1996 to 2002. However, from the 20032021 periods, the new fields explored were mainly in the social sciences and humanities. Finally, we identified several unexplored research fronts, particularly in health ( care planning) and the social sciences and humanities ( demography)."
A cross-domain recommendation model by unified modelling high-order information and rating information,"Cross-domain recommendation models are proposed to enrich the knowledge in the target domain by taking advantage of the data in the auxiliary domain to mitigate sparsity and cold-start user problems. However, most of the existing cross-domain recommendation models are dependent on rating information of items, ignoring high-order information contained in the graph data structure. In this study, we develop a novel cross-domain recommendation model by unified modelling high-order information and rating information to tackle the research gaps. Different from previous research work, we apply heterogeneous graph neural network to extract high-order information among users, items and features; obtain high-order information embeddings of users and items; and then use neural network to extract rating information and obtain user rating information embeddings by a non-linear mapping function MLP (Multilayer Perceptron). Moreover, high-order information embeddings and rating information embeddings are fused in a unified way to complete the final rating prediction, and the gradient descent method is adopted to learn the parameters of the model based on the loss function. Experiments conducted on two real-world data sets including 3,032,642 ratings from two experimental scenarios demonstrate that our model can effectively alleviate the problems of sparsity and cold-start users simultaneously, and significantly outperforms the baseline models using a variety of recommendation accuracy metrics. Â© The Author(s) 2023.","Cross-domain recommendation models are proposed to enrich the knowledge in the target domain by taking advantage of the data in the auxiliary domain to mitigate sparsity and cold-start user problems. However, most of the existing cross-domain recommendation models are dependent on rating information of items, ignoring high-order information contained in the graph data structure. In this study, we develop a novel cross-domain recommendation model by unified modelling high-order information and rating information to tackle the research gaps. Different from previous research work, we apply heterogeneous graph neural network to extract high-order information among users, items and features; obtain high-order information embeddings of users and items; and then use neural network to extract rating information and obtain user rating information embeddings by a non-linear mapping function MLP (Multilayer Perceptron). Moreover, high-order information embeddings and rating information embeddings are fused in a unified way to complete the final rating prediction, and the gradient descent method is adopted to learn the parameters of the model based on the loss function. Experiments conducted on two real-world data sets including 3,032,642 ratings from two experimental scenarios demonstrate that our model can effectively alleviate the problems of sparsity and cold-start users simultaneously, and significantly outperforms the baseline models using a variety of recommendation accuracy metrics."
Predicting and characterising persuasion strategies in misinformation content over social media based on the multi-label classification approach,"Persuasion aims at affecting the audienceâs attitude and behaviour through a series of messages containing persuasion strategies. In the context of misinformation spread, identifying the persuasion strategies is important in order to warn people to be aware of the analogous persuasion attempts in the future. In this work, we address the prediction of persuasion strategies in micro-blogging posts through a multi-label classification approach based on a variety of lexical and semantic features. We conduct our experiments using a set of well-known multi-label classification algorithms, including multi-label decision tree, multi-label k-nearest neighbours, multi-label random forest, binary relevance and classifier chains. The results show that the model incorporating classifier chains and XGBoost algorithm achieves the best subset accuracy of 0.779 and the highest macro F1-score of 0.847. In addition, we evaluated and compared the featuresâ importance for different persuasion strategies and analysed the major errors of miss-out prediction. The findings of this article provide a benchmark for the multi-label classification of persuasion strategies in micro-blogging posts and lead to a better understanding of different persuasion attempts contained in social media misinformation. Â© The Author(s) 2023.","Persuasion aims at affecting the audiences attitude and behaviour through a series of messages containing persuasion strategies. In the context of misinformation spread, identifying the persuasion strategies is important in order to warn people to be aware of the analogous persuasion attempts in the future. In this work, we address the prediction of persuasion strategies in micro-blogging posts through a multi-label classification approach based on a variety of lexical and semantic features. We conduct our experiments using a set of well-known multi-label classification algorithms, including multi-label decision tree, multi-label k-nearest neighbours, multi-label random forest, binary relevance and classifier chains. The results show that the model incorporating classifier chains and XGBoost algorithm achieves the best subset accuracy of 0.779 and the highest macro F1-score of 0.847. In addition, we evaluated and compared the features importance for different persuasion strategies and analysed the major errors of miss-out prediction. The findings of this article provide a benchmark for the multi-label classification of persuasion strategies in micro-blogging posts and lead to a better understanding of different persuasion attempts contained in social media misinformation."
Comparing COVID-19 vaccine passports attitudes across countries by analysing Reddit comments,"Topic mining and sentiment polarity analysis together can adequately represent the topics and attitudes of users. The goal of this article is to use Redditâs location-based subreddits to look at country-level differences in attitudes towards COVID-19 vaccine passports. We used sentiment analysis and latent topic modelling on textual data obtained from 18 Reddit communities concentrating on COVID-19 vaccine passports from 1 January 2021 to 28 February 2022 to study COVID-19 vaccine passportsârelated discussion on Reddit. To discover changes in sentiment and latent topics, 11,168 comments were aggregated and examined by month. The number of comments on postings from country-specific subreddits was positively proportional to the number of new COVID-19 cases reported each day. The more subjective expressions and positive/negative interpretations occurred after July 2021. Communities indicated more positive sentiment than negative sentiment towards vaccine passportsârelated topics, according to polarity analysis. Topic modelling found that community members were concerned about a variety of concerns related to their socioeconomic status. Throughout the topic modelling, keywords suggesting peopleâs privacy concerns and acceptance of various COVID-19 control methods were found. The use of public opinion and topic modelling to analyse vaccine passports could help with important global health informatics concerns associated with their socioeconomic status. Â© The Author(s) 2023.","Topic mining and sentiment polarity analysis together can adequately represent the topics and attitudes of users. The goal of this article is to use Reddits location-based subreddits to look at country-level differences in attitudes towards COVID-19 vaccine passports. We used sentiment analysis and latent topic modelling on textual data obtained from 18 Reddit communities concentrating on COVID-19 vaccine passports from 1 January 2021 to 28 February 2022 to study COVID-19 vaccine passportsrelated discussion on Reddit. To discover changes in sentiment and latent topics, 11,168 comments were aggregated and examined by month. The number of comments on postings from country-specific subreddits was positively proportional to the number of new COVID-19 cases reported each day. The more subjective expressions and positive/negative interpretations occurred after July 2021. Communities indicated more positive sentiment than negative sentiment towards vaccine passportsrelated topics, according to polarity analysis. Topic modelling found that community members were concerned about a variety of concerns related to their socioeconomic status. Throughout the topic modelling, keywords suggesting peoples privacy concerns and acceptance of various COVID-19 control methods were found. The use of public opinion and topic modelling to analyse vaccine passports could help with important global health informatics concerns associated with their socioeconomic status."
Automatic author name disambiguation by differentiable feature selection,"Author name disambiguation (AND) is the task of resolving the ambiguity problem in bibliographic databases, where distinct real-world authors may share the same name or same author may have distinct names. The aim of AND is to split the name-ambiguous entities (articles) into the corresponding authors. Existing AND algorithms mainly focus on designing different similarity metrics between two ambiguous articles. However, most previous methods empirically select and process the features of entities, then use features to predict the similarity by data-driven models. In this article, we are motivated by natural questions: Which features are most useful for splitting name-ambiguous entities? Can they be automatically determined by an optimisation approach rather than heuristic feature engineering? Therefore, we proposed a novel end-to-end differentiable feature selection algorithm, automatically searching the optimal features for AND task (AAND). AAND optimises the discrete feature selection by differentiable Gumbel-Softmax, leading to the joint learning of feature selection policy and similarity prediction model. The experiments are conducted on a benchmark data set, S2AND, which harmonises eight different AND data sets. The results show that the performance of our proposal is superior to the advanced AND methods and feature selection algorithms. Meanwhile, deep insights into AND features are also given. Â© The Author(s) 2023.","Author name disambiguation (AND) is the task of resolving the ambiguity problem in bibliographic databases, where distinct real-world authors may share the same name or same author may have distinct names. The aim of AND is to split the name-ambiguous entities (articles) into the corresponding authors. Existing AND algorithms mainly focus on designing different similarity metrics between two ambiguous articles. However, most previous methods empirically select and process the features of entities, then use features to predict the similarity by data-driven models. In this article, we are motivated by natural questions: Which features are most useful for splitting name-ambiguous entities? Can they be automatically determined by an optimisation approach rather than heuristic feature engineering? Therefore, we proposed a novel end-to-end differentiable feature selection algorithm, automatically searching the optimal features for AND task (AAND). AAND optimises the discrete feature selection by differentiable Gumbel-Softmax, leading to the joint learning of feature selection policy and similarity prediction model. The experiments are conducted on a benchmark data set, S2AND, which harmonises eight different AND data sets. The results show that the performance of our proposal is superior to the advanced AND methods and feature selection algorithms. Meanwhile, deep insights into AND features are also given."
Cognitive and interdisciplinary mobility in the social sciences and humanities: Traces of increased boundary crossing,"This study presents knowledge diffusion analyses for researchers who have been active in the social sciences and humanities. We compare a network based on switches between the main disciplinary classifications of the documents authored throughout their careers to a discipline similarity network. We find that researchers are not exclusively switching between disciplines that are most similar cognitively. Only less than a third of the authors do not switch between disciplines. On the level of the individual researchers, we also study how the cognitive distance travelled relates to boundary crossing. The cognitive distance authors travel is approximated by calculating cosine distances between the vectors of title records for different periods of activity. Moving to new disciplines is found to be positively related to the cognitive distance an author travels throughout her career. Increased specialisation leading to different types of disciplinary boundary work is suggested as a potential explanation for this finding. Â© The Author(s) 2023.","This study presents knowledge diffusion analyses for researchers who have been active in the social sciences and humanities. We compare a network based on switches between the main disciplinary classifications of the documents authored throughout their careers to a discipline similarity network. We find that researchers are not exclusively switching between disciplines that are most similar cognitively. Only less than a third of the authors do not switch between disciplines. On the level of the individual researchers, we also study how the cognitive distance travelled relates to boundary crossing. The cognitive distance authors travel is approximated by calculating cosine distances between the vectors of title records for different periods of activity. Moving to new disciplines is found to be positively related to the cognitive distance an author travels throughout her career. Increased specialisation leading to different types of disciplinary boundary work is suggested as a potential explanation for this finding."
Hi Bixby: Determinants of goal-congruent usage and goal-congruent outcome in the artificial intelligence personal assistant context,"Artificial intelligence (AI) has changed human life in many ways. AI improves the efficiency of manufacturing processes and helps people make decisions. However, we have little understanding of the motivation and goal-congruent behaviours of AI users. This research develops a theoretical model to investigate the key factors influencing goal-congruent usage and goal-congruent outcome of artificial intelligence personal assistants (AIPAs). It presents an empirical validation of the model through partial least squares structural equation modelling (PLS-SEM) with a sample of 253 AIPA users. The findings reveal that perceived usefulness, perceived ease of use and perceived enjoyment enables both goal-congruent usage and goal-congruent outcome. The results illustrate that utilitarian motivation significantly determines perceived usefulness, perceived ease of use and perceived enjoyment. The analysis results indicate that hedonic motivation exerts a significant positive influence on perceived ease of use and perceived enjoyment. The results of this study can be a meaningful guideline for the development of AIPA. Â© The Author(s) 2023.","Artificial intelligence (AI) has changed human life in many ways. AI improves the efficiency of manufacturing processes and helps people make decisions. However, we have little understanding of the motivation and goal-congruent behaviours of AI users. This research develops a theoretical model to investigate the key factors influencing goal-congruent usage and goal-congruent outcome of artificial intelligence personal assistants (AIPAs). It presents an empirical validation of the model through partial least squares structural equation modelling (PLS-SEM) with a sample of 253 AIPA users. The findings reveal that perceived usefulness, perceived ease of use and perceived enjoyment enables both goal-congruent usage and goal-congruent outcome. The results illustrate that utilitarian motivation significantly determines perceived usefulness, perceived ease of use and perceived enjoyment. The analysis results indicate that hedonic motivation exerts a significant positive influence on perceived ease of use and perceived enjoyment. The results of this study can be a meaningful guideline for the development of AIPA."
Organising musicâs structures: The classification of musical forms in Western art music,"This article analyses the classification of musical forms in Western art music. It examines how some sources in the music domain classify musical forms, and the categorisations and complexities inherent within these classifications. It analyses the table of contents from music domain textbooks, treating them as knowledge organisation systems, as well as analysing music domain descriptions of the knowledge organisation of forms. Form is found to be a complicated type of information, with an intriguing relationship to genre. The analysis of domain classifications reveals five key categorisations: texture, sectionalisation, size of structure, definable-ness and medium. Various complexities about form are elicited, such as form-as-process, complicated whole-part form relationships, an interesting spectrum of definable-ness, and the dependency of form on medium and texture. This article examines a rarely discussed type of information, form, and its approach could be usefully extended to other subjects. Â© The Author(s) 2023.","This article analyses the classification of musical forms in Western art music. It examines how some sources in the music domain classify musical forms, and the categorisations and complexities inherent within these classifications. It analyses the table of contents from music domain textbooks, treating them as knowledge organisation systems, as well as analysing music domain descriptions of the knowledge organisation of forms. Form is found to be a complicated type of information, with an intriguing relationship to genre. The analysis of domain classifications reveals five key categorisations: texture, sectionalisation, size of structure, definable-ness and medium. Various complexities about form are elicited, such as form-as-process, complicated whole-part form relationships, an interesting spectrum of definable-ness, and the dependency of form on medium and texture. This article examines a rarely discussed type of information, form, and its approach could be usefully extended to other subjects."
Citation metrics and evaluation of journals and conferences,"Citation analysis aims at evaluating the published scientific manuscripts, their authors and the publication venues (journals/conferences). There are several popular metrics for measuring the impact of the journals, the Impact Factor (IF) being the most popular. Similarly, the (Formula presented.) -index is a popular metric for evaluating and ranking conferences. We have presented a review of metrics for citation analysis, categorised according to their applicability for evaluating journals and conferences. The citation metrics may also be categorised as popularity measuring and prestige measuring. Prestige measuring indicators like SCImago Journal Rank (SJR) and Eigenfactor have already gained popularity for evaluating journals. We discuss their role in evaluating the conferences. Indeed, some conferences have already started mentioning their prestige score in terms of the SJR of their conference proceedings. We also propose a Normalised Immediacy Index ((Formula presented.)), a variant of the Immediacy Index ((Formula presented.)), to measure the immediate relevance of articles published in a journal/conference. It is shown that the proposed metric can be used for immediacy relevance comparison irrespective of the publication schedule of the articles. Spearman correlation was run to determine the relationship between the values of the proposed (Formula presented.) and traditional metrics ((Formula presented.) -index for conferences and IF for journals). A strong, positive monotonic correlation was observed between (Formula presented.) and H-index ((Formula presented.) =.67, (Formula presented.) = 17, (Formula presented.) <.01) for conferences and between (Formula presented.) and IF ((Formula presented.) =.65, (Formula presented.) = 20, (Formula presented.) <.01) for journals. Â© The Author(s) 2023.","Citation analysis aims at evaluating the published scientific manuscripts, their authors and the publication venues (journals/conferences). There are several popular metrics for measuring the impact of the journals, the Impact Factor (IF) being the most popular. Similarly, the (Formula presented.) -index is a popular metric for evaluating and ranking conferences. We have presented a review of metrics for citation analysis, categorised according to their applicability for evaluating journals and conferences. The citation metrics may also be categorised as popularity measuring and prestige measuring. Prestige measuring indicators like SCImago Journal Rank (SJR) and Eigenfactor have already gained popularity for evaluating journals. We discuss their role in evaluating the conferences. Indeed, some conferences have already started mentioning their prestige score in terms of the SJR of their conference proceedings. We also propose a Normalised Immediacy Index ((Formula presented.)), a variant of the Immediacy Index ((Formula presented.)), to measure the immediate relevance of articles published in a journal/conference. It is shown that the proposed metric can be used for immediacy relevance comparison irrespective of the publication schedule of the articles. Spearman correlation was run to determine the relationship between the values of the proposed (Formula presented.) and traditional metrics ((Formula presented.) -index for conferences and IF for journals). A strong, positive monotonic correlation was observed between (Formula presented.) and H-index ((Formula presented.) =.67, (Formula presented.) = 17, (Formula presented.) <.01) for conferences and between (Formula presented.) and IF ((Formula presented.) =.65, (Formula presented.) = 20, (Formula presented.) <.01) for journals."
An ontological data model to support urban flood disaster response,"This work aims to develop an ontological data model to support urban flood disaster response. The preliminary studies of the existing flood ontologies revealed that the developed ontologies do not express the relationships between the different levels of administration involved during flood disasters. So, an ontology named Flood Disaster Support Ontology (FDSO) was developed. The FDSO acts as a data model that can accommodate the various organisations and agents of different levels of the administrative agencies that are part of the flood disaster management process. It depicts their relationships and their activities specifically during an urban flood disaster response. The ontology also organises the resources and the information about them that are possessed by different organisations and may be used by first responders to carry out the activities such as rescue and relief. An amalgamation of two existing methodologies Yet Another Methodology for Ontology Development (YAMO) and NeOn was used to construct the ontology. The ontology was evaluated syntactically using reasoners along with the OOPS!, an OntOlogy pitfall scanner, whereas the semantic evaluation was done through SPARQL queries. The process yielded satisfactory results. Â© The Author(s) 2023.","This work aims to develop an ontological data model to support urban flood disaster response. The preliminary studies of the existing flood ontologies revealed that the developed ontologies do not express the relationships between the different levels of administration involved during flood disasters. So, an ontology named Flood Disaster Support Ontology (FDSO) was developed. The FDSO acts as a data model that can accommodate the various organisations and agents of different levels of the administrative agencies that are part of the flood disaster management process. It depicts their relationships and their activities specifically during an urban flood disaster response. The ontology also organises the resources and the information about them that are possessed by different organisations and may be used by first responders to carry out the activities such as rescue and relief. An amalgamation of two existing methodologies Yet Another Methodology for Ontology Development (YAMO) and NeOn was used to construct the ontology. The ontology was evaluated syntactically using reasoners along with the OOPS!, an OntOlogy pitfall scanner, whereas the semantic evaluation was done through SPARQL queries. The process yielded satisfactory results."
Aspect term extraction via adaptive fusion of sequential and hierarchical representation,"In aspect-based sentiment analysis, a fundamental task is extracting aspect terms from opinionated sentences. Aspect term extraction (ATE) has been found to play a critical role among several scenarios, such as service quality improvement and recommendation systems. While deep learning-based methods have achieved great progress in ATE, they mainly consider sequential semantic information and generally ignore the utilisation of syntactic relations of the whole sentence on overall meanings. Furthermore, performances of these methods may also be diminished by poor handling of relation and text noises. To address these issues, we propose a fused sequential and hierarchical representation (FSHR) model, wherein both sequential and hierarchical representations are generated, which facilitates not only the capture of linear semantic information for predicting meaning-related aspect terms but also the utilisation of syntactic relations over the entire sentence to better identify structure-related aspect terms. Moreover, to refine the aspect representation, we incorporate relation-gate mechanism which selectively activates meaningful syntactic dependency paths and design the multi-way aspect attention which prompts the model to focus on relevant text segments about particular aspects. Eventually, sequential and hierarchical representations are adaptively fused for aspect prediction. Experiment results on four datasets demonstrate that FSHR outperforms competitive baselines, and further extensive analyses reveal the effectiveness of our model. Â© The Author(s) 2023.","In aspect-based sentiment analysis, a fundamental task is extracting aspect terms from opinionated sentences. Aspect term extraction (ATE) has been found to play a critical role among several scenarios, such as service quality improvement and recommendation systems. While deep learning-based methods have achieved great progress in ATE, they mainly consider sequential semantic information and generally ignore the utilisation of syntactic relations of the whole sentence on overall meanings. Furthermore, performances of these methods may also be diminished by poor handling of relation and text noises. To address these issues, we propose a fused sequential and hierarchical representation (FSHR) model, wherein both sequential and hierarchical representations are generated, which facilitates not only the capture of linear semantic information for predicting meaning-related aspect terms but also the utilisation of syntactic relations over the entire sentence to better identify structure-related aspect terms. Moreover, to refine the aspect representation, we incorporate relation-gate mechanism which selectively activates meaningful syntactic dependency paths and design the multi-way aspect attention which prompts the model to focus on relevant text segments about particular aspects. Eventually, sequential and hierarchical representations are adaptively fused for aspect prediction. Experiment results on four datasets demonstrate that FSHR outperforms competitive baselines, and further extensive analyses reveal the effectiveness of our model."
Depth analysis: What happens after papers stand on the giantâs shoulder?,"Scholarly impact has been investigated with great enthusiasm. Scholarly activity has been understood to be accumulative, as Isaac Newtonâs famous quote âstand on the giantâs shoulderâ indicates, and citation reflects such act of knowledge accumulation and development. While citation count is indeed a robust indicator for academic assessment, given the accumulative and back-to-back nature of academic work, is limited since it does not take descending activities of a paper into consideration. This study suggests a novel approach, the Depth Analysis, which scrutinises descending papers â papers that stood on the giantâs shoulder. Also, novel Depth-based paper assessment method and knowledge structure analysis approach are presented. To validate the method, a case study on conference papers in neural computing domain is conducted. The study result shows that Depth Analysis can capture diachronic scholarly impact and knowledge structure shift more thoroughly. Â© The Author(s) 2023.","Scholarly impact has been investigated with great enthusiasm. Scholarly activity has been understood to be accumulative, as Isaac Newtons famous quote stand on the giants shoulder indicates, and citation reflects such act of knowledge accumulation and development. While citation count is indeed a robust indicator for academic assessment, given the accumulative and back-to-back nature of academic work, is limited since it does not take descending activities of a paper into consideration. This study suggests a novel approach, the Depth Analysis, which scrutinises descending papers papers that stood on the giants shoulder. Also, novel Depth-based paper assessment method and knowledge structure analysis approach are presented. To validate the method, a case study on conference papers in neural computing domain is conducted. The study result shows that Depth Analysis can capture diachronic scholarly impact and knowledge structure shift more thoroughly."
Relevance meets calibration: Triple calibration distance design for neighbour-based recommender systems,"Calibrated recommendations are devoted to revealing the various preferences of users with the appropriate proportions in the recommendation list. Most of the existing calibrated-oriented recommendations take an extra postprocessing step to rerank the initial outputs. However, applying this postprocessing strategy may decrease the recommendation relevance, since the origin accurate outputs have been scattered, and they usually ignore the calibration between pairwise users/items. Instead of reranking the recommendation outputs, this article is dedicated to modifying the criterion of neighbour usersâ selection, where we look forward to strengthening the recommendation relevance by calibrating the neighbourhood. We propose the first-order, second-order and the third-order calibration distance based on the motivation that if a user has a similar genre distribution or genre rating schema towards the target user, then his or her suggestions will be more useful for rating prediction. We also provide an equivalent transformation for the original method to speed up the algorithm with solid theoretical proof. Experimental analysis on two publicly available data sets empirically shows that our approaches are better than some of the state-of-the-art methods in terms of recommendation relevance, calibration and efficiency. Â© The Author(s) 2023.","Calibrated recommendations are devoted to revealing the various preferences of users with the appropriate proportions in the recommendation list. Most of the existing calibrated-oriented recommendations take an extra postprocessing step to rerank the initial outputs. However, applying this postprocessing strategy may decrease the recommendation relevance, since the origin accurate outputs have been scattered, and they usually ignore the calibration between pairwise users/items. Instead of reranking the recommendation outputs, this article is dedicated to modifying the criterion of neighbour users selection, where we look forward to strengthening the recommendation relevance by calibrating the neighbourhood. We propose the first-order, second-order and the third-order calibration distance based on the motivation that if a user has a similar genre distribution or genre rating schema towards the target user, then his or her suggestions will be more useful for rating prediction. We also provide an equivalent transformation for the original method to speed up the algorithm with solid theoretical proof. Experimental analysis on two publicly available data sets empirically shows that our approaches are better than some of the state-of-the-art methods in terms of recommendation relevance, calibration and efficiency."
Semantics-aware query expansion using pseudo-relevance feedback,"In this article, a pseudo-relevance feedback (PRF)âbased framework is presented for effective query expansion (QE). As candidate expansion terms, the proposed PRF framework considers the terms that are different morphological variants of the original query terms and are semantically close to them. This strategy of selecting expansion terms is expected to preserve the query intent after expansion. While judging the suitability of an expansion term with respect to a base query, two aspects of relation of the term with the query are considered. The first aspect probes to what extent the candidate term is semantically linked to the original query and the second one checks the extent to which the candidate term can supplement the base query terms. The semantic relationship between a query and expansion terms is modelled using bidirectional encoder representations from transformers (BERT). The degree of similarity is used to estimate the relative importance of the expansion terms with respect to the query. The quantified relative importance is used to assign weights of the expansion terms in the final query. Finally, the expansion terms are grouped into semantic clusters to strengthen the original query intent. A set of experiments was performed on three different Text REtrieval Conference (TREC) collections to experimentally validate the effectiveness of the proposed QE algorithm. The results show that the proposed QE approach yields competitive retrieval effectiveness over the existing state-of-the-art PRF methods in terms of the mean average precision (MAP) and precision P at position 10 (P@10). Â© The Author(s) 2023.","In this article, a pseudo-relevance feedback (PRF)based framework is presented for effective query expansion (QE). As candidate expansion terms, the proposed PRF framework considers the terms that are different morphological variants of the original query terms and are semantically close to them. This strategy of selecting expansion terms is expected to preserve the query intent after expansion. While judging the suitability of an expansion term with respect to a base query, two aspects of relation of the term with the query are considered. The first aspect probes to what extent the candidate term is semantically linked to the original query and the second one checks the extent to which the candidate term can supplement the base query terms. The semantic relationship between a query and expansion terms is modelled using bidirectional encoder representations from transformers (BERT). The degree of similarity is used to estimate the relative importance of the expansion terms with respect to the query. The quantified relative importance is used to assign weights of the expansion terms in the final query. Finally, the expansion terms are grouped into semantic clusters to strengthen the original query intent. A set of experiments was performed on three different Text REtrieval Conference (TREC) collections to experimentally validate the effectiveness of the proposed QE algorithm. The results show that the proposed QE approach yields competitive retrieval effectiveness over the existing state-of-the-art PRF methods in terms of the mean average precision (MAP) and precision P at position 10 (P@10)."
A bibliometric analysis of literature on bibliometrics in recent half-century,"Bibliometrics research has been developed for many years and achieved a great many scientific achievements. The aim of this study is to understand the current research status and development directions of research on bibliometrics. This study implements bibliometric methods based on the Web of Science to analyse the publications, subjects, citation, co-citation, collaboration and keywords of bibliometrics from 1969 to 2022. Bibliometrics is now in the development phase from its cumulative publication curve and citation analysis. Information Science & Library Science and Computer Science are the two major subjects on bibliometric research. Garfield E has the most citations and co-citation frequency in the field of bibliometrics. The number of countries cooperating with the United States is the largest and China is the most productive country; Beijing, London and Madrid are the three sub-network centres of the city cooperation network; Zhang L owns the largest number of author collaborations. Research content on bibliometrics focuses mainly on bibliometrics theory, methods, research evaluation, techniques, tools and applications. The results in this study are beneficial to researchers and readers who are interested in the field of bibliometrics and other related fields to understand the overall picture of bibliometrics, which is conducive to the future development of bibliometrics. Â© The Author(s) 2023.","Bibliometrics research has been developed for many years and achieved a great many scientific achievements. The aim of this study is to understand the current research status and development directions of research on bibliometrics. This study implements bibliometric methods based on the Web of Science to analyse the publications, subjects, citation, co-citation, collaboration and keywords of bibliometrics from 1969 to 2022. Bibliometrics is now in the development phase from its cumulative publication curve and citation analysis. Information Science & Library Science and Computer Science are the two major subjects on bibliometric research. Garfield E has the most citations and co-citation frequency in the field of bibliometrics. The number of countries cooperating with the United States is the largest and China is the most productive country; Beijing, London and Madrid are the three sub-network centres of the city cooperation network; Zhang L owns the largest number of author collaborations. Research content on bibliometrics focuses mainly on bibliometrics theory, methods, research evaluation, techniques, tools and applications. The results in this study are beneficial to researchers and readers who are interested in the field of bibliometrics and other related fields to understand the overall picture of bibliometrics, which is conducive to the future development of bibliometrics."
A review for comparative text mining: From data acquisition to practical application,"Social media provides customers with great opportunities to share their opinions regarding certain products and services. Comparative text, as an important expression form, deserves further exploration, since it contains considerable comparative information between different products and services. In this study, we review existing research on Comparative Text Mining (CTM) in the past 16 years. Basic concepts related to CTM are first described, and a general research framework is subsequently proposed. We then dive into each component of the research framework, ranging from data acquisition, comparative text identification (CTI), comparative relation extraction (CRE), to potential applications. In addition, we conduct extensive experimental analysis on existing methods for CTI and CRE, and clarify their limitations. Accordingly, we provide corresponding insights, and point out future research directions. Â© The Author(s) 2023.","Social media provides customers with great opportunities to share their opinions regarding certain products and services. Comparative text, as an important expression form, deserves further exploration, since it contains considerable comparative information between different products and services. In this study, we review existing research on Comparative Text Mining (CTM) in the past 16 years. Basic concepts related to CTM are first described, and a general research framework is subsequently proposed. We then dive into each component of the research framework, ranging from data acquisition, comparative text identification (CTI), comparative relation extraction (CRE), to potential applications. In addition, we conduct extensive experimental analysis on existing methods for CTI and CRE, and clarify their limitations. Accordingly, we provide corresponding insights, and point out future research directions."
Snippet-based result merging in federated search,"In federated search, the central broker simultaneously forwards the search query to multiple resources. The returned results from those resources are then merged into a single ranked list. An autonomous resource in a federated search system usually does not provide scores for the retrieved documents; even if some of them do, scores from different resources are incomparable due to the heterogeneity in many aspects of those resource involved such as retrieval models and corpus statistics. Many results merging approaches have been proposed in the literature to deal with this problem. However, to the best of our knowledge, none of them has utilised snippets. This article proposes a snippet-based weighting scheme for the query terms involved. It quantifies the importance of each query term from two angles: the frequency of the term and the part in which the term occurs inside a snippet. Three parts â which are URL, title, and description â are given different weights. Experiments are conducted with the TREC 2013 FedWeb data set. The results show that the proposed methods consistently outperform several baseline models. We also find in many instances, a further small slight performance improvement is achievable by an extra measure of weighting each of the resources involved, which can be done in the phase of resource selection. Â© The Author(s) 2023.","In federated search, the central broker simultaneously forwards the search query to multiple resources. The returned results from those resources are then merged into a single ranked list. An autonomous resource in a federated search system usually does not provide scores for the retrieved documents; even if some of them do, scores from different resources are incomparable due to the heterogeneity in many aspects of those resource involved such as retrieval models and corpus statistics. Many results merging approaches have been proposed in the literature to deal with this problem. However, to the best of our knowledge, none of them has utilised snippets. This article proposes a snippet-based weighting scheme for the query terms involved. It quantifies the importance of each query term from two angles: the frequency of the term and the part in which the term occurs inside a snippet. Three parts which are URL, title, and description are given different weights. Experiments are conducted with the TREC 2013 FedWeb data set. The results show that the proposed methods consistently outperform several baseline models. We also find in many instances, a further small slight performance improvement is achievable by an extra measure of weighting each of the resources involved, which can be done in the phase of resource selection."
Language style and recognition of the answers in health Q&A community: Moderating effects of medical terminology,"The rise of social question and answer (Q&A) platforms has changed the model of traditional health-information services. The quality of answers on a Q&A platform is critical to attract users and increase their community engagement. Thus, recognition of the answer was used to measure usersâ acceptance of the answer. A theoretical model of the impact of language style on community recognition of health questions was developed. Nearly 1330 answers from a health question from Zhihu were obtained to verify the model. Results showed that personality reliability, assertiveness, argumentation clarity, commitment, and reverse incentive of health information positively affected answer recognition, while argumentation structure negatively affected answer acceptance. Simultaneously, the degree of use of medical terminology has a significant moderating effect on the relationship between answer recognition and assertiveness, argumentation structure, argumentation clarity, and commitment. Introducing Aristotleâs rhetoric theory to language style and answer recognition could potentially develop healthy communities and disseminate health knowledge. Â© The Author(s) 2023.","The rise of social question and answer (Q&A) platforms has changed the model of traditional health-information services. The quality of answers on a Q&A platform is critical to attract users and increase their community engagement. Thus, recognition of the answer was used to measure users acceptance of the answer. A theoretical model of the impact of language style on community recognition of health questions was developed. Nearly 1330 answers from a health question from Zhihu were obtained to verify the model. Results showed that personality reliability, assertiveness, argumentation clarity, commitment, and reverse incentive of health information positively affected answer recognition, while argumentation structure negatively affected answer acceptance. Simultaneously, the degree of use of medical terminology has a significant moderating effect on the relationship between answer recognition and assertiveness, argumentation structure, argumentation clarity, and commitment. Introducing Aristotles rhetoric theory to language style and answer recognition could potentially develop healthy communities and disseminate health knowledge."
Citation advantage of COVID-19-related publications,"With the global spread of the COVID-19 pandemic, scientists from various disciplines responded quickly to this historical public health emergency. The sudden boom of COVID-19-related papers in a short period of time may bring unexpected influence to some commonly used bibliometric indicators. By a large-scale investigation using Science Citation Index Expanded and Social Sciences Citation Index, this brief communication confirms the citation advantage of COVID-19-related papers empirically through the lens of Essential Science Indicatorsâ highly cited paper. More than 8% of COVID-19-related papers published during 2020 and 2021 were selected as Essential Science Indicators highly cited papers, which was much higher than the set global benchmark value of 1%. The citation advantage of COVID-19-related papers for different Web of Science categories/countries/journal impact factor quartiles was also demonstrated. The distortions of COVID-19-related papersâ citation advantage to some bibliometric indicators such as journal impact factor were discussed at the end of this brief communication. Â© The Author(s) 2023.","With the global spread of the COVID-19 pandemic, scientists from various disciplines responded quickly to this historical public health emergency. The sudden boom of COVID-19-related papers in a short period of time may bring unexpected influence to some commonly used bibliometric indicators. By a large-scale investigation using Science Citation Index Expanded and Social Sciences Citation Index, this brief communication confirms the citation advantage of COVID-19-related papers empirically through the lens of Essential Science Indicators highly cited paper. More than 8% of COVID-19-related papers published during 2020 and 2021 were selected as Essential Science Indicators highly cited papers, which was much higher than the set global benchmark value of 1%. The citation advantage of COVID-19-related papers for different Web of Science categories/countries/journal impact factor quartiles was also demonstrated. The distortions of COVID-19-related papers citation advantage to some bibliometric indicators such as journal impact factor were discussed at the end of this brief communication."
Construction of recipe knowledge graph based on user knowledge demands,"Building a recipe knowledge graph from the perspective of user knowledge demands can provide users with accurate and efficient retrieval results and recipes. Relatively a few ontology studies have been conducted on Chinese recipes. In addition, some existing recipe ontology models are relatively rough, with few dimensions, and others have too many dimensions, that is, they lack versatility and cannot achieve efficient recipe recommendation and query functions. Therefore, this article proposes a general recipe ontology based on user knowledge demands analysed from multi-source recipes. Then, this article selects recipes for common edible flowers with health benefits, an ontology model and constructs a knowledge graph of flower recipes via knowledge extraction, knowledge fusion and other techniques. Finally, the application of constructed flower recipe knowledge graph is discussed, and the results indicated that it can effectively feedback the query results and satisfy the knowledge demands of users. Â© The Author(s) 2023.","Building a recipe knowledge graph from the perspective of user knowledge demands can provide users with accurate and efficient retrieval results and recipes. Relatively a few ontology studies have been conducted on Chinese recipes. In addition, some existing recipe ontology models are relatively rough, with few dimensions, and others have too many dimensions, that is, they lack versatility and cannot achieve efficient recipe recommendation and query functions. Therefore, this article proposes a general recipe ontology based on user knowledge demands analysed from multi-source recipes. Then, this article selects recipes for common edible flowers with health benefits, an ontology model and constructs a knowledge graph of flower recipes via knowledge extraction, knowledge fusion and other techniques. Finally, the application of constructed flower recipe knowledge graph is discussed, and the results indicated that it can effectively feedback the query results and satisfy the knowledge demands of users."
Constructing a multi-layer heterogeneous networks model to explore the public opinion evolution pattern of key users in public health emergencies,"This article aims to discover key users in public health emergencies and explore their public opinion evolution patterns, thus providing theoretical support for the establishment of a clear cyberspace. In this article, a multi-layer heterogeneous network model based on multiple node attributes (user nodes, microblogs nodes, emotional nodes and topic nodes) is constructed at each stage of the public opinion cycle. Specifically, a novel semi-supervised self-training method based on the bidirectional encoder representations from transformers (SSST-BERT) method is proposed to automatically label the fine-grained emotional nodes. The latent Dirichlet allocation (LDA) model is used to construct the topic nodes. Moreover, the degree centrality, betweenness centrality and closeness centrality of the constructed heterogeneous network are adopted to dynamically identify key users. Finally, the emotional states and topic tendencies of key users are explored to obtain the public opinion evolution pattern of emergencies. The experimental results show that the SSST-BERT automatically labels emotion categories with an F1-score of 80.48%. The key users identified by the constructed heterogeneous network are more representative of the opinion status of ordinary users. Analysis of the public opinion status of key users reveals that netizens show more negative emotions such as anger and fear in public health emergencies, and the shift of focus drives the evolution of discussion topics. Â© The Author(s) 2023.","This article aims to discover key users in public health emergencies and explore their public opinion evolution patterns, thus providing theoretical support for the establishment of a clear cyberspace. In this article, a multi-layer heterogeneous network model based on multiple node attributes (user nodes, microblogs nodes, emotional nodes and topic nodes) is constructed at each stage of the public opinion cycle. Specifically, a novel semi-supervised self-training method based on the bidirectional encoder representations from transformers (SSST-BERT) method is proposed to automatically label the fine-grained emotional nodes. The latent Dirichlet allocation (LDA) model is used to construct the topic nodes. Moreover, the degree centrality, betweenness centrality and closeness centrality of the constructed heterogeneous network are adopted to dynamically identify key users. Finally, the emotional states and topic tendencies of key users are explored to obtain the public opinion evolution pattern of emergencies. The experimental results show that the SSST-BERT automatically labels emotion categories with an F1-score of 80.48%. The key users identified by the constructed heterogeneous network are more representative of the opinion status of ordinary users. Analysis of the public opinion status of key users reveals that netizens show more negative emotions such as anger and fear in public health emergencies, and the shift of focus drives the evolution of discussion topics."
Exploring interdisciplinarity of science projects based on the text mining,"Interdisciplinary research has gradually become one of the main driving forces to promote original innovation of scientific research, and how to measure the interdisciplinarity of science project is becoming an important topic in the science foundation managements. Existing researches mainly using methods, such as academic degree or institutional discipline or discipline category mapping of journals, to measure the interdisciplinarity. This study proposes an approach to mine and capture the different or complementary characteristics of interdisciplinarity of projects by combining text mining and machine learning methods. First, we construct the classification system and extract a raw paper and its discipline matrix according to the discipline category of journals where the references were published in. Second, we cut the matrix to summarise the distribution of key disciplines in each paper and extract the text features in the abstract and title to form a training set. Finally, we compare and analyse the classification effects of Naive Bayesian Model, Support Vector Machine and Bidirectional Encoder Representations from Transformers (BERT) model. Then, the model evaluation indicators show that the best classification effect was achieved by the BERT model. Therefore, the deep pre-trained linguistic model BERT is chosen to predict the discipline distribution of each project. In addition, the different aspects of interdisciplinarity are measured using network coherence and discipline diversity indicators. Besides, experts are invited to evaluate and interpret the results. This proposed approach could be applied to deeply understand the discipline integration from a new perspective. Â© The Author(s) 2023.","Interdisciplinary research has gradually become one of the main driving forces to promote original innovation of scientific research, and how to measure the interdisciplinarity of science project is becoming an important topic in the science foundation managements. Existing researches mainly using methods, such as academic degree or institutional discipline or discipline category mapping of journals, to measure the interdisciplinarity. This study proposes an approach to mine and capture the different or complementary characteristics of interdisciplinarity of projects by combining text mining and machine learning methods. First, we construct the classification system and extract a raw paper and its discipline matrix according to the discipline category of journals where the references were published in. Second, we cut the matrix to summarise the distribution of key disciplines in each paper and extract the text features in the abstract and title to form a training set. Finally, we compare and analyse the classification effects of Naive Bayesian Model, Support Vector Machine and Bidirectional Encoder Representations from Transformers (BERT) model. Then, the model evaluation indicators show that the best classification effect was achieved by the BERT model. Therefore, the deep pre-trained linguistic model BERT is chosen to predict the discipline distribution of each project. In addition, the different aspects of interdisciplinarity are measured using network coherence and discipline diversity indicators. Besides, experts are invited to evaluate and interpret the results. This proposed approach could be applied to deeply understand the discipline integration from a new perspective."
Curtailing fake news creation and dissemination in Nigeria: Twitter social network and sentiment analysis approaches,"Influencers create and facilitate dissemination of information in a network and can shape the attitudes and beliefs of members of a social network. Identifying the influencers and their relations, as well as the sentiment of tweets being disseminated in the network on some selected keywords can help curtail fake news creation and dissemination in the network. This study uses a sequential mixed-methods design with a quantitative method followed by qualitative methods. The quantitative data were collected using Mozdeh big data analysis software. Mozdeh software was used to collect tweets through Twitterâs Streaming application programming interface to build a corpus of tweets, collected from 1 January 2016 to 30 June 2021. The study found two major actors/influencers involved in the creation and dissemination of fake news on the tweeter social network studied. The study further found overall Av. Pos. â Av. Neg. was â0.9935. Data collected were on some specific trending keywords on a particular region in Nigeria. Identifying and monitoring the tweets of influencers in a network can aid in debunking fake news immediately after dissemination and discourage the use of offensive words in tweets. The results revealed major influencers responsible for creating and disseminating fake news on some trending issues in Nigeria. Â© The Author(s) 2023.","Influencers create and facilitate dissemination of information in a network and can shape the attitudes and beliefs of members of a social network. Identifying the influencers and their relations, as well as the sentiment of tweets being disseminated in the network on some selected keywords can help curtail fake news creation and dissemination in the network. This study uses a sequential mixed-methods design with a quantitative method followed by qualitative methods. The quantitative data were collected using Mozdeh big data analysis software. Mozdeh software was used to collect tweets through Twitters Streaming application programming interface to build a corpus of tweets, collected from 1 January 2016 to 30 June 2021. The study found two major actors/influencers involved in the creation and dissemination of fake news on the tweeter social network studied. The study further found overall Av. Pos. Av. Neg. was 0.9935. Data collected were on some specific trending keywords on a particular region in Nigeria. Identifying and monitoring the tweets of influencers in a network can aid in debunking fake news immediately after dissemination and discourage the use of offensive words in tweets. The results revealed major influencers responsible for creating and disseminating fake news on some trending issues in Nigeria."
Towards a moderate realist foundation for ontological knowledge organization systems: The question of the naturalness of classifications,"Several authors emphasise the need for a change in classification theory due to the influence of a dogmatic and monistic ontology supported by an outdated essentialism. These claims tend to focus on the fallibility of knowledge, the need for a pluralistic view, and the theoretical burden of observations. Regardless of the legitimacy of these concerns, there is the risk, when not moderate, to fall into the opposite relativistic extreme. Based on a narrative review of the literature, we aim to reflectively discuss the theoretical foundations that can serve as a basis for a realist position supporting pluralistic ontological classifications. The goal is to show that, against rather conventional solutions, objective scientific-based approaches to natural classifications are presented to be viable, allowing a proper distinction between ontological and taxonomic questions. Supported by critical scientific realism, we consider that such an approach is suitable for the development of ontological Knowledge Organisation Systems (KOSs). We believe that ontological perspectivism can provide the necessary adaptation to the different granularities of reality. Â© The Author(s) 2023.","Several authors emphasise the need for a change in classification theory due to the influence of a dogmatic and monistic ontology supported by an outdated essentialism. These claims tend to focus on the fallibility of knowledge, the need for a pluralistic view, and the theoretical burden of observations. Regardless of the legitimacy of these concerns, there is the risk, when not moderate, to fall into the opposite relativistic extreme. Based on a narrative review of the literature, we aim to reflectively discuss the theoretical foundations that can serve as a basis for a realist position supporting pluralistic ontological classifications. The goal is to show that, against rather conventional solutions, objective scientific-based approaches to natural classifications are presented to be viable, allowing a proper distinction between ontological and taxonomic questions. Supported by critical scientific realism, we consider that such an approach is suitable for the development of ontological Knowledge Organisation Systems (KOSs). We believe that ontological perspectivism can provide the necessary adaptation to the different granularities of reality."
Does scientific collaboration variety influence the impact of articles?,"This study attempts to investigate the relationship between scientific collaboration variety and scientific output in a specific field. The indicators were set from co-author variety (co-authorâs academic background variety and co-authorâs network structure variety as independent variables) and article impact (academic impact and social impact as dependent variables). Considering other factors affecting the research results, we also set up control variables (the number of co-authors, proportion of high-level authors and ratio of highly productive authors). We used the Scopus database as the data source and collected all articles published in the dental field in 2018 as data. We used multiple linear regression analyses to examine the impact of co-authorsâ variety on the article impact. The results demonstrate that the relationship between scientific collaboration variety and article impact is complicated, which depends on the type of variety of the cooperative scientist. Conversely, the same variety indicator presents the same results as the correlation analysis of academic and social impact articles. The findings indicate that authors can improve their scientific output by collaborating with similar authors of academic backgrounds or stable groups of authors, which provides guidance for scientific cooperation. Â© The Author(s) 2023.","This study attempts to investigate the relationship between scientific collaboration variety and scientific output in a specific field. The indicators were set from co-author variety (co-authors academic background variety and co-authors network structure variety as independent variables) and article impact (academic impact and social impact as dependent variables). Considering other factors affecting the research results, we also set up control variables (the number of co-authors, proportion of high-level authors and ratio of highly productive authors). We used the Scopus database as the data source and collected all articles published in the dental field in 2018 as data. We used multiple linear regression analyses to examine the impact of co-authors variety on the article impact. The results demonstrate that the relationship between scientific collaboration variety and article impact is complicated, which depends on the type of variety of the cooperative scientist. Conversely, the same variety indicator presents the same results as the correlation analysis of academic and social impact articles. The findings indicate that authors can improve their scientific output by collaborating with similar authors of academic backgrounds or stable groups of authors, which provides guidance for scientific cooperation."
Titles and keywords: âGreat love isnât two people finding the perfect match in one another!â,"Compared with abstracts, titles and keywords are briefer, more condensed, but fragmented textual elements on the first page of research articles (RAs) that summarise their content and represent their themes to enhance the discoverability. Therefore, the hidden, subtle relationship between titles and associated keywords increasingly attracts scholarsâ attention. This research attempts to analyse title lengths and lexical words of two major title types: nominal titles and colonic titles, and investigate the matching coefficient (MC) values between lexical words and keywords assigned by authors. A total number of 505 RA titles from 15 journals in Library and Information Science were examined. Regarding title length and lexical words, the results found statistically significant differences between colonic and nominal titles. However, the results did not support the hypothesis that colonic titles have a higher MC value with author-assigned keywords than nominal titles. In light of significant findings, this research recommends constructing titles that maximise informativity and build effective keywords. Â© The Author(s) 2023.","Compared with abstracts, titles and keywords are briefer, more condensed, but fragmented textual elements on the first page of research articles (RAs) that summarise their content and represent their themes to enhance the discoverability. Therefore, the hidden, subtle relationship between titles and associated keywords increasingly attracts scholars attention. This research attempts to analyse title lengths and lexical words of two major title types: nominal titles and colonic titles, and investigate the matching coefficient values between lexical words and keywords assigned by authors. A total number of 505 RA titles from 15 journals in Library and Information Science were examined. Regarding title length and lexical words, the results found statistically significant differences between colonic and nominal titles. However, the results did not support the hypothesis that colonic titles have a higher MC value with author-assigned keywords than nominal titles. In light of significant findings, this research recommends constructing titles that maximise informativity and build effective keywords."
Hidden influential node selection based on costâbenefit analysis for harmful information propagation control,"The quick development of mobile network and smart devices provides a convenience way for information sharing in online social networks, which also accelerates the propagation of harmful information, thus how to select the hidden influential nodes with lower management cost for reducing the propagation speed of harmful information is an important task. In this article, we propose a greedy hidden influential node selection algorithm based on the epidemic model and costâbenefit analysis. First, we investigate the user behaviour dynamic characteristics from two perspectives of social relationships and interaction behaviours, and then susceptible and infected (SI) epidemic model is applied and user influence is estimated. Second, considering the management cost and benefit of different users, a greedy hidden influential node selection algorithm based on the costâbenefit analysis is proposed. Finally, a series of experiments are conducted using the public social network data set and the data set collected from Sina Weibo, to verify the performance and practicality of the developed method. The experimental results demonstrate that our method outperforms other related methods in harmful information propagation control. Â© The Author(s) 2023.","The quick development of mobile network and smart devices provides a convenience way for information sharing in online social networks, which also accelerates the propagation of harmful information, thus how to select the hidden influential nodes with lower management cost for reducing the propagation speed of harmful information is an important task. In this article, we propose a greedy hidden influential node selection algorithm based on the epidemic model and costbenefit analysis. First, we investigate the user behaviour dynamic characteristics from two perspectives of social relationships and interaction behaviours, and then susceptible and infected (SI) epidemic model is applied and user influence is estimated. Second, considering the management cost and benefit of different users, a greedy hidden influential node selection algorithm based on the costbenefit analysis is proposed. Finally, a series of experiments are conducted using the public social network data set and the data set collected from Sina Weibo, to verify the performance and practicality of the developed method. The experimental results demonstrate that our method outperforms other related methods in harmful information propagation control."
Do the paperâs connections to existing work disclose its citation impact? A study based on graph representation learning,"Influential scientific papers tend to be primarily based on combinations of prior works. However, assessing the potential impact of a new scientific paper remains a challenging task. In this article, we introduce an innovative framework to investigate the relationship between the embedding of citation networks and a paperâs future citation counts, based on the graph representation learning approach. First, we employ three Nobel Prize-winning topic papers from the Web of Science as our data source. Through data preprocessing and direct citation network modelling, we train the struc2vec model to obtain embeddings of papersâ citation network structure. Then, we perform visualisation and analysis on two types of networks. One is the direct-citation network, in which we identify four patterns of linkage between newly published papers and existing knowledge, and the other is the co-citation network, where we measure three structural variation indicators of new papers based on existing research findings. Finally, a statistical test is used to examine the predictive potentials of network embeddings. The results demonstrate that the structural features captured by the graph representation learning model can be used to predict a paperâs citation counts and impact. This article innovatively combines cluster analysis, visual analysis and statistical analysis to gain insights into the relationship between the hard-to-explain structural embeddings of newly published papers in a citation network and their future citations. Â© The Author(s) 2023.","Influential scientific papers tend to be primarily based on combinations of prior works. However, assessing the potential impact of a new scientific paper remains a challenging task. In this article, we introduce an innovative framework to investigate the relationship between the embedding of citation networks and a papers future citation counts, based on the graph representation learning approach. First, we employ three Nobel Prize-winning topic papers from the Web of Science as our data source. Through data preprocessing and direct citation network modelling, we train the struc2vec model to obtain embeddings of papers citation network structure. Then, we perform visualisation and analysis on two types of networks. One is the direct-citation network, in which we identify four patterns of linkage between newly published papers and existing knowledge, and the other is the co-citation network, where we measure three structural variation indicators of new papers based on existing research findings. Finally, a statistical test is used to examine the predictive potentials of network embeddings. The results demonstrate that the structural features captured by the graph representation learning model can be used to predict a papers citation counts and impact. This article innovatively combines cluster analysis, visual analysis and statistical analysis to gain insights into the relationship between the hard-to-explain structural embeddings of newly published papers in a citation network and their future citations."
Analysis and prediction of the formation of new technical phrases for inventive ideation,"Despite the fast pace of technological development, the process of inventive ideation remains fuzzy. Meanwhile, improving innovation efficiency has become critical for research and development (R&D) teams because of the fierce competition. This study claimed that new technical phrases (NTPs) were important carriers of novel inventive ideas, and their formation was key to understanding and improving ideation processes. Therefore, this article proposed a methodology to analyse and predict the formation of NTPs. First, based on the recombinant search theory and link prediction, four variables in the prior co-word network of a phrase that may influence its formation were collected. Thereafter, logistic regression and a classification tree were employed on patent data to explore the effects of these variables on NTPs. Moreover, various machine learning methods were used for developing NTP prediction models, and procedures for applying the prediction models in real-world R&D settings were designed. Finally, a case study was conducted using the proposed methodology for its demonstration and validation in neural network technology. The case study revealed that all the four variables posed significant impact on the formation of NTPs, and the prediction models yielded the highest prediction accuracy of 78.6% on the test set. The proposed methodology would shed light on the ideation process in innovation theory and provide R&D teams with practical tools for generating new technical ideas. Â© The Author(s) 2023.","Despite the fast pace of technological development, the process of inventive ideation remains fuzzy. Meanwhile, improving innovation efficiency has become critical for research and development (R&D) teams because of the fierce competition. This study claimed that new technical phrases (NTPs) were important carriers of novel inventive ideas, and their formation was key to understanding and improving ideation processes. Therefore, this article proposed a methodology to analyse and predict the formation of NTPs. First, based on the recombinant search theory and link prediction, four variables in the prior co-word network of a phrase that may influence its formation were collected. Thereafter, logistic regression and a classification tree were employed on patent data to explore the effects of these variables on NTPs. Moreover, various machine learning methods were used for developing NTP prediction models, and procedures for applying the prediction models in real-world R&D settings were designed. Finally, a case study was conducted using the proposed methodology for its demonstration and validation in neural network technology. The case study revealed that all the four variables posed significant impact on the formation of NTPs, and the prediction models yielded the highest prediction accuracy of 78.6% on the test set. The proposed methodology would shed light on the ideation process in innovation theory and provide R&D teams with practical tools for generating new technical ideas."
Immersive information seekingâA scoping review of information seeking in virtual reality environments,"Recently, virtual reality (VR) technology has become more widespread. Humans increasingly interact with information in VR, and a detailed look into those activities is warranted. Thus, a scoping literature review (PRISMA-ScR) is conducted. It overviews all relevant literature about information-seeking behaviour in VR, focusing on existing models and theories. Out of 536 publications, 37 qualify for this review. Eight publications show an understanding related to information behaviour theories from information science. Pressingly, no publications relate models, frameworks or general theories of information seeking to VR. This review overviews VR-related cognitive and behavioural human factors based on this research gap. Those factors include immersion and presence, affordances, embodiment, cognitive load, human error, flow and engagement. The review is concluded with an explorative framework for future research that is constructed with Marchioniniâs process model of information seeking as a baseline and in conjunction with the discussed human factors. Â© The Author(s) 2023.","Recently, virtual reality (VR) technology has become more widespread. Humans increasingly interact with information in VR, and a detailed look into those activities is warranted. Thus, a scoping literature review (PRISMA-ScR) is conducted. It overviews all relevant literature about information-seeking behaviour in VR, focusing on existing models and theories. Out of 536 publications, 37 qualify for this review. Eight publications show an understanding related to information behaviour theories from information science. Pressingly, no publications relate models, frameworks or general theories of information seeking to VR. This review overviews VR-related cognitive and behavioural human factors based on this research gap. Those factors include immersion and presence, affordances, embodiment, cognitive load, human error, flow and engagement. The review is concluded with an explorative framework for future research that is constructed with Marchioninis process model of information seeking as a baseline and in conjunction with the discussed human factors."
The use of one-on-one interviews in library and information science: A scoping review,"Individual interviews are not commonly used in library and information science (LIS). A content analysis of research papers published between 2015 and 2020 in seven journals yielded 323 articles applying interviews. Selected papers, extracted from the Scopus database, were examined qualitatively and quantitatively to find the description of the type of interview, the use of additional research methods, the number of interviewees and the topics of the studies. Interviews appeared in 11% of the research papers, confirming previous studies. They are often combined with other methods, in particular surveys and observations. Half of the interviews were conducted with up to 15 participants. Individual interviews were used most frequently in studies of information behaviour (19% of the papers), librarians, information retrieval and web search. This study contributes to better understanding interviews as a LIS research method and may facilitate the planning of research projects. Â© The Author(s) 2023.","Individual interviews are not commonly used in library and information science (LIS). A content analysis of research papers published between 2015 and 2020 in seven journals yielded 323 articles applying interviews. Selected papers, extracted from the Scopus database, were examined qualitatively and quantitatively to find the description of the type of interview, the use of additional research methods, the number of interviewees and the topics of the studies. Interviews appeared in 11% of the research papers, confirming previous studies. They are often combined with other methods, in particular surveys and observations. Half of the interviews were conducted with up to 15 participants. Individual interviews were used most frequently in studies of information behaviour (19% of the papers), librarians, information retrieval and web search. This study contributes to better understanding interviews as a LIS research method and may facilitate the planning of research projects."
An extended ontology model for trust evaluation using advanced hybrid ontology,"In the blooming area of Internet technology, the concept of Internet-of-Things (IoT) holds a distinct position that interconnects a large number of smart objects. In the context of social IoT (SIoT), the argument of trust and reliability is evaluated in the presented work. The proposed framework is divided into two blocks, namely Verification Block (VB) and Evaluation Block (EB). VB defines various ontology-based relationships computed for the objects that reflect the security and trustworthiness of an accessed service. While, EB is used for the feedback analysis and proves to be a valuable step that computes and governs the success rate of the service. Support vector machine (SVM) is applied to categorise the trust-based evaluation. The security aspect of the proposed approach is comparatively evaluated for DDoS and malware attacks in terms of success rate, trustworthiness and execution time. The proposed secure ontology-based framework provides better performance compared with existing architectures. Â© The Author(s) 2023.","In the blooming area of Internet technology, the concept of Internet-of-Things (IoT) holds a distinct position that interconnects a large number of smart objects. In the context of social IoT (SIoT), the argument of trust and reliability is evaluated in the presented work. The proposed framework is divided into two blocks, namely Verification Block (VB) and Evaluation Block (EB). VB defines various ontology-based relationships computed for the objects that reflect the security and trustworthiness of an accessed service. While, EB is used for the feedback analysis and proves to be a valuable step that computes and governs the success rate of the service. Support vector machine (SVM) is applied to categorise the trust-based evaluation. The security aspect of the proposed approach is comparatively evaluated for DDoS and malware attacks in terms of success rate, trustworthiness and execution time. The proposed secure ontology-based framework provides better performance compared with existing architectures."
Filtering objectionable information access based on click-through behaviours with deep learning methods,"This study explores URL click-through behaviour to predict the category of usersâ online information accesses and applies the results to progressively filter objectionable accesses during web surfing. Each clicked URL is represented by the embedding technique and fed into the Bidirectional Long Short-Term Memory neural network cascaded with a Conditional Random Field (BiLSTM-CRF) model to predict the category of a userâs access. Large-scale experiments on click-through data from nearly one million real users show that our proposed BiLSTM-CRF model achieves promising results. The proposed method outperforms related approaches by a high accuracy of 0.9492 (near 27% relative improvement) for context-aware category prediction and an F1-score of 0.8995 (about 29% relative improvement) for objectionable access identification. In addition, in real-time filtering simulations, our model gradually achieves a macro-averaging blocking rate of 0.9221, while maintaining a favourably low false-positive rate of 0.0041. Â© The Author(s) 2023.","This study explores URL click-through behaviour to predict the category of users online information accesses and applies the results to progressively filter objectionable accesses during web surfing. Each clicked URL is represented by the embedding technique and fed into the Bidirectional Long Short-Term Memory neural network cascaded with a Conditional Random Field (BiLSTM-CRF) model to predict the category of a users access. Large-scale experiments on click-through data from nearly one million real users show that our proposed BiLSTM-CRF model achieves promising results. The proposed method outperforms related approaches by a high accuracy of 0.9492 (near 27% relative improvement) for context-aware category prediction and an F1-score of 0.8995 (about 29% relative improvement) for objectionable access identification. In addition, in real-time filtering simulations, our model gradually achieves a macro-averaging blocking rate of 0.9221, while maintaining a favourably low false-positive rate of 0.0041."
What Google-based data can tell us about information systems theory: A review,"The aim of this study is to examine the attention and acceptance of popular information systems (IS) theory. Both Google Scholar (GS) and Google Trends (GT) data related to popular IS theories from 2006 to 2020 are collected. The study provides an overview of popular IS theories and tests the pair data to show changes in GSâs total publications and GTâs public interest. Correlation analyses are conducted to investigate the relationships between GS and GT data, as well as between GS and GT data. Trend analysis reveals that there are certain IS theories that researchers and practitioners should pay attention to. In addition, this study provides an overview of recent and future directions in this field. Despite the significant increase in research using Google data, the use of big data to explore the interests and applications of IS theories has been rarely explored. Â© The Author(s) 2023.","The aim of this study is to examine the attention and acceptance of popular information systems (IS) theory. Both Google Scholar (GS) and Google Trends (GT) data related to popular IS theories from 2006 to 2020 are collected. The study provides an overview of popular IS theories and tests the pair data to show changes in GSs total publications and GTs public interest. Correlation analyses are conducted to investigate the relationships between GS and GT data, as well as between GS and GT data. Trend analysis reveals that there are certain IS theories that researchers and practitioners should pay attention to. In addition, this study provides an overview of recent and future directions in this field. Despite the significant increase in research using Google data, the use of big data to explore the interests and applications of IS theories has been rarely explored."
Libraries roles and practices to enhance information resilience: Academic librariansâ perspectives,"COVID-19 has changed the information landscape. The results were the wide spread of information of all kinds and from all sources, leading to increased information disorder (i.e. disinformation, misinformation practices). This study aimed to explore the practices of academic libraries during times of crisis from the staff perception in four public universities in Jordan to correct the information disorder and hence create information resilience among community members. Exploring these practices helps shed light on the librariesâ contribution to creating information resilience practices and hence an information-resilient community. An interview was used to collect deep insights from 26 library staff working in the Information Division at the four academic libraries. The results were mainly directed towards the importance of increasing community awareness and providing access to quality information sources. It also affirmed the importance of information skills to help individuals locate the right and accurate information. It was affirmed that developing and promoting information literacy programmes was the main pillar to countering information disorder and establishing information resilience. The findings will provide insights for other academic libraries on the best practices to create a information resilient community. Â© The Author(s) 2023.","COVID-19 has changed the information landscape. The results were the wide spread of information of all kinds and from all sources, leading to increased information disorder ( disinformation, misinformation practices). This study aimed to explore the practices of academic libraries during times of crisis from the staff perception in four public universities in Jordan to correct the information disorder and hence create information resilience among community members. Exploring these practices helps shed light on the libraries contribution to creating information resilience practices and hence an information-resilient community. An interview was used to collect deep insights from 26 library staff working in the Information Division at the four academic libraries. The results were mainly directed towards the importance of increasing community awareness and providing access to quality information sources. It also affirmed the importance of information skills to help individuals locate the right and accurate information. It was affirmed that developing and promoting information literacy programmes was the main pillar to countering information disorder and establishing information resilience. The findings will provide insights for other academic libraries on the best practices to create a information resilient community."
A transformer-based deep learning model for Persian moral sentiment analysis,"Moral expressions in online communications can have a serious impact on framing discussions and subsequent online behaviours. Despite research on extracting moral sentiment from English text, other low-resource languages, such as Persian, lack enough resources and research about this important topic. We address this issue using the Moral Foundation theory (MFT) as the theoretical moral psychology paradigm. We developed a Twitter data set of 8000 tweets that are manually annotated for moral foundations and also we established a baseline for computing moral sentiment from Persian text. We evaluate a plethora of state-of-the-art machine learning models, both rule-based and neural, including distributed dictionary representation (DDR), long short-term memory (LSTM) and bidirectional encoder representations from transformer (BERT). Our findings show that among different models, fine-tuning a pre-trained Persian BERT language model with a linear network as the classifier yields the best results. Furthermore, we analysed this model to find out which layer of the model contributes most to this superior accuracy. We also proposed an alternative transformer-based model that yields competitive results to the BERT model despite its lower size and faster inference time. The proposed model can be used as a tool for analysing moral sentiment and framing in Persian texts for downstream social and psychological studies. We also hope our work provides some resources for further enhancing the methods for computing moral sentiment in Persian text. Â© The Author(s) 2023.","Moral expressions in online communications can have a serious impact on framing discussions and subsequent online behaviours. Despite research on extracting moral sentiment from English text, other low-resource languages, such as Persian, lack enough resources and research about this important topic. We address this issue using the Moral Foundation theory (MFT) as the theoretical moral psychology paradigm. We developed a Twitter data set of 8000 tweets that are manually annotated for moral foundations and also we established a baseline for computing moral sentiment from Persian text. We evaluate a plethora of state-of-the-art machine learning models, both rule-based and neural, including distributed dictionary representation (DDR), long short-term memory (LSTM) and bidirectional encoder representations from transformer (BERT). Our findings show that among different models, fine-tuning a pre-trained Persian BERT language model with a linear network as the classifier yields the best results. Furthermore, we analysed this model to find out which layer of the model contributes most to this superior accuracy. We also proposed an alternative transformer-based model that yields competitive results to the BERT model despite its lower size and faster inference time. The proposed model can be used as a tool for analysing moral sentiment and framing in Persian texts for downstream social and psychological studies. We also hope our work provides some resources for further enhancing the methods for computing moral sentiment in Persian text."
"Scholarly article retrieval from Web of Science, Scopus and Dimensions: A comparative analysis of retrieval quality","Scholarly databases are now being increasingly used for search and retrieval of research articles in different subject areas. Several previous studies have shown that different databases vary in their coverage of publication sources, and therefore, one may expect that for a given query, they may retrieve different results. However, how do these databases compare in terms of relevance of the retrieved results is relatively unexplored. This study, therefore, attempts to bridge this research gap by carrying out a systematic study of retrieval relevance of the three scholarly databases â Web of Science, Scopus and Dimensions. Five selected queries are used for this purpose. The retrieved results from the three databases for the given queries are first analysed in terms of volume of retrieved records, language of retrieved records, etc. Thereafter, a user-based annotation scheme is used to assess and compare the relevance of retrieved results. The standard measure of normalised discounted cumulative gain (NDCG) and Spearman rank correlation coefficient (SRCC) is computed for the purpose. Results indicate that although the number of retrieved results for the same query differs significantly in the three databases, the databases differ only marginally in retrieval relevance, with Web of Science having a slight edge over other two. Â© The Author(s) 2023.","Scholarly databases are now being increasingly used for search and retrieval of research articles in different subject areas. Several previous studies have shown that different databases vary in their coverage of publication sources, and therefore, one may expect that for a given query, they may retrieve different results. However, how do these databases compare in terms of relevance of the retrieved results is relatively unexplored. This study, therefore, attempts to bridge this research gap by carrying out a systematic study of retrieval relevance of the three scholarly databases Web of Science, Scopus and Dimensions. Five selected queries are used for this purpose. The retrieved results from the three databases for the given queries are first analysed in terms of volume of retrieved records, language of retrieved records, etc. Thereafter, a user-based annotation scheme is used to assess and compare the relevance of retrieved results. The standard measure of normalised discounted cumulative gain (NDCG) and Spearman rank correlation coefficient (SRCC) is computed for the purpose. Results indicate that although the number of retrieved results for the same query differs significantly in the three databases, the databases differ only marginally in retrieval relevance, with Web of Science having a slight edge over other two."
"Revolutions in science: The proposal of an approach for the identification of most important researchers, institutions and countries based on co-citation reference publication year spectroscopy exemplified at research on physical modelling of Earthâs climate","Reference Publication Year Spectroscopy (RPYS) is a bibliometric method originally introduced to reveal the historical roots of research topics or fields. RPYS does not identify the most highly cited papers of the publication set being studied (as is usually done by bibliometric analyses in research evaluation), but instead it indicates most frequently referenced publications â each within a specific reference publication year. In this study, we propose to use the method to identify important researchers, institutions, and countries in the context of breakthrough research. To do so, we focus on research on physical modeling of Earthâs climate and the prediction of global warming as an example. Klaus Hasselmann (KH) and Syukuro Manabe (SM) were both honoured with the Nobel Prize in 2021 for their fundamental contributions to research on physical modeling of Earthâs climate and the prediction of global warming. Our results reveal that RPYS is able to identify most important researchers, institutions, and countries. In our example, all the relevant authorsâ institutions are located in the United States. These institutions are either research centers of two US National Research Administrations (NASA and NOAA) or universities: the University of Arizona, Princeton University, the Massachusetts Institute of Technology (MIT), and the University of Stony Brook. The limitations of our approach to identify important researchers, institutions, and countries in the context of breakthrough research are discussed. Â© The Author(s) 2023.","Reference Publication Year Spectroscopy (RPYS) is a bibliometric method originally introduced to reveal the historical roots of research topics or fields. RPYS does not identify the most highly cited papers of the publication set being studied (as is usually done by bibliometric analyses in research evaluation), but instead it indicates most frequently referenced publications each within a specific reference publication year. In this study, we propose to use the method to identify important researchers, institutions, and countries in the context of breakthrough research. To do so, we focus on research on physical modeling of Earths climate and the prediction of global warming as an example. Klaus Hasselmann (KH) and Syukuro Manabe (SM) were both honoured with the Nobel Prize in 2021 for their fundamental contributions to research on physical modeling of Earths climate and the prediction of global warming. Our results reveal that RPYS is able to identify most important researchers, institutions, and countries. In our example, all the relevant authors institutions are located in the United States. These institutions are either research centers of two US National Research Administrations (NASA and NOAA) or universities: the University of Arizona, Princeton University, the Massachusetts Institute of Technology (MIT), and the University of Stony Brook. The limitations of our approach to identify important researchers, institutions, and countries in the context of breakthrough research are discussed."
Investigating the reviewer assignment problem: A systematic literature review,"The assignment of appropriate reviewers to academic articles, known as the reviewer assignment problem (RAP), has become a crucial issue in academia. While there has been much research on RAP, there has not yet been a systematic literature review (SLR) examining the various approaches, techniques, algorithms and discoveries related to this topic. To conduct the SLR, we identified and evaluated relevant articles from four databases using defined inclusion and exclusion criteria. We analysed the selected articles and extracted information, and assessed their quality. Our review identified 67 articles on RAP published in conferences and journals up to mid-2022. As one of the main challenges in RAP is acquiring open data, we have studied the data sources used by researchers and found that most studies use real data from conferences, bibliographic databases and online academic search engines. RAP is divided into two main phases: (1) finding/recommending expert reviewers and (2) assigning reviewers to submitted manuscripts. In Phase 1, we have identified that decision support systems, recommendation systems, and machine learning-oriented approaches are more commonly used due to better results. In Phase 2, heuristics and metaheuristics are the approaches that present better results and are consequently more commonly used by researchers. Based on the analysed studies, we have identified potential areas for future research that could lead to improved results. Specifically, we suggest exploring the application of deep neural networks for calculating the degree of correspondence and using the Boolean satisfiability problem to optimise the attribution process. Â© The Author(s) 2023.","The assignment of appropriate reviewers to academic articles, known as the reviewer assignment problem (RAP), has become a crucial issue in academia. While there has been much research on RAP, there has not yet been a systematic literature review (SLR) examining the various approaches, techniques, algorithms and discoveries related to this topic. To conduct the SLR, we identified and evaluated relevant articles from four databases using defined inclusion and exclusion criteria. We analysed the selected articles and extracted information, and assessed their quality. Our review identified 67 articles on RAP published in conferences and journals up to mid-2022. As one of the main challenges in RAP is acquiring open data, we have studied the data sources used by researchers and found that most studies use real data from conferences, bibliographic databases and online academic search engines. RAP is divided into two main phases: finding/recommending expert reviewers and assigning reviewers to submitted manuscripts. In Phase 1, we have identified that decision support systems, recommendation systems, and machine learning-oriented approaches are more commonly used due to better results. In Phase 2, heuristics and metaheuristics are the approaches that present better results and are consequently more commonly used by researchers. Based on the analysed studies, we have identified potential areas for future research that could lead to improved results. Specifically, we suggest exploring the application of deep neural networks for calculating the degree of correspondence and using the Boolean satisfiability problem to optimise the attribution process."
A mathematical analysis of the h-index and study of its correlation with some improved metrics: A conceptual approach,"This article aims to establish the h-index in different mathematical aspects and measure the correlation of the h-index with other metrics using the bibliographic data of selected journals in the Library and Information Science (LIS) domain. Using the concept of relation and function, the h-index is expressed. Data collected from authors in three major LIS journals, including h-index, g-index, m-index, total citations and total publications, are analysed using correlation and regression analyses. The findings indicate a high level of relationship between h-index and g-index scores and m-index at starting year of publication. Conversely, a lower level of relationship between the h-index, g-index, starting year of publication, the total number of publications and the number of citations. This is an original study and will be of interest to the researchers of LIS who wants to know this performance indicator in different aspects, the dependency/independency of the h-index with other metrics, and the impact/performance of the three journals over time in terms of h-index, m-index, total citations and the number of publications. The study is limited to analysis of performance-and-citation-related measurements h-index, g-index, m-index, total citations, and the number of publications. Â© The Author(s) 2023.","This article aims to establish the h-index in different mathematical aspects and measure the correlation of the h-index with other metrics using the bibliographic data of selected journals in the Library and Information Science (LIS) domain. Using the concept of relation and function, the h-index is expressed. Data collected from authors in three major LIS journals, including h-index, g-index, m-index, total citations and total publications, are analysed using correlation and regression analyses. The findings indicate a high level of relationship between h-index and g-index scores and m-index at starting year of publication. Conversely, a lower level of relationship between the h-index, g-index, starting year of publication, the total number of publications and the number of citations. This is an original study and will be of interest to the researchers of LIS who wants to know this performance indicator in different aspects, the dependency/independency of the h-index with other metrics, and the impact/performance of the three journals over time in terms of h-index, m-index, total citations and the number of publications. The study is limited to analysis of performance-and-citation-related measurements h-index, g-index, m-index, total citations, and the number of publications."
The effect of knowledge acquisition and knowledge sharing on the use of E-learning,"This descriptive-analytical study was conducted to evaluate the effect of knowledge acquisition and knowledge sharing on the use of e-learning. Data were evaluated by using a questionnaire designed by the researchers, the components of which were based on the Theory of Reasoned Action (TRA). The research population included faculty members, and the research sample was selected based on Morganâs Table. The collected data were analysed using SPSS software, and the final model was presented. The results indicated that knowledge acquisition and knowledge sharing have a significant and direct effect on attitude and subjective norms of faculty members in using e-learning. Other findings showed that attitude and subjective norms control faculty membersâ behavioural intention to use e-learning. It can be concluded that, knowledge acquisition and knowledge sharing are among the influential factors in the successful implementation of e-learning and are effective elements of attitude and subjective norms of users. Â© The Author(s) 2023.","This descriptive-analytical study was conducted to evaluate the effect of knowledge acquisition and knowledge sharing on the use of e-learning. Data were evaluated by using a questionnaire designed by the researchers, the components of which were based on the Theory of Reasoned Action (TRA). The research population included faculty members, and the research sample was selected based on Morgans Table. The collected data were analysed using SPSS software, and the final model was presented. The results indicated that knowledge acquisition and knowledge sharing have a significant and direct effect on attitude and subjective norms of faculty members in using e-learning. Other findings showed that attitude and subjective norms control faculty members behavioural intention to use e-learning. It can be concluded that, knowledge acquisition and knowledge sharing are among the influential factors in the successful implementation of e-learning and are effective elements of attitude and subjective norms of users."
Development of a scale for measuring individual propensity for serendipitous information encounters in an online environment,"This article reports the development of a scale designed to measure an individualâs tendency to engage in serendipitous information encountering behaviour online. We relied on the simplified process model of information encountering to derive an initial item pool. In Study 1, a total of 3037 participants completed a preliminary 24-item scale. Using a split-half principal components analysis and confirmatory factor analysis to determine the underlying factor structure, we reduced the scale to 12 items. In Study 2 (N = 66), we demonstrated the revised scaleâs testâretest reliability and showed that it distinguishes between information encountering and other conceptually related constructs. In Study 3, behavioural data from 304 participants showed that the information encountering scale is associated with various indicators of online information-seeking, such as page visits and visit duration. Collectively, these studies yield a valid and easily implemented scale to better understand online information encountering. We describe the development of this scale and discuss its implications for both the measurement and study of serendipity in online information behaviour. Â© The Author(s) 2023.","This article reports the development of a scale designed to measure an individuals tendency to engage in serendipitous information encountering behaviour online. We relied on the simplified process model of information encountering to derive an initial item pool. In Study 1, a total of 3037 participants completed a preliminary 24-item scale. Using a split-half principal components analysis and confirmatory factor analysis to determine the underlying factor structure, we reduced the scale to 12 items. In Study 2 (N = 66), we demonstrated the revised scales testretest reliability and showed that it distinguishes between information encountering and other conceptually related constructs. In Study 3, behavioural data from 304 participants showed that the information encountering scale is associated with various indicators of online information-seeking, such as page visits and visit duration. Collectively, these studies yield a valid and easily implemented scale to better understand online information encountering. We describe the development of this scale and discuss its implications for both the measurement and study of serendipity in online information behaviour."
The effects of globalisation techniques on feature selection for text classification,"Text classification (TC) is very important and critical task in the 21th century as there exist high volume of electronic data on the Internet. In TC, textual data are characterised by a huge number of highly sparse features/terms. A typical TC consists of many steps and one of the most important steps is undoubtedly feature selection (FS). In this study, we have comprehensively investigated the effects of various globalisation techniques on local feature selection (LFS) methods using datasets with different characteristics such as multi-class unbalanced (MCU), multi-class balanced (MCB), binary-class unbalanced (BCU) and binary-class balanced (BCB). The globalisation techniques used in this study are summation (SUM), weighted-sum (AVG), and maximum (MAX). To investigate the effect of globalisation techniques, we used three LFS methods named as Discriminative Feature Selection (DFSS), odds ratio (OR) and chi-square (CHI2). In the experiments, we have utilised four different benchmark datasets named as Reuters-21578, 20Newsgroup., Enron1, and Polarity in addition to Support Vector Machines (SVM) and Decision Tree (DT) classifiers. According to the experimental results, the most successful globalisation technique is AVG while all situations are taken into account. The experimental results indicate that DFSS method is more successful than OR and CHI2 methods on datasets with MCU and MCB characteristics. However, CHI2 method seems more accurate than OR and DFSS methods on datasets with BCU and BCB characteristics. Also, SVM classifier performed better than DT classifier in most cases. Â© The Author(s) 2020.","Text classification (TC) is very important and critical task in the 21th century as there exist high volume of electronic data on the Internet. In TC, textual data are characterised by a huge number of highly sparse features/terms. A typical TC consists of many steps and one of the most important steps is undoubtedly feature selection (FS). In this study, we have comprehensively investigated the effects of various globalisation techniques on local feature selection (LFS) methods using datasets with different characteristics such as multi-class unbalanced (MCU), multi-class balanced (MCB), binary-class unbalanced (BCU) and binary-class balanced (BCB). The globalisation techniques used in this study are summation (SUM), weighted-sum (AVG), and maximum (MAX). To investigate the effect of globalisation techniques, we used three LFS methods named as Discriminative Feature Selection (DFSS), odds ratio (OR) and chi-square (CHI2). In the experiments, we have utilised four different benchmark datasets named as Reuters-21578, 20Newsgroup., Enron1, and Polarity in addition to Support Vector Machines (SVM) and Decision Tree (DT) classifiers. According to the experimental results, the most successful globalisation technique is AVG while all situations are taken into account. The experimental results indicate that DFSS method is more successful than OR and CHI2 methods on datasets with MCU and MCB characteristics. However, CHI2 method seems more accurate than OR and DFSS methods on datasets with BCU and BCB characteristics. Also, SVM classifier performed better than DT classifier in most cases."
The distinctiveness of author interdisciplinarity: A long-neglected issue in research on interdisciplinarity,"In the research on interdisciplinarity (RID), measures for evaluating the interdisciplinarity of scientific entities (e.g., papers, authors, journals or research areas) have been proposed for a long time. The author interdisciplinarity is very different from the other types of interdisciplinarity because of the complex interpersonal relationships between the connected authors. However, previous work has failed to uncover the distinctiveness of author interdisciplinarity and has regarded it as equivalent to other types of interdisciplinarity. In this work, an extended RaoâStirling diversity measure is proposed, which incorporates the co-author network and a network similarity measure to specifically evaluate the author interdisciplinarity. Moreover, betweenness centrality is used for improving network similarity measure, because of its intrinsic advantage of expressing how an entity loads on different factors in a network, which is highly in line with the characteristic of interdisciplinarity. An experiment on the papers about Public Administration in the Web of Science is conducted; based on the final results, a deeper investigation is performed into by typical authors. The work proposes a novel idea for measuring author interdisciplinarity, which can promote the study of interdisicplinarity measuring in RID. Â© The Author(s) 2020.","In the research on interdisciplinarity (RID), measures for evaluating the interdisciplinarity of scientific entities (, papers, authors, journals or research areas) have been proposed for a long time. The author interdisciplinarity is very different from the other types of interdisciplinarity because of the complex interpersonal relationships between the connected authors. However, previous work has failed to uncover the distinctiveness of author interdisciplinarity and has regarded it as equivalent to other types of interdisciplinarity. In this work, an extended RaoStirling diversity measure is proposed, which incorporates the co-author network and a network similarity measure to specifically evaluate the author interdisciplinarity. Moreover, betweenness centrality is used for improving network similarity measure, because of its intrinsic advantage of expressing how an entity loads on different factors in a network, which is highly in line with the characteristic of interdisciplinarity. An experiment on the papers about Public Administration in the Web of Science is conducted; based on the final results, a deeper investigation is performed into by typical authors. The work proposes a novel idea for measuring author interdisciplinarity, which can promote the study of interdisicplinarity measuring in RID."
A method of semi-automated ontology population from multiple semi-structured data sources,"Organisations use data in different formats: Word documents, Excel spreadsheets, databases, HTML pages and so on. It is not easy to make decisions with such data due to the lack of integration between the different sources and built-in decision-making rules. Decisions can be reached with knowledge bases, which, unlike databases, make it possible to store not only objects, facts and attributes but also more sophisticated patterns such as rules and axioms. The article proposes an ontology-based method for knowledge base creation that allows for the simultaneous integration of semi-structured data sources and extendibility while remaining context independent. At the initial steps of the method, data specification should be performed with the Data Sources Ontology developed by the authors. This ontology provides data structure description that forms supportive knowledge graph. The graphâs schema should be mapped with the domain ontology to be populated. Finally, the data are inserted into the domain ontology according to the mapping rules. Manual input is needed during data specification and data-to-ontology schema mapping. Â© The Author(s) 2020.","Organisations use data in different formats: Word documents, Excel spreadsheets, databases, HTML pages and so on. It is not easy to make decisions with such data due to the lack of integration between the different sources and built-in decision-making rules. Decisions can be reached with knowledge bases, which, unlike databases, make it possible to store not only objects, facts and attributes but also more sophisticated patterns such as rules and axioms. The article proposes an ontology-based method for knowledge base creation that allows for the simultaneous integration of semi-structured data sources and extendibility while remaining context independent. At the initial steps of the method, data specification should be performed with the Data Sources Ontology developed by the authors. This ontology provides data structure description that forms supportive knowledge graph. The graphs schema should be mapped with the domain ontology to be populated. Finally, the data are inserted into the domain ontology according to the mapping rules. Manual input is needed during data specification and data-to-ontology schema mapping."
In-Cites research fronts and its relationship with citations per document and highly cited papers: Spanish universities as a case study,"Research fronts (RFs) represent the most dynamic areas of science and technology and the topics that receive higher attention in a specific field. Their identification has become the focus of global scientific and technological competition. In this study, we performed an analysis of the In-Cites RFs from Web of Science (WoS) in seven Spanish universities as a case study in the period 2015â2019. Our purpose is threefold: (1) to develop a methodology for approximating the âalignmentâ of scientific articles with the RFs, (2) to test if Highly Cited Papers (HCP) from these universities are more aligned with RFs than the rest of the output, and (3) to test if papers aligned with RFs receive more citations per document than those not aligned. The study uses a novel retrieval method, and the analysis is conducted using a coincidence method (comparison with a standard), in which MannâKendallâs test and Pearsonâs correlation are used. The results show that there is alignment between output and the HCP and those better aligned, present greater mean ranks of citations per article. This study shows the usefulness of RFs for orienting the research priorities at the institutional level for the university. Â© The Author(s) 2022.","Research fronts (RFs) represent the most dynamic areas of science and technology and the topics that receive higher attention in a specific field. Their identification has become the focus of global scientific and technological competition. In this study, we performed an analysis of the In-Cites RFs from Web of Science (WoS) in seven Spanish universities as a case study in the period 20152019. Our purpose is threefold: to develop a methodology for approximating the alignment of scientific articles with the RFs, to test if Highly Cited Papers (HCP) from these universities are more aligned with RFs than the rest of the output, and to test if papers aligned with RFs receive more citations per document than those not aligned. The study uses a novel retrieval method, and the analysis is conducted using a coincidence method (comparison with a standard), in which MannKendalls test and Pearsons correlation are used. The results show that there is alignment between output and the HCP and those better aligned, present greater mean ranks of citations per article. This study shows the usefulness of RFs for orienting the research priorities at the institutional level for the university."
"An overview of literature on COVID-19, MERS and SARS: Using text mining and latent Dirichlet allocation","The unprecedented outbreak of COVID-19 is one of the most serious global threats to public health in this century. During this crisis, specialists in information science could play key roles to support the efforts of scientists in the health and medical community for combatting COVID-19. In this article, we demonstrate that information specialists can support health and medical community by applying text mining technique with latent Dirichlet allocation procedure to perform an overview of a mass of coronavirus literature. This overview presents the generic research themes of the coronavirus diseases: COVID-19, MERS and SARS, reveals the representative literature per main research theme and displays a network visualisation to explore the overlapping, similarity and difference among these themes. The overview can help the health and medical communities to extract useful information and interrelationships from coronavirus-related studies. Â© The Author(s) 2020.","The unprecedented outbreak of COVID-19 is one of the most serious global threats to public health in this century. During this crisis, specialists in information science could play key roles to support the efforts of scientists in the health and medical community for combatting COVID-19. In this article, we demonstrate that information specialists can support health and medical community by applying text mining technique with latent Dirichlet allocation procedure to perform an overview of a mass of coronavirus literature. This overview presents the generic research themes of the coronavirus diseases: COVID-19, MERS and SARS, reveals the representative literature per main research theme and displays a network visualisation to explore the overlapping, similarity and difference among these themes. The overview can help the health and medical communities to extract useful information and interrelationships from coronavirus-related studies."
SBTM: A joint sentiment and behaviour topic model for online course discussion forums,"Large quantities of textual posts are increasingly generated in course discussion forums, and the accumulation of these data greatly increases the cognitive loads on online participants. It is imperative for them to automatically identify the potential semantic information derived from these textual discourse interactions. Moreover, existing topic models can discover the latent topics or sentimental polarities from textual data, but these models typically ignore the interactive ways of discussing topics, thus making it difficult to further construct topicsâ semantic space from the perspective of document generation. To solve this issue, we proposed a joint sentiment and behaviour topic model called SBTM, which was an unsupervised approach for automatic analysis of learnersâ discussed posts. The results demonstrated that SBTM was quantitatively effective on both model generalisation and topic exploration, and rich topic content was qualitatively characterised. Furthermore, the model can be potentially employed in some practical applications, such as information summarisation and behaviour-oriented personalised recommendation. Â© The Author(s) 2020.","Large quantities of textual posts are increasingly generated in course discussion forums, and the accumulation of these data greatly increases the cognitive loads on online participants. It is imperative for them to automatically identify the potential semantic information derived from these textual discourse interactions. Moreover, existing topic models can discover the latent topics or sentimental polarities from textual data, but these models typically ignore the interactive ways of discussing topics, thus making it difficult to further construct topics semantic space from the perspective of document generation. To solve this issue, we proposed a joint sentiment and behaviour topic model called SBTM, which was an unsupervised approach for automatic analysis of learners discussed posts. The results demonstrated that SBTM was quantitatively effective on both model generalisation and topic exploration, and rich topic content was qualitatively characterised. Furthermore, the model can be potentially employed in some practical applications, such as information summarisation and behaviour-oriented personalised recommendation."
"Group trip planning and information seeking behaviours by mobile social media users: A study of tourists in Australia, Bangladesh and China","Social media plays an increasingly important role in travel information seeking and decision-making. However, there is limited understanding of how a group of tourists use social media to plan trips collaboratively and the different practices between countries. In this study, we investigated the collaborative information seeking (CIS) and sharing behaviours of mobile social media users from Australia, Bangladesh and China. Specifically, we surveyed a total of 219 participants to explore the differences in CIS behaviours when people were planning a group trip. The findings suggest significant differences among three countries in terms of the motivations of using social media, CIS activities and social interactions outside the group. Key findings include Bangladeshi and Chinese travellers preferred known contacts on social media, while Australian tourists intended to use both known contacts and user-generated contents for seeking information. The findings also show that social interactions employed by individuals are considered as an important complement of and are interwoven with in-group CIS; both contribute to tourism information seeking. Finally, we propose a framework for CIS research in the tourism domain. Â© The Author(s) 2019.","Social media plays an increasingly important role in travel information seeking and decision-making. However, there is limited understanding of how a group of tourists use social media to plan trips collaboratively and the different practices between countries. In this study, we investigated the collaborative information seeking (CIS) and sharing behaviours of mobile social media users from Australia, Bangladesh and China. Specifically, we surveyed a total of 219 participants to explore the differences in CIS behaviours when people were planning a group trip. The findings suggest significant differences among three countries in terms of the motivations of using social media, CIS activities and social interactions outside the group. Key findings include Bangladeshi and Chinese travellers preferred known contacts on social media, while Australian tourists intended to use both known contacts and user-generated contents for seeking information. The findings also show that social interactions employed by individuals are considered as an important complement of and are interwoven with in-group CIS; both contribute to tourism information seeking. Finally, we propose a framework for CIS research in the tourism domain."
Measuring visibility of disciplines on Chinese academic web,"This study proposes a hierarchy affiliation model (departmentâschoolâuniversity) to build network between web entities taking into account the domain names, the topological structure of academic network and the disciplinary characteristics of schools and universities synthetically. The study of the Chinese academic web based on the model shows that at the school level, 68 of 95 disciplines (71.6%) are identified from the directed school network and 71 from the undirected school network, respectively; at the university level, four out of seven broad disciplines are found. Furthermore, according to the comparative result based on three types of relations (hyperlinks, citations and collaborations) among universities, we would like to argue with cautions that the structure on academic web would potentially be more suitable to trace the interests in common between institutions. Â© The Author(s) 2020.","This study proposes a hierarchy affiliation model (departmentschooluniversity) to build network between web entities taking into account the domain names, the topological structure of academic network and the disciplinary characteristics of schools and universities synthetically. The study of the Chinese academic web based on the model shows that at the school level, 68 of 95 disciplines (71.6%) are identified from the directed school network and 71 from the undirected school network, respectively; at the university level, four out of seven broad disciplines are found. Furthermore, according to the comparative result based on three types of relations (hyperlinks, citations and collaborations) among universities, we would like to argue with cautions that the structure on academic web would potentially be more suitable to trace the interests in common between institutions."
A multi-strategy approach for the merging of multiple taxonomies,"Taxonomy merging is an important work to provide a uniform schema for several heterogeneous taxonomies. Previous studies primarily focus on merging two taxonomies in a specific domain, while the merging of multiple taxonomies has been neglected. This article proposes a taxonomy merging approach to automatically merge multiple source taxonomies into a target taxonomy in an asymmetric manner. The approach adopts a strategy of breaking up the whole into parts to decrease the complexity of merging multiple taxonomies and employs a block-based method to reduce the scale of measuring semantic relations between concept pairs. In addition, for the problem of multiple inheritance, a method of topical coverage is proposed. Experiments conducted on synthetic and real-world scenarios indicate that the proposed merging approach is feasible and effective to merge multiple taxonomies. In particular, the proposed approach works well in the aspects of limiting the semantic redundancy and establishing high-quality hierarchical relations between concepts. Â© The Author(s) 2020.","Taxonomy merging is an important work to provide a uniform schema for several heterogeneous taxonomies. Previous studies primarily focus on merging two taxonomies in a specific domain, while the merging of multiple taxonomies has been neglected. This article proposes a taxonomy merging approach to automatically merge multiple source taxonomies into a target taxonomy in an asymmetric manner. The approach adopts a strategy of breaking up the whole into parts to decrease the complexity of merging multiple taxonomies and employs a block-based method to reduce the scale of measuring semantic relations between concept pairs. In addition, for the problem of multiple inheritance, a method of topical coverage is proposed. Experiments conducted on synthetic and real-world scenarios indicate that the proposed merging approach is feasible and effective to merge multiple taxonomies. In particular, the proposed approach works well in the aspects of limiting the semantic redundancy and establishing high-quality hierarchical relations between concepts."
REDI: Towards knowledge graph-powered scholarly information management and research networking,"Academic data management has become an increasingly challenging task as research evolves over time. Essential tasks such as information retrieval and research networking have turned into extremely difficult operations due to an ever-growing number of researchers and scientific articles. Numerous initiatives have emerged in the IT environments to address this issue, especially focused on web technologies. Although those approaches have individually provided solutions for diverse problems, they still can not offer integrated knowledge bases nor flexibility to exploit adequately this information. In this article, we present REDI, a Linked Data-powered framework for academic knowledge management and research networking, which introduces a new perspective of integration. REDI combines information from multiple sources into a consolidated knowledge base through state-of-the-art procedures and leverages semantic web standards to represent the information. Moreover, REDI takes advantage of such knowledge for data visualisation and analysis, which ultimately improves and simplifies many activities including research networking. Â© The Author(s) 2020.","Academic data management has become an increasingly challenging task as research evolves over time. Essential tasks such as information retrieval and research networking have turned into extremely difficult operations due to an ever-growing number of researchers and scientific articles. Numerous initiatives have emerged in the IT environments to address this issue, especially focused on web technologies. Although those approaches have individually provided solutions for diverse problems, they still can not offer integrated knowledge bases nor flexibility to exploit adequately this information. In this article, we present REDI, a Linked Data-powered framework for academic knowledge management and research networking, which introduces a new perspective of integration. REDI combines information from multiple sources into a consolidated knowledge base through state-of-the-art procedures and leverages semantic web standards to represent the information. Moreover, REDI takes advantage of such knowledge for data visualisation and analysis, which ultimately improves and simplifies many activities including research networking."
On the relationship between supervisorâsupervisee gender difference and scientific impact of doctoral dissertations: Evidence from Humanities and Social Sciences in China,"This article explores the relationships between supervisorâsupervisee gender difference and the scientific impact of doctoral dissertations. We use the China Doctoral Dissertations Full-text Database and pay special attention to the fields of Humanities and Social Sciences in China in our empirical study. By establishing regression models, we find that the ranks of the scientific impact regarding doctoral dissertations are femaleâfemale (first), femaleâmale (second), maleâmale (third) and maleâfemale (fourth) pairs (sequence: student gender and then supervisor gender). The finding has many interesting implications for science policy and gender inequality. Â© The Author(s) 2020.","This article explores the relationships between supervisorsupervisee gender difference and the scientific impact of doctoral dissertations. We use the China Doctoral Dissertations Full-text Database and pay special attention to the fields of Humanities and Social Sciences in China in our empirical study. By establishing regression models, we find that the ranks of the scientific impact regarding doctoral dissertations are femalefemale (first), femalemale (second), malemale (third) and malefemale (fourth) pairs (sequence: student gender and then supervisor gender). The finding has many interesting implications for science policy and gender inequality."
Knowledge discovery using SPARQL property path: The case of disease data set,"The Semantic Web allows knowledge discovery on graph-based data sets and facilitates answering complex queries that are extremely difficult to achieve using traditional database approaches. Intuitively, the Semantic Web query language (SPARQL) has a âproperty pathâ feature that enables knowledge discovery in a knowledgebase using its reasoning engine. In this article, we utilise the property path of SPARQL and the other Semantic Web technologies to answer sophisticated queries posed over a disease data set. To this aim, we transform data from a disease web portal to a graph-based data set by designing an ontology, present a template to define the queries and provide a set of conjunctive queries on the data set. We illustrate how the reasoning engine of âproperty pathâ feature of SPARQL can retrieve the results from the designed knowledgebase. The results of this study were verified by two domain experts as well as authorsâ manual exploration on the disease web portal. Â© The Author(s) 2019.","The Semantic Web allows knowledge discovery on graph-based data sets and facilitates answering complex queries that are extremely difficult to achieve using traditional database approaches. Intuitively, the Semantic Web query language (SPARQL) has a property path feature that enables knowledge discovery in a knowledgebase using its reasoning engine. In this article, we utilise the property path of SPARQL and the other Semantic Web technologies to answer sophisticated queries posed over a disease data set. To this aim, we transform data from a disease web portal to a graph-based data set by designing an ontology, present a template to define the queries and provide a set of conjunctive queries on the data set. We illustrate how the reasoning engine of property path feature of SPARQL can retrieve the results from the designed knowledgebase. The results of this study were verified by two domain experts as well as authors manual exploration on the disease web portal."
A semi-hierarchical clustering method for constructing knowledge trees from stackoverflow,"To help students learn how to programme, we have to give them a clear knowledge map and sufficient materials. Question-based websites, such as stackoverflow, are excellent information sources for this goal. However, for beginners, the process can be a little tricky since they may not know how to ask correct questions if they do not have sufficient background knowledge, and a knowledge tree is usually considered more helpful in such a scenario. In this research, a method to infer a knowledge tree automatically from the type of websites and to group documents based on the resulting knowledge tree is proposed. The proposed method mainly addresses two issues: first, the quality of tags cannot be guaranteed, and second, clustering-based methods usually generate the flat schema. The occurrence count and the co-occurrence ratio were used together to identify important tags. Then, an algorithm was developed to infer the hierarchical relationship between tags. Using these tags as centres, the clustering performance is better than applying k-means alone. Â© The Author(s) 2020.","To help students learn how to programme, we have to give them a clear knowledge map and sufficient materials. Question-based websites, such as stackoverflow, are excellent information sources for this goal. However, for beginners, the process can be a little tricky since they may not know how to ask correct questions if they do not have sufficient background knowledge, and a knowledge tree is usually considered more helpful in such a scenario. In this research, a method to infer a knowledge tree automatically from the type of websites and to group documents based on the resulting knowledge tree is proposed. The proposed method mainly addresses two issues: first, the quality of tags cannot be guaranteed, and second, clustering-based methods usually generate the flat schema. The occurrence count and the co-occurrence ratio were used together to identify important tags. Then, an algorithm was developed to infer the hierarchical relationship between tags. Using these tags as centres, the clustering performance is better than applying k-means alone."
Dynamical entropic analysis of scientific concepts,"In the present era of information, the problem of effective knowledge retrieval from a collection of scientific documents becomes especially important for continuous scientific progress. The information available in scientific publications traditionally consists of bibliometric metadata and its semantic component such as title, abstract and text. While the former having a machine-readable format usually used for knowledge mapping and pattern recognition, the latter designed for human interpretation and analysis. Only a few studies use full-text analysis, based on carefully selected scientific ontology, to map the actual structure of the scientific knowledge or uncover similarities between documents. Unfortunately, the presence of common (basic) concepts across semantically unrelated documents creates spurious connections between different topics. We revise the known method based on the entropic information-theoretic measure used for selecting basic concepts and propose to analyse the dynamics of Shannon entropy for more rigorous sorting of concepts by their generality. Â© The Author(s) 2020.","In the present era of information, the problem of effective knowledge retrieval from a collection of scientific documents becomes especially important for continuous scientific progress. The information available in scientific publications traditionally consists of bibliometric metadata and its semantic component such as title, abstract and text. While the former having a machine-readable format usually used for knowledge mapping and pattern recognition, the latter designed for human interpretation and analysis. Only a few studies use full-text analysis, based on carefully selected scientific ontology, to map the actual structure of the scientific knowledge or uncover similarities between documents. Unfortunately, the presence of common (basic) concepts across semantically unrelated documents creates spurious connections between different topics. We revise the known method based on the entropic information-theoretic measure used for selecting basic concepts and propose to analyse the dynamics of Shannon entropy for more rigorous sorting of concepts by their generality."
Proposing an information value chain to improve information services to disabled library patrons using assistive technologies,"Information services offered by academic libraries increasingly rely on assistive technologies (AT) to facilitate disabled patronsâ retrieval and use of information for learning and teaching. However, disabled patronsâ access to AT might not always lead to their use, resulting in the underutilization of information services offered by academic libraries. We adopt an inward-looking, service innovation perspective to improve information services for disabled patrons using AT. The open coding of qualitative responses collected from administrators and librarians in 186 academic libraries in public universities in the United States, reveals 10 mechanisms (i.e. modified work practices), which involve searching, compiling, mixing, framing, sharing, or reusing information, and learning from it. Based on this information-centric reorganisation of work practices, we propose an âinformation value chainâ, like Porterâs value chain, for improving information services to disabled patrons using AT in academic libraries, which is the major theoretical contribution of our study. Â© The Author(s) 2021.","Information services offered by academic libraries increasingly rely on assistive technologies (AT) to facilitate disabled patrons retrieval and use of information for learning and teaching. However, disabled patrons access to AT might not always lead to their use, resulting in the underutilization of information services offered by academic libraries. We adopt an inward-looking, service innovation perspective to improve information services for disabled patrons using AT. The open coding of qualitative responses collected from administrators and librarians in 186 academic libraries in public universities in the United States, reveals 10 mechanisms ( modified work practices), which involve searching, compiling, mixing, framing, sharing, or reusing information, and learning from it. Based on this information-centric reorganisation of work practices, we propose an information value chain, like Porters value chain, for improving information services to disabled patrons using AT in academic libraries, which is the major theoretical contribution of our study."
Link prediction in supernetwork: Risk perception of emergencies,"After an emergency incident occurs, how to identify risks, predict trends and scientifically cope before the crisis erupts is the basic starting point of this study. In this study, a supernetwork model of the risk perception in emergencies is innovatively constructed from the perspective of the governance of risks. This supernetwork model includes three subnetworks: the similar relationship subnetwork that is composed of newly occurring emergencies, the chain relationship subnetwork that is composed of historical emergencies and the co-occurrence relationship subnetwork that is composed of the risk elements for emergencies. Afterwards, the feature similarity algorithm is applied to quantify the relations between newly occurring emergencies and historical emergencies, and then, the link prediction algorithm is applied to predict the risk elements that may be derived from the newly occurring emergencies. This will be beneficial to enhancing the scientific accuracy of decision-making by managers when coping with emergencies risks. Â© The Author(s) 2020.","After an emergency incident occurs, how to identify risks, predict trends and scientifically cope before the crisis erupts is the basic starting point of this study. In this study, a supernetwork model of the risk perception in emergencies is innovatively constructed from the perspective of the governance of risks. This supernetwork model includes three subnetworks: the similar relationship subnetwork that is composed of newly occurring emergencies, the chain relationship subnetwork that is composed of historical emergencies and the co-occurrence relationship subnetwork that is composed of the risk elements for emergencies. Afterwards, the feature similarity algorithm is applied to quantify the relations between newly occurring emergencies and historical emergencies, and then, the link prediction algorithm is applied to predict the risk elements that may be derived from the newly occurring emergencies. This will be beneficial to enhancing the scientific accuracy of decision-making by managers when coping with emergencies risks."
Evaluating the quality of linked open data in digital libraries,"Cultural heritage institutions have recently started to share their metadata as Linked Open Data (LOD) in order to disseminate and enrich them. The publication of large bibliographic data sets as LOD is a challenge that requires the design and implementation of custom methods for the transformation, management, querying and enrichment of the data. In this report, the methodology defined by previous research for the evaluation of the quality of LOD is analysed and adapted to the specific case of Resource Description Framework (RDF) triples containing standard bibliographic information. The specified quality measures are reported in the case of four highly relevant libraries. Â© The Author(s) 2020.","Cultural heritage institutions have recently started to share their metadata as Linked Open Data (LOD) in order to disseminate and enrich them. The publication of large bibliographic data sets as LOD is a challenge that requires the design and implementation of custom methods for the transformation, management, querying and enrichment of the data. In this report, the methodology defined by previous research for the evaluation of the quality of LOD is analysed and adapted to the specific case of Resource Description Framework (RDF) triples containing standard bibliographic information. The specified quality measures are reported in the case of four highly relevant libraries."
Small female citation advantages for US journal articles in medicine,"Female under-representation continues in senior roles within academic medicine, potentially influenced by a perception that female research has less citation impact. This article provides systematic evidence of (a) female participation rates from the perspective of published journal articles in 46 Scopus medical subject categories 1996â2018 and (b) gender differences in citation rates 1996â2014. The results show female proportion increases 1996â2018 in all fields and a female majority of first-authored articles in two-fifths of categories, but substantial differences between fields. A paper is 7.3 times more likely to have a female first author in Obstetrics and Gynaecology than in Orthopaedics and Sports Medicine. Only three fields had a female last author majority by 2018, a probable side effect of ongoing problems with appointing female leaders. Female first-authored research tended to be more cited than male first-authored research in most fields (59%), although with a maximum difference of only 5.1% (log-transformed normalised citations). In contrast, male last-authored research tends to be more cited than female last-authored research, perhaps due to cases where a senior male has attracted substantial funding for a project. These differences increase if team sizes are not accounted for in the calculations. Since female first-authored research is cited slightly more than male first-authored research, properly analysed bibliometric data considering career gaps should not disadvantage female candidates for senior roles. Â© The Author(s) 2020.","Female under-representation continues in senior roles within academic medicine, potentially influenced by a perception that female research has less citation impact. This article provides systematic evidence of (a) female participation rates from the perspective of published journal articles in 46 Scopus medical subject categories 19962018 and (b) gender differences in citation rates 19962014. The results show female proportion increases 19962018 in all fields and a female majority of first-authored articles in two-fifths of categories, but substantial differences between fields. A paper is 7.3 times more likely to have a female first author in Obstetrics and Gynaecology than in Orthopaedics and Sports Medicine. Only three fields had a female last author majority by 2018, a probable side effect of ongoing problems with appointing female leaders. Female first-authored research tended to be more cited than male first-authored research in most fields (59%), although with a maximum difference of only 5.1% (log-transformed normalised citations). In contrast, male last-authored research tends to be more cited than female last-authored research, perhaps due to cases where a senior male has attracted substantial funding for a project. These differences increase if team sizes are not accounted for in the calculations. Since female first-authored research is cited slightly more than male first-authored research, properly analysed bibliometric data considering career gaps should not disadvantage female candidates for senior roles."
Semisupervised sentiment analysis method for online text reviews,"Sentiment analysis plays an important role in understanding individual opinions expressed in websites such as social media and product review sites. The common approaches to sentiment analysis use the sentiments carried by words that express opinions and are based on either supervised or unsupervised learning techniques. The unsupervised learning approach builds a word-sentiment dictionary, but it requires lengthy time periods and high costs to build a reliable dictionary. The supervised learning approach uses machine learning models to learn the sentiment scores of words; however, training a classifier model requires large amounts of labelled text data to achieve a good performance. In this article, we propose a semisupervised approach that performs well despite having only small amounts of labelled data available for training. The proposed method builds a base sentiment dictionary from a small training dataset using a lasso-based ensemble model with minimal human effort. The scores of words not in the training dataset are estimated using an adaptive instance-based learning model. In a pretrained word2vec model space, the sentiment values of the words in the dictionary are propagated to the words that did not exist in the training dataset. Through two experiments, we demonstrate that the performance of the proposed method is comparable to that of supervised learning models trained on large datasets. Â© The Author(s) 2020.","Sentiment analysis plays an important role in understanding individual opinions expressed in websites such as social media and product review sites. The common approaches to sentiment analysis use the sentiments carried by words that express opinions and are based on either supervised or unsupervised learning techniques. The unsupervised learning approach builds a word-sentiment dictionary, but it requires lengthy time periods and high costs to build a reliable dictionary. The supervised learning approach uses machine learning models to learn the sentiment scores of words; however, training a classifier model requires large amounts of labelled text data to achieve a good performance. In this article, we propose a semisupervised approach that performs well despite having only small amounts of labelled data available for training. The proposed method builds a base sentiment dictionary from a small training dataset using a lasso-based ensemble model with minimal human effort. The scores of words not in the training dataset are estimated using an adaptive instance-based learning model. In a pretrained word2vec model space, the sentiment values of the words in the dictionary are propagated to the words that did not exist in the training dataset. Through two experiments, we demonstrate that the performance of the proposed method is comparable to that of supervised learning models trained on large datasets."
Hidden in the light: Scientistsâ online presence on institutional websites and professional networking sites,"The visibility of individual scientists and their academic performance plays a major role in gaining the creditability to get funded and to advance in academic positions. Therefore, web presences are increasingly used to boost oneâs own visibility, disseminate research results and keep up to date with the research of others. However, previous reports show that these channels are not used equally by all scientists. Our study therefore investigates how faculty members (N = 868) at all universities in Lower Saxony (Germany) in the disciplines of physics, biology and chemistry present themselves on institutional websites and professional networking sites. We find that online presentations on institutional websites are mostly rudimentary. In contrast, there are more informative self-presentations on professional networking sites for both established (professors) and less-established (only PhD holders) faculty members. Our figures confirm observations that scientists present themselves online, but less-established ones seem to find less-supportive environments in academic institutions. Â© The Author(s) 2022.","The visibility of individual scientists and their academic performance plays a major role in gaining the creditability to get funded and to advance in academic positions. Therefore, web presences are increasingly used to boost ones own visibility, disseminate research results and keep up to date with the research of others. However, previous reports show that these channels are not used equally by all scientists. Our study therefore investigates how faculty members (N = 868) at all universities in Lower Saxony (Germany) in the disciplines of physics, biology and chemistry present themselves on institutional websites and professional networking sites. We find that online presentations on institutional websites are mostly rudimentary. In contrast, there are more informative self-presentations on professional networking sites for both established (professors) and less-established (only PhD holders) faculty members. Our figures confirm observations that scientists present themselves online, but less-established ones seem to find less-supportive environments in academic institutions."
"Analysis of direct citation, co-citation and bibliographic coupling in scientific topic identification","In our study, we examine the impact of citation network structures on the ability to discern valuable research topics in Computer Science literature. We use the bibliographic information available in the DBLP database to extract candidate phrases from scientific paper abstracts. Following that, we construct citation networks based on direct citation, co-citation and bibliographic coupling relationships between the papers. The candidate research topics, in the form of keyphrases and n-grammes, are subsequently ranked and filtered by a graph-text ranking algorithm. This selection of the highest ranked potential topics is further evaluated by domain experts and through the Wikipedia knowledge base. The results obtained from these citation networks are complementary, returning valid but non-overlapping output phrases between some pairs of networks. In particular, bibliographic coupling appears to capture more unique information than either direct citation or co-citation. These findings point towards the possible added value in combining bibliographic coupling analysis with other structures. At the same time, combining direct citation and co-citation is put into question. We expect our findings to be utilised in method design for research topic identification. Â© The Author(s) 2020.","In our study, we examine the impact of citation network structures on the ability to discern valuable research topics in Computer Science literature. We use the bibliographic information available in the DBLP database to extract candidate phrases from scientific paper abstracts. Following that, we construct citation networks based on direct citation, co-citation and bibliographic coupling relationships between the papers. The candidate research topics, in the form of keyphrases and n-grammes, are subsequently ranked and filtered by a graph-text ranking algorithm. This selection of the highest ranked potential topics is further evaluated by domain experts and through the Wikipedia knowledge base. The results obtained from these citation networks are complementary, returning valid but non-overlapping output phrases between some pairs of networks. In particular, bibliographic coupling appears to capture more unique information than either direct citation or co-citation. These findings point towards the possible added value in combining bibliographic coupling analysis with other structures. At the same time, combining direct citation and co-citation is put into question. We expect our findings to be utilised in method design for research topic identification."
A survey on automatically constructed universal knowledge bases,"A universal knowledge base can be defined as a domain-independent ontology containing instances. Ontologies define the concepts and relations among these concepts and are used to represent a domain of interest. These universal knowledge bases are the elementary units for automated reasoning on the Semantic Web. The Semantic Web is an extension of the World Wide Web which facilitates software agents to share content beyond the limitations of applications and websites. This survey focuses on the most prominent automatically constructed universal knowledge bases including KnowItAll, DBpedia, YAGO, NELL, Probase, BabelNet and Knowledge Vault. We take a closer look at how these knowledge bases are built, in particular at the information extraction and taxonomy generation process and investigate how they are used in practical applications. Due to quality concerns, the most successful and widely employed knowledge bases are manually constructed to maintain high quality, but they suffer from low coverage, high assembly and quality assurance cost. On the contrary, automatic approaches for building knowledge bases try to overcome these drawbacks. Although it is strenuous to achieve the same level of quality as for manual knowledge bases, we found that the surveyed automatically constructed knowledge bases have shown promising results and are useful for many real-world applications. Â© The Author(s) 2020.","A universal knowledge base can be defined as a domain-independent ontology containing instances. Ontologies define the concepts and relations among these concepts and are used to represent a domain of interest. These universal knowledge bases are the elementary units for automated reasoning on the Semantic Web. The Semantic Web is an extension of the World Wide Web which facilitates software agents to share content beyond the limitations of applications and websites. This survey focuses on the most prominent automatically constructed universal knowledge bases including KnowItAll, DBpedia, YAGO, NELL, Probase, BabelNet and Knowledge Vault. We take a closer look at how these knowledge bases are built, in particular at the information extraction and taxonomy generation process and investigate how they are used in practical applications. Due to quality concerns, the most successful and widely employed knowledge bases are manually constructed to maintain high quality, but they suffer from low coverage, high assembly and quality assurance cost. On the contrary, automatic approaches for building knowledge bases try to overcome these drawbacks. Although it is strenuous to achieve the same level of quality as for manual knowledge bases, we found that the surveyed automatically constructed knowledge bases have shown promising results and are useful for many real-world applications."
A generic metamodel for data extraction and generic ontology population,"As the next step in the development of intelligent computing systems is the addition of human expertise and knowledge, it is a priority to build strong computable and well-documented knowledge bases. Ontologies partially respond to this challenge by providing formalisms for knowledge representation. However, one major remaining task is the population of these ontologies with concrete application. Based on Model-Driven Engineering principles, a generic metamodel for the extraction of heterogeneous data is presented in this article. The metamodel has been designed with two objectives, namely (1) the need of genericity regarding the source of collected pieces of knowledge and (2) the intent to stick to a structure close to an ontological structure. As well, an example of instantiation of the metamodel for textual data in chemistry domain and an insight of how this metamodel could be integrated in a larger automated domain independent ontology population framework are given. Â© The Author(s) 2021.","As the next step in the development of intelligent computing systems is the addition of human expertise and knowledge, it is a priority to build strong computable and well-documented knowledge bases. Ontologies partially respond to this challenge by providing formalisms for knowledge representation. However, one major remaining task is the population of these ontologies with concrete application. Based on Model-Driven Engineering principles, a generic metamodel for the extraction of heterogeneous data is presented in this article. The metamodel has been designed with two objectives, namely the need of genericity regarding the source of collected pieces of knowledge and the intent to stick to a structure close to an ontological structure. As well, an example of instantiation of the metamodel for textual data in chemistry domain and an insight of how this metamodel could be integrated in a larger automated domain independent ontology population framework are given."
"Research practices of LIS professionals in Pakistan: A study of attitudes, involvement and competencies","This study analyses the attitudes, involvement and competencies of Pakistani Library Information Science (LIS) professionals towards research. An online survey was carried out by using a questionnaire to collect data from LIS professionals working in various types of libraries in Pakistan. The findings reveal that the overall attitude of the Pakistani LIS professionals towards research is positive. A vast majority of them read research literature, albeit occasionally, while a small majority read the full-text articles. Two local journals, Pakistan Library & Information Science Journal (PLISJ) and Pakistan Journal of Information Management & Libraries (PJIML), are the top read titles. Though research contributions counted towards promotion of LIS professionals, a very small majority of them were currently engaged in research project/s. They do not feel very confident about their research expertise, yet aspire to increase their knowledge of research. Some of the factors that deter LIS professionals from engaging in research are: a lack of time, little support from their organisation, lack of research ideas and lack of research skills. Institutional support in terms of time, money and educational training would enhance opportunities for LIS professionals to produce more research and publication. Â© The Author(s) 2020.","This study analyses the attitudes, involvement and competencies of Pakistani Library Information Science (LIS) professionals towards research. An online survey was carried out by using a questionnaire to collect data from LIS professionals working in various types of libraries in Pakistan. The findings reveal that the overall attitude of the Pakistani LIS professionals towards research is positive. A vast majority of them read research literature, albeit occasionally, while a small majority read the full-text articles. Two local journals, Pakistan Library & Information Science Journal (PLISJ) and Pakistan Journal of Information Management & Libraries (PJIML), are the top read titles. Though research contributions counted towards promotion of LIS professionals, a very small majority of them were currently engaged in research project/ They do not feel very confident about their research expertise, yet aspire to increase their knowledge of research. Some of the factors that deter LIS professionals from engaging in research are: a lack of time, little support from their organisation, lack of research ideas and lack of research skills. Institutional support in terms of time, money and educational training would enhance opportunities for LIS professionals to produce more research and publication."
Understanding social media discontinuance from social cognitive perspective: Evidence from Facebook users,"Based on social cognitive theory, this study proposes a research framework to investigate two different social media discontinuance behaviours: reduced usage and abandoned usage. Specifically, perceived technology overload, information overload and social overload are the environmental factors that induce negative personal states, including dissatisfaction and social media fatigue, which lead to negative behavioural changes, such as reduced usage and abandoned usage of social media. The proposed research model was tested empirically with data collected among Facebook users. The research results indicate that impacts from perceived technology overload, information overload and social overload on social network fatigue and dissatisfaction vary. Dissatisfaction exerts greater impacts on abandoned-usage behaviour than social media fatigue, but similar impacts on reduced-usage behaviour as social media fatigue. In addition, reduced-usage behaviour was found to lead to abandoned-usage behaviour. Finally, we discuss the theoretical and practical contributions that can be gleaned from the proposed research model. Â© The Author(s) 2020.","Based on social cognitive theory, this study proposes a research framework to investigate two different social media discontinuance behaviours: reduced usage and abandoned usage. Specifically, perceived technology overload, information overload and social overload are the environmental factors that induce negative personal states, including dissatisfaction and social media fatigue, which lead to negative behavioural changes, such as reduced usage and abandoned usage of social media. The proposed research model was tested empirically with data collected among Facebook users. The research results indicate that impacts from perceived technology overload, information overload and social overload on social network fatigue and dissatisfaction vary. Dissatisfaction exerts greater impacts on abandoned-usage behaviour than social media fatigue, but similar impacts on reduced-usage behaviour as social media fatigue. In addition, reduced-usage behaviour was found to lead to abandoned-usage behaviour. Finally, we discuss the theoretical and practical contributions that can be gleaned from the proposed research model."
Predicting social media rumours in the context of public health emergencies,"The spread of rumours on social media in the context of public health emergencies often distorts perceptions of public events and obstructs crisis management. Microblog entries about 28 rumour cases are collected on Sina Weibo during the COVID-19 outbreak. The ModalityâAgencyâInteractivityâNavigability model is used to identify the key factors of rumour prediction. To investigate the relationship among information modality, information content, information source and rumour identification, the binary logistic regression model is established based on the features of users and microblog entries. In addition, we propose a multi-feature rumour prediction model based on the Bidirectional Encoder Representations from Transformers (BERT) and Extreme Gradient Boosting (XGBoost) models. The proposed rumour prediction model has the best performance compared with other models. The feature importance is then calculated by the SHapley Additive exPlanations (SHAP), which can also explain the XGBoost results. It is shown that the likelihood that microblog entries are rumours decreases as the values of variables such as user influence and the positive sentiment of comments rise. Microblog entries posted on Thursdays or at noon are more probably to be rumours than those posted at other time. The proposed model can assist emergency management departments in establishing a feasible rumour prediction mechanism to guide public opinion against rumours. Â© The Author(s) 2022.","The spread of rumours on social media in the context of public health emergencies often distorts perceptions of public events and obstructs crisis management. Microblog entries about 28 rumour cases are collected on Sina Weibo during the COVID-19 outbreak. The ModalityAgencyInteractivityNavigability model is used to identify the key factors of rumour prediction. To investigate the relationship among information modality, information content, information source and rumour identification, the binary logistic regression model is established based on the features of users and microblog entries. In addition, we propose a multi-feature rumour prediction model based on the Bidirectional Encoder Representations from Transformers (BERT) and Extreme Gradient Boosting (XGBoost) models. The proposed rumour prediction model has the best performance compared with other models. The feature importance is then calculated by the SHapley Additive exPlanations (SHAP), which can also explain the XGBoost results. It is shown that the likelihood that microblog entries are rumours decreases as the values of variables such as user influence and the positive sentiment of comments rise. Microblog entries posted on Thursdays or at noon are more probably to be rumours than those posted at other time. The proposed model can assist emergency management departments in establishing a feasible rumour prediction mechanism to guide public opinion against rumours."
A collaborative filtering algorithm based on item labels and Hellinger distance for sparse data,"The Neighbourhood-based collaborative filtering (CF) algorithm has been widely used in recommender systems. To enhance the adaptability to the sparse data, a CF with new similarity measure and prediction method is proposed. The new similarity measure is designed based on the Hellinger distance of item labels, which overcomes the problem of depending on common-rated items (co-rated items). In the proposed prediction method, we present a new strategy to solve the problem that the neighbour users do not rate the target item, that is, the most similar item rated by the neighbour user is used to replace the target item. The proposed prediction method can significantly improve the utilisation of neighbours and obviously increase the accuracy of prediction. The experimental results on two benchmark datasets both confirm that the proposed algorithm can effectively alleviate the sparse data problem and improve the recommendation results. Â© The Author(s) 2020.","The Neighbourhood-based collaborative filtering (CF) algorithm has been widely used in recommender systems. To enhance the adaptability to the sparse data, a CF with new similarity measure and prediction method is proposed. The new similarity measure is designed based on the Hellinger distance of item labels, which overcomes the problem of depending on common-rated items (co-rated items). In the proposed prediction method, we present a new strategy to solve the problem that the neighbour users do not rate the target item, that is, the most similar item rated by the neighbour user is used to replace the target item. The proposed prediction method can significantly improve the utilisation of neighbours and obviously increase the accuracy of prediction. The experimental results on two benchmark datasets both confirm that the proposed algorithm can effectively alleviate the sparse data problem and improve the recommendation results."
A new similarity measure for vector space models in text classification and information retrieval,"There are various models, methodologies and algorithms that can be used today for document classification, information retrieval and other text mining applications and systems. One of them is the vector spaceâbased models, where distance metrics or similarity measures lie at the core of such models. Vector spaceâbased model is one of the fast and simple alternatives for the processing of textual data; however, its accuracy, precision and reliability still need significant improvements. In this study, a new similarity measure is proposed, which can be effectively used for vector space models and related algorithms such as k-nearest neighbours (k-NN) and Rocchio as well as some clustering algorithms such as K-means. The proposed similarity measure is tested with some universal benchmark data sets in Turkish and English, and the results are compared with some other standard metrics such as Euclidean distance, Manhattan distance, Chebyshev distance, Canberra distance, BrayâCurtis dissimilarity, Pearson correlation coefficient and Cosine similarity. Some successful and promising results have been obtained, which show that this proposed similarity measure could be alternatively used within all suitable algorithms and models for information retrieval, document clustering and text classification. Â© The Author(s) 2020.","There are various models, methodologies and algorithms that can be used today for document classification, information retrieval and other text mining applications and systems. One of them is the vector spacebased models, where distance metrics or similarity measures lie at the core of such models. Vector spacebased model is one of the fast and simple alternatives for the processing of textual data; however, its accuracy, precision and reliability still need significant improvements. In this study, a new similarity measure is proposed, which can be effectively used for vector space models and related algorithms such as k-nearest neighbours (k-NN) and Rocchio as well as some clustering algorithms such as K-means. The proposed similarity measure is tested with some universal benchmark data sets in Turkish and English, and the results are compared with some other standard metrics such as Euclidean distance, Manhattan distance, Chebyshev distance, Canberra distance, BrayCurtis dissimilarity, Pearson correlation coefficient and Cosine similarity. Some successful and promising results have been obtained, which show that this proposed similarity measure could be alternatively used within all suitable algorithms and models for information retrieval, document clustering and text classification."
Exploring direct citations between citing publications,"This article defines and explores the direct citations between citing publications (DCCPs) of a publication. We construct an ego-centred citation network for each paper that contains all of its citing papers and itself, as well as the citation relationships among them. By utilising a large-scale scholarly dataset from the computer science field in the Microsoft Academic Graph (MAG-CS) dataset, we find that DCCPs exist universally in medium and highly cited papers. For those papers that have DCCPs, DCCPs do occur frequently; highly cited papers tend to contain more DCCPs than others. Meanwhile, the number of DCCPs of papers published in different years does not vary dramatically. This paper also discusses the relationship between DCCPs and some indirect citation relationships (e.g. co-citation and bibliographic coupling). Â© The Author(s) 2020.","This article defines and explores the direct citations between citing publications (DCCPs) of a publication. We construct an ego-centred citation network for each paper that contains all of its citing papers and itself, as well as the citation relationships among them. By utilising a large-scale scholarly dataset from the computer science field in the Microsoft Academic Graph (MAG-CS) dataset, we find that DCCPs exist universally in medium and highly cited papers. For those papers that have DCCPs, DCCPs do occur frequently; highly cited papers tend to contain more DCCPs than others. Meanwhile, the number of DCCPs of papers published in different years does not vary dramatically. This paper also discusses the relationship between DCCPs and some indirect citation relationships ( co-citation and bibliographic coupling)."
Information is essential for competitive and cost-effective public procurement,"Public authorities promote transparent public procurement practices to increase competition and reduce public procurement costs. In this article, we focus on public procurement of the European Union (EU). We employ a multidisciplinary approach to analyse economic effects of information in public procurement. We quantify the information content of 2,390,630 EU public procurement notices published in 22 different languages using natural language processing techniques. Subsequently, we examine the impact of the information content on public procurement outcomes. We find that higher information levels have significant positive effects. Competition is considerably higher when notices contain more information. On average, contract prices would be 6%â8% lower if notices were to contain adequate information. EU governments could save up to (Formula presented.) 80 billion if all public procurement notices were to have detailed information. Based on our comprehensive analysis, we believe that authorities should regulate the information content of notices to promote competition and cost-effectiveness in public procurement. Â© The Author(s) 2022.","Public authorities promote transparent public procurement practices to increase competition and reduce public procurement costs. In this article, we focus on public procurement of the European Union (EU). We employ a multidisciplinary approach to analyse economic effects of information in public procurement. We quantify the information content of 2,390,630 EU public procurement notices published in 22 different languages using natural language processing techniques. Subsequently, we examine the impact of the information content on public procurement outcomes. We find that higher information levels have significant positive effects. Competition is considerably higher when notices contain more information. On average, contract prices would be 6%8% lower if notices were to contain adequate information. EU governments could save up to (Formula presented.) 80 billion if all public procurement notices were to have detailed information. Based on our comprehensive analysis, we believe that authorities should regulate the information content of notices to promote competition and cost-effectiveness in public procurement."
The association between professional stratification and use of online sources: Evidence from the National Dental Practice-Based Research Network,"The use of online information sources in most professions is widespread, and well researched. Less understood is how the use of these sources vary across the strata within a single profession, and how question context affects search behaviour. Using the dental profession as a case of a highly stratified discipline, we examine search preferences for sources by professional strata among dentists in a practice-based network. Results show that variation exists in information search behaviour across professional strata of dental clinicians. This study highlights the importance of addressing information literacy across different levels of a profession. Findings also underscore that search behaviour and source preference vary with perceived question relevance. Â© The Author(s) 2019.","The use of online information sources in most professions is widespread, and well researched. Less understood is how the use of these sources vary across the strata within a single profession, and how question context affects search behaviour. Using the dental profession as a case of a highly stratified discipline, we examine search preferences for sources by professional strata among dentists in a practice-based network. Results show that variation exists in information search behaviour across professional strata of dental clinicians. This study highlights the importance of addressing information literacy across different levels of a profession. Findings also underscore that search behaviour and source preference vary with perceived question relevance."
TIPS: Time-aware Personalised Semantic-based query auto-completion,"With the rapid growth of the Internet, search engines play vital roles in meeting the usersâ information needs. However, formulating information needs to simple queries for canonical users is a problem yet. Therefore, query auto-completion, which is one of the most important characteristics of the search engines, is leveraged to provide a ranked list of queries matching the userâs entered prefix. Although query auto-completion utilises useful information provided by search engine logs, time-, semantic- and context-aware features are still important resources of extra knowledge. Specifically, in this study, a hybrid query auto-completion system called TIPS (Time-aware Personalised Semantic-based query auto-completion) is introduced to combine the well-known systems performing based on popularity and neural language model. Furthermore, this system is supplemented by time-aware features that blend both context and semantic information in a collaborative manner. Experimental studies on the standard AOL dataset are conducted to compare our proposed system with state-of-the-art methods, that is, FactorCell, ConcatCell and Unadapted. The results illustrate the significant superiorities of TIPS in terms of mean reciprocal rank (MRR), especially for short-length prefixes. Â© The Author(s) 2020.","With the rapid growth of the Internet, search engines play vital roles in meeting the users information needs. However, formulating information needs to simple queries for canonical users is a problem yet. Therefore, query auto-completion, which is one of the most important characteristics of the search engines, is leveraged to provide a ranked list of queries matching the users entered prefix. Although query auto-completion utilises useful information provided by search engine logs, time-, semantic- and context-aware features are still important resources of extra knowledge. Specifically, in this study, a hybrid query auto-completion system called TIPS (Time-aware Personalised Semantic-based query auto-completion) is introduced to combine the well-known systems performing based on popularity and neural language model. Furthermore, this system is supplemented by time-aware features that blend both context and semantic information in a collaborative manner. Experimental studies on the standard AOL dataset are conducted to compare our proposed system with state-of-the-art methods, that is, FactorCell, ConcatCell and Unadapted. The results illustrate the significant superiorities of TIPS in terms of mean reciprocal rank (MRR), especially for short-length prefixes."
Identification of rumour stances by considering network topology and social media comments,"Online social media (OSM) has become a hotbed for the rapid dissemination of disinformation or faked news. In order to track and limit the spread of faked news, we study stance identification of comments posted on OSM, where the stance can denote the commentâs semantics. In this article, we propose a framework for identification of rumour stances, combining network topology and OSM comments. We construct a vector matrix of comments and words via OTI (optimisation term frequencyâinverse document frequency). To better identify the stances, we introduce another vector matrix with novel or special attribute, that is, network topology among the users. Variant autoencoder (VAE) is then applied for dimensionality reduction and optimisation of these vector matrices which are then combined into an integrated matrix (Formula presented.), tempered by two parameters (Formula presented.) and (Formula presented.). Finally, the matrix is fed into a neural network for final rumour stance identification. Experimental evaluations show that our proposed approach outperforms some state-of-the-art methods and achieves a high precision of 90.26% and F1-score of 88.58%. Â© The Author(s) 2020.","Online social media (OSM) has become a hotbed for the rapid dissemination of disinformation or faked news. In order to track and limit the spread of faked news, we study stance identification of comments posted on OSM, where the stance can denote the comments semantics. In this article, we propose a framework for identification of rumour stances, combining network topology and OSM comments. We construct a vector matrix of comments and words via OTI (optimisation term frequencyinverse document frequency). To better identify the stances, we introduce another vector matrix with novel or special attribute, that is, network topology among the users. Variant autoencoder (VAE) is then applied for dimensionality reduction and optimisation of these vector matrices which are then combined into an integrated matrix (Formula presented.), tempered by two parameters (Formula presented.) and (Formula presented.). Finally, the matrix is fed into a neural network for final rumour stance identification. Experimental evaluations show that our proposed approach outperforms some state-of-the-art methods and achieves a high precision of 90.26% and F1-score of 88.58%."
Investigating information seeking in physical and online environments with escape room and web search,"Searching and interacting with information is one of the most fundamental behaviours of human beings â something that takes place in both online and physical environments. Yet, most studies of information interaction have focused on only one of these sides. This work aims to connect them by investigating oneâs information interaction behaviours in different physical and online contexts as well as different types of tasks. During Web search (online searching) and Escape Room (physical searching), 31 participantsâ behavioural data during web search (online searching) and escape room (physical searching) were collected through eye-tracker, web browser logs, and wearable video recorder. Analysis of the behavioural data suggests that individuals have a preferred search strategy that they adopt across different tasks and environments. The behavioural pattern, however, was found to be affected by the task type (e.g. problem searching vs exploratory search) and the way information is structured within the environments. Â© The Author(s) 2020.","Searching and interacting with information is one of the most fundamental behaviours of human beings something that takes place in both online and physical environments. Yet, most studies of information interaction have focused on only one of these sides. This work aims to connect them by investigating ones information interaction behaviours in different physical and online contexts as well as different types of tasks. During Web search (online searching) and Escape Room (physical searching), 31 participants behavioural data during web search (online searching) and escape room (physical searching) were collected through eye-tracker, web browser logs, and wearable video recorder. Analysis of the behavioural data suggests that individuals have a preferred search strategy that they adopt across different tasks and environments. The behavioural pattern, however, was found to be affected by the task type ( problem searching vs exploratory search) and the way information is structured within the environments."
Think outside the search box: A comparative study of visual and form-based query builders,"Knowledge workers such as healthcare information professionals, legal researchers and librarians need to create and execute search strategies that are comprehensive, transparent and reproducible. The traditional solution is to use proprietary query-building tools provided by literature database vendors. In the majority of cases, these query builders are designed using a form-based paradigm that requires the user to enter keywords and ontology terms on a line-by-line basis and then combine them using Boolean operators. However, recent years have witnessed significant changes in humanâcomputer interaction technologies, and users can now engage with online information systems using a variety of novel data visualisation techniques. In this article, we evaluate a new approach to query building in which users express concepts as objects on a visual canvas and compare this with a traditional form-based query builder in a laboratory-based user study. The results demonstrate the potential of visual interfaces to mitigate some of the shortcomings associated with form-based interfaces and encourage more exploratory search behaviour. They also demonstrate the value of having a temporary âscratchâ space in query formulation. In addition, the findings highlight an ongoing need for transparency and reproducibility in professional search and raise further questions about how these properties may best be supported. Â© The Author(s) 2022.","Knowledge workers such as healthcare information professionals, legal researchers and librarians need to create and execute search strategies that are comprehensive, transparent and reproducible. The traditional solution is to use proprietary query-building tools provided by literature database vendors. In the majority of cases, these query builders are designed using a form-based paradigm that requires the user to enter keywords and ontology terms on a line-by-line basis and then combine them using Boolean operators. However, recent years have witnessed significant changes in humancomputer interaction technologies, and users can now engage with online information systems using a variety of novel data visualisation techniques. In this article, we evaluate a new approach to query building in which users express concepts as objects on a visual canvas and compare this with a traditional form-based query builder in a laboratory-based user study. The results demonstrate the potential of visual interfaces to mitigate some of the shortcomings associated with form-based interfaces and encourage more exploratory search behaviour. They also demonstrate the value of having a temporary scratch space in query formulation. In addition, the findings highlight an ongoing need for transparency and reproducibility in professional search and raise further questions about how these properties may best be supported."
Information security: Legal regulations in Azerbaijan and abroad,"The article is devoted to information security issues in the world and in Azerbaijan, in particular. The article compares laws and regulations of Azerbaijan and other countries in the cybersecurity policy between them. The article reveals the features of the organisational and legal regulation of the information security system as an integral part of state security. A number of aspects of ensuring information security through legal and technological means, as well as a number of features of ensuring the security of certain categories of information, are highlighted. Recommendations and conclusions from the policies of both jurisdictions are presented. Â© The Author(s) 2020.","The article is devoted to information security issues in the world and in Azerbaijan, in particular. The article compares laws and regulations of Azerbaijan and other countries in the cybersecurity policy between them. The article reveals the features of the organisational and legal regulation of the information security system as an integral part of state security. A number of aspects of ensuring information security through legal and technological means, as well as a number of features of ensuring the security of certain categories of information, are highlighted. Recommendations and conclusions from the policies of both jurisdictions are presented."
Do researchers use open research data? Exploring the relationships between usage trends and metadata quality across scientific disciplines from the Figshare case,"Open research data (ORD) have been considered a driver of scientific transparency. However, data friction, as the phenomenon of data underutilisation for several causes, has also been pointed out. A factor often called into question for ORD low usage is the quality of the ORD and associated metadata. This work aims to illustrate the use of ORD, published by the Figshare scientific repository, concerning their scientific discipline, their type and compared with the quality of their metadata. Considering all the Figshare resources and carrying out a programmatic quality assessment of their metadata, our analysis highlighted two aspects. First, irrespective of the scientific domain considered, most ORD are under-used, but with exceptional cases which concentrate most researchersâ attention. Second, there was no evidence that the use of ORD is associated with good metadata publishing practices. These two findings opened to a reflection about the potential causes of such data friction. Â© The Author(s) 2020.","Open research data (ORD) have been considered a driver of scientific transparency. However, data friction, as the phenomenon of data underutilisation for several causes, has also been pointed out. A factor often called into question for ORD low usage is the quality of the ORD and associated metadata. This work aims to illustrate the use of ORD, published by the Figshare scientific repository, concerning their scientific discipline, their type and compared with the quality of their metadata. Considering all the Figshare resources and carrying out a programmatic quality assessment of their metadata, our analysis highlighted two aspects. First, irrespective of the scientific domain considered, most ORD are under-used, but with exceptional cases which concentrate most researchers attention. Second, there was no evidence that the use of ORD is associated with good metadata publishing practices. These two findings opened to a reflection about the potential causes of such data friction."
Do online reviews have different effects on consumersâ sampling behaviour across product types? Evidence from the software industry,"Previous research shows that online reviews may have different effects for search goods and experience goods. However, as a typical type of experience goods, software can be further divided into different categories based on product characteristics. Little research has been conducted regarding the different effects of online reviews for different types of software. Furthermore, to offer free samples is another common practice of software firms to alleviate consumer uncertainty prior to purchase. To fill the corresponding research gap, this research focuses on the interaction effects between online reviews and free samples for different types of software. Through our empirical analysis, we find that user ratings significantly increase consumersâ sample downloads. Furthermore, consumers download more samples for some categories than for others. Finally, user and editor ratings might have differential effects for different types of software. Â© The Author(s) 2020.","Previous research shows that online reviews may have different effects for search goods and experience goods. However, as a typical type of experience goods, software can be further divided into different categories based on product characteristics. Little research has been conducted regarding the different effects of online reviews for different types of software. Furthermore, to offer free samples is another common practice of software firms to alleviate consumer uncertainty prior to purchase. To fill the corresponding research gap, this research focuses on the interaction effects between online reviews and free samples for different types of software. Through our empirical analysis, we find that user ratings significantly increase consumers sample downloads. Furthermore, consumers download more samples for some categories than for others. Finally, user and editor ratings might have differential effects for different types of software."
Predicting mobile application breakout using sentiment analysis of Facebook posts,"Publishing mobile applications on the official stores is becoming a big business. Many developers are charmed by the billion-dollar success of breakout applications. Thus, in order to ensure success, mobile applications need to sustain top ranking. Previous work on the predictability of mobile applications success aimed to extract from app stores relevant features that influence high rating. In this article, we propose an automated approach to exploit data available on Facebook platform that predicts mobile applications breakout. We collect data from Facebook graph API, then determine sentiment polarity of user comments. We design statistical features to score users sentiment for each post. Then, we compose posts scores with Facebook statistical measures to form a mobile applications breakout dataset. Finally, we use machine learning techniques to build our breakout prediction model. We evaluate our approach with 199 mobile applications and obtain a prediction accuracy of 83.78%. We find that Likes count on a Facebook page is decisive for climbing mobile applications ranking. However, a high rate of negative opinions declines application ranking and deprives mobile application of achieving a breakout. Based on these findings, we provide evidence that user interactions on social networks can influence the success of mobile applications. Â© The Author(s) 2020.","Publishing mobile applications on the official stores is becoming a big business. Many developers are charmed by the billion-dollar success of breakout applications. Thus, in order to ensure success, mobile applications need to sustain top ranking. Previous work on the predictability of mobile applications success aimed to extract from app stores relevant features that influence high rating. In this article, we propose an automated approach to exploit data available on Facebook platform that predicts mobile applications breakout. We collect data from Facebook graph API, then determine sentiment polarity of user comments. We design statistical features to score users sentiment for each post. Then, we compose posts scores with Facebook statistical measures to form a mobile applications breakout dataset. Finally, we use machine learning techniques to build our breakout prediction model. We evaluate our approach with 199 mobile applications and obtain a prediction accuracy of 83.78%. We find that Likes count on a Facebook page is decisive for climbing mobile applications ranking. However, a high rate of negative opinions declines application ranking and deprives mobile application of achieving a breakout. Based on these findings, we provide evidence that user interactions on social networks can influence the success of mobile applications."
"NaLa-Search: A multimodal, interaction-based architecture for faceted search on linked open data","Mobile devices are the technological basis of computational intelligent systems, yet traditional mobile application interfaces tend to rely only on the touch modality. That said, such interfaces could improve humanâcomputer interaction by combining diverse interaction modalities, such as visual, auditory and touch. Also, a lot of information on the Web is published under the Linked Data principles to allow people and computers to share, use and/or reuse high-quality information; however, current tools for searching for, browsing and visualising this kind of data are not fully developed. The goal of this research is to propose a novel architecture called NaLa-Search to effectively explore the Linked Open Data cloud. We present a mobile application that combines voice commands and touch for browsing and searching for such semantic information through faceted search, which is a widely used interaction scheme for exploratory search that is faithful to its richness and practical for real-world use. NaLa-Search was evaluated by real users from the clinical pharmacology domain. In this evaluation, the users had to search and navigate among the DrugBank dataset through voice commands. The evaluation results show that faceted search combined with multiple interaction modalities (e.g. speech and touch) can enhance usersâ interaction with semantic knowledge bases. Â© The Author(s) 2020.","Mobile devices are the technological basis of computational intelligent systems, yet traditional mobile application interfaces tend to rely only on the touch modality. That said, such interfaces could improve humancomputer interaction by combining diverse interaction modalities, such as visual, auditory and touch. Also, a lot of information on the Web is published under the Linked Data principles to allow people and computers to share, use and/or reuse high-quality information; however, current tools for searching for, browsing and visualising this kind of data are not fully developed. The goal of this research is to propose a novel architecture called NaLa-Search to effectively explore the Linked Open Data cloud. We present a mobile application that combines voice commands and touch for browsing and searching for such semantic information through faceted search, which is a widely used interaction scheme for exploratory search that is faithful to its richness and practical for real-world use. NaLa-Search was evaluated by real users from the clinical pharmacology domain. In this evaluation, the users had to search and navigate among the DrugBank dataset through voice commands. The evaluation results show that faceted search combined with multiple interaction modalities ( speech and touch) can enhance users interaction with semantic knowledge bases."
Reusing digital collections from GLAM institutions,"For some decades now, Galleries, Libraries, Archives and Museums (GLAM) institutions have published and provided access to information resources in digital format. Recently, innovative approaches have appeared such as the concept of Labs within GLAM institutions that facilitates the adoption of innovative and creative tools for content delivery and user engagement. In addition, new methods have been proposed to address the publication of digital collections as data sets amenable to computational use. In this article, we propose a methodology to create machine actionable collections following a set of steps. This methodology is then applied to several use cases based on data sets published by relevant GLAM institutions. It intends to encourage institutions to adopt the publication of data sets that support computationally driven research as a core activity. Â© The Author(s) 2020.","For some decades now, Galleries, Libraries, Archives and Museums (GLAM) institutions have published and provided access to information resources in digital format. Recently, innovative approaches have appeared such as the concept of Labs within GLAM institutions that facilitates the adoption of innovative and creative tools for content delivery and user engagement. In addition, new methods have been proposed to address the publication of digital collections as data sets amenable to computational use. In this article, we propose a methodology to create machine actionable collections following a set of steps. This methodology is then applied to several use cases based on data sets published by relevant GLAM institutions. It intends to encourage institutions to adopt the publication of data sets that support computationally driven research as a core activity."
Beyond correlation: Towards matching strategy for causal inference in Information Science,"Correlation has become a fundamental method for information science. However, correlations are limited in making concrete decisions. In this article, we detail how causal inference could be utilised in the field of information science. There are six main steps of implementing matching for causal inference, namely, selecting candidate control variables, determining control variables, calculating similarities among all samples, forming control group, examining the performance of control group and estimating causal effects. As an example, this article applies causal inference to investigate whether Nobel Physics award increases the after-award citations. The method is presented in a step-by-step manner so that researchers can reproduce our analysis in the future. Â© The Author(s) 2020.","Correlation has become a fundamental method for information science. However, correlations are limited in making concrete decisions. In this article, we detail how causal inference could be utilised in the field of information science. There are six main steps of implementing matching for causal inference, namely, selecting candidate control variables, determining control variables, calculating similarities among all samples, forming control group, examining the performance of control group and estimating causal effects. As an example, this article applies causal inference to investigate whether Nobel Physics award increases the after-award citations. The method is presented in a step-by-step manner so that researchers can reproduce our analysis in the future."
An online multi-dimensional opinion dynamic model with misinformation diffusion in emergency events,"Multiple opinions, including many that are negative, are produced in emergency events. These opinions are commonly formed asynchronously based on misinformation. However, most researches on opinion dynamics involving information neglect the asynchronous process of initial opinion formation due to information diffusion. Since online social networks like Sina Weibo act as major avenues for the expression, after analysing online behaviours, an opinion dynamic model is developed with consideration of misinformation diffusion of public opinion. In this model, schemes are developed for opinion interactions in multiple dimensions by introducing characteristics of online communication as another way of opinion interactions besides communication between neighbours. Subsequently, we investigate the impacts of network structure, diffusion rate, repost rate and other factors, which provide insights into understanding online opinion dynamics during emergency events. Furthermore, we conduct simulations to determine the intervention effects of different official responses. Results show that removing comments compulsively exhibits better performance in reducing negative opinion as well as increasing the density of Spreaders. Debunking misinformation by posting early results officially which indicates the probability of the existence of misinformation may lead public opinion in time if it takes a long time to finally confirm the misinformation. Â© The Author(s) 2021.","Multiple opinions, including many that are negative, are produced in emergency events. These opinions are commonly formed asynchronously based on misinformation. However, most researches on opinion dynamics involving information neglect the asynchronous process of initial opinion formation due to information diffusion. Since online social networks like Sina Weibo act as major avenues for the expression, after analysing online behaviours, an opinion dynamic model is developed with consideration of misinformation diffusion of public opinion. In this model, schemes are developed for opinion interactions in multiple dimensions by introducing characteristics of online communication as another way of opinion interactions besides communication between neighbours. Subsequently, we investigate the impacts of network structure, diffusion rate, repost rate and other factors, which provide insights into understanding online opinion dynamics during emergency events. Furthermore, we conduct simulations to determine the intervention effects of different official responses. Results show that removing comments compulsively exhibits better performance in reducing negative opinion as well as increasing the density of Spreaders. Debunking misinformation by posting early results officially which indicates the probability of the existence of misinformation may lead public opinion in time if it takes a long time to finally confirm the misinformation."
A topic analysis method based on a three-dimensional strategic diagram,"With the tremendous growth of scientific literature in recent years, methods of detecting and analysing research topics have become more and more important. This study proposes a topic analysis method combining latent Dirichlet allocation (LDA) and a three-dimensional strategic diagram. This study constructs the three-dimensional strategic diagram by three dimensions of centrality, density and novelty, and we classify topics into seven categories according to their strategic positions. Using this topic analysis method, the paper analyses 62,340 publications in the field of medical informatics between 1991 and 2018. Results show that the research scope of medical informatics has become increasingly interdisciplinary. Data analytical methods and technologies are sub-domains with persistent popularity. New health technologies, drug safety, algorithm optimisation and standardisation of medical information are emerging research topics. We hope the findings could help researchers identify potential research topics and facilitate in-depth analysis of the current state of various fields. Â© The Author(s) 2020.","With the tremendous growth of scientific literature in recent years, methods of detecting and analysing research topics have become more and more important. This study proposes a topic analysis method combining latent Dirichlet allocation (LDA) and a three-dimensional strategic diagram. This study constructs the three-dimensional strategic diagram by three dimensions of centrality, density and novelty, and we classify topics into seven categories according to their strategic positions. Using this topic analysis method, the paper analyses 62,340 publications in the field of medical informatics between 1991 and 2018. Results show that the research scope of medical informatics has become increasingly interdisciplinary. Data analytical methods and technologies are sub-domains with persistent popularity. New health technologies, drug safety, algorithm optimisation and standardisation of medical information are emerging research topics. We hope the findings could help researchers identify potential research topics and facilitate in-depth analysis of the current state of various fields."
On the measurement of scientific leadership,"The (Formula presented.) index was recently proposed to measure the degree of scientific leadership. While the concept is useful and interesting, namely, as a complement to the traditional performance analysis, the metric suffers from important shortcomings. We argue that scientific leadership should be evaluated: (1) taking into account information of the moment the paper is produced/published, and (2) in the specific context of the paper, meaning that only previous work relevant for the paper should be taken into account. Based on these two principles, we introduce an alternative approach, using self-citations as a source of information, which eliminates the shortcomings inherent to the (Formula presented.) index. The new measures proposed in this study can be used to complement the traditional performance assessment, namely, through the application of the h-index. Â© The Author(s) 2020.","The (Formula presented.) index was recently proposed to measure the degree of scientific leadership. While the concept is useful and interesting, namely, as a complement to the traditional performance analysis, the metric suffers from important shortcomings. We argue that scientific leadership should be evaluated: taking into account information of the moment the paper is produced/published, and in the specific context of the paper, meaning that only previous work relevant for the paper should be taken into account. Based on these two principles, we introduce an alternative approach, using self-citations as a source of information, which eliminates the shortcomings inherent to the (Formula presented.) index. The new measures proposed in this study can be used to complement the traditional performance assessment, namely, through the application of the h-index."
A domain knowledge graph construction method based on Wikipedia,"In order to achieve real-time updating of the domain knowledge graph and improve the relationship extraction ability in the construction process, a domain knowledge graph construction method is proposed. Based on the structured knowledge in Wikipediaâs classification system, we acquire concepts and instances contained in subject areas. A relationship extraction algorithm based on co-word analysis is intended to extract the classification relationships in semi-structured open labels. A Bi-GRU remote supervised relationship extraction model based on a multiple-scale attention mechanism and an improved cross-entropy loss function is proposed to obtain the non-classification relationships of concepts in unstructured texts. Experiments show that the proposed model performs better than the existing methods. Based on the obtained concepts, instances and relationships, a domain knowledge graph is constructed and the domain-independent nodes and relationships contained in them are removed through a vector variance algorithm. The effectiveness of the proposed method is verified by constructing a food domain knowledge graph based on Wikipedia. Â© The Author(s) 2020.","In order to achieve real-time updating of the domain knowledge graph and improve the relationship extraction ability in the construction process, a domain knowledge graph construction method is proposed. Based on the structured knowledge in Wikipedias classification system, we acquire concepts and instances contained in subject areas. A relationship extraction algorithm based on co-word analysis is intended to extract the classification relationships in semi-structured open labels. A Bi-GRU remote supervised relationship extraction model based on a multiple-scale attention mechanism and an improved cross-entropy loss function is proposed to obtain the non-classification relationships of concepts in unstructured texts. Experiments show that the proposed model performs better than the existing methods. Based on the obtained concepts, instances and relationships, a domain knowledge graph is constructed and the domain-independent nodes and relationships contained in them are removed through a vector variance algorithm. The effectiveness of the proposed method is verified by constructing a food domain knowledge graph based on Wikipedia."
Cross-lingual text similarity exploiting neural machine translation models,"This article studies cross-lingual text similarity using neural machine translation models. A straightforward approach based on machine translation is to use translated text so as to make the problem monolingual. Another possible approach is to use intermediate states of machine translation models as recently proposed in the related work, which could avoid propagation of translation errors. We aim at improving both approaches independently and then combine the two types of information, that is, translations and intermediate states, in a learning-to-rank framework to compute cross-lingual text similarity. To evaluate the effectiveness and generalisability of our approach, we conduct empirical experiments on EnglishâJapanese and EnglishâHindi translation corpora for a cross-lingual sentence retrieval task. It is demonstrated that our approach using translations and intermediate states outperforms other neural networkâbased approaches and is even comparable with a strong baseline based on a state-of-the-art machine translation system. Â© The Author(s) 2020.","This article studies cross-lingual text similarity using neural machine translation models. A straightforward approach based on machine translation is to use translated text so as to make the problem monolingual. Another possible approach is to use intermediate states of machine translation models as recently proposed in the related work, which could avoid propagation of translation errors. We aim at improving both approaches independently and then combine the two types of information, that is, translations and intermediate states, in a learning-to-rank framework to compute cross-lingual text similarity. To evaluate the effectiveness and generalisability of our approach, we conduct empirical experiments on EnglishJapanese and EnglishHindi translation corpora for a cross-lingual sentence retrieval task. It is demonstrated that our approach using translations and intermediate states outperforms other neural networkbased approaches and is even comparable with a strong baseline based on a state-of-the-art machine translation system."
"Partitioning highly, medium and lowly cited publications","Dividing papers based on their numbers of citations into several groups constitutes one of the most common research practices in bibliometrics and beyond. However, existing dividing methods are both arbitrary and subject to bias. This article proposes a novel approach to partition highly, medium and lowly cited publications based on their citation distribution. We utilise the whole Web of Science (WoS) dataset to demonstrate how to apply this approach to scholarly datasets and examine the robustness of our algorithm in each of the six disciplines under the WoS dataset. The codes that underlie the algorithm are available online. Â© The Author(s) 2020.","Dividing papers based on their numbers of citations into several groups constitutes one of the most common research practices in bibliometrics and beyond. However, existing dividing methods are both arbitrary and subject to bias. This article proposes a novel approach to partition highly, medium and lowly cited publications based on their citation distribution. We utilise the whole Web of Science (WoS) dataset to demonstrate how to apply this approach to scholarly datasets and examine the robustness of our algorithm in each of the six disciplines under the WoS dataset. The codes that underlie the algorithm are available online."
"Twenty-six years of LIS research focus and hot spots, 1990â2016: A co-word analysis","The purpose of this research is to map and analyse the conceptual and thematic structure of library and information science (LIS) research from the perspective of the co-word analysis. The bibliographical records consist of all the research papers published in the LIS core journals between 1990 and 2016 and indexed in Web of Science. âCiteSpaceâ was used to visualise the co-word network of LIS studies. The frequency of co-occurrence and centrality scores in the overall structure of the field showed that the word âScienceâ is the most significant and pivotal keyword among the nodes in the co-word network of LIS literature, and in this respect, the word âLibraryâ is in the second place. However, the results of the social network analysis uncovered that in spite of the high frequency of the word âlibraryâ, the pivotal role of the term has been exposed to decline over the time. The results of the analysis of co-word clusters showed that âinformation seeking and retrievalâ is the most important research focus in the intellectual structure of LIS literature during 1990â2016. Also, analysis of the hot spots of the LIS research based on Kleinberg algorithm indicated that the words âInternetâ and âWorld Wide Webâ have attracted the most attention by LIS scholars during the years under study. Â© The Author(s) 2020.","The purpose of this research is to map and analyse the conceptual and thematic structure of library and information science (LIS) research from the perspective of the co-word analysis. The bibliographical records consist of all the research papers published in the LIS core journals between 1990 and 2016 and indexed in Web of Science. CiteSpace was used to visualise the co-word network of LIS studies. The frequency of co-occurrence and centrality scores in the overall structure of the field showed that the word Science is the most significant and pivotal keyword among the nodes in the co-word network of LIS literature, and in this respect, the word Library is in the second place. However, the results of the social network analysis uncovered that in spite of the high frequency of the word library, the pivotal role of the term has been exposed to decline over the time. The results of the analysis of co-word clusters showed that information seeking and retrieval is the most important research focus in the intellectual structure of LIS literature during 19902016. Also, analysis of the hot spots of the LIS research based on Kleinberg algorithm indicated that the words Internet and World Wide Web have attracted the most attention by LIS scholars during the years under study."
Using social media during job search: The case of 16â24 year olds in Scotland,"Social media are powerful networking platforms that provide users with significant information opportunities. Despite this, little is known about their impact on job search behaviour. Here, interview (participants = 7), focus group (participants = 6) and survey (n = 558) data supplied by young jobseekers in Scotland were analysed to investigate the role of social media in job search. The findings show that Facebook, Twitter and LinkedIn are the most popular platforms for this purpose, and that the type of job sought influences the direction of user behaviour. Frequent social media use for job search is linked with interview invitations. The study also reveals that although most jobseekers use social media for job search sparingly, they are much more likely to do so if advised by a professional. Combined, the findings represent a crucial base of knowledge which can inform careers policy and be used as a platform for further research. Â© The Author(s) 2020.","Social media are powerful networking platforms that provide users with significant information opportunities. Despite this, little is known about their impact on job search behaviour. Here, interview (participants = 7), focus group (participants = 6) and survey (n = 558) data supplied by young jobseekers in Scotland were analysed to investigate the role of social media in job search. The findings show that Facebook, Twitter and LinkedIn are the most popular platforms for this purpose, and that the type of job sought influences the direction of user behaviour. Frequent social media use for job search is linked with interview invitations. The study also reveals that although most jobseekers use social media for job search sparingly, they are much more likely to do so if advised by a professional. Combined, the findings represent a crucial base of knowledge which can inform careers policy and be used as a platform for further research."
Exploiting user network topology and comment semantic for accurate rumour stance recognition on social media,"Online social media (OSM) has become a hotbed for the rapid dissemination of disinformation or fake news. In order to recognise fake news and guide users of OSM, we focus on the stance recognition of comments, posted on OSM on the fake news-related users. In this article, we propose a framework for recognition of rumour stances (we set four categories ââagreeâ, âdisagreeâ, âneutralâ and âqueryâ), combining network topology and comment semantic enhancement (CSE). We first construct a vector matrix of comments via a novel optimised term frequencyâinverse document frequency (OTI). To better recognise stances, we employ another vector matrix with novel or special attributes which comprises the network topology of the OSM users derived from the random walk with restart (RWR) method. In addition, we set a weight parameter for each word in the comments to enhance comment semantic representation, where these parameters are tuned based on sentiment score, topology features and question format words. These vector matrices are optimised and combined into an integrated matrix whose transpose matrix is fed into a neural network (NN) for final rumour stance recognition. Experimental evaluations show that our approach achieves a high precision of 93.96% and F1-score of 92.02% which are superior to baselines and other existing methods. Â© The Author(s) 2020.","Online social media (OSM) has become a hotbed for the rapid dissemination of disinformation or fake news. In order to recognise fake news and guide users of OSM, we focus on the stance recognition of comments, posted on OSM on the fake news-related users. In this article, we propose a framework for recognition of rumour stances (we set four categories agree, disagree, neutral and query), combining network topology and comment semantic enhancement (CSE). We first construct a vector matrix of comments via a novel optimised term frequencyinverse document frequency (OTI). To better recognise stances, we employ another vector matrix with novel or special attributes which comprises the network topology of the OSM users derived from the random walk with restart (RWR) method. In addition, we set a weight parameter for each word in the comments to enhance comment semantic representation, where these parameters are tuned based on sentiment score, topology features and question format words. These vector matrices are optimised and combined into an integrated matrix whose transpose matrix is fed into a neural network (NN) for final rumour stance recognition. Experimental evaluations show that our approach achieves a high precision of 93.96% and F1-score of 92.02% which are superior to baselines and other existing methods."
Factors influencing researchersâ journal selection decisions,"The scholarly publication landscape continues to grow in complexity, presenting researchers with ever-increasing dilemmas regarding journal choice. However, research into the decision-making processes associated with journal choice is limited. This article contributes by reporting on an international survey of researchers in various disciplines and with varying levels of experience. The study examines the extent to which various journal characteristics affect journal selection, perceptions of the extent to which university and national research policies impact on their journal choice, and the influence of academicsâ familiarity, confidence and objectives on journal choice. The most important factors influencing journal choice were as follows: reliability of reviewing, usefulness of reviewersâ feedback, the reputation of the journal and confidence that their article is in scope for the journal. Publishing productivity, publishing experience, researcher role and discipline had little impact on the ranking of journal choice factors, suggesting that the research community is homogeneous. Â© The Author(s) 2020.","The scholarly publication landscape continues to grow in complexity, presenting researchers with ever-increasing dilemmas regarding journal choice. However, research into the decision-making processes associated with journal choice is limited. This article contributes by reporting on an international survey of researchers in various disciplines and with varying levels of experience. The study examines the extent to which various journal characteristics affect journal selection, perceptions of the extent to which university and national research policies impact on their journal choice, and the influence of academics familiarity, confidence and objectives on journal choice. The most important factors influencing journal choice were as follows: reliability of reviewing, usefulness of reviewers feedback, the reputation of the journal and confidence that their article is in scope for the journal. Publishing productivity, publishing experience, researcher role and discipline had little impact on the ranking of journal choice factors, suggesting that the research community is homogeneous."
Online news media website ranking using user-generated content,"News media websites are important online resources that have drawn great attention of text mining researchers. The main aim of this study is to propose a framework for ranking online news websites from different viewpoints. The ranking of news websites provides useful information, which can benefit many news-related tasks such as news retrieval and news recommendation. In the proposed framework, the ranking of news websites is obtained by calculating three measures introduced in the article and based on user-generated content (UGC). Each proposed measure is concerned with the performance of news websites from a particular viewpoint including the completeness of news reports, the diversity of events being covered by the website and its speed. The use of UGC in this framework, as a partly unbiased, real-time and low cost content on the web distinguishes the proposed news website ranking framework from the literature. The results obtained for three prominent news websites, British Broadcasting Corporation (BBC), Cable News Network (CNN) and New York Times (NYTimes), show that BBC has the best performance in terms of news completeness and speed, and NYTimes has the best diversity in comparison with the other two websites. Â© The Author(s) 2020.","News media websites are important online resources that have drawn great attention of text mining researchers. The main aim of this study is to propose a framework for ranking online news websites from different viewpoints. The ranking of news websites provides useful information, which can benefit many news-related tasks such as news retrieval and news recommendation. In the proposed framework, the ranking of news websites is obtained by calculating three measures introduced in the article and based on user-generated content (UGC). Each proposed measure is concerned with the performance of news websites from a particular viewpoint including the completeness of news reports, the diversity of events being covered by the website and its speed. The use of UGC in this framework, as a partly unbiased, real-time and low cost content on the web distinguishes the proposed news website ranking framework from the literature. The results obtained for three prominent news websites, British Broadcasting Corporation (BBC), Cable News Network (CNN) and New York Times (NYTimes), show that BBC has the best performance in terms of news completeness and speed, and NYTimes has the best diversity in comparison with the other two websites."
Discovering informative features in large-scale landmark image collection,"One of the key problems in image retrieval systems is the presence of irrelevant and noisy image content. Such content can cause significant confusion for the system. Therefore, there is a need to represent images with only informative features in order to improve the retrieval performance of the system or any subsequent process. In this article, we propose a method to identify the informative features in a large-scale image collection. We apply the frequent itemset mining (FIM) approach to extract visual features patterns from a list of images of the same object. Then, we generate feature pairs to measure the significance of each feature depending on the co-occurrence with its neighbouring features. In addition, we apply this feature selection technique to localise the landmark in the image. The performance of the proposed method is evaluated in terms of average precision (AP) on two benchmark data sets and found that it gives a comparable retrieval performance over the bag of visual words baseline system and the previous methods. Â© The Author(s) 2020.","One of the key problems in image retrieval systems is the presence of irrelevant and noisy image content. Such content can cause significant confusion for the system. Therefore, there is a need to represent images with only informative features in order to improve the retrieval performance of the system or any subsequent process. In this article, we propose a method to identify the informative features in a large-scale image collection. We apply the frequent itemset mining (FIM) approach to extract visual features patterns from a list of images of the same object. Then, we generate feature pairs to measure the significance of each feature depending on the co-occurrence with its neighbouring features. In addition, we apply this feature selection technique to localise the landmark in the image. The performance of the proposed method is evaluated in terms of average precision (AP) on two benchmark data sets and found that it gives a comparable retrieval performance over the bag of visual words baseline system and the previous methods."
An ensemble clustering approach for topic discovery using implicit text segmentation,"Text segmentation (TS) is the process of dividing multi-topic text collections into cohesive segments using topic boundaries. Similarly, text clustering has been renowned as a major concern when it comes to multi-topic text collections, as they are distinguished by sub-topic structure and their contents are not associated with each other. Existing clustering approaches follow the TS method which relies on word frequencies and may not be suitable to cluster multi-topic text collections. In this work, we propose a new ensemble clustering approach (ECA) is a novel topic-modelling-based clustering approach, which induces the combination of TS and text clustering. We improvised a LDA-onto (LDA-ontology) is a TS-based model, which presents a deterioration of a document into segments (i.e. sub-documents), wherein each sub-document is associated with exactly one sub-topic. We deal with the problem of clustering when it comes to a document that is intrinsically related to various topics and its topical structure is missing. ECA is tested through well-known datasets in order to provide a comprehensive presentation and validation of clustering algorithms using LDA-onto. ECA exhibits the semantic relations of keywords in sub-documents and resultant clusters belong to original documents that they contain. Moreover, present research sheds the light on clustering performances and it indicates that there is no difference over performances (in terms of F-measure) when the number of topics changes. Our findings give above par results in order to analyse the problem of text clustering in a broader spectrum without applying dimension reduction techniques over high sparse data. Specifically, ECA provides an efficient and significant framework than the traditional and segment-based approach, such that achieved results are statistically significant with an average improvement of over 10.2%. For the most part, proposed framework can be evaluated in applications where meaningful data retrieval is useful, such as document summarization, text retrieval, novelty and topic detection. Â© The Author(s) 2020.","Text segmentation (TS) is the process of dividing multi-topic text collections into cohesive segments using topic boundaries. Similarly, text clustering has been renowned as a major concern when it comes to multi-topic text collections, as they are distinguished by sub-topic structure and their contents are not associated with each other. Existing clustering approaches follow the TS method which relies on word frequencies and may not be suitable to cluster multi-topic text collections. In this work, we propose a new ensemble clustering approach (ECA) is a novel topic-modelling-based clustering approach, which induces the combination of TS and text clustering. We improvised a LDA-onto (LDA-ontology) is a TS-based model, which presents a deterioration of a document into segments ( sub-documents), wherein each sub-document is associated with exactly one sub-topic. We deal with the problem of clustering when it comes to a document that is intrinsically related to various topics and its topical structure is missing. ECA is tested through well-known datasets in order to provide a comprehensive presentation and validation of clustering algorithms using LDA-onto. ECA exhibits the semantic relations of keywords in sub-documents and resultant clusters belong to original documents that they contain. Moreover, present research sheds the light on clustering performances and it indicates that there is no difference over performances (in terms of F-measure) when the number of topics changes. Our findings give above par results in order to analyse the problem of text clustering in a broader spectrum without applying dimension reduction techniques over high sparse data. Specifically, ECA provides an efficient and significant framework than the traditional and segment-based approach, such that achieved results are statistically significant with an average improvement of over 10.2%. For the most part, proposed framework can be evaluated in applications where meaningful data retrieval is useful, such as document summarization, text retrieval, novelty and topic detection."
Improving personalised query reformulation with embeddings,"As a mechanism to guide users towards a better representation of their information needs, the query reformulation method generates new queries based on usersâ historical queries. To preserve the original search intent, query reformulations should be context-aware and should attempt to meet usersâ personal information needs. The mainstream method aims to generate candidate queries first, according to their past frequencies, and then score (re-rank) these candidates based on the semantic consistency of terms, dependency among latent semantic topics and user preferences. We exploit embeddings (i.e. term, user and topic embeddings) to use contextual information and individual preferences more effectively to improve personalised query reformulation. Our work involves two major tasks. In the first task, candidate queries are generated from an original query by substituting or adding one term, and the contextual similarities between the terms are calculated based on the term embeddings and augmented with user personalisation. In the second task, the candidate queries generated in the first task are evaluated and scored (re-ranked) according to the consistency of the semantic meaning of the candidate query and the user preferences based on a graphical model with the term, user and topic embeddings. Experiments show that our proposed model yields significant improvements compared with the current state-of-the-art methods. Â© The Author(s) 2020.","As a mechanism to guide users towards a better representation of their information needs, the query reformulation method generates new queries based on users historical queries. To preserve the original search intent, query reformulations should be context-aware and should attempt to meet users personal information needs. The mainstream method aims to generate candidate queries first, according to their past frequencies, and then score (re-rank) these candidates based on the semantic consistency of terms, dependency among latent semantic topics and user preferences. We exploit embeddings ( term, user and topic embeddings) to use contextual information and individual preferences more effectively to improve personalised query reformulation. Our work involves two major tasks. In the first task, candidate queries are generated from an original query by substituting or adding one term, and the contextual similarities between the terms are calculated based on the term embeddings and augmented with user personalisation. In the second task, the candidate queries generated in the first task are evaluated and scored (re-ranked) according to the consistency of the semantic meaning of the candidate query and the user preferences based on a graphical model with the term, user and topic embeddings. Experiments show that our proposed model yields significant improvements compared with the current state-of-the-art methods."
Structuration analysis of e-government studies: A bibliometric analysis based on knowledge maps,"Considering the lack of systematic reviews on e-government research, this study includes research categories, spatial structure, research paradigms and noteworthy future topics in the domain of e-government. We collect 142 keywords from 2646 papers published in the Web of Science from 2000 to 2019 as the study object. Then, we identify four research categories: (1) technology and modelling in e-government, (2) drivers of e-government development, (3) public management and (4) governmental management. From public and government perspectives, we outline the spatial structure which includes theories and practices research and conclude four research paradigms: (1) theoretical modelling and application, (2) e-government development, (3) status of public management and (4) status of governmental management. Finally, we develop a 3D spatial map to analyse noteworthy topics and explore the well-studied themes: government-to-citizen, government-to-government and government-to-business, and the under-studied themes: government-to-civil society organisations and citizens-to-citizens, which helps scholars research e-government roundly. Â© The Author(s) 2020.","Considering the lack of systematic reviews on e-government research, this study includes research categories, spatial structure, research paradigms and noteworthy future topics in the domain of e-government. We collect 142 keywords from 2646 papers published in the Web of Science from 2000 to 2019 as the study object. Then, we identify four research categories: technology and modelling in e-government, drivers of e-government development, public management and governmental management. From public and government perspectives, we outline the spatial structure which includes theories and practices research and conclude four research paradigms: theoretical modelling and application, e-government development, status of public management and status of governmental management. Finally, we develop a 3D spatial map to analyse noteworthy topics and explore the well-studied themes: government-to-citizen, government-to-government and government-to-business, and the under-studied themes: government-to-civil society organisations and citizens-to-citizens, which helps scholars research e-government roundly."
Chemistry research in Europe: A publication analysis (2006â2016),"In this article, chemistry research in 51 different European countries between years 2006 and 2016 was studied using statistical methods. This study consists of two parts: In the first part, different economical, institutional and citation parameters were correlated with the number of publications, citations and chemical industry numbers using principal components analysis and hierarchical cluster analysis. The results of the first part indicated that economical and geographical parameters directly affect the chemistry research outcome. In the second part, research in branches of chemistry and related disciplines such as analytical chemistry, polymer science and physical chemistry were analysed using principal components analysis and hierarchical cluster analysis for each country. Publication data were collected as the number of chemistry publications (in Science Citation IndexâExpanded (SCI-E)) between years 2006 and 2016 in different chemistry subdisciplines and related scientific areas. Results of the second part of the study produced geographical and economical clusters of countries, interestingly, without addition of any geographical data. Â© The Author(s) 2019.","In this article, chemistry research in 51 different European countries between years 2006 and 2016 was studied using statistical methods. This study consists of two parts: In the first part, different economical, institutional and citation parameters were correlated with the number of publications, citations and chemical industry numbers using principal components analysis and hierarchical cluster analysis. The results of the first part indicated that economical and geographical parameters directly affect the chemistry research outcome. In the second part, research in branches of chemistry and related disciplines such as analytical chemistry, polymer science and physical chemistry were analysed using principal components analysis and hierarchical cluster analysis for each country. Publication data were collected as the number of chemistry publications (in Science Citation IndexExpanded (SCI-E)) between years 2006 and 2016 in different chemistry subdisciplines and related scientific areas. Results of the second part of the study produced geographical and economical clusters of countries, interestingly, without addition of any geographical data."
Prediction of online topicsâ popularity patterns,"Popularity prediction of online contents is always a tool of emergency management, business decision-making, and public opinion monitoring. Most previous work has made efforts to predict the volumes or levels of popularity, but patterns of popularity evolution are remaining largely unexplored. Actually, topic popularity patterns can offer more detailed information for event detection and early warning. In this article, we proposed an effective method to discover and predict the popularity patterns of topics on the Internet which combined clustering and classification models. This method does not rely on the early time data of topic propagation, so it can predict the future popularity pattern at the initial stage of topic releasing. First, we chose a time series clustering algorithm K-SC to obtain basic types of topic popularity patterns. Then, through acquiring and evaluating multiple features related to the topics including publisher features, outward characteristics of content and textual ones, we built the prediction model of topic popularity patterns based on machine learning methods. The experimental results show that it is suitable to cluster four basic patterns of topic popularity from the experimental data. Whatâs more, making use of certain initial characteristics, Decision Tree model can effectively predict the popularity pattern of a newly released topic, with an accuracy of 89.4%. Â© The Author(s) 2020.","Popularity prediction of online contents is always a tool of emergency management, business decision-making, and public opinion monitoring. Most previous work has made efforts to predict the volumes or levels of popularity, but patterns of popularity evolution are remaining largely unexplored. Actually, topic popularity patterns can offer more detailed information for event detection and early warning. In this article, we proposed an effective method to discover and predict the popularity patterns of topics on the Internet which combined clustering and classification models. This method does not rely on the early time data of topic propagation, so it can predict the future popularity pattern at the initial stage of topic releasing. First, we chose a time series clustering algorithm K-SC to obtain basic types of topic popularity patterns. Then, through acquiring and evaluating multiple features related to the topics including publisher features, outward characteristics of content and textual ones, we built the prediction model of topic popularity patterns based on machine learning methods. The experimental results show that it is suitable to cluster four basic patterns of topic popularity from the experimental data. Whats more, making use of certain initial characteristics, Decision Tree model can effectively predict the popularity pattern of a newly released topic, with an accuracy of 89.4%."
Museum libraries in Spain: A case study at state level,"Special libraries are essential information and documentation centres for university teachers and researchers due to the quality and richness of their collections. In Spain, it is estimated that there are 2456 special libraries, although many are unknown either generally or among information professionals. These include museum libraries, which are important centres with valuable collections of bibliographic heritage for the area of Humanities and Social Sciences. The aim of this research is to gain an understanding of the real state of these information units and promote the social value of museum libraries in Spain. To do this, a survey was sent to the libraries of state-owned and -managed museums under the General Directorate of Fine Arts and Cultural Property (Ministry of Culture and Sports) of the Government of Spain. This general objective will be accompanied by a review of the scientific literature on various aspects of museum libraries at national and international level. After addressing the research methodology, the results obtained will be discussed and will include the following topics: collection management, library services and staff, economic and technological resources and finally, library management. Conclusions include recommendations for museum librarians and reveal that institutional cooperation is a strategic issue to improve both museum libraries visibility and their social recognition as cultural and research centre. Â© The Author(s) 2020.","Special libraries are essential information and documentation centres for university teachers and researchers due to the quality and richness of their collections. In Spain, it is estimated that there are 2456 special libraries, although many are unknown either generally or among information professionals. These include museum libraries, which are important centres with valuable collections of bibliographic heritage for the area of Humanities and Social Sciences. The aim of this research is to gain an understanding of the real state of these information units and promote the social value of museum libraries in Spain. To do this, a survey was sent to the libraries of state-owned and -managed museums under the General Directorate of Fine Arts and Cultural Property (Ministry of Culture and Sports) of the Government of Spain. This general objective will be accompanied by a review of the scientific literature on various aspects of museum libraries at national and international level. After addressing the research methodology, the results obtained will be discussed and will include the following topics: collection management, library services and staff, economic and technological resources and finally, library management. Conclusions include recommendations for museum librarians and reveal that institutional cooperation is a strategic issue to improve both museum libraries visibility and their social recognition as cultural and research centre."
A spike in the scientific output on social sciences in Vietnam for recent three years: Evidence from bibliometric analysis in Scopus database (2000â2019),"Bibliometric analysis of 3105 publications retrieved from the Scopus database was conducted to evaluate bibliographic content of scientific output on social sciences in Vietnam, for the 2000â2019 period. Our main findings show that the number of publications on social sciences from Vietnam has increased significantly over the last two decades, and there was a spike in the scientific output for the recent three years when the number of publications accounted for 53.76% of the collection. The most productive authors came from a few public research institutes with strong resources as the top 10 institutions participated in 44.22% of the collection. Vietnamese scholars tend not to submit their works to high-ranking journals since five Q1 journals in the top 10 publishing journals published only 6.17% of the collection. For international collaboration, Australia and the United States ranked first and second based on the number of publications and citations. Other countries in top 10 mostly located in Europe and Asia. Research topics were diverse focusing on gender, poverty, HIV, higher education and sustainable development. We suggest that supporting policies and funding need to be provided to help Vietnamese scholars improve their works, and to boost their scientific production in the future. Â© The Author(s) 2020.","Bibliometric analysis of 3105 publications retrieved from the Scopus database was conducted to evaluate bibliographic content of scientific output on social sciences in Vietnam, for the 20002019 period. Our main findings show that the number of publications on social sciences from Vietnam has increased significantly over the last two decades, and there was a spike in the scientific output for the recent three years when the number of publications accounted for 53.76% of the collection. The most productive authors came from a few public research institutes with strong resources as the top 10 institutions participated in 44.22% of the collection. Vietnamese scholars tend not to submit their works to high-ranking journals since five Q1 journals in the top 10 publishing journals published only 6.17% of the collection. For international collaboration, Australia and the United States ranked first and second based on the number of publications and citations. Other countries in top 10 mostly located in Europe and Asia. Research topics were diverse focusing on gender, poverty, HIV, higher education and sustainable development. We suggest that supporting policies and funding need to be provided to help Vietnamese scholars improve their works, and to boost their scientific production in the future."
"Does the use of open, non-anonymous peer review in scholarly publishing introduce bias? Evidence from the F1000Research post-publication open peer review publishing model","As part of moves towards open knowledge practices, making peer review open is cited as a way to enable fuller scrutiny and transparency of assessments around research. There are now many flavours of open peer review in use across scholarly publishing, including where reviews are fully attributable and the reviewer is named. This study examines whether there is any evidence of bias in two areas of common critique of open, non-anonymous (named) peer review â and used in the post-publication, peer review system operated by the open-access scholarly publishing platform F1000Research. First, is there evidence of potential bias where a reviewer based in a specific country assesses the work of an author also based in the same country? Second, are reviewers influenced by being able to see the comments and know the origins of a previous reviewer? Based on over 4 years of open peer review data, we found some weak evidence that being based in the same country as an author may influence a reviewerâs decision, while there was insufficient evidence to conclude that being able to read an existing published review prior to submitting a review encourages conformity. Thus, while immediate publishing of peer review reports appears to be unproblematic, caution may be needed when selecting same-country reviewers in open systems if other studies confirm these results. Â© The Author(s) 2020.","As part of moves towards open knowledge practices, making peer review open is cited as a way to enable fuller scrutiny and transparency of assessments around research. There are now many flavours of open peer review in use across scholarly publishing, including where reviews are fully attributable and the reviewer is named. This study examines whether there is any evidence of bias in two areas of common critique of open, non-anonymous (named) peer review and used in the post-publication, peer review system operated by the open-access scholarly publishing platform F1000Research. First, is there evidence of potential bias where a reviewer based in a specific country assesses the work of an author also based in the same country? Second, are reviewers influenced by being able to see the comments and know the origins of a previous reviewer? Based on over 4 years of open peer review data, we found some weak evidence that being based in the same country as an author may influence a reviewers decision, while there was insufficient evidence to conclude that being able to read an existing published review prior to submitting a review encourages conformity. Thus, while immediate publishing of peer review reports appears to be unproblematic, caution may be needed when selecting same-country reviewers in open systems if other studies confirm these results."
A qualitativeâquantitative study of science mapping by different algorithms: The Polish journals landscape,"By applying different clustering algorithms, the author strived to construct the best visual representation of scientific domains and disciplines in Poland. Journals and their disciplinary categories constituted a data set. A comparative analysis of maps was based on both qualitative and quantitative approaches. Complex patterns of eight maps were evaluated taking into account both the local proximity of disciplines and the whole structure of presented domains. Final clustering quality value was introduced and calculated in reference to the knowledge domains. The authors underlined the role of quantitative and qualitative methods in combination in the mapping evaluation. The best results were obtained with the T-distributed stochastic neighbour embedding (t-SNE) algorithm. This youngest technique may have the biggest potential for semantic information studies and in the scope of broadly understood semantic solutions. Â© The Author(s) 2020.","By applying different clustering algorithms, the author strived to construct the best visual representation of scientific domains and disciplines in Poland. Journals and their disciplinary categories constituted a data set. A comparative analysis of maps was based on both qualitative and quantitative approaches. Complex patterns of eight maps were evaluated taking into account both the local proximity of disciplines and the whole structure of presented domains. Final clustering quality value was introduced and calculated in reference to the knowledge domains. The authors underlined the role of quantitative and qualitative methods in combination in the mapping evaluation. The best results were obtained with the T-distributed stochastic neighbour embedding (t-SNE) algorithm. This youngest technique may have the biggest potential for semantic information studies and in the scope of broadly understood semantic solutions."
Testing the validity of Wikipedia categories for subject matter labelling of open-domain corpus data,"The Wikipedia category system was designed to enable browsing and navigation of Wikipedia. It is also a useful resource for knowledge organisation and document indexing, especially using automatic approaches. However, it has received little attention as a resource for manual indexing. In this article, a hierarchical taxonomy of three-level depth is extracted from the Wikipedia category system. The resulting taxonomy is explored as a lightweight alternative to expert-created knowledge organisation systems (e.g. library classification systems) for the manual labelling of open-domain text corpora. Combining quantitative and qualitative data from a crowd-based text labelling study, the validity of the taxonomy is tested and the results quantified in terms of interrater agreement. While the usefulness of the Wikipedia category system for automatic document indexing is documented in the pertinent literature, our results suggest that at least the taxonomy we derived from it is not a valid instrument for manual subject matter labelling of open-domain text corpora. Â© The Author(s) 2020.","The Wikipedia category system was designed to enable browsing and navigation of Wikipedia. It is also a useful resource for knowledge organisation and document indexing, especially using automatic approaches. However, it has received little attention as a resource for manual indexing. In this article, a hierarchical taxonomy of three-level depth is extracted from the Wikipedia category system. The resulting taxonomy is explored as a lightweight alternative to expert-created knowledge organisation systems ( library classification systems) for the manual labelling of open-domain text corpora. Combining quantitative and qualitative data from a crowd-based text labelling study, the validity of the taxonomy is tested and the results quantified in terms of interrater agreement. While the usefulness of the Wikipedia category system for automatic document indexing is documented in the pertinent literature, our results suggest that at least the taxonomy we derived from it is not a valid instrument for manual subject matter labelling of open-domain text corpora."
Investigating Reddit to detect subreddit and author stereotypes and to evaluate author assortativity,"In recent years, Reddit has attracted the interest of many researchers due to its popularity all over the world. In this article, we aim at providing a contribution to the knowledge of this social network by investigating three of its aspects, interesting from the scientific viewpoint, and, at the same time, by analysing a large number of applications. In particular, we first propose a definition and an analysis of several stereotypes of both subreddits and authors. This analysis is coupled with the definition of three possible orthogonal taxonomies that help us to classify stereotypes in an appropriate way. Then, we investigate the possible existence of author assortativity in this social medium; specifically, we focus on co-posters, that is, authors who submitted posts on the same subreddit. Â© The Author(s) 2020.","In recent years, Reddit has attracted the interest of many researchers due to its popularity all over the world. In this article, we aim at providing a contribution to the knowledge of this social network by investigating three of its aspects, interesting from the scientific viewpoint, and, at the same time, by analysing a large number of applications. In particular, we first propose a definition and an analysis of several stereotypes of both subreddits and authors. This analysis is coupled with the definition of three possible orthogonal taxonomies that help us to classify stereotypes in an appropriate way. Then, we investigate the possible existence of author assortativity in this social medium; specifically, we focus on co-posters, that is, authors who submitted posts on the same subreddit."
Automatic e-content sequencing system for personalised learning environments by using fuzzy AHP based on multiple intelligences,"The use of personalised learning environments (PLEs) and adaptive learning environments (ALEs) in education has increased during the COVID-2019 pandemic. Thus, the need of effective and high-quality PLEs and ALEs has emerged but developing such learning environments has some difficulties such as the need for a huge number of qualified e-contents. Generation of e-contents for these systems is a time-consuming and costly process. Moreover, presenting the same e-content for each student is not effective because each student can have different intelligence type. To overcome these problems, low cost, reusable and dynamic e-contents can be used for PLE and ALE systems. For this purpose, fuzzy analytic hierarchy process (FAHP) was used to provide sequenced e-contents for PLE and ALE systems. This proposed system was used in a case study to investigate its impact on learning process. The performance of the proposed study was compared with analytical hierarchy process (AHP), which is another multi-criteria decision-making (MCDM) method, also further statistical analyses were investigated. The results showed that 84.6% of students showed interest in the proposed system, so the FAHP method can be used for presenting personalised e-content effectively. Â© The Author(s) 2021.","The use of personalised learning environments (PLEs) and adaptive learning environments (ALEs) in education has increased during the COVID-2019 pandemic. Thus, the need of effective and high-quality PLEs and ALEs has emerged but developing such learning environments has some difficulties such as the need for a huge number of qualified e-contents. Generation of e-contents for these systems is a time-consuming and costly process. Moreover, presenting the same e-content for each student is not effective because each student can have different intelligence type. To overcome these problems, low cost, reusable and dynamic e-contents can be used for PLE and ALE systems. For this purpose, fuzzy analytic hierarchy process (FAHP) was used to provide sequenced e-contents for PLE and ALE systems. This proposed system was used in a case study to investigate its impact on learning process. The performance of the proposed study was compared with analytical hierarchy process (AHP), which is another multi-criteria decision-making method, also further statistical analyses were investigated. The results showed that 84.6% of students showed interest in the proposed system, so the FAHP method can be used for presenting personalised e-content effectively."
Optimal policy learning for COVID-19 prevention using reinforcement learning,"COVID-19 has changed the lifestyle of many people due to its rapid human-to-human transmission. The spread started at the end of January 2020, and different countries used different approaches in terms of testing, sanitization, lock down and quarantine centres to control the spread of the virus. People are getting back to working and routine life activities with new normal standards of testing, sanitization, social distancing and lock down. People are regularly tested to identify those who are infected with COVID-19 and isolate them from general public. However, testing all people unnecessarily is an expensive operation in terms of resources usage. There must be an optimal policy to test only those who have higher chances of being COVID-19 positive. Similarly, sanitization is used for individuals and streets to disinfect people and places. However, sanitization is also an expensive operation in terms of resources, and it is not possible to disinfect each and every individual and street. Social separating or lock down or quarantine centres focuses are different methodologies that are utilised to control the human-to-human transmission of the infection and separate the individuals who are contaminated with COVID-19. However, lock down and quarantine centres are expensive operations in terms of resources as it disturbs the affairs of state and the growth of economy. At the same time, it negatively affects the quality of life of a society. It is also not possible to provide resources to all citizens by locking them inside homes or quarantine centres for infinite time. All these parameters are expensive in terms of resources and have an effect on controlling the spread of the virus, quality of life of human, resources and economy. In this article, a novel intelligent method based on reinforcement learning (RL) is built up that quantifies the unique levels of testing, disinfection and lock down alongside its impact on the spread of the infection, personal satisfaction or quality of life, resource use and economy. Different RL algorithms are actualized and agents are prepared with these algorithms to interact with the environment to gain proficiency with the best strategy. The examinations exhibit that deep learningâbased algorithms, for example, DQN and DDPG are performing better than customary RL algorithms, for example, Q-Learning and SARSA. Â© The Author(s) 2020.","COVID-19 has changed the lifestyle of many people due to its rapid human-to-human transmission. The spread started at the end of January 2020, and different countries used different approaches in terms of testing, sanitization, lock down and quarantine centres to control the spread of the virus. People are getting back to working and routine life activities with new normal standards of testing, sanitization, social distancing and lock down. People are regularly tested to identify those who are infected with COVID-19 and isolate them from general public. However, testing all people unnecessarily is an expensive operation in terms of resources usage. There must be an optimal policy to test only those who have higher chances of being COVID-19 positive. Similarly, sanitization is used for individuals and streets to disinfect people and places. However, sanitization is also an expensive operation in terms of resources, and it is not possible to disinfect each and every individual and street. Social separating or lock down or quarantine centres focuses are different methodologies that are utilised to control the human-to-human transmission of the infection and separate the individuals who are contaminated with COVID-19. However, lock down and quarantine centres are expensive operations in terms of resources as it disturbs the affairs of state and the growth of economy. At the same time, it negatively affects the quality of life of a society. It is also not possible to provide resources to all citizens by locking them inside homes or quarantine centres for infinite time. All these parameters are expensive in terms of resources and have an effect on controlling the spread of the virus, quality of life of human, resources and economy. In this article, a novel intelligent method based on reinforcement learning (RL) is built up that quantifies the unique levels of testing, disinfection and lock down alongside its impact on the spread of the infection, personal satisfaction or quality of life, resource use and economy. Different RL algorithms are actualized and agents are prepared with these algorithms to interact with the environment to gain proficiency with the best strategy. The examinations exhibit that deep learningbased algorithms, for example, DQN and DDPG are performing better than customary RL algorithms, for example, Q-Learning and SARSA."
Topic extraction to provide an overview of research activities: The case of the high-temperature superconductor and simulation and modelling,"For those who are not experts in a particular scientific field, it is difficult to understand scientific research trends. Although studies on the extraction of research trends have been conducted, most focus on extracting global trends from large-scale data, and the methods are often complicated. The purpose of this study is to develop a method of obtaining overviews of a scientific field for non-experts by capturing research trends simply and then to verify the method. To extract research topics which should express research trends, text analysis was performed using abstracts over 12 years of articles on high-temperature superconductors. We characterised three topics for the extracted word groups that frequently occurred. For these topics, we studied their appropriateness using a method that has been little used: examining research articles, review literature and co-citations among research articles used to extract the words, comparisons with controlled index terms assigned to the articles and confirming that there were no contradictions. Based on the established method, we have also applied this method to another research field: âsimulation and modellingâ. Although the method used in this article is simple, important topics were extracted, and the relations with the original articles are clear, which can lead to further investigation of the extracted topics. Â© The Author(s) 2020.","For those who are not experts in a particular scientific field, it is difficult to understand scientific research trends. Although studies on the extraction of research trends have been conducted, most focus on extracting global trends from large-scale data, and the methods are often complicated. The purpose of this study is to develop a method of obtaining overviews of a scientific field for non-experts by capturing research trends simply and then to verify the method. To extract research topics which should express research trends, text analysis was performed using abstracts over 12 years of articles on high-temperature superconductors. We characterised three topics for the extracted word groups that frequently occurred. For these topics, we studied their appropriateness using a method that has been little used: examining research articles, review literature and co-citations among research articles used to extract the words, comparisons with controlled index terms assigned to the articles and confirming that there were no contradictions. Based on the established method, we have also applied this method to another research field: simulation and modelling. Although the method used in this article is simple, important topics were extracted, and the relations with the original articles are clear, which can lead to further investigation of the extracted topics."
Exploring research trends in big data across disciplines: A text mining analysis,"Using big data has been a prevailing research trend in various academic fields. However, no studies have explored the scope and structure of big data across disciplines. In this article, we applied topic modeling and word co-occurrence analysis methods to identify key topics from more than 36,000 big data publications across all academic disciplines between 2012 and 2017. The results revealed several topics associated with the storage, collection and analysis of large datasets; the publications were predominantly published in computational fields. Other identified research topics show the influence of big data methods and techniques in areas beyond computer science, such as education, urban informatics, business, health and medical sciences. In fact, the prevalence of these topics has increased over time. In contrast, some themes like parallel computing, network modeling and big data analytic techniques have lost their popularity in recent years. These results probably reflect the maturity of big data core topics and highlight flourishing new research trends pertinent to big data in new domains, especially in social sciences, health and medicine. Findings of this article can be beneficial for researchers and science policymakers to understand the scope and structure of big data in different academic disciplines. Â© The Author(s) 2020.","Using big data has been a prevailing research trend in various academic fields. However, no studies have explored the scope and structure of big data across disciplines. In this article, we applied topic modeling and word co-occurrence analysis methods to identify key topics from more than 36,000 big data publications across all academic disciplines between 2012 and 2017. The results revealed several topics associated with the storage, collection and analysis of large datasets; the publications were predominantly published in computational fields. Other identified research topics show the influence of big data methods and techniques in areas beyond computer science, such as education, urban informatics, business, health and medical sciences. In fact, the prevalence of these topics has increased over time. In contrast, some themes like parallel computing, network modeling and big data analytic techniques have lost their popularity in recent years. These results probably reflect the maturity of big data core topics and highlight flourishing new research trends pertinent to big data in new domains, especially in social sciences, health and medicine. Findings of this article can be beneficial for researchers and science policymakers to understand the scope and structure of big data in different academic disciplines."
From words to connections: Word use similarity as an honest signal conducive to employeesâ digital communication,"Bringing together considerations from three research trends (honest signals of collaboration, socio-semantic networks and homophily theory), we hypothesise that word use similarity and having similar social network positions are linked with the level of employeesâ digital interaction. To verify our hypothesis, we analyse the communication of close to 1600 employees, interacting on the intranet communication forum of a large company. We study their social dynamics and the âhonest signalsâ that, in past research, proved to be conducive to employeesâ engagement and collaboration. We find that word use similarity is the main driver of interaction, much more than other language characteristics or similarity in network position. Our results suggest carefully choosing the language according to the target audience and have practical implications for both company managers and online community administrators. Understanding how to better use language could, for example, support the development of knowledge sharing practices or internal communication campaigns. Â© The Author(s) 2020.","Bringing together considerations from three research trends (honest signals of collaboration, socio-semantic networks and homophily theory), we hypothesise that word use similarity and having similar social network positions are linked with the level of employees digital interaction. To verify our hypothesis, we analyse the communication of close to 1600 employees, interacting on the intranet communication forum of a large company. We study their social dynamics and the honest signals that, in past research, proved to be conducive to employees engagement and collaboration. We find that word use similarity is the main driver of interaction, much more than other language characteristics or similarity in network position. Our results suggest carefully choosing the language according to the target audience and have practical implications for both company managers and online community administrators. Understanding how to better use language could, for example, support the development of knowledge sharing practices or internal communication campaigns."
Sentiment analysis of tweets through Altmetrics: A machine learning approach,"The purpose of the study is to (a) contribute to annotating an Altmetrics dataset across five disciplines, (b) undertake sentiment analysis using various machine learning and natural language processingâbased algorithms, (c) identify the best-performing model and (d) provide a Python library for sentiment analysis of an Altmetrics dataset. First, the researchers gave a set of guidelines to two human annotators familiar with the task of related tweet annotation of scientific literature. They duly labelled the sentiments, achieving an inter-annotator agreement (IAA) of 0.80 (Cohenâs Kappa). Then, the same experiments were run on two versions of the dataset: one with tweets in English and the other with tweets in 23 languages, including English. Using 6388 tweets about 300 papers indexed in Web of Science, the effectiveness of employed machine learning and natural language processing models was measured by comparing with well-known sentiment analysis models, that is, SentiStrength and Sentiment140, as the baseline. It was proved that Support Vector Machine with uni-gram outperformed all the other classifiers and baseline methods employed, with an accuracy of over 85%, followed by Logistic Regression at 83% accuracy and NaÃ¯ve Bayes at 80%. The precision, recall and F1 scores for Support Vector Machine, Logistic Regression and NaÃ¯ve Bayes were (0.89, 0.86, 0.86), (0.86, 0.83, 0.80) and (0.85, 0.81, 0.76), respectively. Â© The Author(s) 2020.","The purpose of the study is to (a) contribute to annotating an Altmetrics dataset across five disciplines, (b) undertake sentiment analysis using various machine learning and natural language processingbased algorithms, identify the best-performing model and provide a Python library for sentiment analysis of an Altmetrics dataset. First, the researchers gave a set of guidelines to two human annotators familiar with the task of related tweet annotation of scientific literature. They duly labelled the sentiments, achieving an inter-annotator agreement (IAA) of 0.80 (Cohens Kappa). Then, the same experiments were run on two versions of the dataset: one with tweets in English and the other with tweets in 23 languages, including English. Using 6388 tweets about 300 papers indexed in Web of Science, the effectiveness of employed machine learning and natural language processing models was measured by comparing with well-known sentiment analysis models, that is, SentiStrength and Sentiment140, as the baseline. It was proved that Support Vector Machine with uni-gram outperformed all the other classifiers and baseline methods employed, with an accuracy of over 85%, followed by Logistic Regression at 83% accuracy and Nave Bayes at 80%. The precision, recall and F1 scores for Support Vector Machine, Logistic Regression and Nave Bayes were (0.89, 0.86, 0.86), (0.86, 0.83, 0.80) and (0.85, 0.81, 0.76), respectively."
The impact of semantic annotation techniques on content-based video lecture recommendation,"Increasing videos available in educational content repositories makes searching difficult, and recommendation systems have been used to help students and teachers receive a content of interest. Speech is an important carrier of information in video lectures and is used by content-based video recommendation systems. Although automatic speech recognition (ASR) transcripts have been used in modern video recommendation systems, it is not clear how annotation techniques work with noisy text. This article presents an analysis on a set of semantic annotation techniques when applied to text extracted from video lecture speech and their impact on two tasks: annotation and similarity analysis. Experiments show that topic models have good results in this scenario. Besides, a new benchmark for this task has been created and researchers can use it to evaluate new techniques. Â© The Author(s) 2020.","Increasing videos available in educational content repositories makes searching difficult, and recommendation systems have been used to help students and teachers receive a content of interest. Speech is an important carrier of information in video lectures and is used by content-based video recommendation systems. Although automatic speech recognition (ASR) transcripts have been used in modern video recommendation systems, it is not clear how annotation techniques work with noisy text. This article presents an analysis on a set of semantic annotation techniques when applied to text extracted from video lecture speech and their impact on two tasks: annotation and similarity analysis. Experiments show that topic models have good results in this scenario. Besides, a new benchmark for this task has been created and researchers can use it to evaluate new techniques."
A study of Turkish emotion classification with pretrained language models,"Emotion classification is a research field that aims to detect the emotions in a text using machine learning methods. In traditional machine learning (TML) methods, feature engineering processes cause the loss of some meaningful information, and classification performance is negatively affected. In addition, the success of modelling using deep learning (DL) approaches depends on the sample size. More samples are needed for Turkish due to the unique characteristics of the language. However, emotion classification data sets in Turkish are quite limited. In this study, the pretrained language model approach was used to create a stronger emotion classification model for Turkish. Well-known pretrained language models were fine-tuned for this purpose. The performances of these fine-tuned models for Turkish emotion classification were comprehensively compared with the performances of TML and DL methods in experimental studies. The proposed approach provides state-of-the-art performance for Turkish emotion classification. Â© The Author(s) 2021.","Emotion classification is a research field that aims to detect the emotions in a text using machine learning methods. In traditional machine learning (TML) methods, feature engineering processes cause the loss of some meaningful information, and classification performance is negatively affected. In addition, the success of modelling using deep learning approaches depends on the sample size. More samples are needed for Turkish due to the unique characteristics of the language. However, emotion classification data sets in Turkish are quite limited. In this study, the pretrained language model approach was used to create a stronger emotion classification model for Turkish. Well-known pretrained language models were fine-tuned for this purpose. The performances of these fine-tuned models for Turkish emotion classification were comprehensively compared with the performances of TML and DL methods in experimental studies. The proposed approach provides state-of-the-art performance for Turkish emotion classification."
An exploratory study of the all-author bibliographic coupling analysis: Taking scientometrics for example,"All-author bibliographic coupling analyses (AABCA) take all authors of the article into account when constructing author coupling relationships. Taking scientometrics as an example, this article takes the papers from 2010 to 2019 as data sample and divides them into two periods (limited to 5 years) to discuss the performance of AABCA in discovering potential academic communities and intellectual structure of this discipline. It is found that when all authors of the paper are considered, the relationship between the bibliographic coupling authors presents a certain regularity and the bibliographic coupling is likely to be passed between different pairs of authors. With the transitivity of the coupling relationship, AABCA can effectively identify and discover the potential academic groups of this discipline, and more fully reflect the degree of cooperation among authors. AABCA is an effective method to reveal the intellectual structure in the field of scientometrics, and it is easier to find some small research topics with weak correlation. In addition, AABCA is also an ideal way to explore the authorâs research interests over time. Â© The Author(s) 2020.","All-author bibliographic coupling analyses (AABCA) take all authors of the article into account when constructing author coupling relationships. Taking scientometrics as an example, this article takes the papers from 2010 to 2019 as data sample and divides them into two periods (limited to 5 years) to discuss the performance of AABCA in discovering potential academic communities and intellectual structure of this discipline. It is found that when all authors of the paper are considered, the relationship between the bibliographic coupling authors presents a certain regularity and the bibliographic coupling is likely to be passed between different pairs of authors. With the transitivity of the coupling relationship, AABCA can effectively identify and discover the potential academic groups of this discipline, and more fully reflect the degree of cooperation among authors. AABCA is an effective method to reveal the intellectual structure in the field of scientometrics, and it is easier to find some small research topics with weak correlation. In addition, AABCA is also an ideal way to explore the authors research interests over time."
Negotiating change: Transition as a central concept for information literacy,"Transition forms a dynamic concept that has been underexplored within information literacy research and practice. This article uses the grounded theory of mitigating risk, which was produced through doctoral research into the information literacy practices of language-learners, as a lens for a more detailed examination of transition and its role within information literacy. This framing demonstrates that information literacy mediates transition through supporting preparation, connection, situatedness and confidence within a new setting and facilitating a shift in identity. This article concludes by discussing the important role that time and temporality, resistance and reflexivity play within transition as well as outlining implications for information literacy instruction and future research into time, affect and materiality. Â© The Author(s) 2020.","Transition forms a dynamic concept that has been underexplored within information literacy research and practice. This article uses the grounded theory of mitigating risk, which was produced through doctoral research into the information literacy practices of language-learners, as a lens for a more detailed examination of transition and its role within information literacy. This framing demonstrates that information literacy mediates transition through supporting preparation, connection, situatedness and confidence within a new setting and facilitating a shift in identity. This article concludes by discussing the important role that time and temporality, resistance and reflexivity play within transition as well as outlining implications for information literacy instruction and future research into time, affect and materiality."
Deep Persian sentiment analysis: Cross-lingual training for low-resource languages,"With the advent of deep neural models in natural language processing tasks, having a large amount of training data plays an essential role in achieving accurate models. Creating valid training data, however, is a challenging issue in many low-resource languages. This problem results in a significant difference between the accuracy of available natural language processing tools for low-resource languages compared with rich languages. To address this problem in the sentiment analysis task in the Persian language, we propose a cross-lingual deep learning framework to benefit from available training data of English. We deployed cross-lingual embedding to model sentiment analysis as a transfer learning model which transfers a model from a rich-resource language to low-resource ones. Our model is flexible to use any cross-lingual word embedding model and any deep architecture for text classification. Our experiments on English Amazon dataset and Persian Digikala dataset using two different embedding models and four different classification networks show the superiority of the proposed model compared with the state-of-the-art monolingual techniques. Based on our experiment, the performance of Persian sentiment analysis improves 22% in static embedding and 9% in dynamic embedding. Our proposed model is general and language-independent; that is, it can be used for any low-resource language, once a cross-lingual embedding is available for the sourceâtarget language pair. Moreover, by benefitting from word-aligned cross-lingual embedding, the only required data for a reliable cross-lingual embedding is a bilingual dictionary that is available between almost all languages and the English language, as a potential source language. Â© The Author(s) 2020.","With the advent of deep neural models in natural language processing tasks, having a large amount of training data plays an essential role in achieving accurate models. Creating valid training data, however, is a challenging issue in many low-resource languages. This problem results in a significant difference between the accuracy of available natural language processing tools for low-resource languages compared with rich languages. To address this problem in the sentiment analysis task in the Persian language, we propose a cross-lingual deep learning framework to benefit from available training data of English. We deployed cross-lingual embedding to model sentiment analysis as a transfer learning model which transfers a model from a rich-resource language to low-resource ones. Our model is flexible to use any cross-lingual word embedding model and any deep architecture for text classification. Our experiments on English Amazon dataset and Persian Digikala dataset using two different embedding models and four different classification networks show the superiority of the proposed model compared with the state-of-the-art monolingual techniques. Based on our experiment, the performance of Persian sentiment analysis improves 22% in static embedding and 9% in dynamic embedding. Our proposed model is general and language-independent; that is, it can be used for any low-resource language, once a cross-lingual embedding is available for the sourcetarget language pair. Moreover, by benefitting from word-aligned cross-lingual embedding, the only required data for a reliable cross-lingual embedding is a bilingual dictionary that is available between almost all languages and the English language, as a potential source language."
Performance-based evaluation of academic libraries in the big data era,"The concept of big data has been extensively considered as a technological modernisation in organisations and educational institutes. Thus, the purpose of this study is to determine whether the modified technology acceptance model (MTAM) is viable for evaluating the performance of librarians in the use of big data analytics in academic libraries. This study used an empirical research method for collecting data from 211 librarians working in Pakistanâs universities. On the basis of the findings of the MTAM analysis by structural equation modelling, the performances of the academic libraries were comprehended through the process of big data. The main influential components of the performance analysis in this study were the big data analytics capabilities, perceived ease of access and the usefulness of big data practices in academic libraries. Subsequently, the utilisation of big data was significantly affected by skills, perceived ease of access and the usefulness of academic libraries. The results also suggested that the various components of the academic libraries lead to effective organisational performance when linked to big data analytics. Â© The Author(s) 2020.","The concept of big data has been extensively considered as a technological modernisation in organisations and educational institutes. Thus, the purpose of this study is to determine whether the modified technology acceptance model (MTAM) is viable for evaluating the performance of librarians in the use of big data analytics in academic libraries. This study used an empirical research method for collecting data from 211 librarians working in Pakistans universities. On the basis of the findings of the MTAM analysis by structural equation modelling, the performances of the academic libraries were comprehended through the process of big data. The main influential components of the performance analysis in this study were the big data analytics capabilities, perceived ease of access and the usefulness of big data practices in academic libraries. Subsequently, the utilisation of big data was significantly affected by skills, perceived ease of access and the usefulness of academic libraries. The results also suggested that the various components of the academic libraries lead to effective organisational performance when linked to big data analytics."
Topic attention encoder: A self-supervised approach for short text clustering,"Short text clustering is a challenging and important task in many practical applications. However, many Bag-of-Wordâbased methods for short text clustering are often limited by the sparsity of text representation, while many sentence embeddingâbased methods fail to capture the document structure dependencies within a text corpus. In considerations of the shortcomings of many existing studies, a topic attention encoder (TAE) is proposed in this study. Given topics derived from corpus by the techniques of topic modelling, the cross-document information is introduced. This encoder assumes the document-topic vector to be the learning target and the concatenating vectors of the word embedding and corresponding topic-word vector to be the input. Also, a self-attention mechanism is employed in the encoder, which aims to extract weights of hidden states adaptively and encode the semantics of each short text document. With captured global dependencies and local semantics, TAE integrates the superiority of Bag-of-Word methods and sentence embedding methods. Finally, categories of benchmarking experiments were conducted by analysing three public data sets. It demonstrates that the proposed TAE outperforms many document representation benchmark methods for short text clustering. Â© The Author(s) 2020.","Short text clustering is a challenging and important task in many practical applications. However, many Bag-of-Wordbased methods for short text clustering are often limited by the sparsity of text representation, while many sentence embeddingbased methods fail to capture the document structure dependencies within a text corpus. In considerations of the shortcomings of many existing studies, a topic attention encoder (TAE) is proposed in this study. Given topics derived from corpus by the techniques of topic modelling, the cross-document information is introduced. This encoder assumes the document-topic vector to be the learning target and the concatenating vectors of the word embedding and corresponding topic-word vector to be the input. Also, a self-attention mechanism is employed in the encoder, which aims to extract weights of hidden states adaptively and encode the semantics of each short text document. With captured global dependencies and local semantics, TAE integrates the superiority of Bag-of-Word methods and sentence embedding methods. Finally, categories of benchmarking experiments were conducted by analysing three public data sets. It demonstrates that the proposed TAE outperforms many document representation benchmark methods for short text clustering."
Influence and performance of user similarity metrics in followee prediction,"Followee recommendation is a problem rapidly gaining importance in Twitter as well as in other micro-blogging communities. Hence, understanding how users select whom to follow becomes crucial for designing accurate and personalised recommendation strategies. This work aims at shedding some light on how homophily drives the formation of user relationships by studying the influence of diverse recommendation factors on tie formation. The selected recommendation factors were studied considering multiple alternatives for assessing them in terms of user similarity. A data analysis comparing the similarity among Twitter users and their followees, regarding two commonly used followee recommendation factors (topology and content) was performed in the context of a followee recommendation task. This study is among the firsts to analyse the effect of the different criteria for followee recommendation in micro-blogging communities, and the importance of thoroughly analysing the different aspects of user relationships to define the concept of user similarity. The study showed how the choice of the different factors and assessment alternatives affects followee recommendation. It also verified the existence of certain patterns regarding friends and random usersâ similarities, which can condition the adequacy of the available similarity metrics. Â© The Author(s) 2020.","Followee recommendation is a problem rapidly gaining importance in Twitter as well as in other micro-blogging communities. Hence, understanding how users select whom to follow becomes crucial for designing accurate and personalised recommendation strategies. This work aims at shedding some light on how homophily drives the formation of user relationships by studying the influence of diverse recommendation factors on tie formation. The selected recommendation factors were studied considering multiple alternatives for assessing them in terms of user similarity. A data analysis comparing the similarity among Twitter users and their followees, regarding two commonly used followee recommendation factors (topology and content) was performed in the context of a followee recommendation task. This study is among the firsts to analyse the effect of the different criteria for followee recommendation in micro-blogging communities, and the importance of thoroughly analysing the different aspects of user relationships to define the concept of user similarity. The study showed how the choice of the different factors and assessment alternatives affects followee recommendation. It also verified the existence of certain patterns regarding friends and random users similarities, which can condition the adequacy of the available similarity metrics."
Intelligent detection of hate speech in Arabic social network: A machine learning approach,"Nowadays, cyber hate speech is increasingly growing, which forms a serious problem worldwide by threatening the cohesion of civil societies. Hate speech relates to using expressions or phrases that are violent, offensive or insulting for a person or a minority of people. In particular, in the Arab region, the number of Arab social media users is growing rapidly, which is accompanied with high increasing rate of cyber hate speech. This drew our attention to aspire healthy online environments that are free of hatred and discrimination. Therefore, this article aims to detect cyber hate speech based on Arabic context over Twitter platform, by applying Natural Language Processing (NLP) techniques, and machine learning methods. The article considers a set of tweets related to racism, journalism, sports orientation, terrorism and Islam. Several types of features and emotions are extracted and arranged in 15 different combinations of data. The processed dataset is experimented using Support Vector Machine (SVM), Naive Bayes (NB), Decision Tree (DT) and Random Forest (RF), in which RF with the feature set of Term Frequency-Inverse Document Frequency (TF-IDF) and profile-related features achieves the best results. Furthermore, a feature importance analysis is conducted based on RF classifier in order to quantify the predictive ability of features in regard to the hate class. Â© The Author(s) 2020.","Nowadays, cyber hate speech is increasingly growing, which forms a serious problem worldwide by threatening the cohesion of civil societies. Hate speech relates to using expressions or phrases that are violent, offensive or insulting for a person or a minority of people. In particular, in the Arab region, the number of Arab social media users is growing rapidly, which is accompanied with high increasing rate of cyber hate speech. This drew our attention to aspire healthy online environments that are free of hatred and discrimination. Therefore, this article aims to detect cyber hate speech based on Arabic context over Twitter platform, by applying Natural Language Processing (NLP) techniques, and machine learning methods. The article considers a set of tweets related to racism, journalism, sports orientation, terrorism and Islam. Several types of features and emotions are extracted and arranged in 15 different combinations of data. The processed dataset is experimented using Support Vector Machine (SVM), Naive Bayes (NB), Decision Tree (DT) and Random Forest (RF), in which RF with the feature set of Term Frequency-Inverse Document Frequency (TF-IDF) and profile-related features achieves the best results. Furthermore, a feature importance analysis is conducted based on RF classifier in order to quantify the predictive ability of features in regard to the hate class."
DeepLink: A novel link prediction framework based on deep learning,"Recently, link prediction has attracted more attention from various disciplines such as computer science, bioinformatics and economics. In link prediction, numerous information such as network topology, profile information and user-generated contents are considered to discover missing links between nodes. Whereas numerous previous researches had focused on the structural features of the networks for link prediction, recent studies have shown more interest in profile and content information, too. So, some of these researches combine structural and content information. However, some issues such as scalability and feature engineering need to be investigated to solve a few remaining problems. Moreover, most of the previous researches are presented only for undirected and unweighted networks. In this article, a novel link prediction framework named âDeepLinkâ is presented, which is based on deep learning techniques. While deep learning has the advantage of extracting automatically the best features for link prediction, many other link prediction algorithms need manual feature engineering. Moreover, in the proposed framework, both structural and content information are employed. The framework is capable of using different structural feature vectors that are prepared by various link prediction methods. It learns all proximity orders that are presented on a network during the structural feature learning. We have evaluated the effectiveness of DeepLink on two real social network datasets, Telegram and irBlogs. On both datasets, the proposed framework outperforms several other structural and hybrid approaches for link prediction. Â© The Author(s) 2019.","Recently, link prediction has attracted more attention from various disciplines such as computer science, bioinformatics and economics. In link prediction, numerous information such as network topology, profile information and user-generated contents are considered to discover missing links between nodes. Whereas numerous previous researches had focused on the structural features of the networks for link prediction, recent studies have shown more interest in profile and content information, too. So, some of these researches combine structural and content information. However, some issues such as scalability and feature engineering need to be investigated to solve a few remaining problems. Moreover, most of the previous researches are presented only for undirected and unweighted networks. In this article, a novel link prediction framework named DeepLink is presented, which is based on deep learning techniques. While deep learning has the advantage of extracting automatically the best features for link prediction, many other link prediction algorithms need manual feature engineering. Moreover, in the proposed framework, both structural and content information are employed. The framework is capable of using different structural feature vectors that are prepared by various link prediction methods. It learns all proximity orders that are presented on a network during the structural feature learning. We have evaluated the effectiveness of DeepLink on two real social network datasets, Telegram and irBlogs. On both datasets, the proposed framework outperforms several other structural and hybrid approaches for link prediction."
What makes a tweet be retweeted? A Bayesian trigram analysis of tweet propagation during the 2015 Colombian political campaign,"This article proposes the use of computationally efficient inverse regression Bayesian method for analysis of tweet propagation of political messages. Our example focuses on the Colombian case, though our method can be used in any election where social media messaging has a direct impact on political outcomes. We find strong evidence that politicians were able to identify the combination of sensitive words to enhance the probability of retweet of the message, which, in turn, had an impact on political outcomes. The contributions of our work entail: (a) an examination of a neglected unit of analysis (trigram) in a language less studied (i.e. Spanish), (b) based on an innovative Bayesian efficient approach and (c) exploiting the predictive power that retweets have on electoral results as an informational diffusion tool in social media. A practical implication of this new methodology is the possibility to adjust political messages as a means to increase voters engagement in political campaigns. Â© The Author(s) 2019.","This article proposes the use of computationally efficient inverse regression Bayesian method for analysis of tweet propagation of political messages. Our example focuses on the Colombian case, though our method can be used in any election where social media messaging has a direct impact on political outcomes. We find strong evidence that politicians were able to identify the combination of sensitive words to enhance the probability of retweet of the message, which, in turn, had an impact on political outcomes. The contributions of our work entail: (a) an examination of a neglected unit of analysis (trigram) in a language less studied ( Spanish), (b) based on an innovative Bayesian efficient approach and exploiting the predictive power that retweets have on electoral results as an informational diffusion tool in social media. A practical implication of this new methodology is the possibility to adjust political messages as a means to increase voters engagement in political campaigns."
Using microdata for international e-Government data exchange: The case of social security domain,"Semantic interoperability issues of international e-Government data exchanges have not been solved up until now. In the case of social security institutions, the data exchange operations have some particularities that make that the non-ambiguous definition of core concepts used in the institutions has a key impact on the success and quality of system interconnections. In this article, we present the result of a research to implement a new metadata specification based in Dublin Core elements for international social security exchanges, named Exchange Social Security Information Metadata (ESSIM). This proposal is based in a semantic approach using Linked Data for Interoperability, with technologies, such as RDF(S), SPARQL, Microdata and JSON-LD, in order to ensure interoperability between social security institutions from different countries. This will help to strengthen the protection of the social security rights of mobile workers by automating the application of international agreements on social security and to improve cross border communication between social security institutions of different countries. For the near future, the goal is to include this specification as part of information and communication technology Guidelines under development by International Social Security Association with the participation of authors of this article. This will facilitate a future adoption of the specification as an international standard. Â© The Author(s) 2019.","Semantic interoperability issues of international e-Government data exchanges have not been solved up until now. In the case of social security institutions, the data exchange operations have some particularities that make that the non-ambiguous definition of core concepts used in the institutions has a key impact on the success and quality of system interconnections. In this article, we present the result of a research to implement a new metadata specification based in Dublin Core elements for international social security exchanges, named Exchange Social Security Information Metadata (ESSIM). This proposal is based in a semantic approach using Linked Data for Interoperability, with technologies, such as RDF(S), SPARQL, Microdata and JSON-LD, in order to ensure interoperability between social security institutions from different countries. This will help to strengthen the protection of the social security rights of mobile workers by automating the application of international agreements on social security and to improve cross border communication between social security institutions of different countries. For the near future, the goal is to include this specification as part of information and communication technology Guidelines under development by International Social Security Association with the participation of authors of this article. This will facilitate a future adoption of the specification as an international standard."
Does the mobility of scientists disrupt their collaboration stability?,"To explore to what extent the mobility of scientists disrupts the stability of their research collaboration, we designed a measure â Collaboration Stability After Moving (CSAM) â for scientists, retrieved 4343 US-related scientistsâ curricula vitae (CVs) from the Open Researcher and Contributor ID (ORCID) website and publication records in the Web of Science database and applied a linear regression model to the dataset. Our findings include the following: (1) the more times a scientist moved, the more she or he is inclined to co-author with previous collaborators, (2) cross-country mobility disrupts the stability of research collaboration more than domestic mobility and (3) the stability of research collaboration correlates with scientistsâ cultural background, cross-country work experience and research areas. Â© The Author(s) 2020.","To explore to what extent the mobility of scientists disrupts the stability of their research collaboration, we designed a measure Collaboration Stability After Moving (CSAM) for scientists, retrieved 4343 US-related scientists curricula vitae (CVs) from the Open Researcher and Contributor ID (ORCID) website and publication records in the Web of Science database and applied a linear regression model to the dataset. Our findings include the following: the more times a scientist moved, the more she or he is inclined to co-author with previous collaborators, cross-country mobility disrupts the stability of research collaboration more than domestic mobility and the stability of research collaboration correlates with scientists cultural background, cross-country work experience and research areas."
Topic modelling and social network analysis of publications and patents in humanoid robot technology,This article presents analysis of data from scientific articles and patents to identify the evolving trends and underlying topics in research on humanoid robots. We used topic modelling based on latent Dirichlet allocation analysis to identify underlying topics in sub-areas in the field. We also used social network analysis to measure the centrality indices of publication keywords to detect important and influential sub-areas and used co-occurrence analysis of keywords to visualise relationships among subfields. The research result is useful to identify evolving topics and areas of current focus in the field of humanoid technology. The results contribute to identify valuable research patterns from publications and to increase understanding of the hidden knowledge themes that are revealed by patents. Â© The Author(s) 2019.,This article presents analysis of data from scientific articles and patents to identify the evolving trends and underlying topics in research on humanoid robots. We used topic modelling based on latent Dirichlet allocation analysis to identify underlying topics in sub-areas in the field. We also used social network analysis to measure the centrality indices of publication keywords to detect important and influential sub-areas and used co-occurrence analysis of keywords to visualise relationships among subfields. The research result is useful to identify evolving topics and areas of current focus in the field of humanoid technology. The results contribute to identify valuable research patterns from publications and to increase understanding of the hidden knowledge themes that are revealed by patents.
How do academia and society react to erroneous or deceitful claims? The case of retracted articlesâ recognition,"Researchers give credit to peer-reviewed, and thus, credible publications through citations. Despite a rigorous reviewing process, certain articles undergo retraction due to disclosure of their ethical or scientific deficiencies. It is, therefore, important to understand how society and academia react to the erroneous or deceitful claims and purge the science of their unreliable results. Applying a matched-pairs research design, this study examined a sample of medicine-related retracted and non-retracted articles matched by their content similarity. The regression analysis revealed similarities in obsolescence trends of the retracted and non-retracted groups. The Generalized Estimating Equations showed that citations are affected by the retraction status, life after retraction, life cycle and the journalsâ previous reputation, with the two formers being the strongest in positively predicting the citations. The retracted papers obtain fewer citations either before or after retraction, implying academiaâs watchful reaction to the low-quality papers even before official announcement of their fallibility. They exhibit an equal or higher social recognition level regarding Tweets and Blog Mentions, while a lower status regarding Mendeley Readership. This could signify social usersâ sensibility regarding scientific quality since they probably publicise the retraction and warn against the retracted items in their tweets or blogs, while avoiding recording them in their Mendeley profiles. Further scrutiny is required to gain insight into the sensibility, if any, about scientific quality. The studyâs originality relies on matching the retracted and non-retracted papers with their topics and neutralising variations in their citation potentials. It is also the first study comparing the groupsâ social impacts. Â© The Author(s) 2020.","Researchers give credit to peer-reviewed, and thus, credible publications through citations. Despite a rigorous reviewing process, certain articles undergo retraction due to disclosure of their ethical or scientific deficiencies. It is, therefore, important to understand how society and academia react to the erroneous or deceitful claims and purge the science of their unreliable results. Applying a matched-pairs research design, this study examined a sample of medicine-related retracted and non-retracted articles matched by their content similarity. The regression analysis revealed similarities in obsolescence trends of the retracted and non-retracted groups. The Generalized Estimating Equations showed that citations are affected by the retraction status, life after retraction, life cycle and the journals previous reputation, with the two formers being the strongest in positively predicting the citations. The retracted papers obtain fewer citations either before or after retraction, implying academias watchful reaction to the low-quality papers even before official announcement of their fallibility. They exhibit an equal or higher social recognition level regarding Tweets and Blog Mentions, while a lower status regarding Mendeley Readership. This could signify social users sensibility regarding scientific quality since they probably publicise the retraction and warn against the retracted items in their tweets or blogs, while avoiding recording them in their Mendeley profiles. Further scrutiny is required to gain insight into the sensibility, if any, about scientific quality. The studys originality relies on matching the retracted and non-retracted papers with their topics and neutralising variations in their citation potentials. It is also the first study comparing the groups social impacts."
Automatic knowledge exchange between ontologies and semantic graphs,"This article presents an innovative knowledge management approach for the validation and transfer of knowledge between semantic networks or ontologies and a target ontology represented in web ontology language (OWL) format. This process has been designed for addressing the quality improvement of ontologies automatically created by learning techniques. The knowledge transfer process is a semi-automatic computer aided method to assist the domain expert to improve the target ontology. To validate our proposal, we have used an automatically generated target ontology. We used knowledge transfer from the well-known Babelnet semantic graph and a manually generated ontology to improve the quality of the target ontology. Finally, to show the suitability of our proposal in the ontology-fixing process, we compare the improved target ontology resulting from the application of the proposed validation and knowledge transfer techniques with its original version. We developed an example of our proposal in our OntologyFixer tool, which is available on a GitHub repository (https://github.com/gabyluna/OntologyFixer/tree/ontofixer_v2). Â© The Author(s) 2022.","This article presents an innovative knowledge management approach for the validation and transfer of knowledge between semantic networks or ontologies and a target ontology represented in web ontology language (OWL) format. This process has been designed for addressing the quality improvement of ontologies automatically created by learning techniques. The knowledge transfer process is a semi-automatic computer aided method to assist the domain expert to improve the target ontology. To validate our proposal, we have used an automatically generated target ontology. We used knowledge transfer from the well-known Babelnet semantic graph and a manually generated ontology to improve the quality of the target ontology. Finally, to show the suitability of our proposal in the ontology-fixing process, we compare the improved target ontology resulting from the application of the proposed validation and knowledge transfer techniques with its original version. We developed an example of our proposal in our OntologyFixer tool, which is available on a GitHub repository (https://github.com/gabyluna/OntologyFixer/tree/ontofixer_v2)."
Understanding the evolution of a scientific field by clustering and visualizing knowledge graphs,"The process of tracking the evolution of a scientific field is arduous. It allows researchers to understand trends in areas of science and predict how they may evolve. Nowadays, most of the automated mechanisms developed to assist researchers in this process do not consider the content of articles to identify changes in its structure, only the articles metadata. These methods are not suited to easily assist researchers to study the concepts that compose an area and its evolution. In this article, we propose a method to track the evolution of a scientific field at a concept level. Our method structures a scientific field using two knowledge graphs, representing distinct periods of the studied field. Then, it clusters them and identifies correspondent clusters between the knowledge graphs, representing the same subareas in distinct time periods. Our solution enables to compare the corresponding clusters, tracking their evolution. We apply and experiment our method in two case studies concerning the artificial intelligence (AI) and the biotechnology (BIO) fields. Findings indicate befitting results regarding the way their evolution can be assessed with our implemented software tool. From our analyses, we perceived evolution in broader subareas of a scientific field, as the growth of the âConvolutional Neural Networkâ area from 2006; to specific ones, as the decrease of research works using mice to study BRAF-mutation lung cancer from 2018. This work contributes with the development of a web application with interactive user interfaces to assist researchers in representing, analysing and tracking the evolution of scientific fields at a concept level. Â© The Author(s) 2020.","The process of tracking the evolution of a scientific field is arduous. It allows researchers to understand trends in areas of science and predict how they may evolve. Nowadays, most of the automated mechanisms developed to assist researchers in this process do not consider the content of articles to identify changes in its structure, only the articles metadata. These methods are not suited to easily assist researchers to study the concepts that compose an area and its evolution. In this article, we propose a method to track the evolution of a scientific field at a concept level. Our method structures a scientific field using two knowledge graphs, representing distinct periods of the studied field. Then, it clusters them and identifies correspondent clusters between the knowledge graphs, representing the same subareas in distinct time periods. Our solution enables to compare the corresponding clusters, tracking their evolution. We apply and experiment our method in two case studies concerning the artificial intelligence (AI) and the biotechnology (BIO) fields. Findings indicate befitting results regarding the way their evolution can be assessed with our implemented software tool. From our analyses, we perceived evolution in broader subareas of a scientific field, as the growth of the Convolutional Neural Network area from 2006; to specific ones, as the decrease of research works using mice to study BRAF-mutation lung cancer from 2018. This work contributes with the development of a web application with interactive user interfaces to assist researchers in representing, analysing and tracking the evolution of scientific fields at a concept level."
Using community information for natural disaster alerts,"Recently, the ceaseless rise in the global average temperature has led to extreme climates in which natural disasters, such as droughts, hurricanes, earthquakes and floods, are becoming increasingly serious. Recent research has found that social media typically reflects disasters earlier than official communication channels. In this study, the idea of collecting information on flood disasters caused during the periods of typhoons and heavy rains for a city from the plain text messages released by social media by means of a term frequency (TF) and sliding window approach is proposed. The dataset analysed here contains a total of 292 articles and 12,484 tweets. This research determines how to establish a warning mechanism, with an added notification time for flooding disasters, and it shows how to provide relevant disaster relief personnel with references. This article contributes by combining social media data with emergency management information cloud (EMIC) data, especially in the context of having a mechanism for warning about flooding disasters. According to the experimental results, a sliding window of 90 min and a sliding gap of 10 min obtained the best F-measure value (F = 0.315). The event studied was Typhoon Megi (September 2016), which caused major flooding in Tainan. For the Typhoon Megi event, the flood disaster location database had 161 streets available for matching. Based on the experimental results, it is possible to obtain a high-precision (90% or higher) accuracy rate from real-time tweet data by exploiting a social media dataset. Â© The Author(s) 2020.","Recently, the ceaseless rise in the global average temperature has led to extreme climates in which natural disasters, such as droughts, hurricanes, earthquakes and floods, are becoming increasingly serious. Recent research has found that social media typically reflects disasters earlier than official communication channels. In this study, the idea of collecting information on flood disasters caused during the periods of typhoons and heavy rains for a city from the plain text messages released by social media by means of a term frequency (TF) and sliding window approach is proposed. The dataset analysed here contains a total of 292 articles and 12,484 tweets. This research determines how to establish a warning mechanism, with an added notification time for flooding disasters, and it shows how to provide relevant disaster relief personnel with references. This article contributes by combining social media data with emergency management information cloud (EMIC) data, especially in the context of having a mechanism for warning about flooding disasters. According to the experimental results, a sliding window of 90 min and a sliding gap of 10 min obtained the best F-measure value (F = 0.315). The event studied was Typhoon Megi (September 2016), which caused major flooding in Tainan. For the Typhoon Megi event, the flood disaster location database had 161 streets available for matching. Based on the experimental results, it is possible to obtain a high-precision (90% or higher) accuracy rate from real-time tweet data by exploiting a social media dataset."
Can we use link-based indicators to find highly cited publications? The case of the Trust Flow score,"The Majesticâs Trust Flow (TF) is a link-based score aimed at measuring the influence of online objects (e.g. scientific publications) by considering the weighted number of links received from trusted websites. This study describes the bibliographic characteristics and impact of those publications with the highest TF score. In order to do this, 20,810 URL-based Digital Object Identifiers (DOIs) were identified and analysed. The results show that these DOIs mainly represent recent publications (57.1% of publications were published between 2010 and 2020), journal articles (93.75%) published in the first SCImago Journal Rank (SJR) quartile (81.7%), written with international collaboration (40.4%) and biased towards the field of medicine (36.9%). While the TF score is a discovering tool with the potential to be used in webometric studies to find influential publications, a few technical limitations jeopardise the general applicability of this indicator for research evaluation at the publication level. Â© The Author(s) 2022.","The Majestics Trust Flow (TF) is a link-based score aimed at measuring the influence of online objects ( scientific publications) by considering the weighted number of links received from trusted websites. This study describes the bibliographic characteristics and impact of those publications with the highest TF score. In order to do this, 20,810 URL-based Digital Object Identifiers (DOIs) were identified and analysed. The results show that these DOIs mainly represent recent publications (57.1% of publications were published between 2010 and 2020), journal articles (93.75%) published in the first SCImago Journal Rank (SJR) quartile (81.7%), written with international collaboration (40.4%) and biased towards the field of medicine (36.9%). While the TF score is a discovering tool with the potential to be used in webometric studies to find influential publications, a few technical limitations jeopardise the general applicability of this indicator for research evaluation at the publication level."
A content-based technique for linking dual language news articles in an archive,"To retrieve a specific news article from a vast archive containing multilingual news articles against a user query or based on similarity among news articles is a challenging task. The task becomes even further complicated when the archive contains articles from a low resourced and morphologically complex language like Urdu, along with English new articles. The article proposes a content-based (lexical) similarity measure, that is, Common Ratio Measure for Dual Language (CRMDL), for linking digital news articles published in various online news sources. The similarity measure links Urdu-to-English news articles during the preservation process using an Urdu-to-English lexicon. A literature review showed that an Urdu-to-English lexicon did not exist, and therefore, the first task was to build a lexicon from multiple sources. The proposed similarity measure, that is, CRMDL, is evaluated rigorously on different data sets, of varying sizes, to assess the effectiveness. The experimental results show that the proposed measure is feasible and effective for similarity computation between Urdu and English news articles, which can obtain, on average, 50% precision and 67% recall. The performance can be improved sufficiently by managing the limitations summarised in the study. Â© The Author(s) 2020.","To retrieve a specific news article from a vast archive containing multilingual news articles against a user query or based on similarity among news articles is a challenging task. The task becomes even further complicated when the archive contains articles from a low resourced and morphologically complex language like Urdu, along with English new articles. The article proposes a content-based (lexical) similarity measure, that is, Common Ratio Measure for Dual Language (CRMDL), for linking digital news articles published in various online news sources. The similarity measure links Urdu-to-English news articles during the preservation process using an Urdu-to-English lexicon. A literature review showed that an Urdu-to-English lexicon did not exist, and therefore, the first task was to build a lexicon from multiple sources. The proposed similarity measure, that is, CRMDL, is evaluated rigorously on different data sets, of varying sizes, to assess the effectiveness. The experimental results show that the proposed measure is feasible and effective for similarity computation between Urdu and English news articles, which can obtain, on average, 50% precision and 67% recall. The performance can be improved sufficiently by managing the limitations summarised in the study."
Text and metadata extraction from scanned Arabic documents using support vector machines,"Text information in scanned documents becomes accessible only when extracted and interpreted by a text recognizer. For a recognizer to work successfully, it must have detailed location information about the regions of the document images that it is asked to analyse. It will need focus on page regions with text skipping non-text regions that include illustrations or photographs. However, text recognizers do not work as logical analyzers. Logical layout analysis automatically determines the function of a document text region, that is, it labels each region as a title, paragraph, or caption, and so on, and thus is an essential part of a document understanding system. In the past, rule-based algorithms have been used to conduct logical layout analysis, using limited size data sets. We here instead focus on supervised learning methods for logical layout analysis. We describe LABA, a system based on multiple support vector machines to perform logical Layout Analysis of scanned Books pages in Arabic. The system detects the function of a text region based on the analysis of various images features and a voting mechanism. For a baseline comparison, we implemented an older but state-of-the-art neural network method. We evaluated LABA using a data set of scanned pages from illustrated Arabic books and obtained high recall and precision values. We also found that the F-measure of LABA is higher for five of the tested six classes compared to the state-of-the-art method. Â© The Author(s) 2020.","Text information in scanned documents becomes accessible only when extracted and interpreted by a text recognizer. For a recognizer to work successfully, it must have detailed location information about the regions of the document images that it is asked to analyse. It will need focus on page regions with text skipping non-text regions that include illustrations or photographs. However, text recognizers do not work as logical analyzers. Logical layout analysis automatically determines the function of a document text region, that is, it labels each region as a title, paragraph, or caption, and so on, and thus is an essential part of a document understanding system. In the past, rule-based algorithms have been used to conduct logical layout analysis, using limited size data sets. We here instead focus on supervised learning methods for logical layout analysis. We describe LABA, a system based on multiple support vector machines to perform logical Layout Analysis of scanned Books pages in Arabic. The system detects the function of a text region based on the analysis of various images features and a voting mechanism. For a baseline comparison, we implemented an older but state-of-the-art neural network method. We evaluated LABA using a data set of scanned pages from illustrated Arabic books and obtained high recall and precision values. We also found that the F-measure of LABA is higher for five of the tested six classes compared to the state-of-the-art method."
"Modelling usersâ perceptions of video information seeking, learning through added value and use of curated digital collections","Information seeking research has provided models of users in the search for information across many different contexts and situations. Digital content curation has emerged as a means for managing information and facilitating user learning by adding âvalueâ to digital content in different ways, enhancing the user experience. Using digital video and Kâ12 education as the context, this study examined factors representing video information seeking, user learning and use of curated video collections both individually and together as user-centred constructs. Two hundred and fifty-two Kâ12 teachers provided perceptions of their own information seeking processes and for different qualities of curated content and collections within the context of searching digital video for applied purposes. Results extracted underlying factors of these concepts and demonstrated significant relationships between them. Findings enabled the expansion of a model to incorporate both usersâ perceptions of information seeking together with user-centred constructs of learning through added value content and use of curated digital collections. Practical implications of the study help establish baselines for future studies for formulating, incorporating and emphasising added value and video curation qualities based on usersâ information seeking within the process. Â© The Author(s) 2020.","Information seeking research has provided models of users in the search for information across many different contexts and situations. Digital content curation has emerged as a means for managing information and facilitating user learning by adding value to digital content in different ways, enhancing the user experience. Using digital video and K12 education as the context, this study examined factors representing video information seeking, user learning and use of curated video collections both individually and together as user-centred constructs. Two hundred and fifty-two K12 teachers provided perceptions of their own information seeking processes and for different qualities of curated content and collections within the context of searching digital video for applied purposes. Results extracted underlying factors of these concepts and demonstrated significant relationships between them. Findings enabled the expansion of a model to incorporate both users perceptions of information seeking together with user-centred constructs of learning through added value content and use of curated digital collections. Practical implications of the study help establish baselines for future studies for formulating, incorporating and emphasising added value and video curation qualities based on users information seeking within the process."
Quantifying and analysing the stages of online information dissemination in different enterprise emergencies: The idea of system cybernetics,"Previous research on information dissemination in emergencies focus on prediction of the volume via abundant models. However, most of these models did not specify different stages of emergencies, and hence making it difficult for public relations (PR) practitioner to make decisions based on needs of each stage in todayâs rapid changing media environments. In this study, we introduce the idea of system cybernetics and the method of system identification into information dissemination perspective. Based on the proposed information accumulation probability distribution continuity (IAPDC) model, we provide a quantitative division of the information accumulation process. The durations of each stage and the time points that each stage begins are stated and defined with a quantitative calculation method. Using empirical data from 83 emergencies in 2016 and 2017 covering Weibo, WeChat Platforms and over 20,000 web media, we verify the effectiveness of this method. Next, we use simulation analysis to demonstrate what effects of parameters have on the dissemination process and how do changes on different stages affect the process. Moreover, we also demonstrate the effects of emergenciesâ attributes on the information dissemination process and on each stage. Our study complements the gaps in existing communication discipline and provides insight for PR practitioner when dealing with enterprise emergencies. Â© The Author(s) 2020.","Previous research on information dissemination in emergencies focus on prediction of the volume via abundant models. However, most of these models did not specify different stages of emergencies, and hence making it difficult for public relations (PR) practitioner to make decisions based on needs of each stage in todays rapid changing media environments. In this study, we introduce the idea of system cybernetics and the method of system identification into information dissemination perspective. Based on the proposed information accumulation probability distribution continuity (IAPDC) model, we provide a quantitative division of the information accumulation process. The durations of each stage and the time points that each stage begins are stated and defined with a quantitative calculation method. Using empirical data from 83 emergencies in 2016 and 2017 covering Weibo, WeChat Platforms and over 20,000 web media, we verify the effectiveness of this method. Next, we use simulation analysis to demonstrate what effects of parameters have on the dissemination process and how do changes on different stages affect the process. Moreover, we also demonstrate the effects of emergencies attributes on the information dissemination process and on each stage. Our study complements the gaps in existing communication discipline and provides insight for PR practitioner when dealing with enterprise emergencies."
Creativity as a key competence in advertising industry: Knowledge management and creative potential stimulation,"The study purpose is to investigate the development of creativity in the process of knowledge management in the advertising industry. The novelty of the research is in the study of creativity and social intelligence management as key competencies of this industry. The study is motivated by the high need for continuous improvement in the quality of training of employees in the advertising industry. The respondents group for the online study was formed by simple randomisation from representatives of the advertising industry in the Russian Federation (42 people) and China (45 people). The training methodology consisted of several online exercises in the form of situational tasks aimed at stimulating creativity as well as the development of communicative, image qualities and social intelligence. Statistical analysis was performed using STATISTICA 13.3 software (StatSoft Inc.). To compare quantitative data in the group of respondents before and after the training, a paired Studentâs t-test was used to test the null hypothesis about the absence of statistically significant differences in the results of tests of creativity and other significant variables for pre- and post-test after applying the training methodology. The null hypothesis was rejected. The research has shown the possibilities of stimulation of creative potential and development of active thinking, its fluency and flexibility by means of training sessions. Â© The Author(s) 2022.","The study purpose is to investigate the development of creativity in the process of knowledge management in the advertising industry. The novelty of the research is in the study of creativity and social intelligence management as key competencies of this industry. The study is motivated by the high need for continuous improvement in the quality of training of employees in the advertising industry. The respondents group for the online study was formed by simple randomisation from representatives of the advertising industry in the Russian Federation (42 people) and China (45 people). The training methodology consisted of several online exercises in the form of situational tasks aimed at stimulating creativity as well as the development of communicative, image qualities and social intelligence. Statistical analysis was performed using STATISTICA 13.3 software (StatSoft Inc.). To compare quantitative data in the group of respondents before and after the training, a paired Students t-test was used to test the null hypothesis about the absence of statistically significant differences in the results of tests of creativity and other significant variables for pre- and post-test after applying the training methodology. The null hypothesis was rejected. The research has shown the possibilities of stimulation of creative potential and development of active thinking, its fluency and flexibility by means of training sessions."
Mining information from sentences through Semantic Web data and Information Extraction tasks,"The Semantic Web provides guidelines for the representation of information about real-world objects (entities) and their relations (properties). This is helpful for the dissemination and consumption of information by people and applications. However, the information is mainly contained within natural language sentences, which do not have a structure or linguistic descriptions ready to be directly processed by computers. Thus, the challenge is to identify and extract the elements of information that can be represented. Hence, this article presents a strategy to extract information from sentences and its representation with Semantic Web standards. Our strategy involves Information Extraction tasks and a hybrid semantic similarity measure to get entities and relations that are later associated with individuals and properties from a Knowledge Base to create RDF triples (SubjectâPredicateâObject structures). The experiments demonstrate the feasibility of our method and that it outperforms the accuracy provided by a pattern-based method from the literature. Â© The Author(s) 2020.","The Semantic Web provides guidelines for the representation of information about real-world objects (entities) and their relations (properties). This is helpful for the dissemination and consumption of information by people and applications. However, the information is mainly contained within natural language sentences, which do not have a structure or linguistic descriptions ready to be directly processed by computers. Thus, the challenge is to identify and extract the elements of information that can be represented. Hence, this article presents a strategy to extract information from sentences and its representation with Semantic Web standards. Our strategy involves Information Extraction tasks and a hybrid semantic similarity measure to get entities and relations that are later associated with individuals and properties from a Knowledge Base to create RDF triples (SubjectPredicateObject structures). The experiments demonstrate the feasibility of our method and that it outperforms the accuracy provided by a pattern-based method from the literature."
Library services and facilities in higher education institutions during coronavirus disease (COVID-19) in Pakistan,"This article attempts to evaluate the library services and facilities during the pandemic situations in Pakistan. This study is fashioned in positivistic tradition of quantitative research design, and a cross-sectional research method was employed. The objective of the study was to examine the library services available to students and faculty members in two public sector academic libraries in Pakistan. A complete list of students and faculty members was collected from the concerned offices along with their WhatsApp numbers and email address. A total of 1736 students and faculty members sampled through proportionate random sampling technique and filled the questionnaire out of 7835. The inclusion criteria to participate was based on enrolled students of public and private sector university, passed at least one semester of BS (4 years) and MA/MSc (2 years) programme in Pakistan, and faculty members using library resources. A structured questionnaire to measure the response of library patrons and consisted of exogenous and endogenous variables and pretested. The study findings showed that COVID-19 pandemic situations affected educational institutions at a large scale. The social distancing rule was opted to minimise the risk of infection and university libraries were also closed down. However, due to online learning transformation, library materials were digitalized and online library services were provided to students and faculty members. Â© The Author(s) 2022.","This article attempts to evaluate the library services and facilities during the pandemic situations in Pakistan. This study is fashioned in positivistic tradition of quantitative research design, and a cross-sectional research method was employed. The objective of the study was to examine the library services available to students and faculty members in two public sector academic libraries in Pakistan. A complete list of students and faculty members was collected from the concerned offices along with their WhatsApp numbers and email address. A total of 1736 students and faculty members sampled through proportionate random sampling technique and filled the questionnaire out of 7835. The inclusion criteria to participate was based on enrolled students of public and private sector university, passed at least one semester of BS (4 years) and MA/MSc (2 years) programme in Pakistan, and faculty members using library resources. A structured questionnaire to measure the response of library patrons and consisted of exogenous and endogenous variables and pretested. The study findings showed that COVID-19 pandemic situations affected educational institutions at a large scale. The social distancing rule was opted to minimise the risk of infection and university libraries were also closed down. However, due to online learning transformation, library materials were digitalized and online library services were provided to students and faculty members."
Supporting information use and task accomplishment: What system features do users like and expect?,"Information systems have been improving in helping users find information. However, they have been less attended to regarding helping searchers in using located information. This research attempts to address the issue of information use by investigating what information systems and features searchers think are helpful in using located information to accomplish information tasks. In all, 32 college students were invited to an information interaction lab, first being interviewed on a recently completed task and then working on a to-be-finished task, both being their real-life tasks of their own choices. Through questionnaires, the study discovered the most favoured existing and expected features helpful for usersâ task completion. Users expected convenient citations, note taking in search result pages and being kept on task. Findings in this study have implications on designing search systems that can better support task accomplishment, in addition to returning search results. Â© The Author(s) 2020.","Information systems have been improving in helping users find information. However, they have been less attended to regarding helping searchers in using located information. This research attempts to address the issue of information use by investigating what information systems and features searchers think are helpful in using located information to accomplish information tasks. In all, 32 college students were invited to an information interaction lab, first being interviewed on a recently completed task and then working on a to-be-finished task, both being their real-life tasks of their own choices. Through questionnaires, the study discovered the most favoured existing and expected features helpful for users task completion. Users expected convenient citations, note taking in search result pages and being kept on task. Findings in this study have implications on designing search systems that can better support task accomplishment, in addition to returning search results."
Which are the influential publications in the Web of Science subject categories over a long period of time? CRExplorer software used for big-data analyses in bibliometrics*,"What are the landmark papers in scientific disciplines? Which papers are indispensable for scientific progress? These are typical questions which are of interest not only for researchers (who frequently know the answers â or guess to know them) but also for the interested general public. Citation counts can be used to identify very useful papers since they reflect the wisdom of the crowd â in this case, the scientists using published results for their research. In this study, we identified with recently developed methods for the program CRExplorer landmark publications in nearly all Web of Science subject categories (WoS-SCs). These are publications which belong more frequently than other publications during the citing years to the top-1â° in their subject area. As examples, we show the results of five subject categories: âInformation Science & Library Scienceâ, âComputer Science, Information Systemsâ, âComputer Science, Software Engineeringâ, âPsychology, Socialâ and, âChemistry, Physicalâ. The results of the other WoS-SCs can be found online at http://crexplorer.net. An analyst of the results should keep in mind that the identification of landmark papers depends on the used methods and data. Small differences in methods and/or data may lead to other results. Â© The Author(s) 2020.","What are the landmark papers in scientific disciplines? Which papers are indispensable for scientific progress? These are typical questions which are of interest not only for researchers (who frequently know the answers or guess to know them) but also for the interested general public. Citation counts can be used to identify very useful papers since they reflect the wisdom of the crowd in this case, the scientists using published results for their research. In this study, we identified with recently developed methods for the program CRExplorer landmark publications in nearly all Web of Science subject categories (WoS-SCs). These are publications which belong more frequently than other publications during the citing years to the top-1 in their subject area. As examples, we show the results of five subject categories: Information Science & Library Science, Computer Science, Information Systems, Computer Science, Software Engineering, Psychology, Social and, Chemistry, Physical. The results of the other WoS-SCs can be found online at http://crexplorer.net. An analyst of the results should keep in mind that the identification of landmark papers depends on the used methods and data. Small differences in methods and/or data may lead to other results."
Information is non-physical: The rules connecting representation and meaning do not obey the laws of physics,"The proposition that information is physical is widely accepted in the scientific community. Information is composed of physical representation, abstract meaning and rules, which interpret representation to meaning. In this article, I demonstrated that the rules connecting representation with meaning cannot be the laws of physics, because all quantities appearing in the laws of physics are physical, observable and measurable, and the meaning, however, is abstract, unobservable and unmeasurable. For linguistic information, the rules that determine how the ordered symbol sequence of language corresponds to its meaning are language vocabulary and grammar, not the laws of physics. This characteristic of information â its rules that link representation with meaning do not obey the laws of physics â ontologically distinguishes information from physical substances that obey the laws of physics, and inspires me to define entities that do not obey the laws of physics (such as information) as non-physical. Â© The Author(s) 2022.","The proposition that information is physical is widely accepted in the scientific community. Information is composed of physical representation, abstract meaning and rules, which interpret representation to meaning. In this article, I demonstrated that the rules connecting representation with meaning cannot be the laws of physics, because all quantities appearing in the laws of physics are physical, observable and measurable, and the meaning, however, is abstract, unobservable and unmeasurable. For linguistic information, the rules that determine how the ordered symbol sequence of language corresponds to its meaning are language vocabulary and grammar, not the laws of physics. This characteristic of information its rules that link representation with meaning do not obey the laws of physics ontologically distinguishes information from physical substances that obey the laws of physics, and inspires me to define entities that do not obey the laws of physics (such as information) as non-physical."
Mapping the social landscape through social media,"Being a habitat of the global village, every place has established connections through the strength and power of social media, piercing through the political boundaries. Social media is a digital platform, where people across the world can interact. This has a number of advantages of being universal, anonymous, easy accessibility, indirect interaction, gathering and sharing information when compared with direct interaction. The easy access to social networking sites (SNSs) such as Facebook, Twitter and blogs has brought about unprecedented opportunities for citizens to voice their opinions loaded with emotions/sentiments. Furthermore, social media can influence human thoughts. A recent incident of public importance had presented an opportunity to map the sentiments, involved around it. Sentiments were extracted from tweets for a week. These sentiments were classified as positive, negative and neutral and were mapped in geographic information system (GIS) environment. It was found that the number of tweets diminished by 91% over a week from 25 August 2017 to 31 August 2017. Maximum tweets emerged from places near the origin of the case (Haryana, Delhi and Punjab). The trend of sentiments was found to be â neutral (47.4%), negative (30%) and positive (22.6%). Interestingly, tweets were also coming from unexpected places such as United States, United Kingdom and West Asia. The result can also be used to assess the spatial distribution of digital penetration in India. The highest concentration was found to be around metropolitan cities, that is, Mumbai, Delhi and lowest in North East India and Jammu & Kashmir indicating the penetration of SNSs. Â© The Author(s) 2019.","Being a habitat of the global village, every place has established connections through the strength and power of social media, piercing through the political boundaries. Social media is a digital platform, where people across the world can interact. This has a number of advantages of being universal, anonymous, easy accessibility, indirect interaction, gathering and sharing information when compared with direct interaction. The easy access to social networking sites (SNSs) such as Facebook, Twitter and blogs has brought about unprecedented opportunities for citizens to voice their opinions loaded with emotions/sentiments. Furthermore, social media can influence human thoughts. A recent incident of public importance had presented an opportunity to map the sentiments, involved around it. Sentiments were extracted from tweets for a week. These sentiments were classified as positive, negative and neutral and were mapped in geographic information system (GIS) environment. It was found that the number of tweets diminished by 91% over a week from 25 August 2017 to 31 August 2017. Maximum tweets emerged from places near the origin of the case (Haryana, Delhi and Punjab). The trend of sentiments was found to be neutral (47.4%), negative (30%) and positive (22.6%). Interestingly, tweets were also coming from unexpected places such as United States, United Kingdom and West Asia. The result can also be used to assess the spatial distribution of digital penetration in India. The highest concentration was found to be around metropolitan cities, that is, Mumbai, Delhi and lowest in North East India and Jammu & Kashmir indicating the penetration of SNSs."
Capture and visualisation of text understanding through semantic annotations and semantic networks for teaching and learning,"During various learning activities, teachers and students need to clarify, explore and share their understanding of reading materials. For doing this, they must make explicit the mental representation constructed during the reading process. In this article, we propose an approach combining semantic annotations and semantic networks as formal means for elicitation, structuring, formalisation, analysis and sharing of teachersâ and studentsâ understanding of textual materials that they are asked to read in learning tasks. In the proposed approach, teachers can create learning tasks, in which students are asked to semantically annotate a text by associating portions of it to resources described in a Knowledge Base (KB) in accordance with a provided ontology. New instances can be created by students or teachers in the KB during the annotation process. We show how semantic networks can be used to visualise extracts of the resulting KB, and to help people organise their comprehension of texts. In addition, teachers can assess student evolution by analysing the semantic networks that each one produces during the reading and annotation process. This approach is implemented by using our annotation tool, integrated with a digital repository and a virtual learning environment. An empirical evaluation of the benefits of the proposed approach in a literature case study confirms that it facilitates information extraction, sharing and analysis, contributing to leverage teaching and learning. Â© The Author(s) 2019.","During various learning activities, teachers and students need to clarify, explore and share their understanding of reading materials. For doing this, they must make explicit the mental representation constructed during the reading process. In this article, we propose an approach combining semantic annotations and semantic networks as formal means for elicitation, structuring, formalisation, analysis and sharing of teachers and students understanding of textual materials that they are asked to read in learning tasks. In the proposed approach, teachers can create learning tasks, in which students are asked to semantically annotate a text by associating portions of it to resources described in a Knowledge Base (KB) in accordance with a provided ontology. New instances can be created by students or teachers in the KB during the annotation process. We show how semantic networks can be used to visualise extracts of the resulting KB, and to help people organise their comprehension of texts. In addition, teachers can assess student evolution by analysing the semantic networks that each one produces during the reading and annotation process. This approach is implemented by using our annotation tool, integrated with a digital repository and a virtual learning environment. An empirical evaluation of the benefits of the proposed approach in a literature case study confirms that it facilitates information extraction, sharing and analysis, contributing to leverage teaching and learning."
Exploring the dominant features of social media for depression detection,"Recently, social media have been used by researchers to detect depressive symptoms in individuals using linguistic data from usersâ posts. In this study, we propose a framework to identify social information as a significant predictor of depression. Using the proposed framework, we develop an application called the Socially Mediated Patient Portal (SMPP), which detects depression-related markers in Facebook users by applying a data-driven approach with machine learning classification techniques. We examined a data set of 4350 users who were evaluated for depression using the Center for Epidemiological Studies Depression (CES-D) scale. From this analysis, we identified a set of features that can distinguish between individuals with and without depression. Finally, we identified the dominant features that adequately assess individuals with and without depression on social media. The model trained on these features will be helpful to physicians in diagnosing mental diseases and psychiatrists in analysing patient behaviour. Â© The Author(s) 2019.","Recently, social media have been used by researchers to detect depressive symptoms in individuals using linguistic data from users posts. In this study, we propose a framework to identify social information as a significant predictor of depression. Using the proposed framework, we develop an application called the Socially Mediated Patient Portal (SMPP), which detects depression-related markers in Facebook users by applying a data-driven approach with machine learning classification techniques. We examined a data set of 4350 users who were evaluated for depression using the Center for Epidemiological Studies Depression (CES-D) scale. From this analysis, we identified a set of features that can distinguish between individuals with and without depression. Finally, we identified the dominant features that adequately assess individuals with and without depression on social media. The model trained on these features will be helpful to physicians in diagnosing mental diseases and psychiatrists in analysing patient behaviour."
Using dates as contextual information for personalised cultural heritage experiences,"We present semantics-based mechanisms that aim to promote reflection on cultural heritage by means of dates (historical events or annual commemorations), owing to their connections to a collection of items and to the visitorsâ interests. We argue that links to specific dates can trigger curiosity, increase retention and guide visitors around the venue following new appealing narratives in subsequent visits. The proposal has been evaluated in a pilot study on the collection of the Archaeological Museum of Tripoli (Greece), for which a team of humanities experts wrote a set of diverse narratives about the exhibits. A year-round calendar was crafted so that certain narratives would be more or less relevant on any given day. Expanding on this calendar, personalised recommendations can be made by sorting out those relevant narratives according to personal events and interests recorded in the profiles of the target users. Evaluation of the associations by experts and potential museum visitors shows that the proposed approach can discover meaningful connections, while many others that are more incidental can still contribute to the intended cognitive phenomena. Â© The Author(s) 2019.","We present semantics-based mechanisms that aim to promote reflection on cultural heritage by means of dates (historical events or annual commemorations), owing to their connections to a collection of items and to the visitors interests. We argue that links to specific dates can trigger curiosity, increase retention and guide visitors around the venue following new appealing narratives in subsequent visits. The proposal has been evaluated in a pilot study on the collection of the Archaeological Museum of Tripoli (Greece), for which a team of humanities experts wrote a set of diverse narratives about the exhibits. A year-round calendar was crafted so that certain narratives would be more or less relevant on any given day. Expanding on this calendar, personalised recommendations can be made by sorting out those relevant narratives according to personal events and interests recorded in the profiles of the target users. Evaluation of the associations by experts and potential museum visitors shows that the proposed approach can discover meaningful connections, while many others that are more incidental can still contribute to the intended cognitive phenomena."
Integrating word status for joint detection of sentiment and aspect in reviews,"A crucial task in sentiment analysis is aspect detection: the step of selecting the aspects on which opinions are expressed. This step anticipates the step of determining whether the opinions on aspects are positive or negative. This article proposes a novel probabilistic generative topic model for aspect-based sentiment analysis which is able to discover the latent structure of a large collection of review documents. The proposed joint sentiment-aspect detection model (SAM) is a generative topic model that incorporates the structure of review sentences for detecting aspects and sentiments simultaneously. The intuitions behind the SAM are that from generating documents by latent single- and multi-word topics, modelling the word distribution for each topic and learning of the prior distribution over topics in sentences of documents. SAM introduces word status so that the model can decide when to sample from a bigram distribution or a unigram distribution and integrates all these components into one combined model for aspect-based sentiment analysis. We evaluate SAM both qualitatively and quantitatively to show that the model is indeed able to perform the task effectively and improves significantly over standard joint sentiment-aspect models. The proposed model can easily be transformed between domains or languages and can detect the polarity of text data at various levels. However, for the quantitative analysis, we mainly focus on presenting the results for the document-level sentiment classification. Â© The Author(s) 2018.","A crucial task in sentiment analysis is aspect detection: the step of selecting the aspects on which opinions are expressed. This step anticipates the step of determining whether the opinions on aspects are positive or negative. This article proposes a novel probabilistic generative topic model for aspect-based sentiment analysis which is able to discover the latent structure of a large collection of review documents. The proposed joint sentiment-aspect detection model (SAM) is a generative topic model that incorporates the structure of review sentences for detecting aspects and sentiments simultaneously. The intuitions behind the SAM are that from generating documents by latent single- and multi-word topics, modelling the word distribution for each topic and learning of the prior distribution over topics in sentences of documents. SAM introduces word status so that the model can decide when to sample from a bigram distribution or a unigram distribution and integrates all these components into one combined model for aspect-based sentiment analysis. We evaluate SAM both qualitatively and quantitatively to show that the model is indeed able to perform the task effectively and improves significantly over standard joint sentiment-aspect models. The proposed model can easily be transformed between domains or languages and can detect the polarity of text data at various levels. However, for the quantitative analysis, we mainly focus on presenting the results for the document-level sentiment classification."
Development of a classification system for Mathematical Logic,"The number of digital resources that exist in repositories and on the Internet in general is enormous. Recovering resources that fit with the userâs specific needs poses a problem. To solve this problem, metainformation is added to the resources. One type of metainformation is the classification of a resource using a classification system that is widely recognised and agreed upon by its users. In this way, each resource is assigned a precise place within the classification system, thus facilitating its location. This article proposes a taxonomy for the classification of the resources (notes, exercises, exams or programmes that could be stored within a digital repository) that are generated within the scope of a Mathematical Logic course for a computer science degree programme. It also describes how to represent the proposed taxonomy using the IMS-VDEX standard and how to integrate it in the LOM and Dublin Core metadata specifications and proposes a set of controlled vocabularies that make it possible to refine the taxonomic metainformation. Â© The Author(s) 2019.","The number of digital resources that exist in repositories and on the Internet in general is enormous. Recovering resources that fit with the users specific needs poses a problem. To solve this problem, metainformation is added to the resources. One type of metainformation is the classification of a resource using a classification system that is widely recognised and agreed upon by its users. In this way, each resource is assigned a precise place within the classification system, thus facilitating its location. This article proposes a taxonomy for the classification of the resources (notes, exercises, exams or programmes that could be stored within a digital repository) that are generated within the scope of a Mathematical Logic course for a computer science degree programme. It also describes how to represent the proposed taxonomy using the IMS-VDEX standard and how to integrate it in the LOM and Dublin Core metadata specifications and proposes a set of controlled vocabularies that make it possible to refine the taxonomic metainformation."
Real-time feedback query expansion technique for supporting scholarly search using citation network analysis,"Scholars routinely search relevant papers to discover and put a new idea into proper context. Despite ongoing advances in scholarly retrieval technologies, locating relevant papers through keyword queries is still quite challenging due to the massive expansion in the size of the research paper repository. To tackle this problem, we propose a novel real-time feedback query expansion technique, which is a two-stage interactive scholarly search process. Upon receiving the initial search query, the retrieval system provides a ranked list of results. In the second stage, a user selects a few relevant papers, from which useful terms are extracted for query expansion. The newly expanded query is run against the index in real time to generate the final list of research papers. In both stages, citation analysis is involved in further improving the quality of the results. The novelty of the approach lies in the combined exploitation of query expansion and citation analysis that may bring the most relevant papers to the top of the search results list. The experimental results on the Association of Computational Linguistics (ACL) Anthology Network data set demonstrate that this technique is effective and robust for locating relevant papers regarding normalised discounted cumulative gain (nDCG), precision and recall rates than several state-of-the-art approaches. Â© The Author(s) 2019.","Scholars routinely search relevant papers to discover and put a new idea into proper context. Despite ongoing advances in scholarly retrieval technologies, locating relevant papers through keyword queries is still quite challenging due to the massive expansion in the size of the research paper repository. To tackle this problem, we propose a novel real-time feedback query expansion technique, which is a two-stage interactive scholarly search process. Upon receiving the initial search query, the retrieval system provides a ranked list of results. In the second stage, a user selects a few relevant papers, from which useful terms are extracted for query expansion. The newly expanded query is run against the index in real time to generate the final list of research papers. In both stages, citation analysis is involved in further improving the quality of the results. The novelty of the approach lies in the combined exploitation of query expansion and citation analysis that may bring the most relevant papers to the top of the search results list. The experimental results on the Association of Computational Linguistics (ACL) Anthology Network data set demonstrate that this technique is effective and robust for locating relevant papers regarding normalised discounted cumulative gain (nDCG), precision and recall rates than several state-of-the-art approaches."
âNo commentâ? A study of commenting on PLOS articles,"Articleâcommenting functionality allows users to add publicly visible comments to an article on a publisherâs website. As well as facilitating forms of post-publication peer review, for publishers of open-access mega-journals (large, broad scope, open-access journals that seek to publish all technically or scientifically sound research) comments are also thought to serve as a means for the community to discuss and communicate the significance and novelty of the research, factors which are not assessed during peer review. In this article we present the results of an analysis of commenting on articles published by the Public Library of Science (PLOS), publisher of the first and best-known mega-journal PLOS ONE, between 2003 and 2016. We find that while overall commenting rates are low, and have declined since 2010, there is substantial variation across different PLOS titles. Using a typology of comments developed for this research, we also find that only around half of comments engage in an academic discussion of the article and that these discussions are most likely to focus on the paperâs technical soundness. Our results suggest that publishers are yet to encourage significant numbers of readers to leave comments, with implications for the effectiveness of commenting as a means of collecting and communicating community perceptions of an articleâs importance. Â© The Author(s) 2019.","Articlecommenting functionality allows users to add publicly visible comments to an article on a publishers website. As well as facilitating forms of post-publication peer review, for publishers of open-access mega-journals (large, broad scope, open-access journals that seek to publish all technically or scientifically sound research) comments are also thought to serve as a means for the community to discuss and communicate the significance and novelty of the research, factors which are not assessed during peer review. In this article we present the results of an analysis of commenting on articles published by the Public Library of Science (PLOS), publisher of the first and best-known mega-journal PLOS ONE, between 2003 and 2016. We find that while overall commenting rates are low, and have declined since 2010, there is substantial variation across different PLOS titles. Using a typology of comments developed for this research, we also find that only around half of comments engage in an academic discussion of the article and that these discussions are most likely to focus on the papers technical soundness. Our results suggest that publishers are yet to encourage significant numbers of readers to leave comments, with implications for the effectiveness of commenting as a means of collecting and communicating community perceptions of an articles importance."
"A study on first citations of patents through a combination of Bradfordâs distribution, Cox regression and life tables method","The current research employs two survival analysis methods: Cox regression and life tables. The first determines the effect of inventor, assignee and country for receiving the first citation by patents. Life tables concern the time-lag between the dates of granting and receiving the first citation by patents. Bradfordâs method is also established as a technique for categorization of patents, inventors, assignees and countries as a prerequisite for survival analysis. The research materials consist of 2837 patents in the area of âpurification, separation, or recovery of hydrocarbon componentsâ which were classified under the classes 585/800 and 585/868 by the United States Patent and Trademark Office (USPTO). The findings showed that Bradfordâs method complies with the distribution of citations of patents, first inventors and assignees. It means that Bradfordâs distribution is well suited for determination of key patents, inventors and assignees in an area too. Cox regression revealed that only the inventorsâ variable decides for receiving the first citation in terms of frequency, degrees of their inventions and citations. Life table data revealed that one half of the first citations were received in the first 10 years. As a conclusion, survival analysis methods provide the possibility for deciding technology lifetime and for predicting the determinants for the flow of knowledge through citation analysis. Â© The Author(s) 2019.","The current research employs two survival analysis methods: Cox regression and life tables. The first determines the effect of inventor, assignee and country for receiving the first citation by patents. Life tables concern the time-lag between the dates of granting and receiving the first citation by patents. Bradfords method is also established as a technique for categorization of patents, inventors, assignees and countries as a prerequisite for survival analysis. The research materials consist of 2837 patents in the area of purification, separation, or recovery of hydrocarbon components which were classified under the classes 585/800 and 585/868 by the United States Patent and Trademark Office (USPTO). The findings showed that Bradfords method complies with the distribution of citations of patents, first inventors and assignees. It means that Bradfords distribution is well suited for determination of key patents, inventors and assignees in an area too. Cox regression revealed that only the inventors variable decides for receiving the first citation in terms of frequency, degrees of their inventions and citations. Life table data revealed that one half of the first citations were received in the first 10 years. As a conclusion, survival analysis methods provide the possibility for deciding technology lifetime and for predicting the determinants for the flow of knowledge through citation analysis."
A user ranking algorithm for efficient information management of community sites using spectral clustering and folksonomy,"Community question answering (CQA) sites are the major platform for information sharing where posts are created by users as questions and answers. A large number of posts are created on a day-to-day basis, which raise the problem of information management of these sites. Multiple techniques are suggested in existing research for efficient management of CQA sites. Many of the existing techniques used the user ranking for managing the CQA sites but ignored the tagging data and user subject area. In this article, a user ranking method is derived using spectral clustering for posts management by considering the tagging data of CQA sites. Folksonomy is used to build relationship between tags, posts and users. The proposed method is developed in three stages. In first stage, the folksonomy relation is created and user similarity graph is built with the help of tag frequency-inverse post frequency and text similarity techniques. In the second stage, spectral clustering algorithm is applied on user similarity graph to group the similar users. Finally, in third stage, rank of users is identified from the clusters based on userâs information. The clustered users and rank of the users are generated as the output of the proposed algorithm that can provide a way of efficient information management. The experimental results show that the proposed user ranking algorithm outperforms the other considered ranking algorithms and can be helpful for information management of CQA sites. Some real-life applications of information management in CQA sites using the proposed work are also demonstrated in this article. Â© The Author(s) 2018.","Community question answering (CQA) sites are the major platform for information sharing where posts are created by users as questions and answers. A large number of posts are created on a day-to-day basis, which raise the problem of information management of these sites. Multiple techniques are suggested in existing research for efficient management of CQA sites. Many of the existing techniques used the user ranking for managing the CQA sites but ignored the tagging data and user subject area. In this article, a user ranking method is derived using spectral clustering for posts management by considering the tagging data of CQA sites. Folksonomy is used to build relationship between tags, posts and users. The proposed method is developed in three stages. In first stage, the folksonomy relation is created and user similarity graph is built with the help of tag frequency-inverse post frequency and text similarity techniques. In the second stage, spectral clustering algorithm is applied on user similarity graph to group the similar users. Finally, in third stage, rank of users is identified from the clusters based on users information. The clustered users and rank of the users are generated as the output of the proposed algorithm that can provide a way of efficient information management. The experimental results show that the proposed user ranking algorithm outperforms the other considered ranking algorithms and can be helpful for information management of CQA sites. Some real-life applications of information management in CQA sites using the proposed work are also demonstrated in this article."
An investigation of cultural objects in conflict zones through the lens of TripAdvisor reviews: A case of South Caucasus,"This study is an investigation of how cultural sites and objects in the former conflict zones of South Caucasus are constructed in user-generated narratives in TripAdvisor reviews and images. An analysis of these reviews and images was found to demonstrate the embodied orientation of reviewersâ narrations, wherein the disputed nature of the cultural sites is mainly voiced in the form of dissatisfaction with the socio-economical situation and services. This study suggests that the forgotten nature of frozen conflicts engendered an erosion of and disconnect from cultural heritage, ties and significance for those who fled the contested areas. Â© The Author(s) 2019.","This study is an investigation of how cultural sites and objects in the former conflict zones of South Caucasus are constructed in user-generated narratives in TripAdvisor reviews and images. An analysis of these reviews and images was found to demonstrate the embodied orientation of reviewers narrations, wherein the disputed nature of the cultural sites is mainly voiced in the form of dissatisfaction with the socio-economical situation and services. This study suggests that the forgotten nature of frozen conflicts engendered an erosion of and disconnect from cultural heritage, ties and significance for those who fled the contested areas."
Emerging scenarios of data infrastructure and novel concepts of digital libraries in intelligent infrastructure for human-centred communities: A qualitative research,"This research investigated the strategic development of a large-scale transdisciplinary area, named intelligent infrastructure for human-centred communities, at Virginia Tech. Within such development, this study explored the future vision and anticipated scenarios of data infrastructure and digital libraries for smart community development. It draws upon the mixed-methods approach combining ethnographic participant observation, document analysis and semi-structured interviews. Grounded in socio-technical framework and rooted in empirical methods, this research produces results that augment design thinking and visioning practice for digital data libraries beyond traditional boundaries. The findings reveal the emerging scenarios around complex adaptive systems, intelligent data infrastructure and future digital libraries all in the context of building infrastructure for human-centred communities. Situated in this advancing reality, the results further discuss the next-generation data and information user experience, smart infrastructure data environment and future library capabilities. The article concludes that a smart library system, whether in its conceptual form of a âdigital octopusâ or a âsmart village data hubâ or an âintelligent virtual assistantâ, will provide intelligence in data gathering, processing, summarising, communication and recommendations. By delivering unified and personalised data solutions, it will offer an end-to-end seamless experience for users throughout their journey of knowledge pursuit. Â© The Author(s) 2018.","This research investigated the strategic development of a large-scale transdisciplinary area, named intelligent infrastructure for human-centred communities, at Virginia Tech. Within such development, this study explored the future vision and anticipated scenarios of data infrastructure and digital libraries for smart community development. It draws upon the mixed-methods approach combining ethnographic participant observation, document analysis and semi-structured interviews. Grounded in socio-technical framework and rooted in empirical methods, this research produces results that augment design thinking and visioning practice for digital data libraries beyond traditional boundaries. The findings reveal the emerging scenarios around complex adaptive systems, intelligent data infrastructure and future digital libraries all in the context of building infrastructure for human-centred communities. Situated in this advancing reality, the results further discuss the next-generation data and information user experience, smart infrastructure data environment and future library capabilities. The article concludes that a smart library system, whether in its conceptual form of a digital octopus or a smart village data hub or an intelligent virtual assistant, will provide intelligence in data gathering, processing, summarising, communication and recommendations. By delivering unified and personalised data solutions, it will offer an end-to-end seamless experience for users throughout their journey of knowledge pursuit."
Exploring the characteristics of crowdsourcing: An online observational study,"This article examines the application of crowdsourcing in research studies. The aim of this study is to understand how crowdsourcing is being used in research by undertaking a content analysis of studies posted to an online site designed to facilitate crowdsourced research. While there are a number of websites that facilitate crowdsourcing, this study provides an analysis only of research studies posted on crowdcrafting.org. Characteristics of crowdsourcing, proposed by EstellÃ©s-Arolas and GonzÃ¡lez-LadrÃ³n-de-Guevara, served as the framework for the content analysis, and research projects were evaluated as to how they addressed each of the proposed criteria. This article concludes with recommendations for researchers undertaking the design and implementation of projects employing crowdsourcing. Â© The Author(s) 2019.","This article examines the application of crowdsourcing in research studies. The aim of this study is to understand how crowdsourcing is being used in research by undertaking a content analysis of studies posted to an online site designed to facilitate crowdsourced research. While there are a number of websites that facilitate crowdsourcing, this study provides an analysis only of research studies posted on crowdcrafting.org. Characteristics of crowdsourcing, proposed by Estell"
An overview of systematic literature reviews in social media marketing,"Systematic literature reviews (SLRs) adopt a specified and transparent approach in order to scope the literature in a field or sub-field. However, there has been little critical comment on their purpose and processes in practice. By undertaking an overview of SLRs in the field of social media (SM) marketing, this article undertakes a critical evaluation of the SLR purposes and processes in a set of recent SLRs and presents a future research agenda for social media marketing. The overview shows that the purposes of SLRs include the following: making sense (of research in a field), developing a concept matrix/taxonomy and supporting research and practice. On SLR processes, while there is some consensus on the stages of the process, there is considerable variation in how these processes are executed. This article offers a resource to inform practice and acts as a platform for further critical debate regarding the nature and value of SLRs. Â© The Author(s) 2019.","Systematic literature reviews (SLRs) adopt a specified and transparent approach in order to scope the literature in a field or sub-field. However, there has been little critical comment on their purpose and processes in practice. By undertaking an overview of SLRs in the field of social media (SM) marketing, this article undertakes a critical evaluation of the SLR purposes and processes in a set of recent SLRs and presents a future research agenda for social media marketing. The overview shows that the purposes of SLRs include the following: making sense (of research in a field), developing a concept matrix/taxonomy and supporting research and practice. On SLR processes, while there is some consensus on the stages of the process, there is considerable variation in how these processes are executed. This article offers a resource to inform practice and acts as a platform for further critical debate regarding the nature and value of SLRs."
Spatial bibliometrics on the city level,"It is very popular in bibliometrics to present results on institutions not only as tabular lists, but also on maps (see, for example, the Leiden Ranking). However, the problem with these visualisations is that institutions are frequently spatially clustered in larger cities whereby institutions are positioned one above the other. In this Brief Communication, we propose as an alternative to visualise bibliometric data on the city rather than the institution level to avoid this problem. Â© The Author(s) 2018.","It is very popular in bibliometrics to present results on institutions not only as tabular lists, but also on maps (see, for example, the Leiden Ranking). However, the problem with these visualisations is that institutions are frequently spatially clustered in larger cities whereby institutions are positioned one above the other. In this Brief Communication, we propose as an alternative to visualise bibliometric data on the city rather than the institution level to avoid this problem."
Low-cost similarity calculation on ontology fusion in knowledge bases,"Ontology fusion in knowledge bases has become less easy, due to the massive capacity involved in the process of semantic similarity calculation. Many similarity calculation methods have been developed, although they are hardly united. This article contributes a low-cost similarity calculation method for ontology fusion, based on the inspiration of binary metrics, with the aim of reducing the size of similarity calculations both spatially and logically. By introducing the definitions of a heterogeneous ontology, entities of ontologies and rules of ontology fusion on the basis of concept fusion and relationship fusion, we put forward the algorithm of main traverse procedure and calculated to be the least cost in time and space in comparison with traditional methods. We adopted three experiments to testify the usability of our approach from the perspective of actual library resources, small datasets and large datasets. In Experiment 1, the bibliographic data from East China Normal University Library were used to show the feasibility and capability of our proposal and present the process of the algorithm. In both Experiments 2 and 3, our approach had at least 88% confidence in detecting accurate merging mappings and also decreased time cost. The test demonstrated a good fusion result. The problem of lower recalls caused by error analysis results from the conflict between the complex structures in ontologies and the recursive functions, which will be improved in the future. Â© The Author(s) 2019.","Ontology fusion in knowledge bases has become less easy, due to the massive capacity involved in the process of semantic similarity calculation. Many similarity calculation methods have been developed, although they are hardly united. This article contributes a low-cost similarity calculation method for ontology fusion, based on the inspiration of binary metrics, with the aim of reducing the size of similarity calculations both spatially and logically. By introducing the definitions of a heterogeneous ontology, entities of ontologies and rules of ontology fusion on the basis of concept fusion and relationship fusion, we put forward the algorithm of main traverse procedure and calculated to be the least cost in time and space in comparison with traditional methods. We adopted three experiments to testify the usability of our approach from the perspective of actual library resources, small datasets and large datasets. In Experiment 1, the bibliographic data from East China Normal University Library were used to show the feasibility and capability of our proposal and present the process of the algorithm. In both Experiments 2 and 3, our approach had at least 88% confidence in detecting accurate merging mappings and also decreased time cost. The test demonstrated a good fusion result. The problem of lower recalls caused by error analysis results from the conflict between the complex structures in ontologies and the recursive functions, which will be improved in the future."
LAZY R-tree: The R-tree with lazy splitting algorithm,"The spatial index is a data structure formed according to the position and shape of the spatial object or the relationship between the spatial objects according to certain rules, and the spatial data is managed by an effective spatial data structure. The quality of a spatial index directly affects the performance of spatial queries. The R-tree index structure is a highly efficient spatial index. According to the R-tree query rule, when performing spatial query, most data that is not related to the query condition can be filtered out, and finally, a few leaf nodes can be accessed to query the data satisfying the condition. Its query performance is affected by factors such as non-leaf node overlap and node space utilisation. This article proposes a lazy splitting method to improve the R-tree construction process. The scheme works as follows: (1) When a node overflows, it creates an overflow node for that node and all overflow nodes are saved in a hash table. (2) If the node continues to insert data, the data are added to its overflow node. (3) When an overflow node is saturated, the node and its overflow node are split into two saturated nodes. We use both simulated and actual data to perform experiments. The experimental results show that an R-tree constructed by the lazy algorithm is superior to an R-tree constructed using the original R-tree PM algorithm or the corner-based splitting (CBS) algorithm based on the number of splits created, the node space used and the efficiency of region queries and k-nearest neighbour (kNN) queries. Â© The Author(s) 2019.","The spatial index is a data structure formed according to the position and shape of the spatial object or the relationship between the spatial objects according to certain rules, and the spatial data is managed by an effective spatial data structure. The quality of a spatial index directly affects the performance of spatial queries. The R-tree index structure is a highly efficient spatial index. According to the R-tree query rule, when performing spatial query, most data that is not related to the query condition can be filtered out, and finally, a few leaf nodes can be accessed to query the data satisfying the condition. Its query performance is affected by factors such as non-leaf node overlap and node space utilisation. This article proposes a lazy splitting method to improve the R-tree construction process. The scheme works as follows: When a node overflows, it creates an overflow node for that node and all overflow nodes are saved in a hash table. If the node continues to insert data, the data are added to its overflow node. When an overflow node is saturated, the node and its overflow node are split into two saturated nodes. We use both simulated and actual data to perform experiments. The experimental results show that an R-tree constructed by the lazy algorithm is superior to an R-tree constructed using the original R-tree PM algorithm or the corner-based splitting (CBS) algorithm based on the number of splits created, the node space used and the efficiency of region queries and k-nearest neighbour (kNN) queries."
Open-access policy and data-sharing practice in UK academia,"Data sharing can be defined as the release of research data that can be used by others. With the recent open-science movement, there has been a call for free access to data, tools and methods in academia. In recent years, subject-based and institutional repositories and data centres have emerged along with online publishing. Many scientific records, including published articles and data, have been made available via new platforms. In the United Kingdom, most major research funders had a data policy and require researchers to include a âdata-sharing planâ when applying for funding. However, there are a number of barriers to the full-scale adoption of data sharing. Those barriers are not only technical, but also psychological and social. A survey was conducted with over 1800 UK-based academics to explore the extent of support of data sharing and the characteristics and factors associated with data-sharing practice. It found that while most academics recognised the importance of sharing research data, most of them had never shared or reused research data. There were differences in the extent of data sharing between different gender, academic disciplines, age and seniority. It also found that the awareness of Research Council UKâs (RCUK) Open-Access (OA) policy, experience of Gold and Green OA publishing, attitudes towards the importance of data sharing and experience of using secondary data were associated with the practice of data sharing. A small group of researchers used social media such as Twitter, blogs and Facebook to promote the research data they had shared online. Our findings contribute to the knowledge and understanding of open science and offer recommendations to academic institutions, journals and funding agencies. Â© The Author(s) 2019.","Data sharing can be defined as the release of research data that can be used by others. With the recent open-science movement, there has been a call for free access to data, tools and methods in academia. In recent years, subject-based and institutional repositories and data centres have emerged along with online publishing. Many scientific records, including published articles and data, have been made available via new platforms. In the United Kingdom, most major research funders had a data policy and require researchers to include a data-sharing plan when applying for funding. However, there are a number of barriers to the full-scale adoption of data sharing. Those barriers are not only technical, but also psychological and social. A survey was conducted with over 1800 UK-based academics to explore the extent of support of data sharing and the characteristics and factors associated with data-sharing practice. It found that while most academics recognised the importance of sharing research data, most of them had never shared or reused research data. There were differences in the extent of data sharing between different gender, academic disciplines, age and seniority. It also found that the awareness of Research Council UKs (RCUK) Open-Access (OA) policy, experience of Gold and Green OA publishing, attitudes towards the importance of data sharing and experience of using secondary data were associated with the practice of data sharing. A small group of researchers used social media such as Twitter, blogs and Facebook to promote the research data they had shared online. Our findings contribute to the knowledge and understanding of open science and offer recommendations to academic institutions, journals and funding agencies."
A knowledge-based model for managing the ontology evolution: case study of maintenance in SONATRACH,"The challenges of the development of a suitable ontology scheme in decision-making environment should be taken in conjunction with the exploitation of more recent technologies. It is expected that the use of ontologies will lead to the construction of more intelligent applications, allowing them to work more specifically at a human conceptual level. We propose in this article an approach that analyses the impact of changes in the ontology on business rules in order to detect inconsistencies that may be generated. In addition, the developed tool provides solutions to repair inconsistencies with the help of domain experts. In our work, business rules are edited from the concepts and properties that are stored in an OWL (Web Ontology Language) ontology named OntoloG. This latter is implemented throughout the use of ProtÃ©gÃ© 4.0.2.with the OWL sub-language. OntoloG has been developed by the knowledge acquisition from documents, collection and capitalisation of business rules process with experts in SONATRACH AVAL. Â© The Author(s) 2018.","The challenges of the development of a suitable ontology scheme in decision-making environment should be taken in conjunction with the exploitation of more recent technologies. It is expected that the use of ontologies will lead to the construction of more intelligent applications, allowing them to work more specifically at a human conceptual level. We propose in this article an approach that analyses the impact of changes in the ontology on business rules in order to detect inconsistencies that may be generated. In addition, the developed tool provides solutions to repair inconsistencies with the help of domain experts. In our work, business rules are edited from the concepts and properties that are stored in an OWL (Web Ontology Language) ontology named OntoloG. This latter is implemented throughout the use of Prot"
Music-search behaviour on a social Q&A site: A cross-gender comparison,"While there have been numerous studies of music-search behaviour, little is known about gendered aspects of how it is carried out on social question and answer sites. The article examines gender differences manifested on one such site with regard to (a) the motivations of the person posing the question, (b) intervening variables that influence music-search behaviour and (c) the formulation of the questions. Results from manual categorisation and other analysis of 17,380 music-relevant questions collected from the site show that males who asked questions did so more often, provided more answers and had more followers than female question-posters. Males tended to include music context information in questions asking for ready reference, whereas females often asked questions in a second-person pronoun aiming for promoting discussion. Such research results add to the current understanding of music-search behaviour and contribute new insights that can inform development of better music services/systems. Â© The Author(s) 2019.","While there have been numerous studies of music-search behaviour, little is known about gendered aspects of how it is carried out on social question and answer sites. The article examines gender differences manifested on one such site with regard to (a) the motivations of the person posing the question, (b) intervening variables that influence music-search behaviour and the formulation of the questions. Results from manual categorisation and other analysis of 17,380 music-relevant questions collected from the site show that males who asked questions did so more often, provided more answers and had more followers than female question-posters. Males tended to include music context information in questions asking for ready reference, whereas females often asked questions in a second-person pronoun aiming for promoting discussion. Such research results add to the current understanding of music-search behaviour and contribute new insights that can inform development of better music services/systems."
Decision tree classification: Ranking journals using IGIDI,"Selection of an attribute for placement of the decision tree at an appropriate position (e.g. root of the tree) is an important decision. Many attribute selection measures such as Information Gain, Gini Index and Entropy have been developed for this purpose. The suitability of an attribute generally depends on the diversity of its values, relevance and dependency. Different attribute selection measures have different criteria for measuring the suitability of an attribute. Diversity Index is a classical statistical measure for determining the diversity of values, and according to our knowledge, it has never been used as an attribute selection method. In this article, we propose a novel attribute selection method for decision tree classification. In the proposed scheme, the average of Information Gain, Gini Index and Diversity Index are taken into account for assigning a weight to the attributes. The attribute with the highest average value is selected for the classification. We have empirically tested our proposed algorithm for classification of different data sets of scientific journals and conferences. We have developed a web-based application named JC-Rank that makes use of our proposed algorithm. We have also compared the results of our proposed technique with some existing decision tree classification algorithms. Â© The Author(s) 2019.","Selection of an attribute for placement of the decision tree at an appropriate position ( root of the tree) is an important decision. Many attribute selection measures such as Information Gain, Gini Index and Entropy have been developed for this purpose. The suitability of an attribute generally depends on the diversity of its values, relevance and dependency. Different attribute selection measures have different criteria for measuring the suitability of an attribute. Diversity Index is a classical statistical measure for determining the diversity of values, and according to our knowledge, it has never been used as an attribute selection method. In this article, we propose a novel attribute selection method for decision tree classification. In the proposed scheme, the average of Information Gain, Gini Index and Diversity Index are taken into account for assigning a weight to the attributes. The attribute with the highest average value is selected for the classification. We have empirically tested our proposed algorithm for classification of different data sets of scientific journals and conferences. We have developed a web-based application named JC-Rank that makes use of our proposed algorithm. We have also compared the results of our proposed technique with some existing decision tree classification algorithms."
Effective feature reduction for link prediction in location-based social networks,"In this study, we investigated feature-based approaches for improving the link prediction performance for location-based social networks (LBSNs) and analysed their performances. We developed new features based on time, common friend detail and place category information of check-in data in order to make use of information in the data which cannot be utilised by the existing features from the literature. We proposed a feature selection method to determine a feature subset that enhances the prediction performance with the removal of redundant features by clustering them. After clustering features, a genetic algorithm is used to determine the ones to select from each cluster. A non-monotonic and feasible feature selection is ensured by the proposed genetic algorithm. Results depict that both new features and the proposed feature selection method improved link prediction performance for LBSNs. Â© The Author(s) 2018.","In this study, we investigated feature-based approaches for improving the link prediction performance for location-based social networks (LBSNs) and analysed their performances. We developed new features based on time, common friend detail and place category information of check-in data in order to make use of information in the data which cannot be utilised by the existing features from the literature. We proposed a feature selection method to determine a feature subset that enhances the prediction performance with the removal of redundant features by clustering them. After clustering features, a genetic algorithm is used to determine the ones to select from each cluster. A non-monotonic and feasible feature selection is ensured by the proposed genetic algorithm. Results depict that both new features and the proposed feature selection method improved link prediction performance for LBSNs."
Cross-lingual text alignment for fine-grained plagiarism detection,"Fast and easy access to a wide range of documents in various languages, in conjunction with the wide availability of translation and editing tools, has led to the need to develop effective tools for detecting cross-lingual plagiarism. Given a suspicious document, cross-lingual plagiarism detection comprises two main subtasks: retrieving documents that are candidate sources for that document and analysing those candidates one by one to determine their similarity to the suspicious document. In this article, we examine the second subtask, also called the detailed analysis subtask, where the goal is to align plagiarised fragments from source and suspicious documents in different languages. Our proposed approach has two main steps: the first step tries to find candidate plagiarised fragments and focuses on high recall, followed by a more precise similarity analysis based on dynamic text alignment that will filter the results by finding alignments between the identified fragments. With these two steps, the proximity of the terms will be considered in different levels of granularity. In both steps, our approach uses a dictionary to obtain translations of individual terms instead of using a machine translation system to convert longer passages from one language to another. We used a weighting scheme to distinct multiple translations of the terms. Experimental results show that our method outperforms the methods used by the systems that achieved the best results in the PAN-2012 and PAN-2014 competitions. Â© The Author(s) 2018.","Fast and easy access to a wide range of documents in various languages, in conjunction with the wide availability of translation and editing tools, has led to the need to develop effective tools for detecting cross-lingual plagiarism. Given a suspicious document, cross-lingual plagiarism detection comprises two main subtasks: retrieving documents that are candidate sources for that document and analysing those candidates one by one to determine their similarity to the suspicious document. In this article, we examine the second subtask, also called the detailed analysis subtask, where the goal is to align plagiarised fragments from source and suspicious documents in different languages. Our proposed approach has two main steps: the first step tries to find candidate plagiarised fragments and focuses on high recall, followed by a more precise similarity analysis based on dynamic text alignment that will filter the results by finding alignments between the identified fragments. With these two steps, the proximity of the terms will be considered in different levels of granularity. In both steps, our approach uses a dictionary to obtain translations of individual terms instead of using a machine translation system to convert longer passages from one language to another. We used a weighting scheme to distinct multiple translations of the terms. Experimental results show that our method outperforms the methods used by the systems that achieved the best results in the PAN-2012 and PAN-2014 competitions."
Parallel sentence extraction to improve cross-language information retrieval from Wikipedia,"Translation language resources, such as bilingual word lists and parallel corpora, are important factors affecting the effectiveness of cross-language information retrieval (CLIR) systems. In particular, when large domain-appropriate parallel corpora are not available, developing an effective CLIR system is particularly difficult. Furthermore, creating a large parallel corpus is costly and requires considerable effort. Therefore, we here demonstrate the construction of parallel corpora from Wikipedia as well as improved query translation, wherein the queries are used for a CLIR system. To do so, we first constructed a bilingual dictionary, termed WikiDic. Then, we evaluated individual language resources and combinations of them in terms of their ability to extract parallel sentences; the combinations of our proposed WikiDic with the translation probability from the Webâs bilingual example sentence pairs and WikiDic was found to be best suited to parallel sentence extraction. Finally, to evaluate the parallel corpus generated from this best combination of language resources, we compared its performance in query translation for CLIR to that of a manually created EnglishâKorean parallel corpus. As a result, the corpus generated by our proposed method achieved a better performance than did the manually created corpus, thus demonstrating the effectiveness of the proposed method for automatic parallel corpus extraction. Not only can the method demonstrated herein be used to inform the construction of other parallel corpora from language resources that are readily available, but also, the parallel sentence extraction method will naturally improve as Wikipedia continues to be used and its content develops. Â© The Author(s) 2021.","Translation language resources, such as bilingual word lists and parallel corpora, are important factors affecting the effectiveness of cross-language information retrieval (CLIR) systems. In particular, when large domain-appropriate parallel corpora are not available, developing an effective CLIR system is particularly difficult. Furthermore, creating a large parallel corpus is costly and requires considerable effort. Therefore, we here demonstrate the construction of parallel corpora from Wikipedia as well as improved query translation, wherein the queries are used for a CLIR system. To do so, we first constructed a bilingual dictionary, termed WikiDic. Then, we evaluated individual language resources and combinations of them in terms of their ability to extract parallel sentences; the combinations of our proposed WikiDic with the translation probability from the Webs bilingual example sentence pairs and WikiDic was found to be best suited to parallel sentence extraction. Finally, to evaluate the parallel corpus generated from this best combination of language resources, we compared its performance in query translation for CLIR to that of a manually created EnglishKorean parallel corpus. As a result, the corpus generated by our proposed method achieved a better performance than did the manually created corpus, thus demonstrating the effectiveness of the proposed method for automatic parallel corpus extraction. Not only can the method demonstrated herein be used to inform the construction of other parallel corpora from language resources that are readily available, but also, the parallel sentence extraction method will naturally improve as Wikipedia continues to be used and its content develops."
Visual analysis of information world maps: An exploration of four methods,"Information researchers increasingly use participatory, arts-based methods to better understand the social contexts of individuals and populations. However, it remains rare to engage in qualitative analysis of the resulting visual artefacts. This article explores approaches to analysing visual media generated through a specific arts-based method, information world mapping (IWM), an interdisciplinary draw-and-talk technique that elicits data about individualsâ social information worlds. Here, we test four approaches to analysing visual media generated through IWM: directed qualitative content analysis (QCA), compositional interpretation, conceptual analysis and visual discourse analysis using situational analysis (SA). QCA was effective in creating an overview of participantsâ information practices, yet raised concern regarding interpretive bias. Using an inductive taxonomy for compositional interpretation, we identified genre conventions for IWMs. Conceptual analysis resulted primarily in a reflection of the research procedures and epistemology. SA, while time-consuming, generated a large amount of rich data, including discourses and power relations that were not identified in previous analysis of textual data. In a reversal of our previous stance that cautioned against IWM analysis, we encourage other researchers to consider integrated or secondary visual analysis of IWMs. Â© The Author(s) 2019.","Information researchers increasingly use participatory, arts-based methods to better understand the social contexts of individuals and populations. However, it remains rare to engage in qualitative analysis of the resulting visual artefacts. This article explores approaches to analysing visual media generated through a specific arts-based method, information world mapping (IWM), an interdisciplinary draw-and-talk technique that elicits data about individuals social information worlds. Here, we test four approaches to analysing visual media generated through IWM: directed qualitative content analysis (QCA), compositional interpretation, conceptual analysis and visual discourse analysis using situational analysis (SA). QCA was effective in creating an overview of participants information practices, yet raised concern regarding interpretive bias. Using an inductive taxonomy for compositional interpretation, we identified genre conventions for IWMs. Conceptual analysis resulted primarily in a reflection of the research procedures and epistemology. SA, while time-consuming, generated a large amount of rich data, including discourses and power relations that were not identified in previous analysis of textual data. In a reversal of our previous stance that cautioned against IWM analysis, we encourage other researchers to consider integrated or secondary visual analysis of IWMs."
Digital identity and the online self: Footprint strategies â An exploratory and comparative research study,"Reflecting on the thousands of diverse research studies of social media representation and digital privacy, this article presents a comprehensive summary of online personal strategies. First, the evolution of academic concepts about digital identity and the online self is summarised. Then, the article investigates the key dynamics of personal strategies and control issues in detail with ideas, experiences, stories and metaphors taken from 60 qualitative interviews from Central and Eastern Europe and Southeast Asia. According to the key findings of this article, the universal patterns of online personal strategies follow mostly conscious decisions, resulting in users maintaining 70% control of their digital footprints. However, the remaining 30% of online activities are unconscious floating with digital dynamics and resulting in a wide range of non-expected consequences from identity theft to kidnapping. In summary, an intercultural and intergenerational model highlights the complexity and diversity of the studied field, providing a reference framework for future studies. The closing section presents a discussion of those findings of this study that are inconsistent with commonplace assumptions and conclusions present in the academic literature, promoting for study those subjects that still need to be extended or explored. Â© The Author(s) 2019.","Reflecting on the thousands of diverse research studies of social media representation and digital privacy, this article presents a comprehensive summary of online personal strategies. First, the evolution of academic concepts about digital identity and the online self is summarised. Then, the article investigates the key dynamics of personal strategies and control issues in detail with ideas, experiences, stories and metaphors taken from 60 qualitative interviews from Central and Eastern Europe and Southeast Asia. According to the key findings of this article, the universal patterns of online personal strategies follow mostly conscious decisions, resulting in users maintaining 70% control of their digital footprints. However, the remaining 30% of online activities are unconscious floating with digital dynamics and resulting in a wide range of non-expected consequences from identity theft to kidnapping. In summary, an intercultural and intergenerational model highlights the complexity and diversity of the studied field, providing a reference framework for future studies. The closing section presents a discussion of those findings of this study that are inconsistent with commonplace assumptions and conclusions present in the academic literature, promoting for study those subjects that still need to be extended or explored."
Integrated framework for criminal network extraction from Web,"Extracting criminalsâ information and discovering their network are techniques that investigators often rely on to get extra information about criminal incidents and potential criminals. With the recent advances of the Web, a.k.a. Web 2.0, it has become a rich source of data which provides a variety of information sources. In this article, we propose an integrated framework that combines a variety of available components and makes use of different sources of information provided on the Web to get a better knowledge about criminals or terrorists (we will use criminals to cover all terrorists in the rest of this article). Our system extracts criminalsâ information and their corresponding network using Web sources, such as online newspapers, official reports, and social media. It uses text analysis to identify key persons and topics from crawled Web documents. We build a criminal graph from the analysed text based on the co-occurrence of mentioning of criminals. Further analysis is applied on the constructed graph to get key people, hidden relationships and interactions between criminals, as well as hierarchical criminal groups within a network. For every process in the framework, we analysed various available works and implementations that could be used in the process. While analysing social media posts, we identified several challenges which show what solutions could be used for that purpose. Finally, we provide a Web application which implements the proposed framework. It also shows how helpful and efficient the system is in extracting and analysing criminal information. Â© The Author(s) 2019.","Extracting criminals information and discovering their network are techniques that investigators often rely on to get extra information about criminal incidents and potential criminals. With the recent advances of the Web, Web 2.0, it has become a rich source of data which provides a variety of information sources. In this article, we propose an integrated framework that combines a variety of available components and makes use of different sources of information provided on the Web to get a better knowledge about criminals or terrorists (we will use criminals to cover all terrorists in the rest of this article). Our system extracts criminals information and their corresponding network using Web sources, such as online newspapers, official reports, and social media. It uses text analysis to identify key persons and topics from crawled Web documents. We build a criminal graph from the analysed text based on the co-occurrence of mentioning of criminals. Further analysis is applied on the constructed graph to get key people, hidden relationships and interactions between criminals, as well as hierarchical criminal groups within a network. For every process in the framework, we analysed various available works and implementations that could be used in the process. While analysing social media posts, we identified several challenges which show what solutions could be used for that purpose. Finally, we provide a Web application which implements the proposed framework. It also shows how helpful and efficient the system is in extracting and analysing criminal information."
Research diversification and its relationship with publication counts and impact: A case study based on Australian professors,"This research aims to investigate whether multi/inter-disciplinary research activities are related to research impact and publication counts of scholars. Since researchers with very high levels of multi/inter-disciplinarity might be able to target complex problems, we would expect them to receive more credits than their colleagues with a stronger disciplinary orientation. We analysed Web of Science (WoS) indexed publications of all associate and full professors from a random sample of Australian universities in physics, chemistry and biology (1980â2014). Australian Fields of Research (FoR) codes assigned to journals were used to calculate the diversification of authorsâ publications. The number of citations in the first 3 years, number of 10% most frequently cited papers, and citation impact percentile were used for impact assessment. A few indicators were used to measure the diversity including âextent of diversification (ED)â (number of distinct FoR codes divided by the number of publications) and âdiversification ratio (DR)â (ratio of the publications falling outside the dominant code to the total number of publications). A total of 47.76% of biologistsâ publications, 35.23% of physicistsâ publications and 20.36% of chemistsâ publications were published in journals assigned to fields other than the Australian associate and full professorsâ fields. Publications from biologists had the largest values of diversification. Women (compared with men) and associate professors (compared with full professors) in chemistry, biology and overall were more probably to publish diversely. ED was negatively correlated with output and citation impact. DR also had a negative but weak correlation with the number of publications and 10% most frequently cited paper. Â© The Author(s) 2019.","This research aims to investigate whether multi/inter-disciplinary research activities are related to research impact and publication counts of scholars. Since researchers with very high levels of multi/inter-disciplinarity might be able to target complex problems, we would expect them to receive more credits than their colleagues with a stronger disciplinary orientation. We analysed Web of Science (WoS) indexed publications of all associate and full professors from a random sample of Australian universities in physics, chemistry and biology (19802014). Australian Fields of Research (FoR) codes assigned to journals were used to calculate the diversification of authors publications. The number of citations in the first 3 years, number of 10% most frequently cited papers, and citation impact percentile were used for impact assessment. A few indicators were used to measure the diversity including extent of diversification (ED) (number of distinct FoR codes divided by the number of publications) and diversification ratio (DR) (ratio of the publications falling outside the dominant code to the total number of publications). A total of 47.76% of biologists publications, 35.23% of physicists publications and 20.36% of chemists publications were published in journals assigned to fields other than the Australian associate and full professors fields. Publications from biologists had the largest values of diversification. Women (compared with men) and associate professors (compared with full professors) in chemistry, biology and overall were more probably to publish diversely. ED was negatively correlated with output and citation impact. DR also had a negative but weak correlation with the number of publications and 10% most frequently cited paper."
Multimodal ensemble approach to identify and rank top-k influential nodes of scholarly literature using Twitter network,"Scholarly literature is an immense network of activities, linked via collaborations or information propagation. Analysing such network can be leveraged by harnessing rich semantic meaning of scholarly graph. Identifying and ranking top-k influential nodes from various domains of scholarly literature using social media data are still infancy. Social networking sites like Twitter provide an opportunity to create inventive graph-based measures to identify and rank influential nodes such as scholars, articles, journal, information spreading media and academic institutions of scholarly literature. Many network-based models such as centrality measures have been proposed to identify influential nodes. The empirical annotation shows that centrality measures for finding influential nodes are high in computational complexity. In addition, notion of these measures have high variance, which signifies an influential node deviation with change in application and nature of information flows in the network. The research aims to propose an ensemble learning approach based on multimodal majority voting influence (MMMVI) to identify and weighted multimodal ensemble average influence (WMMEAI) to rank top-k influential nodes in Twitter network data set of well-known three influential nodes, that is, academic institution, scholar and journal. The empirical analysis has been accomplished to learn practicability and efficiency of the proposed approaches when compared with state-of-the-art approaches. The experimental result shows that the ensemble approach using surface learning models (SLMs) can lead to better identification and ranking of influential nodes with low computational complexity. Â© The Author(s) 2019.","Scholarly literature is an immense network of activities, linked via collaborations or information propagation. Analysing such network can be leveraged by harnessing rich semantic meaning of scholarly graph. Identifying and ranking top-k influential nodes from various domains of scholarly literature using social media data are still infancy. Social networking sites like Twitter provide an opportunity to create inventive graph-based measures to identify and rank influential nodes such as scholars, articles, journal, information spreading media and academic institutions of scholarly literature. Many network-based models such as centrality measures have been proposed to identify influential nodes. The empirical annotation shows that centrality measures for finding influential nodes are high in computational complexity. In addition, notion of these measures have high variance, which signifies an influential node deviation with change in application and nature of information flows in the network. The research aims to propose an ensemble learning approach based on multimodal majority voting influence to identify and weighted multimodal ensemble average influence (WMMEAI) to rank top-k influential nodes in Twitter network data set of well-known three influential nodes, that is, academic institution, scholar and journal. The empirical analysis has been accomplished to learn practicability and efficiency of the proposed approaches when compared with state-of-the-art approaches. The experimental result shows that the ensemble approach using surface learning models (SLMs) can lead to better identification and ranking of influential nodes with low computational complexity."
A bibliometric analysis of topic modelling studies (2000â2017),"Topic modelling is a powerful text mining tool that has been applied in many fields such as software engineering, political and linguistic sciences. To evaluate the development of topic modelling studies, the present study reports a bibliometric analysis of SCIE, SSCI and A&HCI listed articles published from 2000 and 2017. Bibliometric indices for productive authors, countries and institutions are analysed. In addition, thematic changes concerning topic modelling are also examined. Results show that China plays a leading role in this field. Topic modelling has established itself as an important technique in not only natural and formal sciences but also social sciences. LDA, social networks and text analysis are the topics with increasing popularity, while certain models (e.g. pLSA) and applications (e.g. topic detection) are declining in popularity. The findings could help researchers optimise research topic choices, seek collaboration with appropriate partners and stay up-to-date with the development of the field. Â© The Author(s) 2019.","Topic modelling is a powerful text mining tool that has been applied in many fields such as software engineering, political and linguistic sciences. To evaluate the development of topic modelling studies, the present study reports a bibliometric analysis of SCIE, SSCI and A&HCI listed articles published from 2000 and 2017. Bibliometric indices for productive authors, countries and institutions are analysed. In addition, thematic changes concerning topic modelling are also examined. Results show that China plays a leading role in this field. Topic modelling has established itself as an important technique in not only natural and formal sciences but also social sciences. LDA, social networks and text analysis are the topics with increasing popularity, while certain models ( pLSA) and applications ( topic detection) are declining in popularity. The findings could help researchers optimise research topic choices, seek collaboration with appropriate partners and stay up-to-date with the development of the field."
A semantic web methodological framework to evaluate the support of integrity in thesaurus tools,"With the Semantic Web, thesauri regain a relevant role supporting semantic searches and other added-value services. Thesaurus standards define the constructs a thesaurus can have and the integrity rules it must comply with. Thesaurus editors can be helped in their work if thesaurus tools offer them support for integrity, warning when integrity rules are violated and/or helping them to correct these mistakes. The most recent thesaurus standard is ISO 25964, which supersedes ISO 2788, evolving towards concept-based thesauri and better aligned with the Semantic Web approach than the term-based thesauri of ISO 2788. However, the W3C recommendation for Knowledge Organisation System (KOS) representation in the semantic web context is Simple Knowledge Organisation Systems (SKOS), which is in fact prior to ISO 25964. This article focuses on thesaurus integrity and the evolution from ISO 2788 to ISO 25964. Its effect on integrity issues is analysed. A methodological proposal for evaluating integrity support in thesaurus tools, arising from the results of this work, is presented. Its target audience is professionals in charge of thesaurus editing. Besides being adapted to the most recent thesaurus standard, ISO 25964, it also includes the comparison of ISO standards with SKOS. The article concludes with the presentation of the results obtained by applying the framework to three thesaurus tools. Â© The Author(s) 2019.","With the Semantic Web, thesauri regain a relevant role supporting semantic searches and other added-value services. Thesaurus standards define the constructs a thesaurus can have and the integrity rules it must comply with. Thesaurus editors can be helped in their work if thesaurus tools offer them support for integrity, warning when integrity rules are violated and/or helping them to correct these mistakes. The most recent thesaurus standard is ISO 25964, which supersedes ISO 2788, evolving towards concept-based thesauri and better aligned with the Semantic Web approach than the term-based thesauri of ISO 2788. However, the W3C recommendation for Knowledge Organisation System (KOS) representation in the semantic web context is Simple Knowledge Organisation Systems (SKOS), which is in fact prior to ISO 25964. This article focuses on thesaurus integrity and the evolution from ISO 2788 to ISO 25964. Its effect on integrity issues is analysed. A methodological proposal for evaluating integrity support in thesaurus tools, arising from the results of this work, is presented. Its target audience is professionals in charge of thesaurus editing. Besides being adapted to the most recent thesaurus standard, ISO 25964, it also includes the comparison of ISO standards with SKOS. The article concludes with the presentation of the results obtained by applying the framework to three thesaurus tools."
A semantic-based video scene segmentation using a deep neural network,"Video scene segmentation is very important research in the field of computer vision, because it helps in efficient storage, indexing and retrieval of videos. Achieving this kind of scene segmentation cannot be done by just calculating the similarity of low-level features presented in the video; high-level features should also be considered to achieve a better performance. Even though much research has been conducted on video scene segmentation, most of these studies failed to semantically segment a video into scenes. Thus, in this study, we propose a Deep-learning Semantic-based Scene-segmentation model (called DeepSSS) that considers image captioning to segment a video into scenes semantically. First, the DeepSSS performs shot boundary detection by comparing colour histograms and then employs maximum-entropy-applied keyframe extraction. Second, for semantic analysis, using image captioning that benefits from deep learning generates a semantic text description of the keyframes. Finally, by comparing and analysing the generated texts, it assembles the keyframes into a scene grouped under a semantic narrative. That said, DeepSSS considers both low- and high-level features of videos to achieve a more meaningful scene segmentation. By applying DeepSSS to data sets from MS COCO for caption generation and evaluating its semantic scene-segmentation task results with the data sets from TRECVid 2016, we demonstrate quantitatively that DeepSSS outperforms other existing scene-segmentation methods using shot boundary detection and keyframes. Whatâs more, the experiments were done by comparing scenes segmented by humans and scene segmented by the DeepSSS. The results verified that the DeepSSSâ segmentation resembled that of humans. This is a new kind of result that was enabled by semantic analysis, which was impossible by just using low-level features of videos. Â© The Author(s) 2018.","Video scene segmentation is very important research in the field of computer vision, because it helps in efficient storage, indexing and retrieval of videos. Achieving this kind of scene segmentation cannot be done by just calculating the similarity of low-level features presented in the video; high-level features should also be considered to achieve a better performance. Even though much research has been conducted on video scene segmentation, most of these studies failed to semantically segment a video into scenes. Thus, in this study, we propose a Deep-learning Semantic-based Scene-segmentation model (called DeepSSS) that considers image captioning to segment a video into scenes semantically. First, the DeepSSS performs shot boundary detection by comparing colour histograms and then employs maximum-entropy-applied keyframe extraction. Second, for semantic analysis, using image captioning that benefits from deep learning generates a semantic text description of the keyframes. Finally, by comparing and analysing the generated texts, it assembles the keyframes into a scene grouped under a semantic narrative. That said, DeepSSS considers both low- and high-level features of videos to achieve a more meaningful scene segmentation. By applying DeepSSS to data sets from MS COCO for caption generation and evaluating its semantic scene-segmentation task results with the data sets from TRECVid 2016, we demonstrate quantitatively that DeepSSS outperforms other existing scene-segmentation methods using shot boundary detection and keyframes. Whats more, the experiments were done by comparing scenes segmented by humans and scene segmented by the DeepSSS. The results verified that the DeepSSS segmentation resembled that of humans. This is a new kind of result that was enabled by semantic analysis, which was impossible by just using low-level features of videos."
A novel approach to provenance management for privacy preservation,"Provenance determines the origin of the data by tracing and recording the actions that are performed on the data. Therefore, provenance is used in many fields to ensure the reliability and quality of data. In this work, provenance information is used to meet the security needs in information systems. For this purpose, a domain-independent provenance model is proposed. The proposed provenance model is based on the Open Provenance Model and Semantic Web technologies. The goal of the proposed provenance model is to integrate the provenance and security concepts in order to detect privacy violations by querying the provenance data. In order to evaluate the proposed provenance model, we illustrated our domain-independent model by integrating it with an infectious disease domain and implemented the Healthcare Provenance Information System. Â© The Author(s) 2019.","Provenance determines the origin of the data by tracing and recording the actions that are performed on the data. Therefore, provenance is used in many fields to ensure the reliability and quality of data. In this work, provenance information is used to meet the security needs in information systems. For this purpose, a domain-independent provenance model is proposed. The proposed provenance model is based on the Open Provenance Model and Semantic Web technologies. The goal of the proposed provenance model is to integrate the provenance and security concepts in order to detect privacy violations by querying the provenance data. In order to evaluate the proposed provenance model, we illustrated our domain-independent model by integrating it with an infectious disease domain and implemented the Healthcare Provenance Information System."
Using Bayesian networks with hidden variables for identifying trustworthy users in social networks,"The popularity and broad accessibility of online social networks (OSNs) have facilitated effective communication among people, but such networks also pose potential risks that should not be ignored. Interaction through OSNs is complex and can be unsafe, as individuals can be contacted by strangers at any time. This makes the notion of trust a crucial issue in the use of OSNs. However, compared with decision-making processes associated with whether to trust a stranger encountered in everyday life, this task is more difficult to address with regard to OSNs due to the lack of face-to-face communication and prior knowledge between people. In this article, trust evaluation is formalised as a classification problem. We demonstrate how user profiles and historical records can be organised into a logical structure based on Bayesian networks to recognise the trustworthy people without the need to build trust relationships in OSNs. This is possible when a more detailed description of features denoted by hidden variables is considered. We compare the performance of our method with those of six other machine learning methods using Facebook and Twitter datasets, and our results show that our method achieves higher values in accuracy, recall and F1 score. Â© The Author(s) 2019.","The popularity and broad accessibility of online social networks (OSNs) have facilitated effective communication among people, but such networks also pose potential risks that should not be ignored. Interaction through OSNs is complex and can be unsafe, as individuals can be contacted by strangers at any time. This makes the notion of trust a crucial issue in the use of OSNs. However, compared with decision-making processes associated with whether to trust a stranger encountered in everyday life, this task is more difficult to address with regard to OSNs due to the lack of face-to-face communication and prior knowledge between people. In this article, trust evaluation is formalised as a classification problem. We demonstrate how user profiles and historical records can be organised into a logical structure based on Bayesian networks to recognise the trustworthy people without the need to build trust relationships in OSNs. This is possible when a more detailed description of features denoted by hidden variables is considered. We compare the performance of our method with those of six other machine learning methods using Facebook and Twitter datasets, and our results show that our method achieves higher values in accuracy, recall and F1 score."
The classification of rumour standpoints in online social network based on combinatorial classifiers,"It is a fact that most of the rumours related to hot events or emergencies can be propagated rapidly on the hotbed of online social networks. In order to track the standpoints of the participants of rumour topics to regulate the development of rumour, we propose a multi-features model combining classifiers to classify the rumour standpoints, defined as classifying the standpoints of online social network conversations into one of âagreeâ, âdisagreeâ, âcommentâ or âqueryâ on previous comment about the rumour. Testing the performance of the combinatorial model â decision tree with adaptive boosting classifier and extremely randomised trees with adaptive boosting classifier â on different features, that is, structuring the weight matrix based on combination of term frequency (TF), inverse document frequency (IDF) and term frequency â inverse document frequency (TFIDF) method and constructing the features vector with Word2vec method. The experiments show that the combinatorial classifiers that exploit different combination features in the online social network conversations outperform binary classification; especially, the topology of the social network has a highly positive impact on the classification results. Furthermore, the âcommentâ and âqueryâ of rumour standpoints have a better classification effect based on the features of different categories. Â© The Author(s) 2019.","It is a fact that most of the rumours related to hot events or emergencies can be propagated rapidly on the hotbed of online social networks. In order to track the standpoints of the participants of rumour topics to regulate the development of rumour, we propose a multi-features model combining classifiers to classify the rumour standpoints, defined as classifying the standpoints of online social network conversations into one of agree, disagree, comment or query on previous comment about the rumour. Testing the performance of the combinatorial model decision tree with adaptive boosting classifier and extremely randomised trees with adaptive boosting classifier on different features, that is, structuring the weight matrix based on combination of term frequency (TF), inverse document frequency (IDF) and term frequency inverse document frequency (TFIDF) method and constructing the features vector with Word2vec method. The experiments show that the combinatorial classifiers that exploit different combination features in the online social network conversations outperform binary classification; especially, the topology of the social network has a highly positive impact on the classification results. Furthermore, the comment and query of rumour standpoints have a better classification effect based on the features of different categories."
How to identify the roots of broad research topics and fields? The introduction of RPYS sampling using the example of climate change research,"Since the introduction of the reference publication year spectroscopy (RPYS) method and the corresponding programme CRExplorer, many studies have been published revealing the historical roots of topics, fields and researchers. The application of the method was restricted up to now by the available memory of the computer used for running the CRExplorer. Thus, many users could not perform RPYS for broader research fields or topics. In this study, we present various sampling methods to solve this problem: random, systematic and cluster sampling. We introduce the script language of the CRExplorer that can be used to draw many samples from the population data set. Based on a large data set of publications from climate change research, we compare RPYS results using population data with RPYS results using different sampling techniques. From our comparison with the full RPYS (population spectrogram), we conclude that the cluster sampling performs worst and the systematic sampling performs best. The random sampling also performs very well but not as well as the systematic sampling. The study therefore demonstrates the fruitfulness of the sampling approach for applying RPYS. Â© The Author(s) 2019.","Since the introduction of the reference publication year spectroscopy (RPYS) method and the corresponding programme CRExplorer, many studies have been published revealing the historical roots of topics, fields and researchers. The application of the method was restricted up to now by the available memory of the computer used for running the CRExplorer. Thus, many users could not perform RPYS for broader research fields or topics. In this study, we present various sampling methods to solve this problem: random, systematic and cluster sampling. We introduce the script language of the CRExplorer that can be used to draw many samples from the population data set. Based on a large data set of publications from climate change research, we compare RPYS results using population data with RPYS results using different sampling techniques. From our comparison with the full RPYS (population spectrogram), we conclude that the cluster sampling performs worst and the systematic sampling performs best. The random sampling also performs very well but not as well as the systematic sampling. The study therefore demonstrates the fruitfulness of the sampling approach for applying RPYS."
Semantics-preserving optimisation of mapping multi-column key constraints for RDB to RDF transformation,"The relational database (RDB) to resource description framework (RDF) transformation is a major semantic information extraction method because most web data are managed by RDBs. Existing automatic RDB-to-RDF transformation methods generate RDF data without losing the semantics of original relational data. However, two major problems have been observed during the mapping of multi-column key constraints: repetitive data generation and semantic information loss. In this article, we propose an improved RDB-to-RDF transformation method that ensures mapping without the aforementioned problems. Optimised rules are defined to generate an accurate semantic data structure for a multi-column key constraint and to reduce repetitive constraint data. Experimental results show that the proposed method achieves better accuracy in transforming multi-column key constraints and generates compact semantic results without repetitive data. Â© The Author(s) 2020.","The relational database (RDB) to resource description framework (RDF) transformation is a major semantic information extraction method because most web data are managed by RDBs. Existing automatic RDB-to-RDF transformation methods generate RDF data without losing the semantics of original relational data. However, two major problems have been observed during the mapping of multi-column key constraints: repetitive data generation and semantic information loss. In this article, we propose an improved RDB-to-RDF transformation method that ensures mapping without the aforementioned problems. Optimised rules are defined to generate an accurate semantic data structure for a multi-column key constraint and to reduce repetitive constraint data. Experimental results show that the proposed method achieves better accuracy in transforming multi-column key constraints and generates compact semantic results without repetitive data."
Exploiting named entity recognition for improving syntactic-based web service discovery,"Web Services have become essential to the software industry as they represent reusable, remotely accessible functionality and data. Since Web Services must be discovered before being consumed, many discovery approaches applying classic Information Retrieval techniques, which store and process textual service descriptions, have arisen. These efforts are affected by term mismatch: a description relevant to a query can be retrieved only if they share many words. We present an approach to improve Web Service discoverability that automatically augments Web Service descriptions and can be used on top of such existing syntactic-based approaches. We exploit Named Entity Recognition to identify entities in descriptions and expand them with information from public text corpora, for example, Wikidata, mitigating term mismatch since it exploits both synonyms and hypernyms. We evaluated our approach together with classical syntactic-based service discovery approaches using a real 1274-service dataset, achieving up to 15.06% better Recall scores, and up to 17% Precision-at-1, 8% Precision-at-2 and 4% Precision-at-3. Â© The Author(s) 2018.","Web Services have become essential to the software industry as they represent reusable, remotely accessible functionality and data. Since Web Services must be discovered before being consumed, many discovery approaches applying classic Information Retrieval techniques, which store and process textual service descriptions, have arisen. These efforts are affected by term mismatch: a description relevant to a query can be retrieved only if they share many words. We present an approach to improve Web Service discoverability that automatically augments Web Service descriptions and can be used on top of such existing syntactic-based approaches. We exploit Named Entity Recognition to identify entities in descriptions and expand them with information from public text corpora, for example, Wikidata, mitigating term mismatch since it exploits both synonyms and hypernyms. We evaluated our approach together with classical syntactic-based service discovery approaches using a real 1274-service dataset, achieving up to 15.06% better Recall scores, and up to 17% Precision-at-1, 8% Precision-at-2 and 4% Precision-at-3."
The citation advantage for open access science journals with and without article processing charges,"In this study of access models, we compared citation performance in journals that do and do not levy article processing charges (APCs) as part of their business model. We used a sample of journals from the Directory of Open Access Journals (DOAJ) science class and its 13 subclasses and recorded four citation metrics: JIF, H-index, citations per publication (CPP) and quartile rank. We examined 1881 science journals indexed in DOAJ. Thomson Reuters Journal Citation Reports and Web of Science were used to extract JIF, H-index, CPP and quartile category. Overall, the JIF, H-index and CPP indicated that APC and non-APC open access (OA) journals had equal impact. Quartile category ranking indicated a difference in favour of APC journals. In each science subclass, we found significant differences between APC and non-APC journals in all citation metrics except for quartile rank. Discipline-related variations were observed in non-APC journals. Differences in the rank positions of scores in different groups identified citation advantages for non-APC journals in physiology, zoology, microbiology and geology, followed by botany, astronomy and general biology. Impact ranged from moderate to low in physics, chemistry, human anatomy, mathematics, general science and natural history. The results suggest that authors should consider field- and discipline-related differences in the OA citation advantage, especially when they are considering non-APC OA journals categorised in two or more subjects. This may encourage OA publishing at least in the science class. Â© The Author(s) 2019.","In this study of access models, we compared citation performance in journals that do and do not levy article processing charges (APCs) as part of their business model. We used a sample of journals from the Directory of Open Access Journals (DOAJ) science class and its 13 subclasses and recorded four citation metrics: JIF, H-index, citations per publication (CPP) and quartile rank. We examined 1881 science journals indexed in DOAJ. Thomson Reuters Journal Citation Reports and Web of Science were used to extract JIF, H-index, CPP and quartile category. Overall, the JIF, H-index and CPP indicated that APC and non-APC open access (OA) journals had equal impact. Quartile category ranking indicated a difference in favour of APC journals. In each science subclass, we found significant differences between APC and non-APC journals in all citation metrics except for quartile rank. Discipline-related variations were observed in non-APC journals. Differences in the rank positions of scores in different groups identified citation advantages for non-APC journals in physiology, zoology, microbiology and geology, followed by botany, astronomy and general biology. Impact ranged from moderate to low in physics, chemistry, human anatomy, mathematics, general science and natural history. The results suggest that authors should consider field- and discipline-related differences in the OA citation advantage, especially when they are considering non-APC OA journals categorised in two or more subjects. This may encourage OA publishing at least in the science class."
Word-embedding-based pseudo-relevance feedback for Arabic information retrieval,"Pseudo-relevance feedback (PRF) is a very effective query expansion approach, which reformulates queries by selecting expansion terms from top k pseudo-relevant documents. Although standard PRF models have been proven effective to deal with vocabulary mismatch between usersâ queries and relevant documents, expansion terms are selected without considering their similarity to the original query terms. In this article, we propose a method to incorporate word embedding (WE) similarity into PRF models for Arabic information retrieval (IR). The main idea is to select expansion terms using their distribution in the set of top pseudo-relevant documents along with their similarity to the original query terms. Experiments are conducted on the standard Arabic TREC 2001/2002 collection using three neural WE models. The obtained results show that our PRF extensions significantly outperform their baseline PRF models. Moreover, they enhanced the baseline IR model by 22% and 68% for the mean average precision (MAP) and the robustness index (RI), respectively. Â© The Author(s) 2018.","Pseudo-relevance feedback (PRF) is a very effective query expansion approach, which reformulates queries by selecting expansion terms from top k pseudo-relevant documents. Although standard PRF models have been proven effective to deal with vocabulary mismatch between users queries and relevant documents, expansion terms are selected without considering their similarity to the original query terms. In this article, we propose a method to incorporate word embedding (WE) similarity into PRF models for Arabic information retrieval (IR). The main idea is to select expansion terms using their distribution in the set of top pseudo-relevant documents along with their similarity to the original query terms. Experiments are conducted on the standard Arabic TREC 2001/2002 collection using three neural WE models. The obtained results show that our PRF extensions significantly outperform their baseline PRF models. Moreover, they enhanced the baseline IR model by 22% and 68% for the mean average precision (MAP) and the robustness index (RI), respectively."
Spatial information extraction from travel narratives: Analysing the notion of co-occurrence indicating closeness of tourist places,"Recent advancements in social media have generated a myriad of unstructured geospatial data. Travel narratives are among the richest sources of such spatial clues. They are also a reflection of writersâ interaction with places. One of the prevalent ways to model this interaction is a points of interest (POIs) graph depicting popular POIs and routes. A relevant notion is that frequent pairwise occurrences of POIs indicate their geographic proximity. This work presents an empirical interpretation of this theory and constructs spatially enriched POI graphs, a clear augmentation to popularity-based POI graphs. A triplet pattern, rule-based spatial relation extraction technique SpatRE is proposed and compared with standard relation extraction systems Ollie and Stanford OpenIE. A travel blogs data set is also contributed containing labelled spatial relations. The performance is further evaluated on SemEval 2013 benchmark data sets. Finally, spatially enriched POI graphs are qualitatively compared with TripAdvisor and Google Maps to visualise information accuracy. Â© The Author(s) 2019.","Recent advancements in social media have generated a myriad of unstructured geospatial data. Travel narratives are among the richest sources of such spatial clues. They are also a reflection of writers interaction with places. One of the prevalent ways to model this interaction is a points of interest (POIs) graph depicting popular POIs and routes. A relevant notion is that frequent pairwise occurrences of POIs indicate their geographic proximity. This work presents an empirical interpretation of this theory and constructs spatially enriched POI graphs, a clear augmentation to popularity-based POI graphs. A triplet pattern, rule-based spatial relation extraction technique SpatRE is proposed and compared with standard relation extraction systems Ollie and Stanford OpenIE. A travel blogs data set is also contributed containing labelled spatial relations. The performance is further evaluated on SemEval 2013 benchmark data sets. Finally, spatially enriched POI graphs are qualitatively compared with TripAdvisor and Google Maps to visualise information accuracy."
WeChat knowledge service system of university library based on SoLoMo: A holistic design framework,"In this study, we develop a WeChat knowledge service system (WKSS) in university library based on SoLoMo. The aim is to build a comprehensive, open, mobile and smart knowledge service environment. It can realise the interaction between the three users, library and knowledge, and promote the dissemination and sharing of knowledge. By referencing the Internet frontier concept SoLoMo, this study designs a new mobile smart service system, including the system architecture design, the content design and the data association design. Then, this study develops the system, including the running environment configuration, the development of workflow, the core module and the system implementation. This system enables the provision of accurate, specific and more personalised service to each user. It also includes a portable mobile terminal to increase the accuracy of context awareness and enhance user convenience. This study makes up for the shortcomings of the library and increases the functions of personalisation, mobility and intelligence. It extends the way of mobile service in libraries and provides readers with better library mobile services, which was liked by readers. Â© The Author(s) 2019.","In this study, we develop a WeChat knowledge service system (WKSS) in university library based on SoLoMo. The aim is to build a comprehensive, open, mobile and smart knowledge service environment. It can realise the interaction between the three users, library and knowledge, and promote the dissemination and sharing of knowledge. By referencing the Internet frontier concept SoLoMo, this study designs a new mobile smart service system, including the system architecture design, the content design and the data association design. Then, this study develops the system, including the running environment configuration, the development of workflow, the core module and the system implementation. This system enables the provision of accurate, specific and more personalised service to each user. It also includes a portable mobile terminal to increase the accuracy of context awareness and enhance user convenience. This study makes up for the shortcomings of the library and increases the functions of personalisation, mobility and intelligence. It extends the way of mobile service in libraries and provides readers with better library mobile services, which was liked by readers."
Twitter speaks: A case of national disaster situational awareness,"In recent years, we have been faced with a series of natural disasters causing a tremendous amount of financial, environmental and human losses. The unpredictable nature of natural disasters behaviour makes it hard to have a comprehensive situational awareness (SA) to support disaster management. Using opinion surveys is a traditional approach to analyse public concerns during natural disasters; however, this approach is limited, expensive and time-consuming. Luckily, the advent of social media has provided scholars with an alternative means of analysing public concerns. Social media enable users (people) to freely communicate their opinions and disperse information regarding current events including natural disasters. This research emphasises the value of social media analysis and proposes an analytical framework: Twitter Situational Awareness (TwiSA). This framework uses text mining methods including sentiment analysis and topic modelling to create a better SA for disaster preparedness, response and recovery. TwiSA has also effectively deployed on a large number of tweets and tracks the negative concerns of people during the 2015 South Carolina flood. Â© The Author(s) 2019.","In recent years, we have been faced with a series of natural disasters causing a tremendous amount of financial, environmental and human losses. The unpredictable nature of natural disasters behaviour makes it hard to have a comprehensive situational awareness (SA) to support disaster management. Using opinion surveys is a traditional approach to analyse public concerns during natural disasters; however, this approach is limited, expensive and time-consuming. Luckily, the advent of social media has provided scholars with an alternative means of analysing public concerns. Social media enable users (people) to freely communicate their opinions and disperse information regarding current events including natural disasters. This research emphasises the value of social media analysis and proposes an analytical framework: Twitter Situational Awareness (TwiSA). This framework uses text mining methods including sentiment analysis and topic modelling to create a better SA for disaster preparedness, response and recovery. TwiSA has also effectively deployed on a large number of tweets and tracks the negative concerns of people during the 2015 South Carolina flood."
A review of author name disambiguation techniques for the PubMed bibliographic database,"Author names in bibliographic databases often suffer from ambiguity owing to the same author appearing under different names and multiple authors possessing similar names. It creates difficulty in associating a scholarly work with the person who wrote it, thereby introducing inaccuracy in credit attribution, bibliometric analysis, search-by-author in a digital library and expert discovery. A plethora of techniques for disambiguation of author names has been proposed in the literature. In this article, we focus on the research efforts targeted to disambiguate author names specifically in the PubMed bibliographic database. We believe this concentrated review will be useful to the research community because it discusses techniques applied to a very large real database that is actively used worldwide. We make a comprehensive survey of the existing author name disambiguation (AND) approaches that have been applied to the PubMed database: we organise the approaches into a taxonomy; describe the major characteristics of each approach including its performance, strengths, and limitations; and perform a comparative analysis of them. We also identify the datasets from PubMed that are publicly available for researchers to evaluate AND algorithms. Finally, we outline a few directions for future work. Â© The Author(s) 2019.","Author names in bibliographic databases often suffer from ambiguity owing to the same author appearing under different names and multiple authors possessing similar names. It creates difficulty in associating a scholarly work with the person who wrote it, thereby introducing inaccuracy in credit attribution, bibliometric analysis, search-by-author in a digital library and expert discovery. A plethora of techniques for disambiguation of author names has been proposed in the literature. In this article, we focus on the research efforts targeted to disambiguate author names specifically in the PubMed bibliographic database. We believe this concentrated review will be useful to the research community because it discusses techniques applied to a very large real database that is actively used worldwide. We make a comprehensive survey of the existing author name disambiguation (AND) approaches that have been applied to the PubMed database: we organise the approaches into a taxonomy; describe the major characteristics of each approach including its performance, strengths, and limitations; and perform a comparative analysis of them. We also identify the datasets from PubMed that are publicly available for researchers to evaluate AND algorithms. Finally, we outline a few directions for future work."
An efficient attribute reduction algorithm using MapReduce,"Classical attribute reduction algorithms based on attribute significance initiate too many jobs (O(|C|2)) when they run in MapReduce. To improve the efficiencies of these algorithms, we proposed a novel reduction algorithm. Instead of focusing on attribute significance, the notion of a core attribute was applied to construct a new heuristic reduction algorithm, and only |C| jobs were considered to obtain a reduct. The algorithm only included two basic operations: compare and sort. The latter was optimised using the shuffle mechanism in MapReduce, which provided an efficient sorting ability for big data. In particular, we connected jobs in an iterative form to transfer the processing result of the former job to the latter job. Finally, experimental results demonstrated that the proposed attribute reduction algorithm was efficient and significantly improved upon the classical algorithms in runtime and number of jobs. Â© The Author(s) 2019.","Classical attribute reduction algorithms based on attribute significance initiate too many jobs (O(|C|2)) when they run in MapReduce. To improve the efficiencies of these algorithms, we proposed a novel reduction algorithm. Instead of focusing on attribute significance, the notion of a core attribute was applied to construct a new heuristic reduction algorithm, and only |C| jobs were considered to obtain a reduct. The algorithm only included two basic operations: compare and sort. The latter was optimised using the shuffle mechanism in MapReduce, which provided an efficient sorting ability for big data. In particular, we connected jobs in an iterative form to transfer the processing result of the former job to the latter job. Finally, experimental results demonstrated that the proposed attribute reduction algorithm was efficient and significantly improved upon the classical algorithms in runtime and number of jobs."
Analysis and shortcomings of e-recruitment systems: Towards a semantics-based approach addressing knowledge incompleteness and limited domain coverage,"The rapid development of the Internet has led to introducing new methods for e-recruitment and human resources management. These methods aim to systematically address the limitations of conventional recruitment procedures through incorporating natural language processing tools and semantics-based methods. In this context, for a given job post, applicant resumes (usually uploaded as free-text unstructured documents in different formats such as.pdf,.doc or.rtf) are matched/screened out using the conventional keyword-based model enriched by additional resources such as occupational categories and semantics-based techniques. Employing these techniques has proved to be effective in reducing the cost, time, and efforts required in traditional recruitment and candidate selection methods. However, bridging the skill gap - that is, the propensity to precisely detect and extract relevant skills in applicant resumes and job posts - and highlighting the hidden semantic dimensions encoded in applicant resumes are still challenging issues in the process of devising effective e-recruitment systems. This is due to the fact that resources exploited by current e-recruitment systems are obtained from generic domain-independent sources, therefore resulting in knowledge incompleteness and the lack of domain coverage. In this article, we review state-of-the-art e-recruitment approaches and highlight recent advancements in this domain. An e-recruitment framework addressing current shortcomings through the use of multiple cooperative semantic resources, feature extraction techniques and skill relatedness measures is detailed. An instantiation of the proposed framework is proposed and an experimental validation using a real-world recruitment dataset from two employment portals demonstrates the effectiveness of the proposed approach. Â© The Author(s) 2018.","The rapid development of the Internet has led to introducing new methods for e-recruitment and human resources management. These methods aim to systematically address the limitations of conventional recruitment procedures through incorporating natural language processing tools and semantics-based methods. In this context, for a given job post, applicant resumes (usually uploaded as free-text unstructured documents in different formats such as.pdf,.doc or.rtf) are matched/screened out using the conventional keyword-based model enriched by additional resources such as occupational categories and semantics-based techniques. Employing these techniques has proved to be effective in reducing the cost, time, and efforts required in traditional recruitment and candidate selection methods. However, bridging the skill gap - that is, the propensity to precisely detect and extract relevant skills in applicant resumes and job posts - and highlighting the hidden semantic dimensions encoded in applicant resumes are still challenging issues in the process of devising effective e-recruitment systems. This is due to the fact that resources exploited by current e-recruitment systems are obtained from generic domain-independent sources, therefore resulting in knowledge incompleteness and the lack of domain coverage. In this article, we review state-of-the-art e-recruitment approaches and highlight recent advancements in this domain. An e-recruitment framework addressing current shortcomings through the use of multiple cooperative semantic resources, feature extraction techniques and skill relatedness measures is detailed. An instantiation of the proposed framework is proposed and an experimental validation using a real-world recruitment dataset from two employment portals demonstrates the effectiveness of the proposed approach."
A similarity measure based on KullbackâLeibler divergence for collaborative filtering in sparse data,"In the neighbourhood-based collaborative filtering (CF) algorithms, a user similarity measure is used to find other users similar to an active user. Most of the existing user similarity measures rely on the co-rated items. However, there are not enough co-rated items in sparse dataset, which usually leads to poor prediction. In this article, a new similarity scheme is proposed, which breaks free of the constraint of the co-rated items. Moreover, an item similarity measure based on the KullbackâLeibler (KL) divergence is presented, which identifies the relation between items based on the probability density distribution of ratings. Since the item similarity based on KL divergence makes full use of all ratings, it owns better flexibility for sparse datasets. The CF algorithm using our proposed similarity scheme is implemented and compared with some classic CF algorithms. The compared results show that the CF using our similarity has better predictive performance. Therefore, our similarity scheme is a good solution for the sparsity problem and has great potential to be applied to recommendation systems. Â© The Author(s) 2018.","In the neighbourhood-based collaborative filtering (CF) algorithms, a user similarity measure is used to find other users similar to an active user. Most of the existing user similarity measures rely on the co-rated items. However, there are not enough co-rated items in sparse dataset, which usually leads to poor prediction. In this article, a new similarity scheme is proposed, which breaks free of the constraint of the co-rated items. Moreover, an item similarity measure based on the KullbackLeibler (KL) divergence is presented, which identifies the relation between items based on the probability density distribution of ratings. Since the item similarity based on KL divergence makes full use of all ratings, it owns better flexibility for sparse datasets. The CF algorithm using our proposed similarity scheme is implemented and compared with some classic CF algorithms. The compared results show that the CF using our similarity has better predictive performance. Therefore, our similarity scheme is a good solution for the sparsity problem and has great potential to be applied to recommendation systems."
OPPCAT: Ontology population from tabular data,"In order to present large amount of information on the Web to both users and machines, it is urgently needed to structure Web data. E-commerce is one of the areas where increasing data bottlenecks on the Web inhibit data access. Ontological display of the product information enables better product comparison and search applications using the semantics of the product specifications and their corresponding values. In this article, we present a framework called OPPCAT, which is used for semi-automatic ontology population from tabular data in e-commerce stores and product catalogues. As a result, OPPCAT allows tabular data to be used for mass production of ontology content. First, we present the common patterns in tabular data which obstruct semi-automatic production of ontologies. Then, we suggest solutions which automatically fix these errors. Finally, we define an algorithm to build ontology content semi-automatically. Â© The Author(s) 2019.","In order to present large amount of information on the Web to both users and machines, it is urgently needed to structure Web data. E-commerce is one of the areas where increasing data bottlenecks on the Web inhibit data access. Ontological display of the product information enables better product comparison and search applications using the semantics of the product specifications and their corresponding values. In this article, we present a framework called OPPCAT, which is used for semi-automatic ontology population from tabular data in e-commerce stores and product catalogues. As a result, OPPCAT allows tabular data to be used for mass production of ontology content. First, we present the common patterns in tabular data which obstruct semi-automatic production of ontologies. Then, we suggest solutions which automatically fix these errors. Finally, we define an algorithm to build ontology content semi-automatically."
Examining the classification and evolution of novice usersâ mental models of an academic database in the search task completion process,"The main task of this article is to develop a classification system for novice usersâ mental models of an academic database and to elaborate upon the evolution mechanisms of those mental models. In total, 83 undergraduate students, mainly sophomores, who were all novice users of the academic database China National Knowledge Infrastructure (CNKI), participated in the experimental study. Their mental models were measured from the diagrams or pictures and corresponding interpretations they produced to articulate their perceptions of CNKI at five time points. A bottom-up encoding approach and content analysis were used to analyse the research data. The results demonstrated that novice usersâ mental models of the academic database can be classified as either system-oriented or user-oriented perspectives. Six categories were identified in the system-oriented perspective, and three were identified in the user-oriented perspective. It was also found that the evolution of usersâ mental models can be facilitated by retrieval tasks, and it is noteworthy that task type can influence the evolution of usersâ mental models. Furthermore, the evolution process of usersâ mental models can be seen as learning behaviour, which includes a learning session and a forgetting session. Â© The Author(s) 2019.","The main task of this article is to develop a classification system for novice users mental models of an academic database and to elaborate upon the evolution mechanisms of those mental models. In total, 83 undergraduate students, mainly sophomores, who were all novice users of the academic database China National Knowledge Infrastructure (CNKI), participated in the experimental study. Their mental models were measured from the diagrams or pictures and corresponding interpretations they produced to articulate their perceptions of CNKI at five time points. A bottom-up encoding approach and content analysis were used to analyse the research data. The results demonstrated that novice users mental models of the academic database can be classified as either system-oriented or user-oriented perspectives. Six categories were identified in the system-oriented perspective, and three were identified in the user-oriented perspective. It was also found that the evolution of users mental models can be facilitated by retrieval tasks, and it is noteworthy that task type can influence the evolution of users mental models. Furthermore, the evolution process of users mental models can be seen as learning behaviour, which includes a learning session and a forgetting session."
An intrinsic evaluation of the Waterloo spam rankings of the ClueWeb09 and ClueWeb12 datasets,"The ClueWeb09 dataset and its successor, the ClueWeb12 dataset, are two of the largest collections of Web pages released by Text REtrieval Conference (TREC). The ClueWeb datasets were used in various tracks of TREC ran through 2009 to 2017. For every year, approximately 50 new queries are released and a pool of Web pages are judged against these queries by human assessors as relevant, non-relevant or spam. In this article, a ground truth for binary classification (spam vs non-spam) is constructed from Web pages that are judged as spam or relevant under the assumption that a Web page judged as relevant for any query cannot be spam. Based on this ground truth, we evaluate classification performances of the Waterloo spam rankings (Fusion, Britney, GroupX and UK2006), which have been traditionally used to identify and filter spam pages in retrieval systems. The experimental results in terms of the universal binary classification evaluation measures suggest that the Fusion (with threshold = 11%) is the best for the ClueWeb09 dataset. Analysis of the frequency distributions of relevant/spam documents over spam scores reveals that the GroupX is the most powerful at identifying relevant documents, whereas the Fusion is the most powerful at identifying spam documents. It is also confirmed that the effectiveness of the Fusion spam ranking of the ClueWeb12 dataset is not as good as that of the ClueWeb09. Â© The Author(s) 2019.","The ClueWeb09 dataset and its successor, the ClueWeb12 dataset, are two of the largest collections of Web pages released by Text REtrieval Conference (TREC). The ClueWeb datasets were used in various tracks of TREC ran through 2009 to 2017. For every year, approximately 50 new queries are released and a pool of Web pages are judged against these queries by human assessors as relevant, non-relevant or spam. In this article, a ground truth for binary classification (spam vs non-spam) is constructed from Web pages that are judged as spam or relevant under the assumption that a Web page judged as relevant for any query cannot be spam. Based on this ground truth, we evaluate classification performances of the Waterloo spam rankings (Fusion, Britney, GroupX and UK2006), which have been traditionally used to identify and filter spam pages in retrieval systems. The experimental results in terms of the universal binary classification evaluation measures suggest that the Fusion (with threshold = 11%) is the best for the ClueWeb09 dataset. Analysis of the frequency distributions of relevant/spam documents over spam scores reveals that the GroupX is the most powerful at identifying relevant documents, whereas the Fusion is the most powerful at identifying spam documents. It is also confirmed that the effectiveness of the Fusion spam ranking of the ClueWeb12 dataset is not as good as that of the ClueWeb09."
Karyon: A scalable and easy to integrate ontology summarisation framework,"In the current Semantic Web Community, as the size and complexity of ontologies increase, ontology summarisation is becoming more important. There are many studies in the literature that use different approaches and metrics. However, many of these studies are not effective in terms of performance or have integration issues with current technologies. In this study, the popular ontology summarisation metrics are examined focusing on their performance in terms of time, and a number of metrics have been selected accordingly. To increase the accuracy of selections made with chosen metrics, we propose a novel metric: âname inclusionâ. This metric promotes a concept if its name is subsumed by the name of another concept. As the existing summarisation applications have integration issues, we have implemented our summarisation framework to integrate easily with the latest web technologies. Therefore, the algorithm is implemented using Rust language, which performs well and easily integrates with other languages. Â© The Author(s) 2019.","In the current Semantic Web Community, as the size and complexity of ontologies increase, ontology summarisation is becoming more important. There are many studies in the literature that use different approaches and metrics. However, many of these studies are not effective in terms of performance or have integration issues with current technologies. In this study, the popular ontology summarisation metrics are examined focusing on their performance in terms of time, and a number of metrics have been selected accordingly. To increase the accuracy of selections made with chosen metrics, we propose a novel metric: name inclusion. This metric promotes a concept if its name is subsumed by the name of another concept. As the existing summarisation applications have integration issues, we have implemented our summarisation framework to integrate easily with the latest web technologies. Therefore, the algorithm is implemented using Rust language, which performs well and easily integrates with other languages."
A model-based method to improve the quality of ranking in keyword search systems using pseudo-relevance feedback,"Keyword search has been known as an attractive search over databases. One of the challenges in keyword search is to rank the answers (subgraphs) of a keyword query in order to their relevance to the query. Most of the previous studies in this area are highly heuristic ranking functions which were proposed based on a confined analysis of the characteristics of answers. These functions usually reveal serious effectiveness problems due to their failure in recognising the conceptual differences between relatively similar answers. In this article, we propose a novel model-based method to improve the ranking accuracy of answers to a keyword query using pseudo-relevance feedback. The proposed method is built upon a carefully designed model for an answer called Structure-Aware Relevance Model which is estimated based on the textual and structural characteristics of the answer. In this article, we also study how to effectively select from feedback answers those words that are focused on the query topic based on placement and importance of words in the nodes of feedback answers. Extensive experiments conducted on a standard evaluation framework with three real-world datasets confirm the effectiveness of the proposed method. Â© The Author(s) 2018.","Keyword search has been known as an attractive search over databases. One of the challenges in keyword search is to rank the answers (subgraphs) of a keyword query in order to their relevance to the query. Most of the previous studies in this area are highly heuristic ranking functions which were proposed based on a confined analysis of the characteristics of answers. These functions usually reveal serious effectiveness problems due to their failure in recognising the conceptual differences between relatively similar answers. In this article, we propose a novel model-based method to improve the ranking accuracy of answers to a keyword query using pseudo-relevance feedback. The proposed method is built upon a carefully designed model for an answer called Structure-Aware Relevance Model which is estimated based on the textual and structural characteristics of the answer. In this article, we also study how to effectively select from feedback answers those words that are focused on the query topic based on placement and importance of words in the nodes of feedback answers. Extensive experiments conducted on a standard evaluation framework with three real-world datasets confirm the effectiveness of the proposed method."
Effect of knowledge management on software product experience with mediating effect of perceived software process improvement: An empirical study for Indian software industry,"The software development industry is characterised by swift innovation and competition. To survive, software engineering (SE) organisations need to develop high-quality software products in a timely fashion and at low cost. Knowledge-based approaches to software development are extremely supportive to acquiring new knowledge and leveraging existing knowledge from software projects; this enables constant improvement of software development practices. In this empirical study of Indian SE organisations, we study the impact of managing knowledge for perceived software process improvement (PSPI) and its effect on software product quality. Information technology (IT) in knowledge management (KM) is an important facilitator for any SE organisation desiring to exploit evolving technologies for management of their knowledge assets and for carrying out various KM processes of knowledge capture, storage, retrieval and sharing. Surveys collected from Indian SE organisations were analysed to propose a model using a structured equation modelling (SEM) technique. Our findings reveal that the relation between KM and quality of software product is positively mediated by PSPI. These findings reinforce an arena that is of growing importance to researchers and practitioners and which has seen only a limited number of empirical studies to date in the context of Indian SE organisations. Â© The Author(s) 2019.","The software development industry is characterised by swift innovation and competition. To survive, software engineering (SE) organisations need to develop high-quality software products in a timely fashion and at low cost. Knowledge-based approaches to software development are extremely supportive to acquiring new knowledge and leveraging existing knowledge from software projects; this enables constant improvement of software development practices. In this empirical study of Indian SE organisations, we study the impact of managing knowledge for perceived software process improvement (PSPI) and its effect on software product quality. Information technology (IT) in knowledge management (KM) is an important facilitator for any SE organisation desiring to exploit evolving technologies for management of their knowledge assets and for carrying out various KM processes of knowledge capture, storage, retrieval and sharing. Surveys collected from Indian SE organisations were analysed to propose a model using a structured equation modelling (SEM) technique. Our findings reveal that the relation between KM and quality of software product is positively mediated by PSPI. These findings reinforce an arena that is of growing importance to researchers and practitioners and which has seen only a limited number of empirical studies to date in the context of Indian SE organisations."
"Text classification for cognitive domains: A case using lexical, syntactic and semantic features","Various automated classifiers have been implemented to categorise learning-related texts into cognitive domains. However, existing studies have applied limited linguistic features, and most have focused on texts written in English, with little attention given to Chinese. This study has tried to fill the gaps by applying a comprehensive set of features that have rarely been used collectively in previous research, with a focus on Chinese analytical texts. Experiments were conducted for classifier learning and evaluation, where a feature selection procedure significantly improved the classification performance. The results showed that different types of features complemented each other in forming strong collective representations of the original texts, and the discriminant nature of the features can be reasonably explained by language usage phenomena. The proposed approach could potentially be applied to other datasets of analytical writings involving cognitive domains, and the text features explored could be reused and further refined in future studies. Â© The Author(s) 2018.","Various automated classifiers have been implemented to categorise learning-related texts into cognitive domains. However, existing studies have applied limited linguistic features, and most have focused on texts written in English, with little attention given to Chinese. This study has tried to fill the gaps by applying a comprehensive set of features that have rarely been used collectively in previous research, with a focus on Chinese analytical texts. Experiments were conducted for classifier learning and evaluation, where a feature selection procedure significantly improved the classification performance. The results showed that different types of features complemented each other in forming strong collective representations of the original texts, and the discriminant nature of the features can be reasonably explained by language usage phenomena. The proposed approach could potentially be applied to other datasets of analytical writings involving cognitive domains, and the text features explored could be reused and further refined in future studies."
iLDA: An interactive latent Dirichlet allocation model to improve topic quality,"User-generated content has been an increasingly important data source for analysing user interests in both industries and academic research. Since the proposal of the basic latent Dirichlet allocation (LDA) model, plenty of LDA variants have been developed to learn knowledge from unstructured user-generated contents. An intractable limitation for LDA and its variants is that low-quality topics whose meanings are confusing may be generated. To handle this problem, this article proposes an interactive strategy to generate high-quality topics with clear meanings by integrating subjective knowledge derived from human experts and objective knowledge learned by LDA. The proposed interactive latent Dirichlet allocation (iLDA) model develops deterministic and stochastic approaches to obtain subjective topic-word distribution from human experts, combines the subjective and objective topic-word distributions by a linear weighted-sum method, and provides the inference process to draw topics and words from a comprehensive topic-word distribution. The proposed model is a significant effort to integrate human knowledge with LDA-based models by interactive strategy. The experiments on two real-world corpora show that the proposed iLDA model can draw high-quality topics with the assistance of subjective knowledge from human experts. It is robust under various conditions and offers fundamental supports for the applications of LDA-based topic modelling. Â© The Author(s) 2019.","User-generated content has been an increasingly important data source for analysing user interests in both industries and academic research. Since the proposal of the basic latent Dirichlet allocation (LDA) model, plenty of LDA variants have been developed to learn knowledge from unstructured user-generated contents. An intractable limitation for LDA and its variants is that low-quality topics whose meanings are confusing may be generated. To handle this problem, this article proposes an interactive strategy to generate high-quality topics with clear meanings by integrating subjective knowledge derived from human experts and objective knowledge learned by LDA. The proposed interactive latent Dirichlet allocation (iLDA) model develops deterministic and stochastic approaches to obtain subjective topic-word distribution from human experts, combines the subjective and objective topic-word distributions by a linear weighted-sum method, and provides the inference process to draw topics and words from a comprehensive topic-word distribution. The proposed model is a significant effort to integrate human knowledge with LDA-based models by interactive strategy. The experiments on two real-world corpora show that the proposed iLDA model can draw high-quality topics with the assistance of subjective knowledge from human experts. It is robust under various conditions and offers fundamental supports for the applications of LDA-based topic modelling."
Data hiding technique in steganography for information security using number theory,"In the current era, due to the widespread availability of the Internet, it is extremely easy for people to communicate and share multimedia contents with each other. However, at the same time, secure transfer of personal and copyrighted material has become a critical issue. Consequently, secure means of data transfer are the most urgent need of the time. Steganography is the science and art of protecting the secret data from an unauthorised access. The steganographic approaches conceal secret data into a cover file of type audio, video, text and/or image. The actual challenge in steganography is to achieve high robustness and capacity without bargaining on the imperceptibility of the cover file. In this article, an efficient steganography method is proposed for the transfer of secret data in digital images using number theory. For this purpose, the proposed method represents the cover image using the Fibonacci sequence. The representation of an image in the Fibonacci sequence allows increasing the bit planes from 8-bit to 12-bit planes. The experimental results of the proposed method in comparison with other existing steganographic methods exhibit that our method not only achieves high embedding of secret data but also gives high quality of stego images in terms of peak signal-to-noise ratio (PSNR). Furthermore, the robustness of the technique is also evaluated in the presence of salt and pepper noise attack on the cover images. Â© The Author(s) 2018.","In the current era, due to the widespread availability of the Internet, it is extremely easy for people to communicate and share multimedia contents with each other. However, at the same time, secure transfer of personal and copyrighted material has become a critical issue. Consequently, secure means of data transfer are the most urgent need of the time. Steganography is the science and art of protecting the secret data from an unauthorised access. The steganographic approaches conceal secret data into a cover file of type audio, video, text and/or image. The actual challenge in steganography is to achieve high robustness and capacity without bargaining on the imperceptibility of the cover file. In this article, an efficient steganography method is proposed for the transfer of secret data in digital images using number theory. For this purpose, the proposed method represents the cover image using the Fibonacci sequence. The representation of an image in the Fibonacci sequence allows increasing the bit planes from 8-bit to 12-bit planes. The experimental results of the proposed method in comparison with other existing steganographic methods exhibit that our method not only achieves high embedding of secret data but also gives high quality of stego images in terms of peak signal-to-noise ratio (PSNR). Furthermore, the robustness of the technique is also evaluated in the presence of salt and pepper noise attack on the cover images."
An effective social recommendation method based on user reputation model and rating profile enhancement,"Trust-aware recommender systems are advanced approaches which have been developed based on social information to provide relevant suggestions to users. These systems can alleviate cold start and data sparsity problems in recommendation methods through trust relations. However, the lack of sufficient trust information can reduce the efficiency of these methods. Moreover, diversity and novelty are important measures for providing more attractive suggestions to users. In this article, a reputation-based approach is proposed to improve trust-aware recommender systems by enhancing rating profiles of the users who have insufficient ratings and trust information. In particular, we use a user reliability measure to determine the effectiveness of the rating profiles and trust networks of users in predicting unseen items. Then, a novel user reputation model is introduced based on the combination of the rating profiles and trust networks. The main idea of the proposed method is to enhance the rating profiles of the users who have low user reliability measure by adding a number of virtual ratings. To this end, the proposed user reputation model is used to predict the virtual ratings. In addition, the diversity, novelty and reliability measures of items are considered in the proposed rating profile enhancement mechanism. Therefore, the proposed method can improve the recommender systems about the cold start and data sparsity problems and also the diversity, novelty and reliability measures. Experimental results based on three real-world datasets show that the proposed method achieves higher performance than other recommendation methods. Â© The Author(s) 2018.","Trust-aware recommender systems are advanced approaches which have been developed based on social information to provide relevant suggestions to users. These systems can alleviate cold start and data sparsity problems in recommendation methods through trust relations. However, the lack of sufficient trust information can reduce the efficiency of these methods. Moreover, diversity and novelty are important measures for providing more attractive suggestions to users. In this article, a reputation-based approach is proposed to improve trust-aware recommender systems by enhancing rating profiles of the users who have insufficient ratings and trust information. In particular, we use a user reliability measure to determine the effectiveness of the rating profiles and trust networks of users in predicting unseen items. Then, a novel user reputation model is introduced based on the combination of the rating profiles and trust networks. The main idea of the proposed method is to enhance the rating profiles of the users who have low user reliability measure by adding a number of virtual ratings. To this end, the proposed user reputation model is used to predict the virtual ratings. In addition, the diversity, novelty and reliability measures of items are considered in the proposed rating profile enhancement mechanism. Therefore, the proposed method can improve the recommender systems about the cold start and data sparsity problems and also the diversity, novelty and reliability measures. Experimental results based on three real-world datasets show that the proposed method achieves higher performance than other recommendation methods."
Finding top performers through email patterns analysis,"In the information economy, individualsâ work performance is closely associated with their digital communication strategies. This study combines social network and semantic analysis to develop a method to identify top performers based on email communication. By reviewing existing literature, we identified the indicators that quantify email communication into measurable dimensions. To empirically examine the predictive power of the proposed indicators, we collected 2 million email archive of 578 executives in an international service company. Panel regression was employed to derive interpretable association between email indicators and top performance. The results suggest that top performers tend to assume central network positions and have high responsiveness to emails. In email contents, top performers use more positive and complex language, with low emotionality, but rich in influential words that are probably reused by co-workers. To better explore the predictive power of the email indicators, we employed AdaBoost machine learning models, which achieved 83.56% accuracy in identifying top performers. With cluster analysis, we further find three categories of top performers, ânetworkersâ with central network positions, âinfluencersâ with influential ideas and âpositivistsâ with positive sentiments. The findings suggest that top performers have distinctive email communication patterns, laying the foundation for grounding email communication competence in theory. The proposed email analysis method also provides a tool to evaluate the different types of individual communication styles. Â© The Author(s) 2019.","In the information economy, individuals work performance is closely associated with their digital communication strategies. This study combines social network and semantic analysis to develop a method to identify top performers based on email communication. By reviewing existing literature, we identified the indicators that quantify email communication into measurable dimensions. To empirically examine the predictive power of the proposed indicators, we collected 2 million email archive of 578 executives in an international service company. Panel regression was employed to derive interpretable association between email indicators and top performance. The results suggest that top performers tend to assume central network positions and have high responsiveness to emails. In email contents, top performers use more positive and complex language, with low emotionality, but rich in influential words that are probably reused by co-workers. To better explore the predictive power of the email indicators, we employed AdaBoost machine learning models, which achieved 83.56% accuracy in identifying top performers. With cluster analysis, we further find three categories of top performers, networkers with central network positions, influencers with influential ideas and positivists with positive sentiments. The findings suggest that top performers have distinctive email communication patterns, laying the foundation for grounding email communication competence in theory. The proposed email analysis method also provides a tool to evaluate the different types of individual communication styles."
Aspect-based summarisation using distributed clustering and single-objective optimisation,"In the user reviews of various domains, there is an increase in the accumulation of reviews in the web that presents a lot of difficulties to the readers. So it becomes necessary to generate a summary which represents the entire review in a concise manner. It is required for each feature or aspect in the reviews for the ease of users. The aspect-based summarisation plays a vital role in the field of opinion mining. This article proposes an aspect summarisation framework using sentence scoring clustering and weight-based single-objective optimisation technique by utilising evolutionary algorithm. The system uses MapReduce framework to incorporate the proposed combinerâbased optimised clustering approach. Then a novel single-objective optimisation with genetic algorithm is developed. Its purpose is to retrieve top sentences from each cluster to generate feature-based summary. The accuracy of the system-generated summary is evaluated using the Recall Oriented Understanding for Gisting Evaluation tool kit using human standard reference summaries. The system is able to achieve more promising results when compared with other standard featureâbased summarisation systems. Â© The Author(s) 2019.","In the user reviews of various domains, there is an increase in the accumulation of reviews in the web that presents a lot of difficulties to the readers. So it becomes necessary to generate a summary which represents the entire review in a concise manner. It is required for each feature or aspect in the reviews for the ease of users. The aspect-based summarisation plays a vital role in the field of opinion mining. This article proposes an aspect summarisation framework using sentence scoring clustering and weight-based single-objective optimisation technique by utilising evolutionary algorithm. The system uses MapReduce framework to incorporate the proposed combinerbased optimised clustering approach. Then a novel single-objective optimisation with genetic algorithm is developed. Its purpose is to retrieve top sentences from each cluster to generate feature-based summary. The accuracy of the system-generated summary is evaluated using the Recall Oriented Understanding for Gisting Evaluation tool kit using human standard reference summaries. The system is able to achieve more promising results when compared with other standard featurebased summarisation systems."
"Factors influencing the information needs and information access channels of farmers: An empirical study in Guangdong, China","Information plays an important role in meeting the quantitative and qualitative goals of agriculture in the 21st century. As an emerging economy in a developing continent, China has already made many interventions to use information technology to support agricultural development. However, information service in some rural areas is still severely limited. The overall impact of the changing information environment on the farmersâ information needs and access channels has not been fully studied. Thus, this study systematically investigates the characteristics of the information needs and channels of farmers in Guangdong, China. We have collected 4006 questionnaire samples and used correlation analysis to explore the relationships between farmersâ information needs and access channel preferences. The results indicate that individual characteristic factors, social factors and family factors have different degrees of influence on farmersâ information needs and access channel preferences. These findings can provide a reference for information construction in the rural areas of Guangdong Province and thus promote its economic development. This study can also provide useful insights for policy-makers and researchers from other developing countries to formulate implementation plans to promote agricultural development. Â© The Author(s) 2019.","Information plays an important role in meeting the quantitative and qualitative goals of agriculture in the 21st century. As an emerging economy in a developing continent, China has already made many interventions to use information technology to support agricultural development. However, information service in some rural areas is still severely limited. The overall impact of the changing information environment on the farmers information needs and access channels has not been fully studied. Thus, this study systematically investigates the characteristics of the information needs and channels of farmers in Guangdong, China. We have collected 4006 questionnaire samples and used correlation analysis to explore the relationships between farmers information needs and access channel preferences. The results indicate that individual characteristic factors, social factors and family factors have different degrees of influence on farmers information needs and access channel preferences. These findings can provide a reference for information construction in the rural areas of Guangdong Province and thus promote its economic development. This study can also provide useful insights for policy-makers and researchers from other developing countries to formulate implementation plans to promote agricultural development."
Loops in publication citation networks,"Traditionally, publication citation networks are regarded as acyclic, that is, no loops in the network as an earlier published article cannot cite a later published article. However, due to the accessibility of pre-print versions of articles, there might be some loops in a publication citation network. This article presents a descriptive statistic on loops in publication citation networks of computer science and physics by employing a network-based indicator, namely, strongly connected component (SCC). By employing computer science and physics disciplines publications from the Web of Science database as examples, this article examines the count of loops, how the count changes over time and how the count relates to the published year difference between publications within the loop in the citation network. Some common structural patterns are also extracted and analysed; we observe that the two disciplines share the most frequent patterns though there exist some minor differences. Moreover, we find that self-citations in terms of authors, authorsâ institutions and journals contribute to the formation of loops in publication citation networks. Â© The Author(s) 2019.","Traditionally, publication citation networks are regarded as acyclic, that is, no loops in the network as an earlier published article cannot cite a later published article. However, due to the accessibility of pre-print versions of articles, there might be some loops in a publication citation network. This article presents a descriptive statistic on loops in publication citation networks of computer science and physics by employing a network-based indicator, namely, strongly connected component (SCC). By employing computer science and physics disciplines publications from the Web of Science database as examples, this article examines the count of loops, how the count changes over time and how the count relates to the published year difference between publications within the loop in the citation network. Some common structural patterns are also extracted and analysed; we observe that the two disciplines share the most frequent patterns though there exist some minor differences. Moreover, we find that self-citations in terms of authors, authors institutions and journals contribute to the formation of loops in publication citation networks."
Twitter sentiment analysis using fuzzy integral classifier fusion,"A thorough analysis of peopleâs sentiment about a business, an event or an individual is necessary for business development, event analysis and popularity assessment. Social networks are rich sources of obtaining user opinions about people, events and products. Sentiment analysis conducted using multiple user comments and messages on microblogs is an interesting field of data mining and natural language processing (NLP). Different techniques and algorithms have recently been developed for conducting sentiment analysis on Twitter. Different proposed classification and pure NLP-based methods have different behaviours in predicting sentiment orientation. In this study, we combined the results of the classic classifiers and NLP-based methods to propose a new approach for Twitter sentiment analysis. The proposed method uses a fuzzy measure for determining the importance of each classifier to make the final decision. Fuzzy measures are used with the Choquet fuzzy integral for fusing the classifier outputs in order to generate the final label. Our experiments with different Twitter sentiment datasets show that fuzzy integral-based classifier fusion improves the average accuracy of sentiment classification. Â© The Author(s) 2019.","A thorough analysis of peoples sentiment about a business, an event or an individual is necessary for business development, event analysis and popularity assessment. Social networks are rich sources of obtaining user opinions about people, events and products. Sentiment analysis conducted using multiple user comments and messages on microblogs is an interesting field of data mining and natural language processing (NLP). Different techniques and algorithms have recently been developed for conducting sentiment analysis on Twitter. Different proposed classification and pure NLP-based methods have different behaviours in predicting sentiment orientation. In this study, we combined the results of the classic classifiers and NLP-based methods to propose a new approach for Twitter sentiment analysis. The proposed method uses a fuzzy measure for determining the importance of each classifier to make the final decision. Fuzzy measures are used with the Choquet fuzzy integral for fusing the classifier outputs in order to generate the final label. Our experiments with different Twitter sentiment datasets show that fuzzy integral-based classifier fusion improves the average accuracy of sentiment classification."
Concept-LDA: Incorporating Babelfy into LDA for aspect extraction,"Latent Dirichlet allocation (LDA) is one of the probabilistic topic models; it discovers the latent topic structure in a document collection. The basic assumption under LDA is that documents are viewed as a probabilistic mixture of latent topics; a topic has a probability distribution over words and each document is modelled on the basis of a bag-of-words model. The topic models such as LDA are sufficient in learning hidden topics but they do not take into account the deeper semantic knowledge of a document. In this article, we propose a novel method based on topic modelling to determine the latent aspects of online review documents. In the proposed model, which is called Concept-LDA, the feature space of reviews is enriched with the concepts and named entities, which are extracted from Babelfy to obtain topics that contain not only co-occurred words but also semantically related words. The performance in terms of topic coherence and topic quality is reported over 10 publicly available datasets, and it is demonstrated that Concept-LDA achieves better topic representations than an LDA model alone, as measured by topic coherence and F-measure. The learned topic representation by Concept-LDA leads to accurate and an easy aspect extraction task in an aspect-based sentiment analysis system. Â© The Author(s) 2019.","Latent Dirichlet allocation (LDA) is one of the probabilistic topic models; it discovers the latent topic structure in a document collection. The basic assumption under LDA is that documents are viewed as a probabilistic mixture of latent topics; a topic has a probability distribution over words and each document is modelled on the basis of a bag-of-words model. The topic models such as LDA are sufficient in learning hidden topics but they do not take into account the deeper semantic knowledge of a document. In this article, we propose a novel method based on topic modelling to determine the latent aspects of online review documents. In the proposed model, which is called Concept-LDA, the feature space of reviews is enriched with the concepts and named entities, which are extracted from Babelfy to obtain topics that contain not only co-occurred words but also semantically related words. The performance in terms of topic coherence and topic quality is reported over 10 publicly available datasets, and it is demonstrated that Concept-LDA achieves better topic representations than an LDA model alone, as measured by topic coherence and F-measure. The learned topic representation by Concept-LDA leads to accurate and an easy aspect extraction task in an aspect-based sentiment analysis system."
Document recommendation based on the analysis of group trust and user weightings,"Collaborative filtering (CF) has been applied in various domains to resolve problems related to information overload. In a knowledge-intensive environment, most works are processed through teamwork. A user on a team can reference task-related documents from other trusted members to support work on the task. However, the traditional personalised recommender systems no longer meet the demand of teams or groups. Therefore, this work proposes a novel document recommendation method based on a group-based trust model. Our method will analyse the degrees of trust among users in a group and then identify the trustworthy users. The proposed group trust consists of a hybrid personal trust (HPT) model and usersâ importance (i.e. usersâ activity, similarity and reputation) in a group. Group-based trust is then integrated with the user-based CF to recommend documents to users. The experiments demonstrate that the proposed method can provide better performance than other trust-based recommendation methods; it not only obtains reliable trust values to increase the accuracy of predictions but also enhances the recommendation quality. Â© The Author(s) 2019.","Collaborative filtering (CF) has been applied in various domains to resolve problems related to information overload. In a knowledge-intensive environment, most works are processed through teamwork. A user on a team can reference task-related documents from other trusted members to support work on the task. However, the traditional personalised recommender systems no longer meet the demand of teams or groups. Therefore, this work proposes a novel document recommendation method based on a group-based trust model. Our method will analyse the degrees of trust among users in a group and then identify the trustworthy users. The proposed group trust consists of a hybrid personal trust (HPT) model and users importance ( users activity, similarity and reputation) in a group. Group-based trust is then integrated with the user-based CF to recommend documents to users. The experiments demonstrate that the proposed method can provide better performance than other trust-based recommendation methods; it not only obtains reliable trust values to increase the accuracy of predictions but also enhances the recommendation quality."
Automatically refining synonym extraction results: Cleaning and ranking,"Synonyms are crucial resources for many semantic applications, and the issue of synonym extraction has been studied extensively. However, extraction accuracy still cannot meet the practical demands. In addition, manually refining extraction results is time consuming. This article focuses on refining synonym extraction results by cleaning and ranking. A new graph model, the synonym graph, is proposed for the purpose of transforming the synonym extraction result of each word into a directed graph. Following this, two approaches for refining synonym extraction results are proposed based on the synonym graph. The first approach divides each extraction result into two parts â synonyms and noise â and detects noise by analysing the connectivity of the synonym graph. The second approach ranks the words in each extraction result by computing their semantic distance in the synonym graph. This approach was found to be more flexible than the first. The results of the experiments conducted in this study indicate that the performance of both of our proposed approaches is effective. In particular, they were found to perform well with datasets containing large synonym extraction results, which is important to reducing the cost of refining. Â© The Author(s) 2018.","Synonyms are crucial resources for many semantic applications, and the issue of synonym extraction has been studied extensively. However, extraction accuracy still cannot meet the practical demands. In addition, manually refining extraction results is time consuming. This article focuses on refining synonym extraction results by cleaning and ranking. A new graph model, the synonym graph, is proposed for the purpose of transforming the synonym extraction result of each word into a directed graph. Following this, two approaches for refining synonym extraction results are proposed based on the synonym graph. The first approach divides each extraction result into two parts synonyms and noise and detects noise by analysing the connectivity of the synonym graph. The second approach ranks the words in each extraction result by computing their semantic distance in the synonym graph. This approach was found to be more flexible than the first. The results of the experiments conducted in this study indicate that the performance of both of our proposed approaches is effective. In particular, they were found to perform well with datasets containing large synonym extraction results, which is important to reducing the cost of refining."
A cohort study of how faculty in LIS schools perceive and engage with open-access publishing,"This article presents results from a survey of faculty in North American Library and Information Studies (LIS) schools about their attitudes towards and experience with open-access publishing. As a follow-up to a similar survey conducted in 2013, the article also outlines the differences in beliefs about and engagement with open access that have occurred between 2013 and 2018. Although faculty in LIS schools are proponents of free access to research, journal publication choices remain informed by traditional considerations such as prestige and impact factor. Engagement with open access has increased significantly, while perceptions of open access have remained relatively stable between 2013 and 2018. Nonetheless, those faculty who have published in an open-access journal or are more knowledgeable about open access tend to be more convinced about the quality of open-access publications and less apprehensive about open-access publishing than those who have no publishing experience with open-access journals or who are less knowledgeable about various open-access modalities. Willingness to comply with gold open-access mandates has increased significantly since 2013. Â© The Author(s) 2019.","This article presents results from a survey of faculty in North American Library and Information Studies (LIS) schools about their attitudes towards and experience with open-access publishing. As a follow-up to a similar survey conducted in 2013, the article also outlines the differences in beliefs about and engagement with open access that have occurred between 2013 and 2018. Although faculty in LIS schools are proponents of free access to research, journal publication choices remain informed by traditional considerations such as prestige and impact factor. Engagement with open access has increased significantly, while perceptions of open access have remained relatively stable between 2013 and 2018. Nonetheless, those faculty who have published in an open-access journal or are more knowledgeable about open access tend to be more convinced about the quality of open-access publications and less apprehensive about open-access publishing than those who have no publishing experience with open-access journals or who are less knowledgeable about various open-access modalities. Willingness to comply with gold open-access mandates has increased significantly since 2013."
Clickbait detection using multiple categorisation techniques,"Clickbaits are online articles with deliberately designed misleading titles for luring more and more readers to open the intended web page. Clickbaits are used to tempt visitors to click on a particular link either to monetise the landing page or to spread the false news for sensationalisation. The presence of clickbaits on any news aggregator portal may lead to unpleasant experience to readers. Automatic detection of clickbait headlines from news headlines has been a challenging issue for the machine learning community. A lot of methods have been proposed for preventing clickbait articles in recent past. However, the recent techniques available in detecting clickbaits are not much robust. This article proposes a hybrid categorisation technique for separating clickbait and non-clickbait articles by integrating different features, sentence structure and clustering. During preliminary categorisation, the headlines are separated using 11 features. After that, the headlines are recategorised using sentence formality and syntactic similarity measures. In the last phase, the headlines are again recategorised by applying clustering using word vector similarity based on t-stochastic neighbourhood embedding (t-SNE) approach. After categorisation of these headlines, machine learning models are applied to the dataset to evaluate machine learning algorithms. The obtained experimental results indicate that the proposed hybrid model is more robust, reliable and efficient than any individual categorisation techniques for the dataset we have used. Â© The Author(s) 2019.","Clickbaits are online articles with deliberately designed misleading titles for luring more and more readers to open the intended web page. Clickbaits are used to tempt visitors to click on a particular link either to monetise the landing page or to spread the false news for sensationalisation. The presence of clickbaits on any news aggregator portal may lead to unpleasant experience to readers. Automatic detection of clickbait headlines from news headlines has been a challenging issue for the machine learning community. A lot of methods have been proposed for preventing clickbait articles in recent past. However, the recent techniques available in detecting clickbaits are not much robust. This article proposes a hybrid categorisation technique for separating clickbait and non-clickbait articles by integrating different features, sentence structure and clustering. During preliminary categorisation, the headlines are separated using 11 features. After that, the headlines are recategorised using sentence formality and syntactic similarity measures. In the last phase, the headlines are again recategorised by applying clustering using word vector similarity based on t-stochastic neighbourhood embedding (t-SNE) approach. After categorisation of these headlines, machine learning models are applied to the dataset to evaluate machine learning algorithms. The obtained experimental results indicate that the proposed hybrid model is more robust, reliable and efficient than any individual categorisation techniques for the dataset we have used."
Ontology population: Approaches and design aspects,"Ontologies provide a means to store knowledge in a machine-readable format. Ontology population is the task of updating an ontology with new facts from an input knowledge resource. These facts are represented in a structured format and integrated thereafter into the existing knowledge in the ontology. Textual resources are the dominant online knowledge resources that contain a large number of facts expressed either explicitly or implicitly. Hence, the automatic processing of the extensive knowledge available in these resources has recently gained increasing interest. This study discusses the major components of ontology population process and the different design aspects to be considered when building ontology population systems. In addition, this research explains the different approaches and techniques adopted to carry out the task of ontology population. The possible choices of the design aspects and the related issues are identified and analysed using a set of representative ontology population systems. This study concludes by describing the remaining open issues that should be further explored in ontology population. Â© The Author(s) 2018.","Ontologies provide a means to store knowledge in a machine-readable format. Ontology population is the task of updating an ontology with new facts from an input knowledge resource. These facts are represented in a structured format and integrated thereafter into the existing knowledge in the ontology. Textual resources are the dominant online knowledge resources that contain a large number of facts expressed either explicitly or implicitly. Hence, the automatic processing of the extensive knowledge available in these resources has recently gained increasing interest. This study discusses the major components of ontology population process and the different design aspects to be considered when building ontology population systems. In addition, this research explains the different approaches and techniques adopted to carry out the task of ontology population. The possible choices of the design aspects and the related issues are identified and analysed using a set of representative ontology population systems. This study concludes by describing the remaining open issues that should be further explored in ontology population."
A deep learning-based quality assessment model of collaboratively edited documents: A case study of Wikipedia,"Wikipedia is becoming increasingly critical in helping people obtain information and knowledge. Its leading advantage is that users can not only access information but also modify it. However, this presents a challenging issue: how can we measure the quality of a Wikipedia article? The existing approaches assess Wikipedia quality by statistical models or traditional machine learning algorithms. However, their performance is not satisfactory. Moreover, most existing models fail to extract complete information from articles, which degrades the modelâs performance. In this article, we first survey related works and summarise a comprehensive feature framework. Then, state-of-the-art deep learning models are introduced and applied to assess Wikipedia quality. Finally, a comparison among deep learning models and traditional machine learning models is conducted to validate the effectiveness of the proposed model. The models are compared extensively in terms of their training and classification performance. Moreover, the importance of each feature and the importance of different feature sets are analysed separately. Â© The Author(s) 2019.","Wikipedia is becoming increasingly critical in helping people obtain information and knowledge. Its leading advantage is that users can not only access information but also modify it. However, this presents a challenging issue: how can we measure the quality of a Wikipedia article? The existing approaches assess Wikipedia quality by statistical models or traditional machine learning algorithms. However, their performance is not satisfactory. Moreover, most existing models fail to extract complete information from articles, which degrades the models performance. In this article, we first survey related works and summarise a comprehensive feature framework. Then, state-of-the-art deep learning models are introduced and applied to assess Wikipedia quality. Finally, a comparison among deep learning models and traditional machine learning models is conducted to validate the effectiveness of the proposed model. The models are compared extensively in terms of their training and classification performance. Moreover, the importance of each feature and the importance of different feature sets are analysed separately."
Contextual weighting approach to compute term weight in layered vector space model,"The World Wide Web (WWW) is the largest available repository of information. This huge amount of information put forward the challenges of retrieval of trustworthy information from WWW. It defies researchers with new issues of diversity and complexity while retrieving the web information. Information retrieval from the web demands approaches that span beyond conventional information retrieval. Heterogeneity, complexity and the huge volume of web information requires a unique approach to retrieve information. Besides, end-users introduce some difficulties in the retrieval process. Sometimes queries submitted by the user are subtle and ambiguous. The primary concern in information retrieval is the issue of predicting the relevance of documents. In this article, a new approach is proposed that rationally separates web document into five layers, namely, title, header, hyperlink, meta tag and body layer. The proposed method effectively combines the textual information and structural evidence of web document for retrieving information from Web. In the proposed layered vector space model, each layer has an allocated priority which is used to compute weight factor for these layers. The proposed method deduces equation that effectively combines priority of the layer and length of the layer to calculate the weight of the layer. Â© The Author(s) 2019.","The World Wide Web (WWW) is the largest available repository of information. This huge amount of information put forward the challenges of retrieval of trustworthy information from WWW. It defies researchers with new issues of diversity and complexity while retrieving the web information. Information retrieval from the web demands approaches that span beyond conventional information retrieval. Heterogeneity, complexity and the huge volume of web information requires a unique approach to retrieve information. Besides, end-users introduce some difficulties in the retrieval process. Sometimes queries submitted by the user are subtle and ambiguous. The primary concern in information retrieval is the issue of predicting the relevance of documents. In this article, a new approach is proposed that rationally separates web document into five layers, namely, title, header, hyperlink, meta tag and body layer. The proposed method effectively combines the textual information and structural evidence of web document for retrieving information from Web. In the proposed layered vector space model, each layer has an allocated priority which is used to compute weight factor for these layers. The proposed method deduces equation that effectively combines priority of the layer and length of the layer to calculate the weight of the layer."
Who are influential in Q&A communities? A measure of V-Constraint based on knowledge diffusion capability,"Recent years have witnessed a surge of research on the identification of key users in online communities. However, seldom research has focused on their knowledge diffusion capabilities. The purpose of this study is to propose a new measure to find key users who perform well in knowledge diffusion in order to promote usersâ active participation in Q&A communities. In particular, this article develops an improved measure consolidating both usersâ structural hole and knowledge diffusion capability and evaluates its performance through a field study involving 230,000 users and more than 132 million network relations of users. Our results show that our proposed measure can be used to detect key users who occupy structural holesâ advantages in social networks. In addition, key users detected by our proposed measure generally perform well on nearly all dimensions of knowledge diffusion capability compared with other measures of key users. Our study entails important theoretical and practical contributions. Â© The Author(s) 2018.","Recent years have witnessed a surge of research on the identification of key users in online communities. However, seldom research has focused on their knowledge diffusion capabilities. The purpose of this study is to propose a new measure to find key users who perform well in knowledge diffusion in order to promote users active participation in Q&A communities. In particular, this article develops an improved measure consolidating both users structural hole and knowledge diffusion capability and evaluates its performance through a field study involving 230,000 users and more than 132 million network relations of users. Our results show that our proposed measure can be used to detect key users who occupy structural holes advantages in social networks. In addition, key users detected by our proposed measure generally perform well on nearly all dimensions of knowledge diffusion capability compared with other measures of key users. Our study entails important theoretical and practical contributions."
A study of the determinants of postgraduate studentsâ satisfaction of using online research databases,"In todayâs technology-enriched environment, online research databases have become an indispensable tool in learning and research activities. Libraries worldwide have incorporated them as an integral part of their current service delivery. This study intends to determine whether the technology satisfaction model (TSM) validly measures postgraduate studentsâ satisfaction of using online research databases in an academic library. It further explores the relationships among antecedents within the model. To measure the components of the TSM, the stratified random sampling strategy was used to conduct a survey with 300 postgraduate students from six faculties of COMSATS University Islamabad (CUI) in Pakistan. The data were statistically analysed using the Rasch model and structural equation modelling (SEM). The results of the study illustrate direct and indirect associations among the exogenous, endogenous and mediating variables of the TSM, with the studied postgraduate studentsâ computer self-efficacy playing a crucial role in influencing their satisfaction. The TSM shows that 69% of the variance in the postgraduate studentsâ satisfaction can be explained by perceived usefulness, computer self-efficacy and perceived ease of use, which confirm that the TSM has a strong predictive power and is viable for measuring endogenous variable. The findings also make a significant contribution to the testing of databases for academics, service providers and researchers in higher education. Â© The Author(s) 2019.","In todays technology-enriched environment, online research databases have become an indispensable tool in learning and research activities. Libraries worldwide have incorporated them as an integral part of their current service delivery. This study intends to determine whether the technology satisfaction model (TSM) validly measures postgraduate students satisfaction of using online research databases in an academic library. It further explores the relationships among antecedents within the model. To measure the components of the TSM, the stratified random sampling strategy was used to conduct a survey with 300 postgraduate students from six faculties of COMSATS University Islamabad (CUI) in Pakistan. The data were statistically analysed using the Rasch model and structural equation modelling (SEM). The results of the study illustrate direct and indirect associations among the exogenous, endogenous and mediating variables of the TSM, with the studied postgraduate students computer self-efficacy playing a crucial role in influencing their satisfaction. The TSM shows that 69% of the variance in the postgraduate students satisfaction can be explained by perceived usefulness, computer self-efficacy and perceived ease of use, which confirm that the TSM has a strong predictive power and is viable for measuring endogenous variable. The findings also make a significant contribution to the testing of databases for academics, service providers and researchers in higher education."
A case study for block-based linked data generation: Recipes as jigsaw puzzles,"This article is a proof-of-concept case study to evaluate the functionality of a block metaphorâbased linked data generator. In this work, we chose to produce linked data repository of recipes, which provide a medium for people to share their regional and healthy recipes with the masses. However, the same approach can also be adapted easily to other domains. Therefore, the applicability of our approach extends well beyond the food domain that we are considering in this article. As a medium for information sharing and understanding between heterogeneous systems, ontologies will play an important role in the realisation of the Internet of things (IoT) vision. Therefore, an ontology-based recipe repository would also be one of the basic blocks of a smart kitchen environment. However, building ontologies is a challenging task, especially for users who are not conversant in the ontology building languages. This article proposes an approach that can be used even by non-experts and facilitates the sharing and searching of recipe data. In our case, we exploit the features of the block paradigm to publish recipes in Linked Data format. In this way, users do not have to know the OWL (Web Ontology Language) syntax and the text input is kept minimal. As far as we know, this article is the first study that produces linked data using Blockly in the literature. We also conducted a user-based evaluation of the proposed approach using the System Usability Scale (SUS) questionnaire. Â© The Author(s) 2019.","This article is a proof-of-concept case study to evaluate the functionality of a block metaphorbased linked data generator. In this work, we chose to produce linked data repository of recipes, which provide a medium for people to share their regional and healthy recipes with the masses. However, the same approach can also be adapted easily to other domains. Therefore, the applicability of our approach extends well beyond the food domain that we are considering in this article. As a medium for information sharing and understanding between heterogeneous systems, ontologies will play an important role in the realisation of the Internet of things (IoT) vision. Therefore, an ontology-based recipe repository would also be one of the basic blocks of a smart kitchen environment. However, building ontologies is a challenging task, especially for users who are not conversant in the ontology building languages. This article proposes an approach that can be used even by non-experts and facilitates the sharing and searching of recipe data. In our case, we exploit the features of the block paradigm to publish recipes in Linked Data format. In this way, users do not have to know the OWL (Web Ontology Language) syntax and the text input is kept minimal. As far as we know, this article is the first study that produces linked data using Blockly in the literature. We also conducted a user-based evaluation of the proposed approach using the System Usability Scale (SUS) questionnaire."
HOMPer: A new hybrid system for opinion mining in the Persian language,"Opinion mining is a subfield of data mining and natural language processing that concerns with extracting usersâ opinion and attitude towards products or services from their comments on the Web. Persian opinion mining, in contrast to its counterpart in English, is a totally new field of study and hence, it has not received the attention it deserves. Existing methods for opinion mining in the Persian language may be classified into machine learningâ and lexicon-based approaches. These methods have been proposed and successfully used for polarity-detection problem. However, when they should be used for more complex tasks like rating prediction, their results are not desirable. In this study, first an exhaustive investigation of machine learningâ and lexicon-based methods is performed. Then, a new hybrid method is proposed for rating-prediction problem in the Persian language. Finally, the effect of machine learning component, feature-selection method, normalisation method and combination level are investigated. The experimental results on a large data set containing 16,000 Persian customersâ review show that this proposed system achieves higher performance in comparison to NaÃ¯ve Bayes algorithm and a pure lexicon-based method. Moreover, results demonstrate that this proposed method may also be successfully used for polarity detection. Â© The Author(s) 2019.","Opinion mining is a subfield of data mining and natural language processing that concerns with extracting users opinion and attitude towards products or services from their comments on the Web. Persian opinion mining, in contrast to its counterpart in English, is a totally new field of study and hence, it has not received the attention it deserves. Existing methods for opinion mining in the Persian language may be classified into machine learning and lexicon-based approaches. These methods have been proposed and successfully used for polarity-detection problem. However, when they should be used for more complex tasks like rating prediction, their results are not desirable. In this study, first an exhaustive investigation of machine learning and lexicon-based methods is performed. Then, a new hybrid method is proposed for rating-prediction problem in the Persian language. Finally, the effect of machine learning component, feature-selection method, normalisation method and combination level are investigated. The experimental results on a large data set containing 16,000 Persian customers review show that this proposed system achieves higher performance in comparison to Nave Bayes algorithm and a pure lexicon-based method. Moreover, results demonstrate that this proposed method may also be successfully used for polarity detection."
Deception detection methods incorporating discourse network metrics in synchronous computer-mediated communication,"The prevalence of deception in computer-mediated communication and the risk of misjudgement based on deceptive information call for effective detection methods of deception. Extant models for online deception detection rely mainly on verbal behaviours of participants while largely ignoring context. Discourse behaviour analysis, which can better investigate the information in context, has been proved effective for online deception detection; nevertheless, these discourse behaviours have been analysed in isolation without referring to other behaviours in context. To achieve the ultimate goal of effective prediction of deception in synchronous computer-mediated communication, this research exploits temporal networks in uncovering the dynamics of deception behaviours, proposes novel deception detection methods using discourse network metrics as predictive features, and empirically evaluates the performances of deception detection methods incorporating three types of predictive features (non-discourse features, discourse features and discourse network metrics). The results suggest that discourse network features are more effective in detecting deception and incorporating these features with non-discourse and discourse features can significantly improve the performance of deception detection. The findings not only demonstrate the efficacy of structural features in deception detection but also offer both methodological and theoretical contributions to deception detection from the perspective of temporal network. Â© The Author(s) 2019.","The prevalence of deception in computer-mediated communication and the risk of misjudgement based on deceptive information call for effective detection methods of deception. Extant models for online deception detection rely mainly on verbal behaviours of participants while largely ignoring context. Discourse behaviour analysis, which can better investigate the information in context, has been proved effective for online deception detection; nevertheless, these discourse behaviours have been analysed in isolation without referring to other behaviours in context. To achieve the ultimate goal of effective prediction of deception in synchronous computer-mediated communication, this research exploits temporal networks in uncovering the dynamics of deception behaviours, proposes novel deception detection methods using discourse network metrics as predictive features, and empirically evaluates the performances of deception detection methods incorporating three types of predictive features (non-discourse features, discourse features and discourse network metrics). The results suggest that discourse network features are more effective in detecting deception and incorporating these features with non-discourse and discourse features can significantly improve the performance of deception detection. The findings not only demonstrate the efficacy of structural features in deception detection but also offer both methodological and theoretical contributions to deception detection from the perspective of temporal network."
Indicator of quality for environmental articles on Wikipedia at the higher education level,"Wikipedia is important in higher education because students and scholars often use it. Nevertheless, the issue of Wikipediaâs quality is an obstacle for its use at the higher education level. In order to contribute to this discussion, we have proposed âVerifiability by respected sourcesâ as an indicator for assessing the quality of Wikipedia articles at the higher education level and conducted an analysis of the most frequently visited articles in the category of Environment on Wikipedia. Results show that these articles contain many unreferenced statements, so their usage at the higher education level is problematic. Therefore, we also propose specific steps for relevant actors that could help to improve the quality of Wikipedia. Â© The Author(s) 2019.","Wikipedia is important in higher education because students and scholars often use it. Nevertheless, the issue of Wikipedias quality is an obstacle for its use at the higher education level. In order to contribute to this discussion, we have proposed Verifiability by respected sources as an indicator for assessing the quality of Wikipedia articles at the higher education level and conducted an analysis of the most frequently visited articles in the category of Environment on Wikipedia. Results show that these articles contain many unreferenced statements, so their usage at the higher education level is problematic. Therefore, we also propose specific steps for relevant actors that could help to improve the quality of Wikipedia."
AuthorâSubjectâTopic model for reviewer recommendation,"Interdisciplinary studies are becoming increasingly popular, and research domains of many experts are becoming diverse. This phenomenon brings difficulty in recommending experts to review interdisciplinary submissions. In this study, an AuthorâSubjectâTopic (AST) model is proposed with two versions. In the model, reviewersâ subject information is embedded to analyse topic distributions of submissions and reviewersâ publications. The major difference between the AST and AuthorâTopic models lies in the introduction of a âSubjectâ layer, which supervises the generation of hierarchical topics and allows sharing of subjects among authors. To evaluate the performance of the AST model, papers in Information System and Management (a typical interdisciplinary domain) in a famous Chinese academic library are investigated. Comparative experiments are conducted, which show the effectiveness of the AST model in topic distribution analysis and reviewer recommendation for interdisciplinary studies. Â© The Author(s) 2018.","Interdisciplinary studies are becoming increasingly popular, and research domains of many experts are becoming diverse. This phenomenon brings difficulty in recommending experts to review interdisciplinary submissions. In this study, an AuthorSubjectTopic (AST) model is proposed with two versions. In the model, reviewers subject information is embedded to analyse topic distributions of submissions and reviewers publications. The major difference between the AST and AuthorTopic models lies in the introduction of a Subject layer, which supervises the generation of hierarchical topics and allows sharing of subjects among authors. To evaluate the performance of the AST model, papers in Information System and Management (a typical interdisciplinary domain) in a famous Chinese academic library are investigated. Comparative experiments are conducted, which show the effectiveness of the AST model in topic distribution analysis and reviewer recommendation for interdisciplinary studies."
Spam profiles detection on social networks using computational intelligence methods: The effect of the lingual context,"In online social networks, spam profiles represent one of the most serious security threats over the Internet; if they do not stop producing bad advertisements, they can be exploited by criminals for various purposes. This article addresses the nature and the characteristics of spam profiles in a social network like Twitter to improve spam detection, based on a number of publicly available language-independent features. In order to investigate the effectiveness of these features in spam detection, four datasets are extracted for four different language contexts (i.e. Arabic, English, Korean and Spanish), and a fifth is formed by combining them all. We conduct our experiments using a set of five well-known classification algorithms in spam detection field, k-Nearest Neighbours (k-NN), Random Forest (RF), Naive Bayes (NB), Decision Tree (DT) (J48) and Multilayer Perceptron (MLP) classifiers, along with five filter-based feature selection methods, namely, Information Gain, Chi-square, ReliefF, Correlation and Significance. The results show oscillating performance of each classifier across all datasets, but improved classification results with feature selection. In addition, detailed analysis and comparisons are carried out on two different levels: in the first level, we compare the selected featuresâ importance among the feature selection methods, whereas in the second level, we observe the relations and the importance of the selected features across all datasets. The findings of this article lead to a better understanding of social spam and improving detection methods by considering the various important features resulting from the different lingual contexts. Â© The Author(s) 2019.","In online social networks, spam profiles represent one of the most serious security threats over the Internet; if they do not stop producing bad advertisements, they can be exploited by criminals for various purposes. This article addresses the nature and the characteristics of spam profiles in a social network like Twitter to improve spam detection, based on a number of publicly available language-independent features. In order to investigate the effectiveness of these features in spam detection, four datasets are extracted for four different language contexts ( Arabic, English, Korean and Spanish), and a fifth is formed by combining them all. We conduct our experiments using a set of five well-known classification algorithms in spam detection field, k-Nearest Neighbours (k-NN), Random Forest (RF), Naive Bayes (NB), Decision Tree (DT) (J48) and Multilayer Perceptron (MLP) classifiers, along with five filter-based feature selection methods, namely, Information Gain, Chi-square, ReliefF, Correlation and Significance. The results show oscillating performance of each classifier across all datasets, but improved classification results with feature selection. In addition, detailed analysis and comparisons are carried out on two different levels: in the first level, we compare the selected features importance among the feature selection methods, whereas in the second level, we observe the relations and the importance of the selected features across all datasets. The findings of this article lead to a better understanding of social spam and improving detection methods by considering the various important features resulting from the different lingual contexts."
Mapping the efficiency of international scientific collaboration between cities worldwide,"International scientific collaboration, a fundamental phenomenon of science, has been studied from several perspectives for decades. In the spatial aspect of science, cities have generally been considered by their publication output or by their citation impact. Only a minority of scientometric studies focus on exploring collaboration patterns of cities. In this visualisation, we go beyond the well-known approaches and map international scientific collaboration patterns of the most prominent science hubs considering both the quantity and the impact of papers produced in the collaboration. The analysis involves 245 cities and the collaboration matrix contains a total number of 7718 international collaboration links. Results show that USâEurope co-publication links are more efficient in terms of producing highly cited papers than those international links that Asian cities have built in scientific collaboration. Â© The Author(s) 2019.","International scientific collaboration, a fundamental phenomenon of science, has been studied from several perspectives for decades. In the spatial aspect of science, cities have generally been considered by their publication output or by their citation impact. Only a minority of scientometric studies focus on exploring collaboration patterns of cities. In this visualisation, we go beyond the well-known approaches and map international scientific collaboration patterns of the most prominent science hubs considering both the quantity and the impact of papers produced in the collaboration. The analysis involves 245 cities and the collaboration matrix contains a total number of 7718 international collaboration links. Results show that USEurope co-publication links are more efficient in terms of producing highly cited papers than those international links that Asian cities have built in scientific collaboration."
Understanding data search as a socio-technical practice,"Open research data are heralded as having the potential to increase effectiveness, productivity and reproducibility in science, but little is known about the actual practices involved in data search. The socio-technical problem of locating data for reuse is often reduced to the technological dimension of designing data search systems. We combine a bibliometric study of the current academic discourse around data search with interviews with data seekers. In this article, we explore how adopting a contextual, socio-technical perspective can help to understand user practices and behaviour and ultimately help to improve the design of data discovery systems. Â© The Author(s) 2019.","Open research data are heralded as having the potential to increase effectiveness, productivity and reproducibility in science, but little is known about the actual practices involved in data search. The socio-technical problem of locating data for reuse is often reduced to the technological dimension of designing data search systems. We combine a bibliometric study of the current academic discourse around data search with interviews with data seekers. In this article, we explore how adopting a contextual, socio-technical perspective can help to understand user practices and behaviour and ultimately help to improve the design of data discovery systems."
Hotel recommendation system by bipartite networks and link prediction,"With the rapid growth of the Internet in recent years, online social media has become very important for people. People often use social media tools to communicate and share their ideas or experiences regardless of time and place. One of the areas where the use of these tools is widespread is tourism. It is one of the hardest tasks to find a suitable hotel for travellers. There are many websites where accommodation services of the tourism enterprises are evaluated. People share their experiences about the hotels they stayed through these websites. Positive or negative comments are an effective factor in hotel selection. The purpose of this study is to construct a hotel recommendation system based on userâs location using online hotel reviews. The reviews crawled from TripAdvisor.com were filtered according to their total scores, and a dataset was obtained consisting of users and reviews of their liked hotels. For the recommendation task, first, three different bipartite networks consisting of users and hotels were modelled, which were global, country and city based. Then, in these networks, it was recommended hotels to users for their next choice with link prediction methods, using the common hotels where the users prefer to stay in the past. The most successful results were obtained in the city-based network. With this study, it was tried to reduce the time spent reading the online reviews and finding the suitable hotel. To the best of our knowledge, this is the first study on recommending a hotel using online reviews by bipartite networks. Â© The Author(s) 2019.","With the rapid growth of the Internet in recent years, online social media has become very important for people. People often use social media tools to communicate and share their ideas or experiences regardless of time and place. One of the areas where the use of these tools is widespread is tourism. It is one of the hardest tasks to find a suitable hotel for travellers. There are many websites where accommodation services of the tourism enterprises are evaluated. People share their experiences about the hotels they stayed through these websites. Positive or negative comments are an effective factor in hotel selection. The purpose of this study is to construct a hotel recommendation system based on users location using online hotel reviews. The reviews crawled from TripAdvisor.com were filtered according to their total scores, and a dataset was obtained consisting of users and reviews of their liked hotels. For the recommendation task, first, three different bipartite networks consisting of users and hotels were modelled, which were global, country and city based. Then, in these networks, it was recommended hotels to users for their next choice with link prediction methods, using the common hotels where the users prefer to stay in the past. The most successful results were obtained in the city-based network. With this study, it was tried to reduce the time spent reading the online reviews and finding the suitable hotel. To the best of our knowledge, this is the first study on recommending a hotel using online reviews by bipartite networks."
University studentsâ mobile news consumption activities and evaluative/affective reactions to political news during election campaigns: A diary study,"Smartphones have now become routinely used tools for peopleâs everyday life news consumption. This article presents a diary study involving 49 university students in the United States documenting their process of consuming political news via smartphones. Participants reported the information about the news and used 23 pairs of semantic differential scales to evaluate and express their affective reactions to the news. Among 176 political news items submitted, the highest proportion was election news. Significant demographic differences were found in participantsâ choices of semantic adjectives. Differences were also found in proportions of election and non-election news submitted. A higher proportion of election news was marked as âlightâ, âstaleâ, âshallowâ, âworthlessâ, âdishonestâ and âharmfulâ than those labelled for non-election news. Findings provide valuable insights into university studentsâ political news mobile consumption activities and their assessment and sentiment surrounding election news and general political news during the 2016 US Presidential election campaign. Â© The Author(s) 2019.","Smartphones have now become routinely used tools for peoples everyday life news consumption. This article presents a diary study involving 49 university students in the United States documenting their process of consuming political news via smartphones. Participants reported the information about the news and used 23 pairs of semantic differential scales to evaluate and express their affective reactions to the news. Among 176 political news items submitted, the highest proportion was election news. Significant demographic differences were found in participants choices of semantic adjectives. Differences were also found in proportions of election and non-election news submitted. A higher proportion of election news was marked as light, stale, shallow, worthless, dishonest and harmful than those labelled for non-election news. Findings provide valuable insights into university students political news mobile consumption activities and their assessment and sentiment surrounding election news and general political news during the 2016 US Presidential election campaign."
Knowledge-sharing and collaborative behaviour: An empirical study on a Portuguese higher education institution,"Collaboration has been considered a way to address the challenges of the 21st century, fostering the necessary innovation, growth and productivity for all parties involved. Several studies reveal that collaboration can be strongly influenced by knowledge sharing. The literature suggests that this topic is quite relevant and that there is an evident lack of empirical studies that properly investigate the relationship between knowledge-sharing and collaborative behaviour in Higher Education Institutions (HEIs). In this context, the purpose of this work is to examine whether knowledge-sharing intention has a positive relationship with collaborative behaviour among professors and researchers in a public Portuguese HEI, taking into account other constructs that can have effect on the knowledge-sharing intention. In order to reach this objective, a conceptual research model was developed based on the theory of reasoned action. The empirical study was conducted based on a questionnaire, and the data analysis was performed using partial least squares. The results indicate that intrinsic motivation and networking are the factors that positively affect the attitude towards knowledge sharing. Nevertheless, it is concluded that trust is the variable that more strongly affects the knowledge-sharing intention. Finally, the study identified that knowledge-sharing intention has a positive influence in collaborative behaviour. It is considered that this study can contribute to support institutionsâ management in defining strategies and developing actions in order to promote an organisational culture based on knowledge management that significantly leads to knowledge-sharing and collaboration relationships. Â© The Author(s) 2019.","Collaboration has been considered a way to address the challenges of the 21st century, fostering the necessary innovation, growth and productivity for all parties involved. Several studies reveal that collaboration can be strongly influenced by knowledge sharing. The literature suggests that this topic is quite relevant and that there is an evident lack of empirical studies that properly investigate the relationship between knowledge-sharing and collaborative behaviour in Higher Education Institutions (HEIs). In this context, the purpose of this work is to examine whether knowledge-sharing intention has a positive relationship with collaborative behaviour among professors and researchers in a public Portuguese HEI, taking into account other constructs that can have effect on the knowledge-sharing intention. In order to reach this objective, a conceptual research model was developed based on the theory of reasoned action. The empirical study was conducted based on a questionnaire, and the data analysis was performed using partial least squares. The results indicate that intrinsic motivation and networking are the factors that positively affect the attitude towards knowledge sharing. Nevertheless, it is concluded that trust is the variable that more strongly affects the knowledge-sharing intention. Finally, the study identified that knowledge-sharing intention has a positive influence in collaborative behaviour. It is considered that this study can contribute to support institutions management in defining strategies and developing actions in order to promote an organisational culture based on knowledge management that significantly leads to knowledge-sharing and collaboration relationships."
On classification of abstracts obtained from medical journals,"Classification of medical documents was mostly carried out on English data sets and these studies were performed on hospital records rather than academic texts. The main reasons behind this situation are the lack of publicly available data sets and the tasks being costly and time-consuming. As the first contribution of this study, two data sets including Turkish and English counterparts of the same abstracts published in Turkish medical journals were constructed. Turkish is one of the widely used agglutinative languages worldwide and English is a good example of non-agglutinative languages. While English abstracts were obtained automatically from MEDLINE database with a computer program, Turkish counterparts of these documents were collected manually from the Internet. As the second contribution of this study, an extensive comparison on classification of abstracts obtained from Turkish medical journals was made by using these two equivalent data sets. Features were extracted from text documents with three different approaches: unigram, bigram and hybrid. Hybrid approach includes a combination of unigram and bigram features. In the experiments, three different feature selection methods and seven different classifiers were utilised. According to the results on both data sets, classification performance of the English abstracts outperformed the Turkish counterparts. Maximum accuracies were obtained from the combination of unigram features, distinguishing feature selector (DFS) and multinomial naÃ¯ve Bayes (MNB) classifier for both data sets. Unigram features were generally more efficient than bigram and hybrid features. However, analysis of top-10 features indicated that nearly half of the features were translations of each other for Turkish and English data sets. Â© The Author(s) 2019.","Classification of medical documents was mostly carried out on English data sets and these studies were performed on hospital records rather than academic texts. The main reasons behind this situation are the lack of publicly available data sets and the tasks being costly and time-consuming. As the first contribution of this study, two data sets including Turkish and English counterparts of the same abstracts published in Turkish medical journals were constructed. Turkish is one of the widely used agglutinative languages worldwide and English is a good example of non-agglutinative languages. While English abstracts were obtained automatically from MEDLINE database with a computer program, Turkish counterparts of these documents were collected manually from the Internet. As the second contribution of this study, an extensive comparison on classification of abstracts obtained from Turkish medical journals was made by using these two equivalent data sets. Features were extracted from text documents with three different approaches: unigram, bigram and hybrid. Hybrid approach includes a combination of unigram and bigram features. In the experiments, three different feature selection methods and seven different classifiers were utilised. According to the results on both data sets, classification performance of the English abstracts outperformed the Turkish counterparts. Maximum accuracies were obtained from the combination of unigram features, distinguishing feature selector (DFS) and multinomial nave Bayes (MNB) classifier for both data sets. Unigram features were generally more efficient than bigram and hybrid features. However, analysis of top-10 features indicated that nearly half of the features were translations of each other for Turkish and English data sets."
Does open access citation advantage depend on paper topics?,"Research topics vary in their citation potential. In a metric-wise scientific milieu, it would be probable that authors tend to select citation-attractive topics especially when choosing open access (OA) outlets that are more likely to attract citations. Applying a matched-pairs study design, this research aims to examine the role of research topics in the citation advantage of OA papers. Using a comparative citation analysis method, it investigates a sample of papers published in 47 Elsevier article processing charges (APC)-funded journals in different access models including non-open access (NOA), APC, Green and mixed Green-APC. The contents of the papers are analysed using natural language processing techniques at the title and abstract level and served as a basis to match the NOA papers to their peers in the OA models. The publication years and journals are controlled for in order to avoid their impacts on the citation numbers. According to the results, the OA citation advantage that is observed in the whole sample still holds even for the highly similar OA and NOA papers. This implies that the OA citation surplus is not an artefact of the OA and NOA papersâ differences in their topics and, therefore, in their citation potential. This leads to the conclusion that OA authorsâ self-selectivity, if it exists at all, is not responsible for the OA citation advantage, at least as far as selection of topics with probably higher citation potentials is concerned. Â© The Author(s) 2019.","Research topics vary in their citation potential. In a metric-wise scientific milieu, it would be probable that authors tend to select citation-attractive topics especially when choosing open access (OA) outlets that are more likely to attract citations. Applying a matched-pairs study design, this research aims to examine the role of research topics in the citation advantage of OA papers. Using a comparative citation analysis method, it investigates a sample of papers published in 47 Elsevier article processing charges (APC)-funded journals in different access models including non-open access (NOA), APC, Green and mixed Green-APC. The contents of the papers are analysed using natural language processing techniques at the title and abstract level and served as a basis to match the NOA papers to their peers in the OA models. The publication years and journals are controlled for in order to avoid their impacts on the citation numbers. According to the results, the OA citation advantage that is observed in the whole sample still holds even for the highly similar OA and NOA papers. This implies that the OA citation surplus is not an artefact of the OA and NOA papers differences in their topics and, therefore, in their citation potential. This leads to the conclusion that OA authors self-selectivity, if it exists at all, is not responsible for the OA citation advantage, at least as far as selection of topics with probably higher citation potentials is concerned."
ASA: A framework for Arabic sentiment analysis,"Sentiment analysis (SA), also known as opinion mining, is a growing important research area. Generally, it helps to automatically determine if a text expresses a positive, negative or neutral sentiment. It enables to mine the huge increasing resources of shared opinions such as social networks, review sites and blogs. In fact, SA is used by many fields and for various languages such as English and Arabic. However, since Arabic is a highly inflectional and derivational language, it raises many challenges. In fact, SA of Arabic text should handle such complex morphology. To better handle these challenges, we decided to provide the research community and Arabic users with a new efficient framework for Arabic Sentiment Analysis (ASA). Our primary goal is to improve the performance of ASA by exploiting deep learning while varying the preprocessing techniques. For that, we implement and evaluate two deep learning models namely convolutional neural network (CNN) and long short-term memory (LSTM) models. The framework offers various preprocessing techniques for ASA (including stemming, normalisation, tokenization and stop words). As a result of this work, we first provide a new rich and publicly available Arabic corpus called Moroccan Sentiment Analysis Corpus (MSAC). Second, the proposed framework demonstrates improvement in ASA. In fact, the experimental results prove that deep learning models have a better performance for ASA than classical approaches (support vector machines, naive Bayes classifiers and maximum entropy). They also show the key role of morphological features in Arabic Natural Language Processing (NLP). Â© The Author(s) 2019.","Sentiment analysis (SA), also known as opinion mining, is a growing important research area. Generally, it helps to automatically determine if a text expresses a positive, negative or neutral sentiment. It enables to mine the huge increasing resources of shared opinions such as social networks, review sites and blogs. In fact, SA is used by many fields and for various languages such as English and Arabic. However, since Arabic is a highly inflectional and derivational language, it raises many challenges. In fact, SA of Arabic text should handle such complex morphology. To better handle these challenges, we decided to provide the research community and Arabic users with a new efficient framework for Arabic Sentiment Analysis (ASA). Our primary goal is to improve the performance of ASA by exploiting deep learning while varying the preprocessing techniques. For that, we implement and evaluate two deep learning models namely convolutional neural network (CNN) and long short-term memory (LSTM) models. The framework offers various preprocessing techniques for ASA (including stemming, normalisation, tokenization and stop words). As a result of this work, we first provide a new rich and publicly available Arabic corpus called Moroccan Sentiment Analysis Corpus (MSAC). Second, the proposed framework demonstrates improvement in ASA. In fact, the experimental results prove that deep learning models have a better performance for ASA than classical approaches (support vector machines, naive Bayes classifiers and maximum entropy). They also show the key role of morphological features in Arabic Natural Language Processing (NLP)."
Mining layered technological information in scientific papers: A semi-supervised method,"Tech mining is the application of text mining tools to science and technology information resources. The ever-increasing volume of scientific outputs is a boom to technological innovation, but it also complicates efforts to obtain useful and concise information for problem solving. This challenge extends to tech mining, where the development of techniques compatible with big data is an urgent issue. This article introduces a semi-supervised method for extracting layered technological information from scientific papers in order to extend the reach of tech mining. Our method starts with several pre-set seed patterns used to extract candidate phrases by matching the dependency tree of each sentence. Then, after a series of judgements, phrases are divided into two categories: âmain techniqueâ and âtech-componentâ. (A technique, for the purposes of this study, is a method or tool used in the article being analysed.) In order to generate new patterns for subsequent iterations, a weighted pattern learning method is also adopted. Finally, multiple iterations of the method are applied to extract technological information from each paper. A dataset from the field of optical switcher is used to verify the methodâs effectiveness. Our findings are that (1) by two loops of extraction process in each iteration, our method realises the layered technological information extraction, which contains the âpartâwholeâ relationships between main techniques and tech-components; (2) the recall rate for main techniques is superior to the baseline after iterating 23 rounds; (3) when layering is disregarded, in the aspect of the precision and the volume of techniques, the new method is higher than that for the baseline; and (4) adjusting another two parameters can optimise the efficiency â however, the effect is neither pronounced nor straightforward. Â© The Author(s) 2018.","Tech mining is the application of text mining tools to science and technology information resources. The ever-increasing volume of scientific outputs is a boom to technological innovation, but it also complicates efforts to obtain useful and concise information for problem solving. This challenge extends to tech mining, where the development of techniques compatible with big data is an urgent issue. This article introduces a semi-supervised method for extracting layered technological information from scientific papers in order to extend the reach of tech mining. Our method starts with several pre-set seed patterns used to extract candidate phrases by matching the dependency tree of each sentence. Then, after a series of judgements, phrases are divided into two categories: main technique and tech-component. (A technique, for the purposes of this study, is a method or tool used in the article being analysed.) In order to generate new patterns for subsequent iterations, a weighted pattern learning method is also adopted. Finally, multiple iterations of the method are applied to extract technological information from each paper. A dataset from the field of optical switcher is used to verify the methods effectiveness. Our findings are that by two loops of extraction process in each iteration, our method realises the layered technological information extraction, which contains the partwhole relationships between main techniques and tech-components; the recall rate for main techniques is superior to the baseline after iterating 23 rounds; when layering is disregarded, in the aspect of the precision and the volume of techniques, the new method is higher than that for the baseline; and adjusting another two parameters can optimise the efficiency however, the effect is neither pronounced nor straightforward."
Evaluating the performance of government websites: An automatic assessment system based on the TFN-AHP methodology,"Government websites are currently important for providing information and services to citizens. It is a crucial task to evaluate the performance of each government website. The traditional evaluation processes based on experts are criticised as being subjective and cannot work in real time. This article proposes a framework and automatic assessment system for evaluating the performance of government websites in real time. To test the proposed framework and the automatic assessment system, we evaluate and classify 70 websites from Shaanxi Province of China. This article provides guidance for government agencies, managers of government websites and researchers. Â© The Author(s) 2019.","Government websites are currently important for providing information and services to citizens. It is a crucial task to evaluate the performance of each government website. The traditional evaluation processes based on experts are criticised as being subjective and cannot work in real time. This article proposes a framework and automatic assessment system for evaluating the performance of government websites in real time. To test the proposed framework and the automatic assessment system, we evaluate and classify 70 websites from Shaanxi Province of China. This article provides guidance for government agencies, managers of government websites and researchers."
BlindDate recommender: A context-aware ontology-based dating recommendation platform,"Online dating sites have become popular platforms for those individuals who utilise the Internet to develop a personal or romantic relationship. Unlike typical recommenders systems, which attempt to suggest items such as films, songs, books and so on. According to a userâs interests, dating recommender systems provide services that people can use to find potential romantic partners. Since these services have a higher expectancy of users, online dating sites are considering the introduction of recommender systems in order to build an improved dating network. Different kinds of techniques based on content-based, collaborative filtering or hybrid techniques exist. In this article, we introduce BlindDate recommender, a context-based platform that utilises semantic technologies to describe usersâ preferences more precisely. We utilise DBPedia repositories to obtain information that is subsequently used to enrich a previously generated ontology model. The instances inserted into the ontology enable the matching algorithms that we have generated to identify potential matches between users. In order to validate the performance of the platform, we utilise a real-world data set that has produced relevant results enhancing the accuracy compared with other well-known approaches and identifying the discriminant parameters used in the dating domain. More specifically, the proposed approach attains 0.79, 0.8 and 0.55 in the I-Precision, I-Recall and I-F-measure, respectively, when employed in separate topics. Â© The Author(s) 2018.","Online dating sites have become popular platforms for those individuals who utilise the Internet to develop a personal or romantic relationship. Unlike typical recommenders systems, which attempt to suggest items such as films, songs, books and so on. According to a users interests, dating recommender systems provide services that people can use to find potential romantic partners. Since these services have a higher expectancy of users, online dating sites are considering the introduction of recommender systems in order to build an improved dating network. Different kinds of techniques based on content-based, collaborative filtering or hybrid techniques exist. In this article, we introduce BlindDate recommender, a context-based platform that utilises semantic technologies to describe users preferences more precisely. We utilise DBPedia repositories to obtain information that is subsequently used to enrich a previously generated ontology model. The instances inserted into the ontology enable the matching algorithms that we have generated to identify potential matches between users. In order to validate the performance of the platform, we utilise a real-world data set that has produced relevant results enhancing the accuracy compared with other well-known approaches and identifying the discriminant parameters used in the dating domain. More specifically, the proposed approach attains 0.79, 0.8 and 0.55 in the I-Precision, I-Recall and I-F-measure, respectively, when employed in separate topics."
Lotkaâs law for the Brazilian scientific output published in journals,"Lotkaâs law is a power law for the frequency of scholarly publications. We show that Lotkaâs law cannot be dismissed after considering a massive sample of the number of publications of Brazilian researchers in journals listed on the SCImago Journal Rank and the Journal Citation Reports. For the SCImago Journal Rank, we found a power law with the Pareto exponent of 0.4 beyond the threshold of 50 papers. This means computing the âaverage number of publicationsâ of either a researcher or a discipline is of no practical significance. Â© The Author(s) 2018.","Lotkas law is a power law for the frequency of scholarly publications. We show that Lotkas law cannot be dismissed after considering a massive sample of the number of publications of Brazilian researchers in journals listed on the SCImago Journal Rank and the Journal Citation Reports. For the SCImago Journal Rank, we found a power law with the Pareto exponent of 0.4 beyond the threshold of 50 papers. This means computing the average number of publications of either a researcher or a discipline is of no practical significance."
Change point detection in social networks using a multivariate exponentially weighted moving average chart,"Although the significant role of social networks in communications between individuals has attracted researchersâ attention to the social networks, only few authors investigated social network monitoring in their studies. Most of the existing studies in this context suffer from the following three main drawbacks: (1) using the case-based network attributes such as person experiences and departments instead of the main attributes such as network density and centrality attributes, (2) monitoring the social attributes separately with the assumption that they are independent of each other and (3) ignoring detection of real time of change in the network. To overcome the above-mentioned disadvantages, this research develops a statistical method for monitoring the connections among actors in the social networks with the four most important network attributes consisting of (1) network density, (2) degree centrality, (3) betweenness centrality and (4) closeness centrality. To this end, a multivariate exponentially weighted moving average (MEWMA) control chart is used for simultaneous monitoring of these four correlated attributes. Furthermore, since the control chart usually does not alert a signal in the exact time of change due to type II error, this study presents a change point detection method to reduce cost and time required for diagnosing the control chart signal. Eventually, the efficiency of the proposed approach in comparison with the existing methods is evaluated through a simulation procedure. The results indicate that the suggested method has better performance than the univariate approach in detecting change point. Â© The Author(s) 2019.","Although the significant role of social networks in communications between individuals has attracted researchers attention to the social networks, only few authors investigated social network monitoring in their studies. Most of the existing studies in this context suffer from the following three main drawbacks: using the case-based network attributes such as person experiences and departments instead of the main attributes such as network density and centrality attributes, monitoring the social attributes separately with the assumption that they are independent of each other and ignoring detection of real time of change in the network. To overcome the above-mentioned disadvantages, this research develops a statistical method for monitoring the connections among actors in the social networks with the four most important network attributes consisting of network density, degree centrality, betweenness centrality and closeness centrality. To this end, a multivariate exponentially weighted moving average (MEWMA) control chart is used for simultaneous monitoring of these four correlated attributes. Furthermore, since the control chart usually does not alert a signal in the exact time of change due to type II error, this study presents a change point detection method to reduce cost and time required for diagnosing the control chart signal. Eventually, the efficiency of the proposed approach in comparison with the existing methods is evaluated through a simulation procedure. The results indicate that the suggested method has better performance than the univariate approach in detecting change point."
Deep learning in Arabic sentiment analysis: An overview,"Sentiment analysis became a very motivating area in both academic and industrial fields due to the exponential increase of the online published reviews and recommendations. To solve the problem of analysing and classifying those reviews and recommendations, several techniques have been proposed. Lately, deep neural networks showed promising outcomes in sentiment analysis. The growing number of Arab users on the Internet along with the increasing amount of published Arabic reviews and comments encouraged researchers to apply deep learning to analyse them. This article is a comprehensive overview of research works that utilised the deep learning approach for Arabic sentiment analysis. Â© The Author(s) 2019.","Sentiment analysis became a very motivating area in both academic and industrial fields due to the exponential increase of the online published reviews and recommendations. To solve the problem of analysing and classifying those reviews and recommendations, several techniques have been proposed. Lately, deep neural networks showed promising outcomes in sentiment analysis. The growing number of Arab users on the Internet along with the increasing amount of published Arabic reviews and comments encouraged researchers to apply deep learning to analyse them. This article is a comprehensive overview of research works that utilised the deep learning approach for Arabic sentiment analysis."
DelibAnalysis: Understanding the quality of online political discourse with machine learning,"This article proposes an automated methodology for the analysis of online political discourse. Drawing from the discourse quality index (DQI) by Steenbergen et al., it applies a machine learningâbased quantitative approach to measuring the discourse quality of political discussions online. The DelibAnalysis framework aims to provide an accessible, replicable methodology for the measurement of discourse quality that is both platform and language agnostic. The framework uses a simplified version of the DQI to train a classifier, which can then be used to predict the discourse quality of any non-coded comment in a given political discussion online. The objective of this research is to provide a systematic framework for the automated discourse quality analysis of large datasets and, in applying this framework, to yield insight into the structure and features of political discussions online. Â© The Author(s) 2019.","This article proposes an automated methodology for the analysis of online political discourse. Drawing from the discourse quality index (DQI) by Steenbergen et al., it applies a machine learningbased quantitative approach to measuring the discourse quality of political discussions online. The DelibAnalysis framework aims to provide an accessible, replicable methodology for the measurement of discourse quality that is both platform and language agnostic. The framework uses a simplified version of the DQI to train a classifier, which can then be used to predict the discourse quality of any non-coded comment in a given political discussion online. The objective of this research is to provide a systematic framework for the automated discourse quality analysis of large datasets and, in applying this framework, to yield insight into the structure and features of political discussions online."
Sequential patterns rule-based approach for opinion target extraction from customer reviews,"Aspect extraction or opinion target extraction is the key task of sentiment analysis, which aims to identify targets of peopleâs sentiments. This is the most important task of aspect-based sentiment analysis as without the aspects, there is no much use of extracted opinions. Recent approaches have shown the significance of dependency-based rules for the given task. These rules are heavily dependent on the dependency parser and generated with the help of grammatical rules. In this article, we are proposing to learn from userâs behaviour to identify the relation among aspects and opinions. The use of sequential patterns has been proposed for the extraction of aspects. The key purpose of this research is to study the impact of sequential pattern mining in the phase of aspect extraction. Our experimental results show that the approach proposed in our work produced better results as compared with the state-of-the-art approaches. Â© The Author(s) 2018.","Aspect extraction or opinion target extraction is the key task of sentiment analysis, which aims to identify targets of peoples sentiments. This is the most important task of aspect-based sentiment analysis as without the aspects, there is no much use of extracted opinions. Recent approaches have shown the significance of dependency-based rules for the given task. These rules are heavily dependent on the dependency parser and generated with the help of grammatical rules. In this article, we are proposing to learn from users behaviour to identify the relation among aspects and opinions. The use of sequential patterns has been proposed for the extraction of aspects. The key purpose of this research is to study the impact of sequential pattern mining in the phase of aspect extraction. Our experimental results show that the approach proposed in our work produced better results as compared with the state-of-the-art approaches."
A hybrid recommender system for the mining of consumer preferences from their reviews,"Product review sites are widespread on the Internet and are rapidly gaining in popularity among consumers. This already large volume of user-generated content is dramatically growing every day, making it hard for consumers to filter out the worthwhile information which appears on the various review sites. There commendation system plays a significant role in solving the problem of information overload. This study proposes a framework which integrates a collaborative filtering approach and an opinion mining technique for movie recommendation. Within the proposed framework, sentiment analysis is first applied to the usersâ reviews to detect consumer opinions about the movie they have watched and to explore the individualâs preference profile. Traditional recommendation models are overly dependent on preference ratings and often suffer from the problem of âdata sparsityâ. Experimental results obtained from real online reviews show that our proposed method is effective in dealing with insufficient data and is more accurate and efficient than existing traditional methods. Â© The Author(s) 2019.","Product review sites are widespread on the Internet and are rapidly gaining in popularity among consumers. This already large volume of user-generated content is dramatically growing every day, making it hard for consumers to filter out the worthwhile information which appears on the various review sites. There commendation system plays a significant role in solving the problem of information overload. This study proposes a framework which integrates a collaborative filtering approach and an opinion mining technique for movie recommendation. Within the proposed framework, sentiment analysis is first applied to the users reviews to detect consumer opinions about the movie they have watched and to explore the individuals preference profile. Traditional recommendation models are overly dependent on preference ratings and often suffer from the problem of data sparsity. Experimental results obtained from real online reviews show that our proposed method is effective in dealing with insufficient data and is more accurate and efficient than existing traditional methods."
A meta-heuristic framework based on clustering and preprocessed datasets for solving the link prediction problem,"This study presents a solution to a problem commonly known as link prediction problem. Link prediction problem interests in predicting the possibility of appearing a connection between two nodes of a network, while there is no connection between these nodes in the present state of the network. Finding a solution to link prediction problem attracts variety of computer science fields such as data mining and machine learning. This attraction is due to its importance for many applications such as social networks, bioinformatics and co-authorship networks. Towards solving this problem, Evolutionary Link Prediction (EVO-LP) framework is proposed, presented, analysed and tested. EVO-LP is a framework that includes dataset preprocessing approach and a meta-heuristic algorithm based on clustering for prediction. EVO-LP is divided into preprocessing stage and link prediction stage. Feature extraction, data under-sampling and feature selection are utilised in the preprocessing stage, while in the prediction stage, a meta-heuristic algorithm based on clustering is used as an optimiser. Experimental results on a number of real networks show that EVO-LP improves the prediction quality with low time complexity. Â© The Author(s) 2018.","This study presents a solution to a problem commonly known as link prediction problem. Link prediction problem interests in predicting the possibility of appearing a connection between two nodes of a network, while there is no connection between these nodes in the present state of the network. Finding a solution to link prediction problem attracts variety of computer science fields such as data mining and machine learning. This attraction is due to its importance for many applications such as social networks, bioinformatics and co-authorship networks. Towards solving this problem, Evolutionary Link Prediction (EVO-LP) framework is proposed, presented, analysed and tested. EVO-LP is a framework that includes dataset preprocessing approach and a meta-heuristic algorithm based on clustering for prediction. EVO-LP is divided into preprocessing stage and link prediction stage. Feature extraction, data under-sampling and feature selection are utilised in the preprocessing stage, while in the prediction stage, a meta-heuristic algorithm based on clustering is used as an optimiser. Experimental results on a number of real networks show that EVO-LP improves the prediction quality with low time complexity."
A linked open data framework to enhance the discoverability and impact of culture heritage,"Cultural heritage institutions have recently begun to consider the benefits of sharing their collections using linked open data to disseminate and enrich their metadata. As datasets become very large, challenges appear, such as ingestion, management, querying and enrichment. Furthermore, each institution has particular features related to important aspects such as vocabularies and interoperability, which make it difficult to generalise this process and provide one-for-all solutions. In order to improve the user experience as regards information retrieval systems, researchers have identified that further refinements are required for the recognition and extraction of implicit relationships expressed in natural language. We introduce a framework for the enrichment and disambiguation of locations in text using open knowledge bases such as Wikidata and GeoNames. The framework has been successfully used to publish a dataset based on information from the Biblioteca Virtual Miguel de Cervantes, thus illustrating how semantic enrichment can help information retrieval. The methods applied in order to automate the enrichment process, which build upon open source software components, are described herein. Â© The Author(s) 2018.","Cultural heritage institutions have recently begun to consider the benefits of sharing their collections using linked open data to disseminate and enrich their metadata. As datasets become very large, challenges appear, such as ingestion, management, querying and enrichment. Furthermore, each institution has particular features related to important aspects such as vocabularies and interoperability, which make it difficult to generalise this process and provide one-for-all solutions. In order to improve the user experience as regards information retrieval systems, researchers have identified that further refinements are required for the recognition and extraction of implicit relationships expressed in natural language. We introduce a framework for the enrichment and disambiguation of locations in text using open knowledge bases such as Wikidata and GeoNames. The framework has been successfully used to publish a dataset based on information from the Biblioteca Virtual Miguel de Cervantes, thus illustrating how semantic enrichment can help information retrieval. The methods applied in order to automate the enrichment process, which build upon open source software components, are described herein."
DIC-DOC-K-means: Dissimilarity-based Initial Centroid selection for DOCument clustering using K-means for improving the effectiveness of text document clustering,"In this article, a new initial centroid selection for a K-means document clustering algorithm, namely, Dissimilarity-based Initial Centroid selection for DOCument clustering using K-means (DIC-DOC-K-means), to improve the performance of text document clustering is proposed. The first centroid is the document having the minimum standard deviation of its term frequency. Each of the other subsequent centroids is selected based on the dissimilarities of the previously selected centroids. For comparing the performance of the proposed DIC-DOC-K-means algorithm, the results of the K-means, K-means++ and weighted average of terms-based initial centroid selection + K-means (Weight_Avg_Initials + K-means) clustering algorithms are considered. The results show that the proposed DIC-DOC-K-means algorithm performs significantly better than the K-means, K-means++ and Weight_Avg_Initials+ K-means clustering algorithms for Reuters-21578 and WebKB with respect to purity, entropy and F-measure for most of the cluster sizes. The cluster sizes used for Reuters-8 are 8, 16, 24 and 32 and those for WebKB are 4, 8, 12 and 16. The results of the proposed DIC-DOC-K-means give a better performance for the number of clusters that are equal to the number of classes in the data set. Â© The Author(s) 2018.","In this article, a new initial centroid selection for a K-means document clustering algorithm, namely, Dissimilarity-based Initial Centroid selection for DOCument clustering using K-means (DIC-DOC-K-means), to improve the performance of text document clustering is proposed. The first centroid is the document having the minimum standard deviation of its term frequency. Each of the other subsequent centroids is selected based on the dissimilarities of the previously selected centroids. For comparing the performance of the proposed DIC-DOC-K-means algorithm, the results of the K-means, K-means++ and weighted average of terms-based initial centroid selection + K-means (Weight_Avg_Initials + K-means) clustering algorithms are considered. The results show that the proposed DIC-DOC-K-means algorithm performs significantly better than the K-means, K-means++ and Weight_Avg_Initials+ K-means clustering algorithms for Reuters-21578 and WebKB with respect to purity, entropy and F-measure for most of the cluster sizes. The cluster sizes used for Reuters-8 are 8, 16, 24 and 32 and those for WebKB are 4, 8, 12 and 16. The results of the proposed DIC-DOC-K-means give a better performance for the number of clusters that are equal to the number of classes in the data set."
Bucketed common vector scaling for authorship attribution in heterogeneous web collections: A scaling approach for authorship attribution,"Domain, genre and topic influences on author style adversely affect the performance of authorship attribution (AA) in multi-genre and multi-domain data sets. Although recent approaches to AA tasks focus on suggesting new feature sets and sampling techniques to improve the robustness of a classification system, they do not incorporate domain-specific properties to reduce the negative impact of irrelevant features on AA. This study presents a novel scaling approach, namely, bucketed common vector scaling, to efficiently reduce negative domain influence without reducing the dimensionality of existing features; therefore, this approach is easily transferable and applicable in a classification system. Classification performances on English-language competition data sets consisting of emails and articles and Turkish-language web documents consisting of blogs, articles and tweets indicate that our approach is very competitive to top-performing approaches in English competition data sets and is significantly improving the top classification performance in mixed-domain experiments on blogs, articles and tweets. Â© The Author(s) 2019.","Domain, genre and topic influences on author style adversely affect the performance of authorship attribution (AA) in multi-genre and multi-domain data sets. Although recent approaches to AA tasks focus on suggesting new feature sets and sampling techniques to improve the robustness of a classification system, they do not incorporate domain-specific properties to reduce the negative impact of irrelevant features on AA. This study presents a novel scaling approach, namely, bucketed common vector scaling, to efficiently reduce negative domain influence without reducing the dimensionality of existing features; therefore, this approach is easily transferable and applicable in a classification system. Classification performances on English-language competition data sets consisting of emails and articles and Turkish-language web documents consisting of blogs, articles and tweets indicate that our approach is very competitive to top-performing approaches in English competition data sets and is significantly improving the top classification performance in mixed-domain experiments on blogs, articles and tweets."
An improved evidence-based aggregation method for sentiment analysis,"Sentiment analysis is one of the natural language processing tasks used to find reviews expressed in online texts and classify them into different classes. One of the most important factors affecting the efficiency of sentiment analysis methods is the aggregation algorithm used for scores combination. Recently, DempsterâShafer algorithm has been used for scores aggregation. This algorithm has a higher precision than common methods such as average, weighed average, product and voting, but the problem with this algorithm is the aggregation of a dominant high or low score that is always selected by the algorithm as the overall score. In the current research, a new method is proposed for scores aggregation that employs both the most and the second probable classes to predict the final score. The proposed approach considers every review as a set of sentences each of which has its own sentiment orientation and score and computes the probability of belonging of every sentence to different classes in a five-star scale using a pure lexicon-based system. These probabilities are then used for document-level sentiment detection. To this aim, two-point structure is used to improve the DempsterâShafer aggregation algorithm. The proposed method is applied to review datasets of TripAdvisor and CitySearch which have been used in previous studies. The obtained results show that in comparison with the original DempsterâShafer aggregation method, the precision of the proposed method for both datasets is 23% and 27% higher, respectively. Â© The Author(s) 2019.","Sentiment analysis is one of the natural language processing tasks used to find reviews expressed in online texts and classify them into different classes. One of the most important factors affecting the efficiency of sentiment analysis methods is the aggregation algorithm used for scores combination. Recently, DempsterShafer algorithm has been used for scores aggregation. This algorithm has a higher precision than common methods such as average, weighed average, product and voting, but the problem with this algorithm is the aggregation of a dominant high or low score that is always selected by the algorithm as the overall score. In the current research, a new method is proposed for scores aggregation that employs both the most and the second probable classes to predict the final score. The proposed approach considers every review as a set of sentences each of which has its own sentiment orientation and score and computes the probability of belonging of every sentence to different classes in a five-star scale using a pure lexicon-based system. These probabilities are then used for document-level sentiment detection. To this aim, two-point structure is used to improve the DempsterShafer aggregation algorithm. The proposed method is applied to review datasets of TripAdvisor and CitySearch which have been used in previous studies. The obtained results show that in comparison with the original DempsterShafer aggregation method, the precision of the proposed method for both datasets is 23% and 27% higher, respectively."
"Corrigendum to Spam Profiles Detection on Social Networks Using Computational Intelligence Methods: The Effect of The Lingual Context (Journal of Information Science, (2019), 10.1177/0165551519861599)","In this article, the authors were listed in the incorrect order. The article has now been updated to reflect the correct author name order: Alaâ M Al-Zoubi, Jaâfar Alqatawna, Hossam Faris and Mohammad A Hassonah. Â© The Author(s) 2019.","In this article, the authors were listed in the incorrect order. The article has now been updated to reflect the correct author name order: Ala M Al-Zoubi, Jafar Alqatawna, Hossam Faris and Mohammad A Hassonah."
Can social news websites pay for content and curation? The SteemIt cryptocurrency model,"SteemIt is a Reddit-like social news site that pays members for posting and curating content. It uses micropayments backed by a tradeable currency, exploiting the Bitcoin cryptocurrency generation model to finance content provision in conjunction with advertising. If successful, this paradigm might change the way in which volunteer-based sites operate. This article investigates 925,092 new membersâ first posts for insights into what drives financial success in the site. Initial blog posts on average received US$0.01, although the maximum accrued was US$20,680.83. Longer, more sentiment-rich or more positive comments with personal information received the greatest financial reward in contrast to more informational or topical content. Thus, there is a clear financial value in starting with a friendly introduction rather than immediately attempting to provide useful content, despite the latter being the ultimate site goal. Follow-up posts also tended to be more successful when more personal, suggesting that interpersonal communication rather than quality content provision has driven the site so far. It remains to be seen whether the model of small typical rewards and the possibility that a post might generate substantially more are enough to incentivise long-term participation or a greater focus on informational posts in the long term. Â© The Author(s) 2017.","SteemIt is a Reddit-like social news site that pays members for posting and curating content. It uses micropayments backed by a tradeable currency, exploiting the Bitcoin cryptocurrency generation model to finance content provision in conjunction with advertising. If successful, this paradigm might change the way in which volunteer-based sites operate. This article investigates 925,092 new members first posts for insights into what drives financial success in the site. Initial blog posts on average received US$0.01, although the maximum accrued was US$20,680.83. Longer, more sentiment-rich or more positive comments with personal information received the greatest financial reward in contrast to more informational or topical content. Thus, there is a clear financial value in starting with a friendly introduction rather than immediately attempting to provide useful content, despite the latter being the ultimate site goal. Follow-up posts also tended to be more successful when more personal, suggesting that interpersonal communication rather than quality content provision has driven the site so far. It remains to be seen whether the model of small typical rewards and the possibility that a post might generate substantially more are enough to incentivise long-term participation or a greater focus on informational posts in the long term."
Similarity versus relatedness: A novel approach in extractive Persian document summarisation,"Automatic text summarisation is the process of creating a summary from one or more documents by eliminating the details and preserving the worthwhile information. This article presents a single/multi-document summariser using a novel clustering method for creating summaries. First, a feature selection phase is employed. Then, FarsNet, the Persian WordNet, is utilised to extract the semantic information of words. Therefore, the input sentences are categorised into three main clusters: similarity, relatedness and coherency. Each similarity cluster contains similar sentences to its core, while each relatedness cluster contains sentences that are related (but not similar) to its core. The coherency clusters show the sentences that should be kept together to preserve the coherency of the summary. Finally, the centroid of each similarity cluster having the most feature score is added to an empty summary. The summary is enlarged by including related sentences from relatedness clusters and excluding similar sentences to its content iteratively. Coherency clusters are applied to the created summary in the last step. The proposed method has been compared with three known existing text summarisation systems and techniques for the Persian language: FarsiSum, Parsumist and Ijaz. Our proposed method leads to improvement in experimental results on different measurements including precision, recall, F-measure, ROUGE-N and ROUGE-L. Â© The Author(s) 2017.","Automatic text summarisation is the process of creating a summary from one or more documents by eliminating the details and preserving the worthwhile information. This article presents a single/multi-document summariser using a novel clustering method for creating summaries. First, a feature selection phase is employed. Then, FarsNet, the Persian WordNet, is utilised to extract the semantic information of words. Therefore, the input sentences are categorised into three main clusters: similarity, relatedness and coherency. Each similarity cluster contains similar sentences to its core, while each relatedness cluster contains sentences that are related (but not similar) to its core. The coherency clusters show the sentences that should be kept together to preserve the coherency of the summary. Finally, the centroid of each similarity cluster having the most feature score is added to an empty summary. The summary is enlarged by including related sentences from relatedness clusters and excluding similar sentences to its content iteratively. Coherency clusters are applied to the created summary in the last step. The proposed method has been compared with three known existing text summarisation systems and techniques for the Persian language: FarsiSum, Parsumist and Ijaz. Our proposed method leads to improvement in experimental results on different measurements including precision, recall, F-measure, ROUGE-N and ROUGE-"
From mud to the museum: Metadata challenges in archaeology,"An archaeological site is a palimpsest in which the evidence of the depositional episodes is destroyed through the excavation processes; all that remains are the artefacts and their documentary evidence manifested in registers, datasets, dig diaries and reports. While the reports may represent the end product of a specific excavation, the archaeological record tells a story; it is interpretative and dynamic, with later excavations adding new knowledge and narratives. Museums preserve the artefacts but unless the documentary evidence is preserved in standard formats, it cannot be easily re-used by the archaeology community to create that knowledge; nor can museums provide the narratives for the general public whose cultural heritage it is. This article presents a case study from the Ness of Brodgar excavations that examines possibilities for reconciling one part of the data of an archaeological dig, the small finds register (SFR) and its sparse amount of descriptive metadata, with the potentiality of data re-use and with the requirements of a museum that may have custody of the artefacts. It maps and enriches messy domain-specific ontologies to standard archaeological and cultural heritage ontologies and taxonomies using simple natural language processing, linked open data and the museum CIDOC conceptual reference model (CRM). This research, in examining the application of ontology mapping tools, explores common practices and processes that are useful in any discipline within the cultural heritage domain. Â© The Author(s) 2017.","An archaeological site is a palimpsest in which the evidence of the depositional episodes is destroyed through the excavation processes; all that remains are the artefacts and their documentary evidence manifested in registers, datasets, dig diaries and reports. While the reports may represent the end product of a specific excavation, the archaeological record tells a story; it is interpretative and dynamic, with later excavations adding new knowledge and narratives. Museums preserve the artefacts but unless the documentary evidence is preserved in standard formats, it cannot be easily re-used by the archaeology community to create that knowledge; nor can museums provide the narratives for the general public whose cultural heritage it is. This article presents a case study from the Ness of Brodgar excavations that examines possibilities for reconciling one part of the data of an archaeological dig, the small finds register (SFR) and its sparse amount of descriptive metadata, with the potentiality of data re-use and with the requirements of a museum that may have custody of the artefacts. It maps and enriches messy domain-specific ontologies to standard archaeological and cultural heritage ontologies and taxonomies using simple natural language processing, linked open data and the museum CIDOC conceptual reference model (CRM). This research, in examining the application of ontology mapping tools, explores common practices and processes that are useful in any discipline within the cultural heritage domain."
SGSG: Semantic graph-based storyline generation in Twitter,"Twitter is a popular microblogging service that has become a great medium for exploring emerging events and breaking news. Unfortunately, the explosive rate of information entering Twitter makes the users experience information overload. Since a great deal of tweets revolve around news events, summarising the storyline of these events can be advantageous to users, allowing them to conveniently access relevant and key information scattered over numerous tweets and, consequently, draw concise conclusions. A storyline shows the evolution of a story through time and sketches the correlations among its significant events. In this article, we propose a novel framework for generating a storyline of news events from a social point of view. Utilising powerful concepts from graph theory, we identify the significant events, summarise them and generate a coherent storyline of their evolution with reasonable computational cost for large datasets. Our approach models a storyline as a directed tree of socially salient events evolving over time in which nodes represent main events and edges capture the semantic relations between related events. We evaluate our proposed method against human-generated storylines, as well as the previous state-of-the-art storyline generation algorithm, on two large-scale datasets, one consisting of English tweets and the other one consisting of Persian tweets. We find that the results of our method are superior to the previous best algorithm and can be comparable with human-generated storylines. Â© The Author(s) 2018.","Twitter is a popular microblogging service that has become a great medium for exploring emerging events and breaking news. Unfortunately, the explosive rate of information entering Twitter makes the users experience information overload. Since a great deal of tweets revolve around news events, summarising the storyline of these events can be advantageous to users, allowing them to conveniently access relevant and key information scattered over numerous tweets and, consequently, draw concise conclusions. A storyline shows the evolution of a story through time and sketches the correlations among its significant events. In this article, we propose a novel framework for generating a storyline of news events from a social point of view. Utilising powerful concepts from graph theory, we identify the significant events, summarise them and generate a coherent storyline of their evolution with reasonable computational cost for large datasets. Our approach models a storyline as a directed tree of socially salient events evolving over time in which nodes represent main events and edges capture the semantic relations between related events. We evaluate our proposed method against human-generated storylines, as well as the previous state-of-the-art storyline generation algorithm, on two large-scale datasets, one consisting of English tweets and the other one consisting of Persian tweets. We find that the results of our method are superior to the previous best algorithm and can be comparable with human-generated storylines."
A hybrid approach for article recommendation in research social networks,"With the prevalence of research social networks, determining effective methods for recommending scientific articles to online scholars has become a challenging and complex task. Current studies on article recommendation works are focused on digital libraries and reference sharing websites while studies on research social networking websites have seldom been conducted. Existing content-based approaches or collaborative filtering approaches suffer from the problem of data sparsity. The quality information of articles has been largely ignored in previous studies, thus raising the need for a unified recommendation framework. We propose a hybrid approach to combine relevance, connectivity and quality to recommend scientific articles. The effectiveness of the proposed framework and methods is verified using a user study on a real research social network website. The results demonstrate that our proposed methods outperform baseline methods. Â© The Author(s) 2017.","With the prevalence of research social networks, determining effective methods for recommending scientific articles to online scholars has become a challenging and complex task. Current studies on article recommendation works are focused on digital libraries and reference sharing websites while studies on research social networking websites have seldom been conducted. Existing content-based approaches or collaborative filtering approaches suffer from the problem of data sparsity. The quality information of articles has been largely ignored in previous studies, thus raising the need for a unified recommendation framework. We propose a hybrid approach to combine relevance, connectivity and quality to recommend scientific articles. The effectiveness of the proposed framework and methods is verified using a user study on a real research social network website. The results demonstrate that our proposed methods outperform baseline methods."
Evaluation of websitesâ compliance to legal and ethical guidelines: A fuzzy logicâbased methodology,"Privacy issues are a top priority in web design. However, websitesâ evaluation methods do not consider legal and ethical issues. This article proposes a fuzzy logicâbased methodology for evaluating websitesâ compliance with legal and ethical principles. Using fuzzy Delphi and fuzzy numbers, the methodology develops the Fuzzy Legal and Ethical Compliance Index (FLECI) that addresses the inherited vagueness of the evaluation process and calculates websitesâ conformity to legal and ethical guidelines. To illustrate the proposed methodology, this research collects data and then evaluates and classifies 100 websites with respect to their privacy policies using fuzzy equivalence. This article provides a foundation for the development of comprehensive website evaluation methods that include privacy and ethical issues in their evaluations. Future research can investigate the applicability of the proposed methodology and the fuzzy numbers calculated in this article in websites across industries and cultural activities. Â© The Author(s) 2017.","Privacy issues are a top priority in web design. However, websites evaluation methods do not consider legal and ethical issues. This article proposes a fuzzy logicbased methodology for evaluating websites compliance with legal and ethical principles. Using fuzzy Delphi and fuzzy numbers, the methodology develops the Fuzzy Legal and Ethical Compliance Index (FLECI) that addresses the inherited vagueness of the evaluation process and calculates websites conformity to legal and ethical guidelines. To illustrate the proposed methodology, this research collects data and then evaluates and classifies 100 websites with respect to their privacy policies using fuzzy equivalence. This article provides a foundation for the development of comprehensive website evaluation methods that include privacy and ethical issues in their evaluations. Future research can investigate the applicability of the proposed methodology and the fuzzy numbers calculated in this article in websites across industries and cultural activities."
A comparative study of three teaching methods on student information literacy in stand-alone credit-bearing university courses,"Three teaching methods, applied to credit-bearing information literacy (IL) university courses, were evaluated and compared. The effects of lecture-based learning (LBL), project-based learning (PjBL) and problem-based learning (PBL) were investigated using the information literacy test (ILT) as an assessment tool, with regard to the total ILT score, specific IL contents according to the five ACRL standards and students' mental skills according to the Bloom's cognitive categories. While all three teaching methods showed a significant improvement in the ILT post-test, the active-learning groups of PjBL and PBL scored significantly better than the LBL group. The most notable positive difference was observed in students' effective access to information related to database searching skills, in the intellectual property/ethics issues and in the cognitive category of comprehension. The PjBL and PBL post-test results did not differ significantly, indicating that both active learning methods resulted in similar improvements of students' IL. Â© Chartered Institute of Library and Information Professionals.","Three teaching methods, applied to credit-bearing information literacy university courses, were evaluated and compared. The effects of lecture-based learning (LBL), project-based learning (PjBL) and problem-based learning (PBL) were investigated using the information literacy test (ILT) as an assessment tool, with regard to the total ILT score, specific IL contents according to the five ACRL standards and students' mental skills according to the Bloom's cognitive categories. While all three teaching methods showed a significant improvement in the ILT post-test, the active-learning groups of PjBL and PBL scored significantly better than the LBL group. The most notable positive difference was observed in students' effective access to information related to database searching skills, in the intellectual property/ethics issues and in the cognitive category of comprehension. The PjBL and PBL post-test results did not differ significantly, indicating that both active learning methods resulted in similar improvements of students'"
Enabling smart objects discovery via constructing hypergraphs of heterogeneous IoT interactions,"Recent advances in the Internet of Things (IoT) have led to the rise of a new paradigm: Social Internet of Things (SIoT). However, the new paradigm, as inspired by the idea that smart objects will soon have a certain degree of social consciousness, is still in its infant state for several reasons. Most of the related works are far from embracing the social aspects of smart objects and the dynamicity of inter-object social relations. Furthermore, there is yet to be a coherent structure for organising and managing IoT objects that elicit social-like features. To fully understand how and to what extent these objects mimic the behaviours of humans, we first model SIoT by scrutinising the distinct characteristics and structural facets of human-centric social networks. To elaborate, we describe the process of profiling the IoT objects that become social and classify various inter-object social relationships. Afterwards, a novel discovery mechanism, which utilises our hypergraph-based overlay network model, is proposed. To test the feasibility of the proposed approach, we have performed several experiments on our smart home automation demo box built with various sensors and actuators. Â© 2016, Â© The Author(s) 2016.","Recent advances in the Internet of Things (IoT) have led to the rise of a new paradigm: Social Internet of Things (SIoT). However, the new paradigm, as inspired by the idea that smart objects will soon have a certain degree of social consciousness, is still in its infant state for several reasons. Most of the related works are far from embracing the social aspects of smart objects and the dynamicity of inter-object social relations. Furthermore, there is yet to be a coherent structure for organising and managing IoT objects that elicit social-like features. To fully understand how and to what extent these objects mimic the behaviours of humans, we first model SIoT by scrutinising the distinct characteristics and structural facets of human-centric social networks. To elaborate, we describe the process of profiling the IoT objects that become social and classify various inter-object social relationships. Afterwards, a novel discovery mechanism, which utilises our hypergraph-based overlay network model, is proposed. To test the feasibility of the proposed approach, we have performed several experiments on our smart home automation demo box built with various sensors and actuators."
Predictive aspect-based sentiment classification of online tourist reviews,"With the increase of online tourists reviews, discovering sentimental idea regarding a tourist place through the posted reviews is becoming a challenging task. The presence of various aspects discussed in user reviews makes it even harder to accurately extract and classify the sentiments. Aspect-based sentiment analysis aims to extract and classify userâs positive or negative orientation towards each aspect. Although several aspect-based sentiment classification methods have been proposed in the past, limited work has been targeted towards the automatic extraction of implicit, infrequent and co-referential aspects. Moreover, existing methods lack the ability to accurately classify the overall polarity of multi-aspect sentiments. This study aims to develop a predictive framework for aspect-based extraction and classification. The proposed framework utilises the semantic relations among review phrases to extract implicit and infrequent aspects for accurate sentiment predictions. Experiments have been performed using real-world data sets crawled from predominant tourist websites such as TripAdvisor and OpenTable. Experimental results and comparison with previously reported findings prove that the predictive framework not only extracts the aspects effectively but also improves the prediction accuracy of aspects. Â© The Author(s) 2018.","With the increase of online tourists reviews, discovering sentimental idea regarding a tourist place through the posted reviews is becoming a challenging task. The presence of various aspects discussed in user reviews makes it even harder to accurately extract and classify the sentiments. Aspect-based sentiment analysis aims to extract and classify users positive or negative orientation towards each aspect. Although several aspect-based sentiment classification methods have been proposed in the past, limited work has been targeted towards the automatic extraction of implicit, infrequent and co-referential aspects. Moreover, existing methods lack the ability to accurately classify the overall polarity of multi-aspect sentiments. This study aims to develop a predictive framework for aspect-based extraction and classification. The proposed framework utilises the semantic relations among review phrases to extract implicit and infrequent aspects for accurate sentiment predictions. Experiments have been performed using real-world data sets crawled from predominant tourist websites such as TripAdvisor and OpenTable. Experimental results and comparison with previously reported findings prove that the predictive framework not only extracts the aspects effectively but also improves the prediction accuracy of aspects."
Understanding Twitter use by major LIS professional organisations in the United States,"Although Twitter has been widely adopted by professional organisations, there has been a lack of understanding and research on its utilisation. This article presents a study that looks into how five major library and information science (LIS) professional organisations in the United States use Twitter, including the American Library Association (ALA), Special Libraries Association (SLA), Association for Library and Information Science Education (ALISE), Association for Information Science and Technology (ASIS&T) and the iSchools. Specifically explored are the characteristics of Twitter usage, such as prevalent topics or contents, type of users involved, as well as the user influence based on number of mentions and retweets. The article also presents the network interactions among the LIS associations on Twitter. A systematic Twitter analysis framework of descriptive analytics, content analytics, user analysis and network analytics with relevant metrics used in this study can be applied to other studies of Twitter use. Â© 2017, Â© The Author(s) 2017.","Although Twitter has been widely adopted by professional organisations, there has been a lack of understanding and research on its utilisation. This article presents a study that looks into how five major library and information science (LIS) professional organisations in the United States use Twitter, including the American Library Association (ALA), Special Libraries Association (SLA), Association for Library and Information Science Education (ALISE), Association for Information Science and Technology (ASIS&T) and the iSchools. Specifically explored are the characteristics of Twitter usage, such as prevalent topics or contents, type of users involved, as well as the user influence based on number of mentions and retweets. The article also presents the network interactions among the LIS associations on Twitter. A systematic Twitter analysis framework of descriptive analytics, content analytics, user analysis and network analytics with relevant metrics used in this study can be applied to other studies of Twitter use."
News events prediction using Markov logic networks,"Predicting future events from text data has been a controversial and much disputed topic in the field of text analytics. However, far too little attention has been paid to efficient prediction in textual environments. This study has aimed to develop a novel and efficient method for news event prediction. The proposed method is based on Markov logic networks (MLNs) framework, which enables us to concisely represent complex events by full expressivity of first-order logic (FOL), as well as to reason uncertain event with probabilities. In our framework, we first extract text news events via an event representation model at a semantic level and then transform them into web ontology language (OWL) as a posteriori knowledge. A set of domain-specific causal rules in FOL associated with weights were also fed into the system as a priori (common-sense) knowledge. Additionally, several large-scale ontologies including DBpedia, VerbNet and WordNet were used to model common-sense logic rules as contextual knowledge. Finally, all types of such knowledge were integrated into OWL for performing causal inference. The resulted OWL knowledge base is augmented by MLN, which uses weighted first-order formulas to represent probabilistic knowledge. Empirical evaluation of real news showed that our method of news event prediction was better than the baselines in terms of precision, coverage and diversity. Â© 2016, Â© The Author(s) 2016.","Predicting future events from text data has been a controversial and much disputed topic in the field of text analytics. However, far too little attention has been paid to efficient prediction in textual environments. This study has aimed to develop a novel and efficient method for news event prediction. The proposed method is based on Markov logic networks (MLNs) framework, which enables us to concisely represent complex events by full expressivity of first-order logic (FOL), as well as to reason uncertain event with probabilities. In our framework, we first extract text news events via an event representation model at a semantic level and then transform them into web ontology language (OWL) as a posteriori knowledge. A set of domain-specific causal rules in FOL associated with weights were also fed into the system as a priori (common-sense) knowledge. Additionally, several large-scale ontologies including DBpedia, VerbNet and WordNet were used to model common-sense logic rules as contextual knowledge. Finally, all types of such knowledge were integrated into OWL for performing causal inference. The resulted OWL knowledge base is augmented by MLN, which uses weighted first-order formulas to represent probabilistic knowledge. Empirical evaluation of real news showed that our method of news event prediction was better than the baselines in terms of precision, coverage and diversity."
Similarity-based link prediction in social networks: A path and node combined approach,"With the rapid development of the Internet, the computational analysis of social networks has grown to be a salient issue. Various research analyses social network topics, and a considerable amount of attention has been devoted to the issue of link prediction. Link prediction aims to predict the interactions that might occur between two entities in the network. To this aim, this study proposed a novel path and node combined approach and constructed a methodology for measuring node similarities. The method was illustrated with five real datasets obtained from different types of social networks. An extensive comparison of the proposed method against existing link prediction algorithms was performed to demonstrate that the path and node combined approach achieved much higher mean average precision (MAP) and area under the curve (AUC) values than those that only consider common nodes (e.g. Common Neighbours and Adamic/Adar) or paths (e.g. Random Walk with Restart and FriendLink). The results imply that two nodes are more likely to establish a link if they have more common neighbours of lower degrees. The weight of the path connecting two nodes is inversely proportional to the product of degrees of nodes on the pathway. The combination of node and topological features can substantially improve the performance of similarity-based link prediction, compared with node-dependent and path-dependent approaches. The experiments also demonstrate that the path-dependent approaches outperform the node-dependent appraoches. This indicates that topological features of networks may contribute more to improving performance than node features. Â© Chartered Institute of Library and Information Professionals.","With the rapid development of the Internet, the computational analysis of social networks has grown to be a salient issue. Various research analyses social network topics, and a considerable amount of attention has been devoted to the issue of link prediction. Link prediction aims to predict the interactions that might occur between two entities in the network. To this aim, this study proposed a novel path and node combined approach and constructed a methodology for measuring node similarities. The method was illustrated with five real datasets obtained from different types of social networks. An extensive comparison of the proposed method against existing link prediction algorithms was performed to demonstrate that the path and node combined approach achieved much higher mean average precision (MAP) and area under the curve (AUC) values than those that only consider common nodes ( Common Neighbours and Adamic/Adar) or paths ( Random Walk with Restart and FriendLink). The results imply that two nodes are more likely to establish a link if they have more common neighbours of lower degrees. The weight of the path connecting two nodes is inversely proportional to the product of degrees of nodes on the pathway. The combination of node and topological features can substantially improve the performance of similarity-based link prediction, compared with node-dependent and path-dependent approaches. The experiments also demonstrate that the path-dependent approaches outperform the node-dependent appraoches. This indicates that topological features of networks may contribute more to improving performance than node features."
An ensemble scheme based on language function analysis and feature engineering for text genre classification,"Text genre classification is the process of identifying functional characteristics of text documents. The immense quantity of text documents available on the web can be properly filtered, organised and retrieved with the use of text genre classification, which may have potential use on several other tasks of natural language processing and information retrieval. Genre may refer to several aspects of text documents, such as function and purpose. The language function analysis (LFA) concentrates on single aspect of genres and it aims to classify text documents into three abstract classes, such as expressive, appellative and informative. Text genre classification is typically performed by supervised machine learning algorithms. The extraction of an efficient feature set to represent text documents is an essential task for building a robust classification scheme with high predictive performance. In addition, ensemble learning, which combines the outputs of individual classifiers to obtain a robust classification scheme, is a promising research field in machine learning research. In this regard, this article presents an extensive comparative analysis of different feature engineering schemes (such as features used in authorship attribution, linguistic features, character n-grams, part of speech n-grams and the frequency of the most discriminative words) and five different base learners (NaÃ¯ve Bayes, support vector machines, logistic regression, k-nearest neighbour and Random Forest) in conjunction with ensemble learning methods (such as Boosting, Bagging and Random Subspace). Based on the empirical analysis, an ensemble classification scheme is presented, which integrates Random Subspace ensemble of Random Forest with four types of features (features used in authorship attribution, character n-grams, part of speech n-grams and the frequency of the most discriminative words). For LFA corpus, the highest average predictive performance obtained by the proposed scheme is 94.43%. Â© 2016, Â© The Author(s) 2016.","Text genre classification is the process of identifying functional characteristics of text documents. The immense quantity of text documents available on the web can be properly filtered, organised and retrieved with the use of text genre classification, which may have potential use on several other tasks of natural language processing and information retrieval. Genre may refer to several aspects of text documents, such as function and purpose. The language function analysis (LFA) concentrates on single aspect of genres and it aims to classify text documents into three abstract classes, such as expressive, appellative and informative. Text genre classification is typically performed by supervised machine learning algorithms. The extraction of an efficient feature set to represent text documents is an essential task for building a robust classification scheme with high predictive performance. In addition, ensemble learning, which combines the outputs of individual classifiers to obtain a robust classification scheme, is a promising research field in machine learning research. In this regard, this article presents an extensive comparative analysis of different feature engineering schemes (such as features used in authorship attribution, linguistic features, character n-grams, part of speech n-grams and the frequency of the most discriminative words) and five different base learners (Nave Bayes, support vector machines, logistic regression, k-nearest neighbour and Random Forest) in conjunction with ensemble learning methods (such as Boosting, Bagging and Random Subspace). Based on the empirical analysis, an ensemble classification scheme is presented, which integrates Random Subspace ensemble of Random Forest with four types of features (features used in authorship attribution, character n-grams, part of speech n-grams and the frequency of the most discriminative words). For LFA corpus, the highest average predictive performance obtained by the proposed scheme is 94.43%."
Modelling information diffusion based on non-dominated friends in social networks,"In recent years, social networks have played a strong role in diffusing information among people all around the globe. Therefore, the ability to analyse the diffusion pattern is essential. A diffusion model can identify the information dissemination pattern in a social network. One of the most important components of a diffusion model is information perception which determines the source each node receives its information from. Previous studies have assumed information perception to be just based on a single factor, that is, each individual receives information from their friend with the highest amount of information, whereas in reality, there exist other factors, such as trust, that affect the decision of people for selecting the friend who would supply information. These factors might be in conflict with each other, and modelling diffusion process with respect to a single factor can give rise to unacceptable results with respect to the other factors. In this article, we propose a novel information diffusion model based on non-dominated friends (IDNDF). Non-dominated friends are a set of friends of a node for whom there is no friend better than them in the set based on all considered factors, considering different factors simultaneously significantly enhance the proposed information diffusion model. Moreover, our model gives a chance to all non-dominated friends to be selected. Also, IDNDF allows having partial knowledge by each node of the social network. Finally, IDNDF is applicable to different types of data, including well-known real social networks like Epinions, WikiPedia, Advogato and so on. Extensive experiments are performed to assess the performance of the proposed model. The results show the efficiency of the IDNDF in diffusion of information in varieties of social networks. Â© 2016, Â© The Author(s) 2016.","In recent years, social networks have played a strong role in diffusing information among people all around the globe. Therefore, the ability to analyse the diffusion pattern is essential. A diffusion model can identify the information dissemination pattern in a social network. One of the most important components of a diffusion model is information perception which determines the source each node receives its information from. Previous studies have assumed information perception to be just based on a single factor, that is, each individual receives information from their friend with the highest amount of information, whereas in reality, there exist other factors, such as trust, that affect the decision of people for selecting the friend who would supply information. These factors might be in conflict with each other, and modelling diffusion process with respect to a single factor can give rise to unacceptable results with respect to the other factors. In this article, we propose a novel information diffusion model based on non-dominated friends (IDNDF). Non-dominated friends are a set of friends of a node for whom there is no friend better than them in the set based on all considered factors, considering different factors simultaneously significantly enhance the proposed information diffusion model. Moreover, our model gives a chance to all non-dominated friends to be selected. Also, IDNDF allows having partial knowledge by each node of the social network. Finally, IDNDF is applicable to different types of data, including well-known real social networks like Epinions, WikiPedia, Advogato and so on. Extensive experiments are performed to assess the performance of the proposed model. The results show the efficiency of the IDNDF in diffusion of information in varieties of social networks."
Overall quality assessment of SKOS thesauri: An AHP-based approach,"The article proposes a methodology for a thesauri quality assessment that supports decision-makers in selecting thesauri by exploiting an overall quality measure. This measure takes into account the subjective perceptions of the decision-maker according to the reuse of thesauri in a specific application context. The analytic hierarchy process methodology is adopted to capture both subjective and objective facets involved in the thesauri quality assessment, thus providing a ranking of the thesauri assessed. Our methodology is applied to a set of thesauri by using user-driven application contexts. A step-by-step explanation of how the approach supports the decision process in the creation, maintenance and exploitation of a framework of linked thesauri is provided. Â© 2016, Â© The Author(s) 2016.","The article proposes a methodology for a thesauri quality assessment that supports decision-makers in selecting thesauri by exploiting an overall quality measure. This measure takes into account the subjective perceptions of the decision-maker according to the reuse of thesauri in a specific application context. The analytic hierarchy process methodology is adopted to capture both subjective and objective facets involved in the thesauri quality assessment, thus providing a ranking of the thesauri assessed. Our methodology is applied to a set of thesauri by using user-driven application contexts. A step-by-step explanation of how the approach supports the decision process in the creation, maintenance and exploitation of a framework of linked thesauri is provided."
Academicsâ attitudes towards peer review in scholarly journals and the effect of role and discipline,"This research contributes to the knowledge on academicsâ attitudes towards peer review, through an international and inter-disciplinary survey of academics, which profiles academicsâ views on the value of peer review, its benefits and the prevalence of unethical practices. Generally, academics regarded peer review as beneficial to improving their article and felt that peer review contributed significantly to the effectiveness of scholarly communication. Academics agreed that peer review could improve the readability and quality of the published paper, as well as check for accuracy, appropriate methodology, novelty and relevance to the journal. There are significant differences in the views of respondents on the basis of role, with those involved as reviewers and editors being less positive about peer review than authors. In addition, there is evidence of some disciplinary differences in views on the benefits of peer review. Â© The Author(s) 2017.","This research contributes to the knowledge on academics attitudes towards peer review, through an international and inter-disciplinary survey of academics, which profiles academics views on the value of peer review, its benefits and the prevalence of unethical practices. Generally, academics regarded peer review as beneficial to improving their article and felt that peer review contributed significantly to the effectiveness of scholarly communication. Academics agreed that peer review could improve the readability and quality of the published paper, as well as check for accuracy, appropriate methodology, novelty and relevance to the journal. There are significant differences in the views of respondents on the basis of role, with those involved as reviewers and editors being less positive about peer review than authors. In addition, there is evidence of some disciplinary differences in views on the benefits of peer review."
Taxonomic framework for factors influencing ERMS adoption in organisations of higher professional education,"An electronic records management system (ERMS) is tightly linked with most of the daily activities of educational organisations and leads to enhance their performance and decision-making. The aim of this article is to identify the significant factors that could influence the ERMS adoption in higher professional education (HPE). The methodology of this article started with identifying the factors through theory analysis and literature and also recommended by experts. Technologyâorganisationâenvironment (TOE) theory was used for factor classification. Qualitative approach was used through the interview with experts to validate and verify the proposed framework. This article presents the results of a study which identifies the issues involved in the utilisation and adoption of ERMS. More than 100 previous works and six well-known theories were critically reviewed to identify the main factors for successful ERMS adoption in different areas with the aim of proposing a taxonomic framework that can depict and identify the main factors that have an impact on the success of ERMS adoption. The proposed framework includes 11 factors categorised into three dimensions. The framework is validated and verified by experts. The adoption factors identified here provide a sound theoretical basis for research to understand, support and facilitate the adoption of ERMS to HPE benefit. The proposed framework could help to improve educational outcomes and the successful implementation of ERMS. Â© The Author(s) 2018.","An electronic records management system (ERMS) is tightly linked with most of the daily activities of educational organisations and leads to enhance their performance and decision-making. The aim of this article is to identify the significant factors that could influence the ERMS adoption in higher professional education (HPE). The methodology of this article started with identifying the factors through theory analysis and literature and also recommended by experts. Technologyorganisationenvironment (TOE) theory was used for factor classification. Qualitative approach was used through the interview with experts to validate and verify the proposed framework. This article presents the results of a study which identifies the issues involved in the utilisation and adoption of ERMS. More than 100 previous works and six well-known theories were critically reviewed to identify the main factors for successful ERMS adoption in different areas with the aim of proposing a taxonomic framework that can depict and identify the main factors that have an impact on the success of ERMS adoption. The proposed framework includes 11 factors categorised into three dimensions. The framework is validated and verified by experts. The adoption factors identified here provide a sound theoretical basis for research to understand, support and facilitate the adoption of ERMS to HPE benefit. The proposed framework could help to improve educational outcomes and the successful implementation of ERMS."
A novel method for content-based image retrieval to improve the effectiveness of the bag-of-words model using a support vector machine,"The advancements in the multimedia technologies result in the growth of the image databases. To retrieve images from such image databases using visual attributes of the images is a challenging task due to the close visual appearance among the visual attributes of these images, which also introduces the issue of the semantic gap. In this article, we recommend a novel method established on the bag-of-words (BoW) model, which perform visual words integration of the local intensity order pattern (LIOP) feature and local binary pattern variance (LBPV) feature to reduce the issue of the semantic gap and enhance the performance of the content-based image retrieval (CBIR). The recommended method uses LIOP and LBPV features to build two smaller size visual vocabularies (one from each feature), which are integrated together to build a larger size of the visual vocabulary, which also contains complementary features of both descriptors. Because for efficient CBIR, the smaller size of the visual vocabulary improves the recall, while the bigger size of the visual vocabulary improves the precision or accuracy of the CBIR. The comparative analysis of the recommended method is performed on three image databases, namely, WANG-1K, WANG-1.5K and Holidays. The experimental analysis of the recommended method on these image databases proves its robust performance as compared with the recent CBIR methods. Â© The Author(s) 2018.","The advancements in the multimedia technologies result in the growth of the image databases. To retrieve images from such image databases using visual attributes of the images is a challenging task due to the close visual appearance among the visual attributes of these images, which also introduces the issue of the semantic gap. In this article, we recommend a novel method established on the bag-of-words (BoW) model, which perform visual words integration of the local intensity order pattern (LIOP) feature and local binary pattern variance (LBPV) feature to reduce the issue of the semantic gap and enhance the performance of the content-based image retrieval (CBIR). The recommended method uses LIOP and LBPV features to build two smaller size visual vocabularies (one from each feature), which are integrated together to build a larger size of the visual vocabulary, which also contains complementary features of both descriptors. Because for efficient CBIR, the smaller size of the visual vocabulary improves the recall, while the bigger size of the visual vocabulary improves the precision or accuracy of the CBIR. The comparative analysis of the recommended method is performed on three image databases, namely, WANG-1K, WANG-1.5K and Holidays. The experimental analysis of the recommended method on these image databases proves its robust performance as compared with the recent CBIR methods."
Information overload and coping strategies in the big data context: Evidence from the hospitality sector,"With the advent of technology, a greater amount of information is available in a greater variety of formats that are accessible through a greater variety of media and communication channels, resulting in a much more complex and rich information environment for business managers. Many businesses are seeing the development of big data as unique opportunity and also experience it as demanding in terms of managerial skills and organisational capability to deal with it. Existing literature provides managerial prescriptions and systemic guidelines to make use of this information, but does not provide empirical evidence on how practising managers actually deal with information overload and make sense of the available data. This article discusses the findings from an interpretive case study of five organisations from the hotel industry within the hospitality sector. It was found that the volume and pace coupled with the qualitative and unsolicited nature of information caused information overload to managers. To cope with this phenomenon at personal level, managers used a combination of filtering, withdrawal and summarising strategies. At organisational level, the practice of summarising evolved into development and use of interactive dashboards. Â© The Author(s) 2018.","With the advent of technology, a greater amount of information is available in a greater variety of formats that are accessible through a greater variety of media and communication channels, resulting in a much more complex and rich information environment for business managers. Many businesses are seeing the development of big data as unique opportunity and also experience it as demanding in terms of managerial skills and organisational capability to deal with it. Existing literature provides managerial prescriptions and systemic guidelines to make use of this information, but does not provide empirical evidence on how practising managers actually deal with information overload and make sense of the available data. This article discusses the findings from an interpretive case study of five organisations from the hotel industry within the hospitality sector. It was found that the volume and pace coupled with the qualitative and unsolicited nature of information caused information overload to managers. To cope with this phenomenon at personal level, managers used a combination of filtering, withdrawal and summarising strategies. At organisational level, the practice of summarising evolved into development and use of interactive dashboards."
Augmented intuitive dissimilarity metric for clustering of Web user sessions,"Clustering is a very useful technique to categorise Web users with common browsing activities, access patterns and navigational behaviour. Web user clustering is used to build Web visitor profiles that make the core of a personalised information recommender system. These systems are used to comprehend Web users surfing activities by offering tailored content to Web users with similar interests. The principle objective of Web user sessions clustering is to maximise the intra-group while minimising the inter-group similarity. Efficient clustering of Web users' sessions not only depend on the clustering algorithm's nature but also depend on how well user concerns are captured and accommodated by the dissimilarity measure that are used. Determining the right dissimilarity measure to capture the access behaviour of the Web user is very significant for substantial clustering. In this paper, an intuitive dissimilarity measure is presented to estimate a Web user's concern from augmented Web user sessions. The proposed usage dissimilarity measure between two Web user sessions is based on the accessing page relevance, the syntactic structure of page URL and hierarchical structure of the website. This proposed intuitive dissimilarity measure was used with K-Medoids Clustering algorithm for experimentation and results were compared with other independent dissimilarity measures. The worth of the generated clusters were evaluated by two unsupervised cluster validity indexes. The experimental results show that intuitive augmented session dissimilarity measure is more realistic and superior as compared to the other independent dissimilarity measures regarding cluster validity indexes. Â© The Author(s) 2016.","Clustering is a very useful technique to categorise Web users with common browsing activities, access patterns and navigational behaviour. Web user clustering is used to build Web visitor profiles that make the core of a personalised information recommender system. These systems are used to comprehend Web users surfing activities by offering tailored content to Web users with similar interests. The principle objective of Web user sessions clustering is to maximise the intra-group while minimising the inter-group similarity. Efficient clustering of Web users' sessions not only depend on the clustering algorithm's nature but also depend on how well user concerns are captured and accommodated by the dissimilarity measure that are used. Determining the right dissimilarity measure to capture the access behaviour of the Web user is very significant for substantial clustering. In this paper, an intuitive dissimilarity measure is presented to estimate a Web user's concern from augmented Web user sessions. The proposed usage dissimilarity measure between two Web user sessions is based on the accessing page relevance, the syntactic structure of page URL and hierarchical structure of the website. This proposed intuitive dissimilarity measure was used with K-Medoids Clustering algorithm for experimentation and results were compared with other independent dissimilarity measures. The worth of the generated clusters were evaluated by two unsupervised cluster validity indexes. The experimental results show that intuitive augmented session dissimilarity measure is more realistic and superior as compared to the other independent dissimilarity measures regarding cluster validity indexes."
Kurdish stemmer pre-processing steps for improving information retrieval,"The rapid increase in the quantity of Kurdish documents over the last several years has created a need for improving information accuracy and precision in text classification and retrieval. Language stemming is an imperative pre-processing step for increasing the possibility of matching terms in a document in text classification tasks. Stemming helps reduce the total number of searchable terms within a document or query. This article proposes an active approach for stemming Kurdish Sorani texts to reduce variations of words to single terms or stems. The outcomes of the process, described in this article, demonstrate that decreasing the dimensionality of feature vectors in documents will increase the effectiveness of retrieval when the stemming process is used. This process applied for Kurdish Sorani can be adapted and applied in Kurdish Kurmanji as well for greater efficiency and effectiveness in digital text classification and applications. Â© 2017, Â© The Author(s) 2017.","The rapid increase in the quantity of Kurdish documents over the last several years has created a need for improving information accuracy and precision in text classification and retrieval. Language stemming is an imperative pre-processing step for increasing the possibility of matching terms in a document in text classification tasks. Stemming helps reduce the total number of searchable terms within a document or query. This article proposes an active approach for stemming Kurdish Sorani texts to reduce variations of words to single terms or stems. The outcomes of the process, described in this article, demonstrate that decreasing the dimensionality of feature vectors in documents will increase the effectiveness of retrieval when the stemming process is used. This process applied for Kurdish Sorani can be adapted and applied in Kurdish Kurmanji as well for greater efficiency and effectiveness in digital text classification and applications."
A study of neighbour selection strategies for POI recommendation in LBSNs,"Location-based recommender systems (LBRSs) are gaining importance with the proliferation of location-based services provided by mobile devices as well as user-generated content in social networks. Collaborative approaches for recommendation rely on the opinions of like-minded people, so-called neighbours, for prediction. Thus, an adequate selection of such neighbours becomes essential for achieving good prediction results. The aim of this work is to explore different strategies to select neighbours in the context of a collaborative filteringâbased recommender system for POI (places of interest) recommendations. Whereas standard methods are based on user similarity to delimit a neighbourhood, in this work several strategies are proposed based on direct social relationships and geographical information extracted from location-based social networks (LBSNs). The impact of the different strategies proposed has been evaluated and compared against the traditional collaborative filtering approach using a dataset from a popular network as Foursquare. In general terms, the proposed strategies for selecting neighbours based on the different elements available in a LBSN achieve better results than the traditional collaborative filtering approach. Our findings can be helpful both to researchers in the recommender systems area and to recommender system developers in the context of LBSNs, since they can take into account our results to design and provide more effective services considering the huge amount of knowledge produced in LBSNs. Â© The Author(s) 2018.","Location-based recommender systems (LBRSs) are gaining importance with the proliferation of location-based services provided by mobile devices as well as user-generated content in social networks. Collaborative approaches for recommendation rely on the opinions of like-minded people, so-called neighbours, for prediction. Thus, an adequate selection of such neighbours becomes essential for achieving good prediction results. The aim of this work is to explore different strategies to select neighbours in the context of a collaborative filteringbased recommender system for POI (places of interest) recommendations. Whereas standard methods are based on user similarity to delimit a neighbourhood, in this work several strategies are proposed based on direct social relationships and geographical information extracted from location-based social networks (LBSNs). The impact of the different strategies proposed has been evaluated and compared against the traditional collaborative filtering approach using a dataset from a popular network as Foursquare. In general terms, the proposed strategies for selecting neighbours based on the different elements available in a LBSN achieve better results than the traditional collaborative filtering approach. Our findings can be helpful both to researchers in the recommender systems area and to recommender system developers in the context of LBSNs, since they can take into account our results to design and provide more effective services considering the huge amount of knowledge produced in LBSNs."
The digitally extended self: A lexicological analysis of personal data,"Individualâs privacy, especially with regard to their personal data, is increasingly an area of concern as people interact with a wider and more pervasive set of digital services. Unfortunately, the terminology around personal data is used inconsistently, the concepts are unclear and there is a poor understanding of their relationships. This is a challenge to those who need to discuss personal data in precise terms, for example, legislators, academics and service providers who seek informed consent from their users. In this article, we present a lexicological analysis of the terms used to describe personal data, use this analysis to identify common concepts and propose a model of the digitally extended self that shows how these concepts of personal data fit together. We then validate the model against key publications and show in practice how it can be used to describe personal data in three scenarios. Our work shows that there is no clearly delineated kernel of personal data, but rather that there are layers of personal data, with different qualities, sources and claims of ownership, which extend out from the individual and form the digitally extended self. Â© The Author(s) 2017.","Individuals privacy, especially with regard to their personal data, is increasingly an area of concern as people interact with a wider and more pervasive set of digital services. Unfortunately, the terminology around personal data is used inconsistently, the concepts are unclear and there is a poor understanding of their relationships. This is a challenge to those who need to discuss personal data in precise terms, for example, legislators, academics and service providers who seek informed consent from their users. In this article, we present a lexicological analysis of the terms used to describe personal data, use this analysis to identify common concepts and propose a model of the digitally extended self that shows how these concepts of personal data fit together. We then validate the model against key publications and show in practice how it can be used to describe personal data in three scenarios. Our work shows that there is no clearly delineated kernel of personal data, but rather that there are layers of personal data, with different qualities, sources and claims of ownership, which extend out from the individual and form the digitally extended self."
SOLD: A node-Splitting algorithm for R-tree based on Objectsâ Locations Distribution,"Spatial data indexing methods are of extreme importance as they massively build up as a result of the explosive growth in capturing data with spatial features. No matter how much the data size is, eventually it will reside on disk pages. Disk pages have to be properly indexed to preserve spatial properties of objects, optimise disk space usage and improve objectsâ retrieval performance. One of the most popular spatial data indexes is the R-tree which is a height balanced tree data structure, where leaf nodes resemble disk pages and contain pointers to objectsâ locations. A single tree node can host up to a maximum number of objects, where any more insertion makes it an overflown node and it has to be split. Better splits lead to better index performance and more utilisation of disk space. In this work, we introduce a new way of finding the most proper split for an overflown node in the R-tree index. The proposed work scans â in a linear cost â the overflown nodeâs objects once to identify the distribution of objectsâ locations (minimum bounding rectangles (MBRs)) in relative to its nodeâs bounding rectangle (nodeâs MBR). It uses objectsâ locations to calculate â for each main axis â the split quality factors: expected overlap between resulting nodes, objects distribution evenness among resulting nodes and the perimeter of resulting nodes. The axis with better combined quality factors values is selected as the split axis. The Splitting based on Objectsâ Locations Distribution (SOLD) algorithm was implemented and tested against two other splitting algorithms, experiments using synthetic and real data files showed good results and it outperformed both algorithms in index creation tests and data retrieval tests. Â© The Author(s) 2018.","Spatial data indexing methods are of extreme importance as they massively build up as a result of the explosive growth in capturing data with spatial features. No matter how much the data size is, eventually it will reside on disk pages. Disk pages have to be properly indexed to preserve spatial properties of objects, optimise disk space usage and improve objects retrieval performance. One of the most popular spatial data indexes is the R-tree which is a height balanced tree data structure, where leaf nodes resemble disk pages and contain pointers to objects locations. A single tree node can host up to a maximum number of objects, where any more insertion makes it an overflown node and it has to be split. Better splits lead to better index performance and more utilisation of disk space. In this work, we introduce a new way of finding the most proper split for an overflown node in the R-tree index. The proposed work scans in a linear cost the overflown nodes objects once to identify the distribution of objects locations (minimum bounding rectangles (MBRs)) in relative to its nodes bounding rectangle (nodes MBR). It uses objects locations to calculate for each main axis the split quality factors: expected overlap between resulting nodes, objects distribution evenness among resulting nodes and the perimeter of resulting nodes. The axis with better combined quality factors values is selected as the split axis. The Splitting based on Objects Locations Distribution (SOLD) algorithm was implemented and tested against two other splitting algorithms, experiments using synthetic and real data files showed good results and it outperformed both algorithms in index creation tests and data retrieval tests."
The mobile Internet underclass: Reality or hyperbole?,"Scholars have warned that because mobile Internet access offers lower levels of functionality and content availability, operates on less open and flexible platforms and contributes to diminished levels of user engagement, content creation and information seeking compared with traditional fixed solutions, mobile Internet has created a mobile Internet âunderclassâ. As a growing proportion of the online population is now âmobile onlyâ, it is imperative to understand the extent to which mobile Internet may represent a meaningful substitute for traditional PC-based Internet access. Based on operational data from a major Chinese telecommunications carrier, this study provides a comparative analysis of the mobile Internet usage patterns associated with different access channels. Our results show that, in terms of mobile Internet usage, there is not enough evidence to support the âmobile underclassâ argument. However, our analysis reveals that the primary concern about the mobile âunderclassâ remains as the first-order digital divide âaccessâ issue. Â© The Author(s) 2017.","Scholars have warned that because mobile Internet access offers lower levels of functionality and content availability, operates on less open and flexible platforms and contributes to diminished levels of user engagement, content creation and information seeking compared with traditional fixed solutions, mobile Internet has created a mobile Internet underclass. As a growing proportion of the online population is now mobile only, it is imperative to understand the extent to which mobile Internet may represent a meaningful substitute for traditional PC-based Internet access. Based on operational data from a major Chinese telecommunications carrier, this study provides a comparative analysis of the mobile Internet usage patterns associated with different access channels. Our results show that, in terms of mobile Internet usage, there is not enough evidence to support the mobile underclass argument. However, our analysis reveals that the primary concern about the mobile underclass remains as the first-order digital divide access issue."
Explore the research front of a specific research theme based on a novel technique of enhanced co-word analysis,"Discovering the research front of a specific topic remains a significant challenge for researchers in all scientific areas. Over the last decade, burst term detection (BTD) in text streams has become a useful technique for bibliometrics and science mapping. It has been argued that analytical methods based on BTD can indicate certain facets of a research front. To integrate BTD into the framework of traditional co-word analysis, association rule mining between keywords and burst terms (ARM-KB) is introduced to enhance traditional co-word analysis and present a new facet of the research front for a field of science. Based on ARM-KB, possible connections between keywords and burst terms are built, which can facilitate the exploration of a research front from a three-dimensional perspective, through co-word analysis, burst term clues, and association rules. In the case study, the research fronts of anticancer based on nanomedicine (ABN) are explored. Based on theoretical and empirical analyses, ARM-KB can be used as a valuable new technique or a supplement to traditional bibliometrics in the exploration of scientific frontiers. Â© 2016, Â© The Author(s) 2016.","Discovering the research front of a specific topic remains a significant challenge for researchers in all scientific areas. Over the last decade, burst term detection (BTD) in text streams has become a useful technique for bibliometrics and science mapping. It has been argued that analytical methods based on BTD can indicate certain facets of a research front. To integrate BTD into the framework of traditional co-word analysis, association rule mining between keywords and burst terms (ARM-KB) is introduced to enhance traditional co-word analysis and present a new facet of the research front for a field of science. Based on ARM-KB, possible connections between keywords and burst terms are built, which can facilitate the exploration of a research front from a three-dimensional perspective, through co-word analysis, burst term clues, and association rules. In the case study, the research fronts of anticancer based on nanomedicine (ABN) are explored. Based on theoretical and empirical analyses, ARM-KB can be used as a valuable new technique or a supplement to traditional bibliometrics in the exploration of scientific frontiers."
The effects of the research excellence framework research impact agenda on early- and mid-career researchers in library and information science,"Early- and mid-career researchers will shape the future of library and information science (LIS) research and it is crucial they be well placed to engage with the research impact agenda. Their understanding of research impact may influence their capacity to be returned to research excellence framework (REF), the UKâs research quality assessment tool, as well as their ability to access research funding. This article reports the findings of a qualitative study exploring how the research impact agenda is influencing early- and mid-career researcher behaviour. Semi-structured interviews were conducted with 14 early- and mid-career researchers. While enthusiastic about creating lasting impact, participants lack effective institutional support to maximise their own research impact. Participants demonstrate uncertainty about what REF impact is. The authors conclude that while there is evidence LIS academics engage with practice to maximise impact, they lack support in building impact and the discipline needs to do more to create opportunities for the academy and the profession to coalesce to identify objects for and deliver impactful research. Â© The Author(s) 2017.","Early- and mid-career researchers will shape the future of library and information science (LIS) research and it is crucial they be well placed to engage with the research impact agenda. Their understanding of research impact may influence their capacity to be returned to research excellence framework (REF), the UKs research quality assessment tool, as well as their ability to access research funding. This article reports the findings of a qualitative study exploring how the research impact agenda is influencing early- and mid-career researcher behaviour. Semi-structured interviews were conducted with 14 early- and mid-career researchers. While enthusiastic about creating lasting impact, participants lack effective institutional support to maximise their own research impact. Participants demonstrate uncertainty about what REF impact is. The authors conclude that while there is evidence LIS academics engage with practice to maximise impact, they lack support in building impact and the discipline needs to do more to create opportunities for the academy and the profession to coalesce to identify objects for and deliver impactful research."
Enterprise search and discovery capability: The factors and generative mechanisms for user satisfaction,"Many organisations are re-creating the âGoogle-likeâ experience behind their firewall to exploit their information. However, surveys show dissatisfaction with enterprise search is commonplace. No prior study has investigated unsolicited user feedback from an enterprise search user interface to understand the underlying reasons for dissatisfaction. A mixed-methods longitudinal study was undertaken analysing feedback from over 1000 users and interviewing search service staff in a multinational corporation. Results show that 62% of dissatisfaction events were due to human (information and search literacy) rather than technology factors. Cognitive biases and the âGoogle Habitusâ influence expectations and information behaviour and are postulated as deep underlying generative mechanisms. The current literature focuses on âstructureâ (technology and information quality) as the reason for enterprise search satisfaction, agency (search literacy) appears downplayed. Organisations which emphasise âsystems thinkingâ and bimodal approaches towards search strategy and information behaviour may improve capabilities. Â© The Author(s) 2018.","Many organisations are re-creating the Google-like experience behind their firewall to exploit their information. However, surveys show dissatisfaction with enterprise search is commonplace. No prior study has investigated unsolicited user feedback from an enterprise search user interface to understand the underlying reasons for dissatisfaction. A mixed-methods longitudinal study was undertaken analysing feedback from over 1000 users and interviewing search service staff in a multinational corporation. Results show that 62% of dissatisfaction events were due to human (information and search literacy) rather than technology factors. Cognitive biases and the Google Habitus influence expectations and information behaviour and are postulated as deep underlying generative mechanisms. The current literature focuses on structure (technology and information quality) as the reason for enterprise search satisfaction, agency (search literacy) appears downplayed. Organisations which emphasise systems thinking and bimodal approaches towards search strategy and information behaviour may improve capabilities."
A social recommender system by combining social network and sentiment similarity: A case study of healthcare,"Social recommender systems aim to support user preferences and help users make better decisions in social media. The social network and the social context are two vital elements in social recommender systems. In this contribution, we propose a new framework for a social recommender system based on both network structure analysis and social context mining. Exponential random graph models (ERGMs) are able to capture and simulate the complex structure of a micro-blog network. We derive the prediction formula from ERGMs for recommending micro-blog users. Then, a primary recommendation list is created by analysing the micro-blog network structure. In the next step, we calculate the sentiment similarities of micro-blog users based on a sentiment feature set which is extracted from users' tweets. Sentiment similarities are used to filter the primary recommendation list and find users who have similar attitudes on the same topic. The goal of those two steps is to make the social recommender system much more precise and to satisfy users' psychological preferences. At the end, we use this new framework deal with big real-world data. The recommendation results of diabetes accounts of Weibo show that our method outperforms other social recommender systems. Â© Chartered Institute of Library and Information Professionals.","Social recommender systems aim to support user preferences and help users make better decisions in social media. The social network and the social context are two vital elements in social recommender systems. In this contribution, we propose a new framework for a social recommender system based on both network structure analysis and social context mining. Exponential random graph models (ERGMs) are able to capture and simulate the complex structure of a micro-blog network. We derive the prediction formula from ERGMs for recommending micro-blog users. Then, a primary recommendation list is created by analysing the micro-blog network structure. In the next step, we calculate the sentiment similarities of micro-blog users based on a sentiment feature set which is extracted from users' tweets. Sentiment similarities are used to filter the primary recommendation list and find users who have similar attitudes on the same topic. The goal of those two steps is to make the social recommender system much more precise and to satisfy users' psychological preferences. At the end, we use this new framework deal with big real-world data. The recommendation results of diabetes accounts of Weibo show that our method outperforms other social recommender systems."
Extraction of proteinâprotein interactions (PPIs) from the literature by deep convolutional neural networks with various feature embeddings,"The automatic extraction of proteinâprotein interactions (PPIs) reported in scientific publications are of great significance for biomedical researchers in that they could efficiently grasp the recent research results about biochemical events and molecular processes for conducting their original studies. This article introduces a deep convolutional neural network (DCNN) equipped with various feature embeddings to battle the limitations of the existing machine learning-based PPI extraction methods. The proposed model learns and optimises word embeddings based on the publicly available word vectors and also exploits position embeddings to identify the locations of the target protein names in sentences. Furthermore, it can employ various linguistic feature embeddings to improve the PPI extraction. The intensive experiments using AIMed data set known as the most difficult collection not only show the superiority of the suggested model but also indicate important implications in optimising the network parameters and hyperparameters. Â© 2016, Â© The Author(s) 2016.","The automatic extraction of proteinprotein interactions (PPIs) reported in scientific publications are of great significance for biomedical researchers in that they could efficiently grasp the recent research results about biochemical events and molecular processes for conducting their original studies. This article introduces a deep convolutional neural network (DCNN) equipped with various feature embeddings to battle the limitations of the existing machine learning-based PPI extraction methods. The proposed model learns and optimises word embeddings based on the publicly available word vectors and also exploits position embeddings to identify the locations of the target protein names in sentences. Furthermore, it can employ various linguistic feature embeddings to improve the PPI extraction. The intensive experiments using AIMed data set known as the most difficult collection not only show the superiority of the suggested model but also indicate important implications in optimising the network parameters and hyperparameters."
Towards a knowledge-based probabilistic and context-aware social recommender system,"In this article, we propose (1) a knowledge-based probabilistic collaborative filtering (CF) recommendation approach using both an ontology-based semantic similarity metric and a latent Dirichlet allocation (LDA) model-based recommendation technique and (2) a context-aware software architecture and system with the objective of validating the recommendation approach in the eating domain (foodservice places). The ontology on which the similarity metric is based is additionally leveraged to model and reason about usersâ contexts; the proposed LDA model also guides the usersâ context modelling to some extent. An evaluation method in the form of a comparative analysis based on traditional information retrieval (IR) metrics and a reference ranking-based evaluation metric (correctly ranked places) is presented towards the end of this article to reliably assess the efficacy and effectiveness of our recommendation approach, along with its utility from the userâs perspective. Our recommendation approach achieves higher average precision and recall values (8% and 7.40%, respectively) in the best-case scenario when compared with a CF approach that employs a baseline similarity metric. In addition, when compared with a partial implementation that does not consider usersâ preferences for topics, the comprehensive implementation of our recommendation approach achieves higher average values of correctly ranked places (2.5 of 5 versus 1.5 of 5). Â© The Author(s) 2017.","In this article, we propose a knowledge-based probabilistic collaborative filtering (CF) recommendation approach using both an ontology-based semantic similarity metric and a latent Dirichlet allocation (LDA) model-based recommendation technique and a context-aware software architecture and system with the objective of validating the recommendation approach in the eating domain (foodservice places). The ontology on which the similarity metric is based is additionally leveraged to model and reason about users contexts; the proposed LDA model also guides the users context modelling to some extent. An evaluation method in the form of a comparative analysis based on traditional information retrieval (IR) metrics and a reference ranking-based evaluation metric (correctly ranked places) is presented towards the end of this article to reliably assess the efficacy and effectiveness of our recommendation approach, along with its utility from the users perspective. Our recommendation approach achieves higher average precision and recall values (8% and 7.40%, respectively) in the best-case scenario when compared with a CF approach that employs a baseline similarity metric. In addition, when compared with a partial implementation that does not consider users preferences for topics, the comprehensive implementation of our recommendation approach achieves higher average values of correctly ranked places (2.5 of 5 versus 1.5 of 5)."
A discourse-aware neural network-based text model for document-level text classification,"Capturing semantics scattered across entire text is one of the important issues for Natural Language Processing (NLP) tasks. It would be particularly critical with long text embodying a flow of themes. This article proposes a new text modelling method that can handle thematic flows of text with Deep Neural Networks (DNNs) in such a way that discourse information and distributed representations of text are incorporate. Unlike previous DNN-based document models, the proposed model enables discourse-aware analysis of text and composition of sentence-level distributed representations guided by the discourse structure. More specifically, our method identifies Elementary Discourse Units (EDUs) and their discourse relations in a given document by applying Rhetorical Structure Theory (RST)-based discourse analysis. The result is fed into a tree-structured neural network that reflects the discourse information including the structure of the document and the discourse roles and relation types. We evaluate the document model for two document-level text classification tasks, sentiment analysis and sarcasm detection, with comparisons against the reference systems that also utilise discourse information. In addition, we conduct additional experiments to evaluate the impact of neural network types and adopted discourse factors on modelling documents vis-Ã -vis the two classification tasks. Furthermore, we investigate the effects of various learning methods, input units on the quality of the proposed discourse-aware document model. Â© The Author(s) 2017.","Capturing semantics scattered across entire text is one of the important issues for Natural Language Processing (NLP) tasks. It would be particularly critical with long text embodying a flow of themes. This article proposes a new text modelling method that can handle thematic flows of text with Deep Neural Networks (DNNs) in such a way that discourse information and distributed representations of text are incorporate. Unlike previous DNN-based document models, the proposed model enables discourse-aware analysis of text and composition of sentence-level distributed representations guided by the discourse structure. More specifically, our method identifies Elementary Discourse Units (EDUs) and their discourse relations in a given document by applying Rhetorical Structure Theory (RST)-based discourse analysis. The result is fed into a tree-structured neural network that reflects the discourse information including the structure of the document and the discourse roles and relation types. We evaluate the document model for two document-level text classification tasks, sentiment analysis and sarcasm detection, with comparisons against the reference systems that also utilise discourse information. In addition, we conduct additional experiments to evaluate the impact of neural network types and adopted discourse factors on modelling documents vis--vis the two classification tasks. Furthermore, we investigate the effects of various learning methods, input units on the quality of the proposed discourse-aware document model."
A two-phase sentiment analysis approach for judgement prediction,"Factual scenario analysis of a judgement is critical to judges during sentencing. With the increasing number of legal cases, professionals typically endure heavy workloads on a daily basis. Although a few previous studies have applied information technology to legal cases, according to our research, no prior studies have predicted a pending judgement using legal documents. In this article, we introduce an innovative solution to predict relevant rulings. The proposed approach employs text mining methods to extract features from precedents and applies a text classifier to automatically classify judgements according to sentiment analysis. This approach can assist legal experts or litigants in predicting possible judgements. Experimental results from a judgement data set reveal that our approach is a satisfactory method for judgement classification. Â© The Author(s) 2017.","Factual scenario analysis of a judgement is critical to judges during sentencing. With the increasing number of legal cases, professionals typically endure heavy workloads on a daily basis. Although a few previous studies have applied information technology to legal cases, according to our research, no prior studies have predicted a pending judgement using legal documents. In this article, we introduce an innovative solution to predict relevant rulings. The proposed approach employs text mining methods to extract features from precedents and applies a text classifier to automatically classify judgements according to sentiment analysis. This approach can assist legal experts or litigants in predicting possible judgements. Experimental results from a judgement data set reveal that our approach is a satisfactory method for judgement classification."
There's no shortcut: Building understanding from information in ultrarunning,"Now that information proliferates, information science should turn its attention towards higher order epistemic aims, such as understanding. Before systems to support the building of understanding can be designed, the process of building understanding must be explored. This article discusses the findings from an interpretative phenomenological analysis study on the information experience of participants in a 100-mile footrace which reveal how these participants have built understanding in their athletic pursuits. Three ways in which ultrarunners build understanding - by taking time, by undergoing struggle and by incorporating multiple perspectives - are described. The ensuing discussion leads to three questions that can guide the future development of information systems that support understanding: First, how can information science slow people down? Second, how can information science encourage people to willingly struggle? And, third, how can information science stimulate analogical thinking?. Â© Chartered Institute of Library and Information Professionals.","Now that information proliferates, information science should turn its attention towards higher order epistemic aims, such as understanding. Before systems to support the building of understanding can be designed, the process of building understanding must be explored. This article discusses the findings from an interpretative phenomenological analysis study on the information experience of participants in a 100-mile footrace which reveal how these participants have built understanding in their athletic pursuits. Three ways in which ultrarunners build understanding - by taking time, by undergoing struggle and by incorporating multiple perspectives - are described. The ensuing discussion leads to three questions that can guide the future development of information systems that support understanding: First, how can information science slow people down? Second, how can information science encourage people to willingly struggle? And, third, how can information science stimulate analogical thinking?."
Assessing the health information source perceptions of tweens using card-sorting exercises,"As young people are increasingly turning to the Internet to meet their information needs, it is imperative to investigate their perceptions regarding various potential sources of health information. A series of card-sorting exercises were administered to new participants in an after-school programme (HackHealth) to find out which sources of health information these greater Washington DC metro area middle school students would turn to, which they would not and their reasons behind these judgements. The findings revealed that participants were very aware of the importance of trustworthiness when looking for health information and they valued both professional expertise based on formal education and expertise born of personal experience with a particular health condition. However, they also valued convenience, ease and speed, and sometimes sacrificed information quality. Some important implications of these findings for healthcare and information professionals are identified and suggestions for future research in this area are offered. Â© 2017, Â© The Author(s) 2017.","As young people are increasingly turning to the Internet to meet their information needs, it is imperative to investigate their perceptions regarding various potential sources of health information. A series of card-sorting exercises were administered to new participants in an after-school programme (HackHealth) to find out which sources of health information these greater Washington DC metro area middle school students would turn to, which they would not and their reasons behind these judgements. The findings revealed that participants were very aware of the importance of trustworthiness when looking for health information and they valued both professional expertise based on formal education and expertise born of personal experience with a particular health condition. However, they also valued convenience, ease and speed, and sometimes sacrificed information quality. Some important implications of these findings for healthcare and information professionals are identified and suggestions for future research in this area are offered."
Evaluating mobile music services in China: An exploration in user experience,"Most digital music repositories and services have mobile applications (apps) that facilitate convenient access for users via smartphones. Although China has one of the largest music listener populations in the world, there is little research evaluating Chinese online or mobile music services. To bridge this gap, this study evaluated mobile apps of three of the most popular Chinese music services from the userâs perspective, using usability testing and semi-structured interviews with a sample of active users in China. Nielsenâs 10 user experience heuristics and four criteria in recommender evaluation were examined. Results identified criteria that create a positive user experience, and those that need further improvement. This study contributes to the literature in user-centred evaluation in music information retrieval (MIR) and music digital libraries (MDL), and provides practical insights for music application design, use and evaluation. Â© The Author(s) 2018.","Most digital music repositories and services have mobile applications (apps) that facilitate convenient access for users via smartphones. Although China has one of the largest music listener populations in the world, there is little research evaluating Chinese online or mobile music services. To bridge this gap, this study evaluated mobile apps of three of the most popular Chinese music services from the users perspective, using usability testing and semi-structured interviews with a sample of active users in China. Nielsens 10 user experience heuristics and four criteria in recommender evaluation were examined. Results identified criteria that create a positive user experience, and those that need further improvement. This study contributes to the literature in user-centred evaluation in music information retrieval (MIR) and music digital libraries , and provides practical insights for music application design, use and evaluation."
"Relationships between work task types, complexity and dwell time of information resources","Information seeking research often reports about types of information resources, ways of acquiring them and opinions on their importance in various professions. Based on self-reporting, these findings are affected by human memory and rationalisation. This article proposes a new way of studying information resource use â based on dwell time in the context provided by concrete work tasks. We use log data of 21 information workers from six organisations to analyse how work task complexity is connected to the time used in various information resources; how task complexity is connected to information resource use in different task types. Unlike traditionally, our findings consist of objective data on which resource types are used, and for how long, in work tasks of varying complexity and type. For example, the findings suggest that growing work task complexity increases the dwell time in local personal computer (PC) resources; these resources are especially popular in intellectual tasks. Such findings help understand factors affecting information resource use. Likewise, they help focus attention on most time-consuming aspects of task-based information interaction when developing support for work. Â© 2017, Â© The Author(s) 2017.","Information seeking research often reports about types of information resources, ways of acquiring them and opinions on their importance in various professions. Based on self-reporting, these findings are affected by human memory and rationalisation. This article proposes a new way of studying information resource use based on dwell time in the context provided by concrete work tasks. We use log data of 21 information workers from six organisations to analyse how work task complexity is connected to the time used in various information resources; how task complexity is connected to information resource use in different task types. Unlike traditionally, our findings consist of objective data on which resource types are used, and for how long, in work tasks of varying complexity and type. For example, the findings suggest that growing work task complexity increases the dwell time in local personal computer (PC) resources; these resources are especially popular in intellectual tasks. Such findings help understand factors affecting information resource use. Likewise, they help focus attention on most time-consuming aspects of task-based information interaction when developing support for work."
A novel algorithm for extracting the user reviews from web pages,"Extracting the user reviews in websites such as forums, blogs, newspapers, commerce, trips, etc. is crucial for text processing applications (e.g. sentiment analysis, trend detection/monitoring and recommendation systems) which are needed to deal with structured data. Traditional algorithms have three processes consisting of Document Object Model (DOM) tree creation, extraction of features obtained from this tree and machine learning. However, these algorithms increase time complexity of extraction process. This study proposes a novel algorithm that involves two complementary stages. The first stage determines which HTML tags correspond to review layout for a web domain by using the DOM tree as well as its features and decision tree learning. The second stage extracts review layout for web pages in a web domain using the found tags obtained from the first stage. This stage is more time-efficient, being approximately 21 times faster compared to the first stage. Moreover, it achieves a relatively high accuracy of 96.67% in our experiments of review block extraction. Â© Chartered Institute of Library and Information Professionals.","Extracting the user reviews in websites such as forums, blogs, newspapers, commerce, trips, etc. is crucial for text processing applications ( sentiment analysis, trend detection/monitoring and recommendation systems) which are needed to deal with structured data. Traditional algorithms have three processes consisting of Document Object Model (DOM) tree creation, extraction of features obtained from this tree and machine learning. However, these algorithms increase time complexity of extraction process. This study proposes a novel algorithm that involves two complementary stages. The first stage determines which HTML tags correspond to review layout for a web domain by using the DOM tree as well as its features and decision tree learning. The second stage extracts review layout for web pages in a web domain using the found tags obtained from the first stage. This stage is more time-efficient, being approximately 21 times faster compared to the first stage. Moreover, it achieves a relatively high accuracy of 96.67% in our experiments of review block extraction."
Why do online consumers experience information overload? An extension of communication theory,"People surfing the Internet are faced with an onslaught of messages from multiple sources, which can overwhelm receivers. In contrast to previous studies, which have used âchoice overloadâ to represent the amount of information provided to consumers, this study used âinformation overloadâ theory to represent the abundance of information received by consumers in online shopping environments. Borrowing from the concepts of the communication model, this study investigated the antecedents of perceived information overload, including information characteristics (message), the information source, the system interface (channel) and recipientsâ motivation (receiver). A total of 15 adults with more than 3 years of online shopping experience participated in a focus group discussion. By integrating focus group results and the results of previous studies into a theoretical framework, this study developed and empirically tested a structural equation model of online information overload among 456 PChome customers. The results indicated that the complexity and ambiguity of information characteristics, number of brand alternatives offered by the information source and system interface all positively affect consumersâ perceived information overload. Furthermore, information recipientsâ motivation not only negatively affected consumersâ perceived information overload but also moderated the relationship between the number of brand alternatives and consumersâ perceived information overload. Â© 2016, Â© The Author(s) 2016.","People surfing the Internet are faced with an onslaught of messages from multiple sources, which can overwhelm receivers. In contrast to previous studies, which have used choice overload to represent the amount of information provided to consumers, this study used information overload theory to represent the abundance of information received by consumers in online shopping environments. Borrowing from the concepts of the communication model, this study investigated the antecedents of perceived information overload, including information characteristics (message), the information source, the system interface (channel) and recipients motivation (receiver). A total of 15 adults with more than 3 years of online shopping experience participated in a focus group discussion. By integrating focus group results and the results of previous studies into a theoretical framework, this study developed and empirically tested a structural equation model of online information overload among 456 PChome customers. The results indicated that the complexity and ambiguity of information characteristics, number of brand alternatives offered by the information source and system interface all positively affect consumers perceived information overload. Furthermore, information recipients motivation not only negatively affected consumers perceived information overload but also moderated the relationship between the number of brand alternatives and consumers perceived information overload."
Bayesian NaÃ¯ve Bayes classifiers to text classification,"Text classification is the task of assigning predefined categories to natural language documents, and it can provide conceptual views of document collections. The NaÃ¯ve Bayes (NB) classifier is a family of simple probabilistic classifiers based on a common assumption that all features are independent of each other, given the category variable, and it is often used as the baseline in text classification. However, classical NB classifiers with multinomial, Bernoulli and Gaussian event models are not fully Bayesian. This study proposes three Bayesian counterparts, where it turns out that classical NB classifier with Bernoulli event model is equivalent to Bayesian counterpart. Finally, experimental results on 20 newsgroups and WebKB data sets show that the performance of Bayesian NB classifier with multinomial event model is similar to that of classical counterpart, but Bayesian NB classifier with Gaussian event model is obviously better than classical counterpart. Â© 2016, Â© The Author(s) 2016.","Text classification is the task of assigning predefined categories to natural language documents, and it can provide conceptual views of document collections. The Nave Bayes (NB) classifier is a family of simple probabilistic classifiers based on a common assumption that all features are independent of each other, given the category variable, and it is often used as the baseline in text classification. However, classical NB classifiers with multinomial, Bernoulli and Gaussian event models are not fully Bayesian. This study proposes three Bayesian counterparts, where it turns out that classical NB classifier with Bernoulli event model is equivalent to Bayesian counterpart. Finally, experimental results on 20 newsgroups and WebKB data sets show that the performance of Bayesian NB classifier with multinomial event model is similar to that of classical counterpart, but Bayesian NB classifier with Gaussian event model is obviously better than classical counterpart."
DISC: Disambiguating homonyms using graph structural clustering,"Author name ambiguity degrades information retrieval, database integration, search results and, more importantly, correct attributions in bibliographic databases. Some unresolved issues include how to ascertain the actual number of authors, how to improve the performance and how to make the method more effective in terms of representative clustering metrics (average cluster purity, average author purity, K-metric, pairwise precision, pairwise recall, pairwise-F1, cluster precision, cluster recall and cluster-F1). It is a non-trivial task to disambiguate authors using only the implicit bibliographic information. An effective method âDISCâ is proposed that uses graph community detection algorithm, feature vectors and graph operations to disambiguate homonyms. The citation data set is pre-processed and ambiguous author blocks are formed. A co-authors graph is constructed using authors and their co-authorâs relationships. A graph structural clustering âgSkeletonCluâ is applied to identify hubs, outliers and clusters of nodes in a co-authorâs graph. Homonyms are resolved by splitting these clusters of nodes across the hub if their feature vector similarity is less than a predefined threshold. DISC utilises only co-authors and titles that are available in almost all bibliographic databases. With little modifications, DISC can also be used for entity disambiguation. To validate the DISC performance, experiments are performed on two Arnetminer data sets and compared with five previous unsupervised methods. Despite using limited bibliographic metadata, DISC achieves on average K-metric, pairwise-F1, and cluster-F1 of 92%, 84% and 74%, respectively, using Arnetminer-S and 86%, 80% and 57%, respectively, using Arnetminer-L. About 77.5% and 73.2% clusters are within the range (ground truth clusters Â± 3) in Arnetminer-S and Arnetminer-L, respectively. Â© The Author(s) 2018.","Author name ambiguity degrades information retrieval, database integration, search results and, more importantly, correct attributions in bibliographic databases. Some unresolved issues include how to ascertain the actual number of authors, how to improve the performance and how to make the method more effective in terms of representative clustering metrics (average cluster purity, average author purity, K-metric, pairwise precision, pairwise recall, pairwise-F1, cluster precision, cluster recall and cluster-F1). It is a non-trivial task to disambiguate authors using only the implicit bibliographic information. An effective method DISC is proposed that uses graph community detection algorithm, feature vectors and graph operations to disambiguate homonyms. The citation data set is pre-processed and ambiguous author blocks are formed. A co-authors graph is constructed using authors and their co-authors relationships. A graph structural clustering gSkeletonClu is applied to identify hubs, outliers and clusters of nodes in a co-authors graph. Homonyms are resolved by splitting these clusters of nodes across the hub if their feature vector similarity is less than a predefined threshold. DISC utilises only co-authors and titles that are available in almost all bibliographic databases. With little modifications, DISC can also be used for entity disambiguation. To validate the DISC performance, experiments are performed on two Arnetminer data sets and compared with five previous unsupervised methods. Despite using limited bibliographic metadata, DISC achieves on average K-metric, pairwise-F1, and cluster-F1 of 92%, 84% and 74%, respectively, using Arnetminer-S and 86%, 80% and 57%, respectively, using Arnetminer- About 77.5% and 73.2% clusters are within the range (ground truth clusters 3) in Arnetminer-S and Arnetminer-L, respectively."
"Request-based, secured and energy-efficient (RBSEE) architecture for handling IoT big data","The technological advancements in the field of computing are giving rise to the generation of gigantic volumes of data which are beyond the handling capabilities of the conventionally available tools, techniques and systems. These types of data are known as big data. Moreover with the emergence of Internet of Things (IoT), these types of data have increased in multiple folds in 7Vs (volume, variety, veracity, value, variability, velocity and visualisation). There are several techniques prevalent in todayâs time for handling these types of huge data. Hadoop is one such open source framework which has emerged as a de facto technology for handling such huge datasets. In an IoT ecosystem, real-time handling of requests is an imperative requirement; however, Hadoop has certain limitations while handling these types of requests. In this article, we present an energy-efficient architecture for effective, secured and real-time handling of IoT big data. The proposed approach adopts atrain distributed system (ADS) to construct the core architecture. This study uses software-defined networking (SDN) framework for energy-efficient and optimal routing of data and requests from source to destination, and vice versa. Furthermore, to ensure secured handling of IoT big data, the proposed approach uses âTwofishâ cryptographic technique for encrypting the information captured by the sensors. Finally, the concept of ârequest-typeâ identifying unit has been proposed. Instead of handling all the requests in an identical way, the proposed approach works by characterising the requests on the basis of certain criteria and parameters, which are identified here. Â© The Author(s) 2018.","The technological advancements in the field of computing are giving rise to the generation of gigantic volumes of data which are beyond the handling capabilities of the conventionally available tools, techniques and systems. These types of data are known as big data. Moreover with the emergence of Internet of Things (IoT), these types of data have increased in multiple folds in 7Vs (volume, variety, veracity, value, variability, velocity and visualisation). There are several techniques prevalent in todays time for handling these types of huge data. Hadoop is one such open source framework which has emerged as a de facto technology for handling such huge datasets. In an IoT ecosystem, real-time handling of requests is an imperative requirement; however, Hadoop has certain limitations while handling these types of requests. In this article, we present an energy-efficient architecture for effective, secured and real-time handling of IoT big data. The proposed approach adopts atrain distributed system (ADS) to construct the core architecture. This study uses software-defined networking (SDN) framework for energy-efficient and optimal routing of data and requests from source to destination, and vice versa. Furthermore, to ensure secured handling of IoT big data, the proposed approach uses Twofish cryptographic technique for encrypting the information captured by the sensors. Finally, the concept of request-type identifying unit has been proposed. Instead of handling all the requests in an identical way, the proposed approach works by characterising the requests on the basis of certain criteria and parameters, which are identified here."
The embeddedness of collaborative information seeking in information culture,"Professionally, people often conduct their work in settings containing a range of different collaborative situations and work practices in which people handle information and work activities. Still, work tasks are usually considered and perceived as individual activities although the technology and the characteristics of the tasks require collaborative and cooperative handling processes. This viewpoint still produces technologies that, in general, assume individual information management and decision-making. Based on previous research on information culture (IC) and collaborative information seeking (CIS), this paper proposes an integrated framework where both environmental (cultural) as well as collaborative aspects of organisational information behaviour are present. This kind of framework would be useful in studies looking into how information is retrieved, how information is organised and managed, and how information is used as a resource in collaborative settings. It gives a more holistic perspective to information use and practices in organisations where culture, collaboration and awareness are especially brought to common attention for effective information management in organisations. Â© The Author(s) 2016.","Professionally, people often conduct their work in settings containing a range of different collaborative situations and work practices in which people handle information and work activities. Still, work tasks are usually considered and perceived as individual activities although the technology and the characteristics of the tasks require collaborative and cooperative handling processes. This viewpoint still produces technologies that, in general, assume individual information management and decision-making. Based on previous research on information culture and collaborative information seeking (CIS), this paper proposes an integrated framework where both environmental (cultural) as well as collaborative aspects of organisational information behaviour are present. This kind of framework would be useful in studies looking into how information is retrieved, how information is organised and managed, and how information is used as a resource in collaborative settings. It gives a more holistic perspective to information use and practices in organisations where culture, collaboration and awareness are especially brought to common attention for effective information management in organisations."
User identification across online social networks in practice: pitfalls and solutions,"To take advantage of the full range of services that online social networks (OSNs) offer, people commonly open several accounts on diverse OSNs where they leave lots of different types of profile information. The integration of these pieces of information from various sources can be achieved by identifying individuals across social networks. In this article, we address the problem of user identification by treating it as a classification task. Relying on common public attributes available through the official application programming interface (API) of social networks, we propose different methods for building negative instances that go beyond usual random selection so as to investigate the effectiveness of each method in training the classifier. Two test sets with different levels of discrimination are set up to evaluate the robustness of our different classifiers. The effectiveness of the approach is measured in real conditions by matching profiles gathered from Google+, Facebook and Twitter. Â© The Author(s) 2016.","To take advantage of the full range of services that online social networks (OSNs) offer, people commonly open several accounts on diverse OSNs where they leave lots of different types of profile information. The integration of these pieces of information from various sources can be achieved by identifying individuals across social networks. In this article, we address the problem of user identification by treating it as a classification task. Relying on common public attributes available through the official application programming interface (API) of social networks, we propose different methods for building negative instances that go beyond usual random selection so as to investigate the effectiveness of each method in training the classifier. Two test sets with different levels of discrimination are set up to evaluate the robustness of our different classifiers. The effectiveness of the approach is measured in real conditions by matching profiles gathered from Google+, Facebook and Twitter."
Feature engineering for detecting spammers on Twitter: Modelling and analysis,"Twitter is a social networking website that has gained a lot of popularity around the world in the last decade. This popularity made Twitter a common target for spammers and malicious users to spread unwanted advertisements, viruses and phishing attacks. In this article, we review the latest research works to determine the most effective features that were investigated for spam detection in the literature. These features are collected to build a comprehensive data set that can be used to develop more robust and accurate spammer detection models. The new data set is tested using popular classifiers (Naive Bayes, support vector machines, multilayer perceptron neural networks, Decision Trees, Random forests and k-Nearest Neighbour). The prediction performance of these classifiers is evaluated and compared based on different evaluation metrics. Moreover, a further analysis is carried out to identify the features that have higher impact on the accuracy of spam detection. Three different techniques are used and compared for this analysis: change of mean square error (CoM), information gain (IG) and Relief-F method. Top five features identified by each technique are used again to build the detection models. Experimental results show that most of the developed classifiers obtained high evaluation results based on the comprehensive data set constructed in this work. Experiments also reveal the important role of some features like the reputation of the account, average length of the tweet, average mention per tweet, age of the account, and the average time between posts in the process of identifying spammers in the social network. Â© 2017, Â© The Author(s) 2017.","Twitter is a social networking website that has gained a lot of popularity around the world in the last decade. This popularity made Twitter a common target for spammers and malicious users to spread unwanted advertisements, viruses and phishing attacks. In this article, we review the latest research works to determine the most effective features that were investigated for spam detection in the literature. These features are collected to build a comprehensive data set that can be used to develop more robust and accurate spammer detection models. The new data set is tested using popular classifiers (Naive Bayes, support vector machines, multilayer perceptron neural networks, Decision Trees, Random forests and k-Nearest Neighbour). The prediction performance of these classifiers is evaluated and compared based on different evaluation metrics. Moreover, a further analysis is carried out to identify the features that have higher impact on the accuracy of spam detection. Three different techniques are used and compared for this analysis: change of mean square error (CoM), information gain (IG) and Relief-F method. Top five features identified by each technique are used again to build the detection models. Experimental results show that most of the developed classifiers obtained high evaluation results based on the comprehensive data set constructed in this work. Experiments also reveal the important role of some features like the reputation of the account, average length of the tweet, average mention per tweet, age of the account, and the average time between posts in the process of identifying spammers in the social network."
Effects of relevance criteria and subjective factors on web image searching behaviour,"Searching for images is an everyday activity. Nevertheless, even a highly skilled searcher often struggles to find what they are looking for. This article studies the factors that affect usersâ online web image search behaviour, investigating (1) the use of criteria in making image relevance judgements and (2) the effect of familiarity, difficulty and satisfaction. The study includes 48 users who performed four online image search tasks using Google Images. Simulated work scenarios, questionnaires and screen capture recordings were used to collect data of their image search behaviour. The results show in judging image relevance, users may apply similar criterion, however, the importance of these criteria depends on the type of image search. Similarly, ratings of usersâ perception on subjective aspects of performing image search shows they were task dependent. Usersâ perception on subjective aspects of performing image search did not always correspond with their actual search behaviour. Correlation analysis shows that subjective factors cannot be definitively measured by using only one component of search behaviour. Future work includes further analysis on the effects of topic familiarity and satisfaction. Â© 2016, Â© The Author(s) 2016.","Searching for images is an everyday activity. Nevertheless, even a highly skilled searcher often struggles to find what they are looking for. This article studies the factors that affect users online web image search behaviour, investigating the use of criteria in making image relevance judgements and the effect of familiarity, difficulty and satisfaction. The study includes 48 users who performed four online image search tasks using Google Images. Simulated work scenarios, questionnaires and screen capture recordings were used to collect data of their image search behaviour. The results show in judging image relevance, users may apply similar criterion, however, the importance of these criteria depends on the type of image search. Similarly, ratings of users perception on subjective aspects of performing image search shows they were task dependent. Users perception on subjective aspects of performing image search did not always correspond with their actual search behaviour. Correlation analysis shows that subjective factors cannot be definitively measured by using only one component of search behaviour. Future work includes further analysis on the effects of topic familiarity and satisfaction."
HybRecSys: Content-based contextual hybrid venue recommender system,"The popularity of location-based social networks has prompted researchers to study recommendation systems for location-based services. When used separately, each existing venue recommendation system algorithm has its own drawbacks (e.g. cold start, data sparsity, scalability). Another issue is that critical information about context is not commonly used in venue recommendation systems. This article proposes a hybrid recommendation model that combines contextual information, user-based and item-based collaborative filtering and content-based filtering. For this purpose, we collected user visit histories, venue-related information (distance, category, popularity and price) and contextual information (weather, season, date and time of visits) related to individual user visits from Twitter, Foursquare and Weather Underground. Experimental evaluation of the proposed hybrid system (HybRecSys) using a real-world dataset shows better results than baseline approaches. Â© The Author(s) 2018.","The popularity of location-based social networks has prompted researchers to study recommendation systems for location-based services. When used separately, each existing venue recommendation system algorithm has its own drawbacks ( cold start, data sparsity, scalability). Another issue is that critical information about context is not commonly used in venue recommendation systems. This article proposes a hybrid recommendation model that combines contextual information, user-based and item-based collaborative filtering and content-based filtering. For this purpose, we collected user visit histories, venue-related information (distance, category, popularity and price) and contextual information (weather, season, date and time of visits) related to individual user visits from Twitter, Foursquare and Weather Underground. Experimental evaluation of the proposed hybrid system (HybRecSys) using a real-world dataset shows better results than baseline approaches."
Lexicon-based sentiment analysis: Comparative evaluation of six sentiment lexicons,"This article introduces a new general-purpose sentiment lexicon called WKWSCI Sentiment Lexicon and compares it with five existing lexicons: Hu & Liu Opinion Lexicon, Multi-perspective Question Answering (MPQA) Subjectivity Lexicon, General Inquirer, National Research Council Canada (NRC) Word-Sentiment Association Lexicon and Semantic Orientation Calculator (SO-CAL) lexicon. The effectiveness of the sentiment lexicons for sentiment categorisation at the document level and sentence level was evaluated using an Amazon product review data set and a news headlines data set. WKWSCI, MPQA, Hu & Liu and SO-CAL lexicons are equally good for product review sentiment categorisation, obtaining accuracy rates of 75%â77% when appropriate weights are used for different categories of sentiment words. However, when a training corpus is not available, Hu & Liu obtained the best accuracy with a simple-minded approach of counting positive and negative words for both document-level and sentence-level sentiment categorisation. The WKWSCI lexicon obtained the best accuracy of 69% on the news headlines sentiment categorisation task, and the sentiment strength values obtained a Pearson correlation of 0.57 with human-assigned sentiment values. It is recommended that the Hu & Liu lexicon be used for product review texts and the WKWSCI lexicon for non-review texts. Â© The Author(s) 2017.","This article introduces a new general-purpose sentiment lexicon called WKWSCI Sentiment Lexicon and compares it with five existing lexicons: Hu & Liu Opinion Lexicon, Multi-perspective Question Answering (MPQA) Subjectivity Lexicon, General Inquirer, National Research Council Canada (NRC) Word-Sentiment Association Lexicon and Semantic Orientation Calculator (SO-CAL) lexicon. The effectiveness of the sentiment lexicons for sentiment categorisation at the document level and sentence level was evaluated using an Amazon product review data set and a news headlines data set. WKWSCI, MPQA, Hu & Liu and SO-CAL lexicons are equally good for product review sentiment categorisation, obtaining accuracy rates of 75%77% when appropriate weights are used for different categories of sentiment words. However, when a training corpus is not available, Hu & Liu obtained the best accuracy with a simple-minded approach of counting positive and negative words for both document-level and sentence-level sentiment categorisation. The WKWSCI lexicon obtained the best accuracy of 69% on the news headlines sentiment categorisation task, and the sentiment strength values obtained a Pearson correlation of 0.57 with human-assigned sentiment values. It is recommended that the Hu & Liu lexicon be used for product review texts and the WKWSCI lexicon for non-review texts."
A study of semantic integration across archaeological data and reports in different languages,"This study investigates the semantic integration of data extracted from archaeological datasets with information extracted via natural language processing (NLP) across different languages. The investigation follows a broad theme relating to wooden objects and their dating via dendrochronological techniques, including types of wooden material, samples taken and wooden objects including shipwrecks. The outcomes are an integrated RDF dataset coupled with an associated interactive research demonstrator query builder application. The semantic framework combines the CIDOC Conceptual Reference Model (CRM) with the Getty Art and Architecture Thesaurus (AAT). The NLP, data cleansing and integration methods are described in detail together with illustrative scenarios from the web application Demonstrator. Reflections and recommendations from the study are discussed. The Demonstrator is a novel SPARQL web application, with CRM/AAT-based data integration. Functionality includes the combination of free text and semantic search with browsing on semantic links, hierarchical and associative relationship thesaurus query expansion. Queries concern wooden objects (e.g. samples of beech wood keels), optionally from a given date range, with automatic expansion over AAT hierarchies of wood types and specialised associative relationships. Following a âmapping patternâ approach (via the STELETO tool) ensured validity and consistency of all RDF output. The user is shielded from the complexity of the underlying semantic framework by a query builder user interface. The study demonstrates the feasibility of connecting information extracted from datasets and grey literature reports in different languages and semantic cross-searching of the integrated information. The semantic linking of textual reports and datasets opens new possibilities for integrative research across diverse resources. Â© The Author(s) 2018.","This study investigates the semantic integration of data extracted from archaeological datasets with information extracted via natural language processing (NLP) across different languages. The investigation follows a broad theme relating to wooden objects and their dating via dendrochronological techniques, including types of wooden material, samples taken and wooden objects including shipwrecks. The outcomes are an integrated RDF dataset coupled with an associated interactive research demonstrator query builder application. The semantic framework combines the CIDOC Conceptual Reference Model (CRM) with the Getty Art and Architecture Thesaurus (AAT). The NLP, data cleansing and integration methods are described in detail together with illustrative scenarios from the web application Demonstrator. Reflections and recommendations from the study are discussed. The Demonstrator is a novel SPARQL web application, with CRM/AAT-based data integration. Functionality includes the combination of free text and semantic search with browsing on semantic links, hierarchical and associative relationship thesaurus query expansion. Queries concern wooden objects ( samples of beech wood keels), optionally from a given date range, with automatic expansion over AAT hierarchies of wood types and specialised associative relationships. Following a mapping pattern approach (via the STELETO tool) ensured validity and consistency of all RDF output. The user is shielded from the complexity of the underlying semantic framework by a query builder user interface. The study demonstrates the feasibility of connecting information extracted from datasets and grey literature reports in different languages and semantic cross-searching of the integrated information. The semantic linking of textual reports and datasets opens new possibilities for integrative research across diverse resources."
Enriching a thesaurus as a better question-answering tool and information retrieval aid,"This article reports the method of enriching a thesaurus by differentiating the related term relationship with specific semantic relations and expanding related terms. It also tests the usefulness of an enriched thesaurus as a better question-answering tool and information retrieval aid based on usersâ perceptions. A small portion of the Education Resources Information Center (ERIC) thesaurus was enriched, and two enriched mini-thesauri were compiled with different levels of detail. A total of 22 participants were recruited to test the usefulness of the three mini-thesauri for facilitating question-answering and information retrieval within the ERIC Abstracts of the EBSCOHost Database and on the Web using Google. The experimental results suggest that the enriched thesauri are better question-answering tools and information retrieval aids than the original thesaurus. The findings imply that thesauri enriched with semantic relations are useful in question-answering and modern information retrieval, although the role of the traditional thesaurus in modern information retrieval has diminished. Â© The Author(s) 2017.","This article reports the method of enriching a thesaurus by differentiating the related term relationship with specific semantic relations and expanding related terms. It also tests the usefulness of an enriched thesaurus as a better question-answering tool and information retrieval aid based on users perceptions. A small portion of the Education Resources Information Center (ERIC) thesaurus was enriched, and two enriched mini-thesauri were compiled with different levels of detail. A total of 22 participants were recruited to test the usefulness of the three mini-thesauri for facilitating question-answering and information retrieval within the ERIC Abstracts of the EBSCOHost Database and on the Web using Google. The experimental results suggest that the enriched thesauri are better question-answering tools and information retrieval aids than the original thesaurus. The findings imply that thesauri enriched with semantic relations are useful in question-answering and modern information retrieval, although the role of the traditional thesaurus in modern information retrieval has diminished."
The INNOVance Lexicon: Organisation of terms and concepts about construction products,"The construction sector is also a knowledge-intensive domain, in which effective and unambiguous communication and knowledge sharing are, at the same time, both essential yet difficult to accomplish. This is primarily due to the several professionals interacting and facing situations involving diverse resources, processes and activities. Each of them brings a different background and perspective, often generating poorly integrated information. Knowledge Organization Systems (KOSs) are crucial for ensuring completeness, consistency and quality of information. Despite the international trend to encourage the development and use of controlled vocabularies, especially classification systems, until recently in Italy the national coordination policy has not been effective enough. This article describes the first national attempt made, the INNOVance Lexicon that collects and organises knowledge about construction products. It combines taxonomic, terminological and semantic aspects of knowledge and it is a reference language to support information exchange and sharing in collaborative context. Â© 2017, Â© The Author(s) 2017.","The construction sector is also a knowledge-intensive domain, in which effective and unambiguous communication and knowledge sharing are, at the same time, both essential yet difficult to accomplish. This is primarily due to the several professionals interacting and facing situations involving diverse resources, processes and activities. Each of them brings a different background and perspective, often generating poorly integrated information. Knowledge Organization Systems (KOSs) are crucial for ensuring completeness, consistency and quality of information. Despite the international trend to encourage the development and use of controlled vocabularies, especially classification systems, until recently in Italy the national coordination policy has not been effective enough. This article describes the first national attempt made, the INNOVance Lexicon that collects and organises knowledge about construction products. It combines taxonomic, terminological and semantic aspects of knowledge and it is a reference language to support information exchange and sharing in collaborative context."
Personalised health document summarisation exploiting Unified Medical Language System and topic-based clustering for mobile healthcare,"According to the growing interest in mobile healthcare, multi-document summarisation techniques are increasingly required to cope with health information overload and effectively deliver personalised online healthcare information. However, because of the peculiarities of medical terminology and the diversity of subtopics in health documents, multi-document summarisation must consider technical aspects that are different from those of the general domain. In this article, we propose a personalised health document summarisation system that provides a reliable personal health-related summary to general healthcare consumers via mobile devices. Our system generates a personalised summary from multiple online health documents by exploiting biomedical concepts, semantic types and semantic relations extracted from the Unified Medical Language System (UMLS) and analysing individual health records derived from mobile personal health record (PHR) applications. Furthermore, to increase the diversity and coverage of summarised results and to display them in a user-friendly manner on mobile devices, we create a summary that is categorised into subtopics by grouping semantically related sentences through topic-based clustering. The experimental evaluations demonstrate the effectiveness of our proposed system. Â© The Author(s) 2017.","According to the growing interest in mobile healthcare, multi-document summarisation techniques are increasingly required to cope with health information overload and effectively deliver personalised online healthcare information. However, because of the peculiarities of medical terminology and the diversity of subtopics in health documents, multi-document summarisation must consider technical aspects that are different from those of the general domain. In this article, we propose a personalised health document summarisation system that provides a reliable personal health-related summary to general healthcare consumers via mobile devices. Our system generates a personalised summary from multiple online health documents by exploiting biomedical concepts, semantic types and semantic relations extracted from the Unified Medical Language System (UMLS) and analysing individual health records derived from mobile personal health record (PHR) applications. Furthermore, to increase the diversity and coverage of summarised results and to display them in a user-friendly manner on mobile devices, we create a summary that is categorised into subtopics by grouping semantically related sentences through topic-based clustering. The experimental evaluations demonstrate the effectiveness of our proposed system."
Mining opinionated product features using WordNet lexicographer files,"Online customer reviews are an important assessment tool for businesses as they contain feedback that is valuable from the customer perspective. These reviews provide a significant basis on which potential customers can select the product that best meets their preferences. In online reviews, customers describe positive or negative experiences with a product or service or any part of it (i.e. features). Consumers frequently experience difficulty finding the desired product for comparison because of the massive number of online reviews. The automatic extraction of important product features is necessary to support customers in search of relevant product features. These features are the criteria that make it possible for customers to characterise different types of products. This article proposes a domain independent approach for identifying explicit opinionated features and attributes that are strongly related to a specific domain product using lexicographer files in WordNet. In our approach, N_gram analysis and the SentiStrength opinion lexicon have been employed to support the extraction of opinionated features. The empirical evaluation of the proposed system using online reviews of two popular datasets of supervised and unsupervised systems showed that our approach achieved competitive results for feature extraction from product reviews. Â© 2016, Â© The Author(s) 2016.","Online customer reviews are an important assessment tool for businesses as they contain feedback that is valuable from the customer perspective. These reviews provide a significant basis on which potential customers can select the product that best meets their preferences. In online reviews, customers describe positive or negative experiences with a product or service or any part of it ( features). Consumers frequently experience difficulty finding the desired product for comparison because of the massive number of online reviews. The automatic extraction of important product features is necessary to support customers in search of relevant product features. These features are the criteria that make it possible for customers to characterise different types of products. This article proposes a domain independent approach for identifying explicit opinionated features and attributes that are strongly related to a specific domain product using lexicographer files in WordNet. In our approach, N_gram analysis and the SentiStrength opinion lexicon have been employed to support the extraction of opinionated features. The empirical evaluation of the proposed system using online reviews of two popular datasets of supervised and unsupervised systems showed that our approach achieved competitive results for feature extraction from product reviews."
Bibliometrics of sentiment analysis literature,"This article provides a bibliometric study of the sentiment analysis literature based on Web of Science (WoS) until the end of 2016 to evaluate current research trends, quantitatively and qualitatively. We concentrate on the analysis of scientific documents, distribution of subject categories, languages of documents and languages that have been more investigated in sentiment analysis, most prolific and impactful authors and institutions, venues of publications and their geographic distribution, most cited and hot documents, trends of keywords and future works. Our investigations demonstrate that the most frequent subject categories in this field are computer science, engineering, telecommunications, linguistics, operations research and management science, information science and library science, business and economics, automation and control systems, robotics and social sciences. In addition, the most active venue of publication in this field is Lecture Notes in Computer Science (LNCS). The United States, China and Singapore have the most prolific or impactful institutions. A keyword analysis demonstrates that sentiment analysis is a more accepted term than opinion mining. Twitter is the most used social network for sentiment analysis and Support Vector Machine (SVM) is the most used classification method. We also present the most cited and hot documents in this field and authorsâ suggestions for future works. Â© The Author(s) 2018.","This article provides a bibliometric study of the sentiment analysis literature based on Web of Science (WoS) until the end of 2016 to evaluate current research trends, quantitatively and qualitatively. We concentrate on the analysis of scientific documents, distribution of subject categories, languages of documents and languages that have been more investigated in sentiment analysis, most prolific and impactful authors and institutions, venues of publications and their geographic distribution, most cited and hot documents, trends of keywords and future works. Our investigations demonstrate that the most frequent subject categories in this field are computer science, engineering, telecommunications, linguistics, operations research and management science, information science and library science, business and economics, automation and control systems, robotics and social sciences. In addition, the most active venue of publication in this field is Lecture Notes in Computer Science (LNCS). The United States, China and Singapore have the most prolific or impactful institutions. A keyword analysis demonstrates that sentiment analysis is a more accepted term than opinion mining. Twitter is the most used social network for sentiment analysis and Support Vector Machine (SVM) is the most used classification method. We also present the most cited and hot documents in this field and authors suggestions for future works."
Big data to knowledge â Harnessing semiotic relationships of data quality and skills in genome curation work,"This article aims to understand the views of genomic scientists with regard to the data quality assurances associated with semiotics and dataâinformationâknowledge (DIK). The resulting communication of signs generated from genomic curation work, was found within different semantic levels of DIK that correlate specific data quality dimensions with their respective skills. Syntactic data quality dimensions were ranked the highest among all other semiotic data quality dimensions, which indicated that scientists spend great efforts for handling data wrangling activities in genome curation work. Semantic- and pragmatic-related sign communications were about meaningful interpretation, thus required additional adaptive and interpretative skills to deal with data quality issues. This expanded concept of âcurationâ as sign/semiotic was not previously explored from the practical to the theoretical perspectives. The findings inform policy makers and practitioners to develop framework and cyberinfrastructure that facilitate the initiatives and advocacies of âBig Data to Knowledgeâ by funding agencies. The findings from this study can also help plan data quality assurance policies and thus maximise the efficiency of genomic data management. Our results give strong support to the relevance of data quality skills communication for relationship with data quality assurance in genome curation activities. Â© The Author(s) 2018.","This article aims to understand the views of genomic scientists with regard to the data quality assurances associated with semiotics and datainformationknowledge (DIK). The resulting communication of signs generated from genomic curation work, was found within different semantic levels of DIK that correlate specific data quality dimensions with their respective skills. Syntactic data quality dimensions were ranked the highest among all other semiotic data quality dimensions, which indicated that scientists spend great efforts for handling data wrangling activities in genome curation work. Semantic- and pragmatic-related sign communications were about meaningful interpretation, thus required additional adaptive and interpretative skills to deal with data quality issues. This expanded concept of curation as sign/semiotic was not previously explored from the practical to the theoretical perspectives. The findings inform policy makers and practitioners to develop framework and cyberinfrastructure that facilitate the initiatives and advocacies of Big Data to Knowledge by funding agencies. The findings from this study can also help plan data quality assurance policies and thus maximise the efficiency of genomic data management. Our results give strong support to the relevance of data quality skills communication for relationship with data quality assurance in genome curation activities."
Attributional style of emotions and its relationship with usersâ search behaviour,"This study aimed to assess the usersâ style of attribution of emotions in information retrieval process based on Weinerâs attribution theory of emotion. The research method in the present research is descriptive and the type of study is practical. The population of this study consisted of MA students of different humanistic science branches studying at Imam-Reza International University. A sample of 72 students was selected. The required information were collected through a questionnaire of attribution style and two researcher-made questionnaires. Results showed that the majority of the users attributed their failure and success in information retrieval to internal causes. Also according to Weinerâs theory, they mentioned âeffortâ as a factor in their success and âinabilityâ and âinadequate effortâ as their main failure factors. Research showed that individuals who attribute their emotions to internal factors are more satisfied with their search. On the other hand, it was found that there is a significant relationship between the overall style of userâs attribution and their style of attribution in information retrieval. Â© The Author(s) 2018.","This study aimed to assess the users style of attribution of emotions in information retrieval process based on Weiners attribution theory of emotion. The research method in the present research is descriptive and the type of study is practical. The population of this study consisted of MA students of different humanistic science branches studying at Imam-Reza International University. A sample of 72 students was selected. The required information were collected through a questionnaire of attribution style and two researcher-made questionnaires. Results showed that the majority of the users attributed their failure and success in information retrieval to internal causes. Also according to Weiners theory, they mentioned effort as a factor in their success and inability and inadequate effort as their main failure factors. Research showed that individuals who attribute their emotions to internal factors are more satisfied with their search. On the other hand, it was found that there is a significant relationship between the overall style of users attribution and their style of attribution in information retrieval."
A cross-institutional analysis of data-related curricula in information science programmes: A focused look at the iSchools,"Our rapidly growing, data-driven culture is motivating curriculum change in nearly every discipline, not the least of which is information science. This article explores this change specifically within the iSchool community, in which information science is a major unifying discipline. A cross-institutional analysis of data-related curricula was conducted across 65 iSchools. Results show that a majority of iSchools examined (37 out of 65, 56.9%) currently offer some form of data-related education, particularly at the masterâs level, and that approximately 15% of their formal degree offerings have a data focus. Overall, iSchools have a greater emphasis on data science and big data analytics, with only a few programmes providing focused curricula in the area of digital curation. Recommendations are made for iSchools to leverage the interdisciplinary nature of information science, publish curricula and track graduate success so that iSchools may excel in educating information professionals in the data area. Future data education in iSchools may benefit from further interdisciplinary data education, including data curation curricula. Â© The Author(s) 2018.","Our rapidly growing, data-driven culture is motivating curriculum change in nearly every discipline, not the least of which is information science. This article explores this change specifically within the iSchool community, in which information science is a major unifying discipline. A cross-institutional analysis of data-related curricula was conducted across 65 iSchools. Results show that a majority of iSchools examined (37 out of 65, 56.9%) currently offer some form of data-related education, particularly at the masters level, and that approximately 15% of their formal degree offerings have a data focus. Overall, iSchools have a greater emphasis on data science and big data analytics, with only a few programmes providing focused curricula in the area of digital curation. Recommendations are made for iSchools to leverage the interdisciplinary nature of information science, publish curricula and track graduate success so that iSchools may excel in educating information professionals in the data area. Future data education in iSchools may benefit from further interdisciplinary data education, including data curation curricula."
A fast multi-level algorithm for community detection in directed online social networks,"The discovery of underlying community structures plays a significant role in online social network (OSN) analysis. Many previous methods suffer from inaccuracy or incompleteness in community descriptions because of the multiple factors affecting OSNs and the high computational complexity caused by the large scale of these networks. We present a new community detection approach that focuses on two aspects. First, it relies on a combination of user interests and cohesiveness in describing community structures. Second, it introduces a multi-level community discovery algorithm for large-scale OSN datasets. The algorithm consists of three steps: (1) network coarsening based on the combination of two categories of properties, (2) stochastic inference to find an initial community assignment over the coarsest network and (3) projection and refinement of this assignment to obtain the final community detection result by solving a semi-supervised learning problem. The combination of user interests and cohesiveness leads to a complete and well-interpreted description of the communities embedded in OSNs, and the multi-level algorithm speeds up the computation process and improves the likelihood of finding the global optimal solution by reducing the parameter space. Experiments conducted on both synthetic and real datasets demonstrate the effectiveness and efficiency of our method. Â© The Author(s) 2017.","The discovery of underlying community structures plays a significant role in online social network (OSN) analysis. Many previous methods suffer from inaccuracy or incompleteness in community descriptions because of the multiple factors affecting OSNs and the high computational complexity caused by the large scale of these networks. We present a new community detection approach that focuses on two aspects. First, it relies on a combination of user interests and cohesiveness in describing community structures. Second, it introduces a multi-level community discovery algorithm for large-scale OSN datasets. The algorithm consists of three steps: network coarsening based on the combination of two categories of properties, stochastic inference to find an initial community assignment over the coarsest network and projection and refinement of this assignment to obtain the final community detection result by solving a semi-supervised learning problem. The combination of user interests and cohesiveness leads to a complete and well-interpreted description of the communities embedded in OSNs, and the multi-level algorithm speeds up the computation process and improves the likelihood of finding the global optimal solution by reducing the parameter space. Experiments conducted on both synthetic and real datasets demonstrate the effectiveness and efficiency of our method."
Knowledge sharing in a non-native language context: Challenges and strategies,"Knowledge sharing is a language-based activity. With the rise of multilingual workforces and the adoption of common corporate languages, such as English, knowledge sharing is happening in non-native language contexts more than ever before. This article explores the challenges and strategies of knowledge sharing in a non-native language at the individual level. For this purpose, an exploratory case study was conducted in a multinational organisation. Results show that the use of a non-native language can make knowledge sharing an ambiguous and costly process, eroding some of the benefits of knowledge sharing. It was found that employees adopt three different strategies to deal with problems in the knowledge-sharing process caused using a non-native language. These strategies â namely, discourse adjustment, language adjustment and media adjustment â play an important role in the successful exchange of knowledge between linguistically diverse individuals. Â© 2017, Â© The Author(s) 2017.","Knowledge sharing is a language-based activity. With the rise of multilingual workforces and the adoption of common corporate languages, such as English, knowledge sharing is happening in non-native language contexts more than ever before. This article explores the challenges and strategies of knowledge sharing in a non-native language at the individual level. For this purpose, an exploratory case study was conducted in a multinational organisation. Results show that the use of a non-native language can make knowledge sharing an ambiguous and costly process, eroding some of the benefits of knowledge sharing. It was found that employees adopt three different strategies to deal with problems in the knowledge-sharing process caused using a non-native language. These strategies namely, discourse adjustment, language adjustment and media adjustment play an important role in the successful exchange of knowledge between linguistically diverse individuals."
A three-layer model on usersâ interests mining,"With the advent of big data era, social media plays an important role in many areas such as security and finance. Researchers pay more attention on mining usersâ interests through the social media data. A three-layer model (TLM) based on keyword extracting is proposed to mine usersâ interests, which includes candidate words extracting, semantic structures analysing and interest words ranking. The TLM mainly focuses on both self-importance and semantic-importance of interest words. In addition, the TLM also considers the interest drifting to track long-term and short-term interests of users. Experiments conducted on 10 SINA Weibo datasets show that TLM is more efficient than existing methods to identify usersâ interests based on hit rate. Â© 2017, Â© The Author(s) 2017.","With the advent of big data era, social media plays an important role in many areas such as security and finance. Researchers pay more attention on mining users interests through the social media data. A three-layer model (TLM) based on keyword extracting is proposed to mine users interests, which includes candidate words extracting, semantic structures analysing and interest words ranking. The TLM mainly focuses on both self-importance and semantic-importance of interest words. In addition, the TLM also considers the interest drifting to track long-term and short-term interests of users. Experiments conducted on 10 SINA Weibo datasets show that TLM is more efficient than existing methods to identify users interests based on hit rate."
Towards enhancement of a lexicon-based approach for Saudi dialect sentiment analysis,"Sentiment analysis (SA) techniques are applied to assess aspects of language that are used to express feelings, evaluations and opinions in areas such as customer sentiment extraction. Most studies have focused on SA techniques for widely used languages such as English, but less attention has been paid to Arabic, particularly the Saudi dialect. Most Arabic SA studies have built systems using supervised approaches that are domain dependent; hence, they achieve low performance when applied to a new domain different from the learning domain, and they require manually labelled training data, which are usually difficult to obtain. In this article, we propose a novel lexicon-based algorithm for Saudi dialect SA that features domain independence. We created an annotated Saudi dialect dataset and built a large-scale lexicon for the Saudi dialect. Then, we developed our weighted lexicon-based algorithm. The proposed algorithm mines the associations between polarity and non-polarity words for the dataset and then weights these words based on their associations. During algorithm development, we also proposed novel rules for handling some linguistic features such as negation and supplication. Several experiments were performed to evaluate the performance of the proposed algorithm. Â© 2017, Â© The Author(s) 2017.","Sentiment analysis (SA) techniques are applied to assess aspects of language that are used to express feelings, evaluations and opinions in areas such as customer sentiment extraction. Most studies have focused on SA techniques for widely used languages such as English, but less attention has been paid to Arabic, particularly the Saudi dialect. Most Arabic SA studies have built systems using supervised approaches that are domain dependent; hence, they achieve low performance when applied to a new domain different from the learning domain, and they require manually labelled training data, which are usually difficult to obtain. In this article, we propose a novel lexicon-based algorithm for Saudi dialect SA that features domain independence. We created an annotated Saudi dialect dataset and built a large-scale lexicon for the Saudi dialect. Then, we developed our weighted lexicon-based algorithm. The proposed algorithm mines the associations between polarity and non-polarity words for the dataset and then weights these words based on their associations. During algorithm development, we also proposed novel rules for handling some linguistic features such as negation and supplication. Several experiments were performed to evaluate the performance of the proposed algorithm."
'Being in a knowledge space': Information behaviour of cult media fan communities,"This article describes the first two parts of a three-stage study investigating the information behaviour of fans and fan communities, focusing on fans of cult media. A literature analysis shows that information practices are an inherent and major part of fan activities, and that fans are practitioners of new forms of information consumption and production, showing sophisticated activities of information organisation and dissemination. A subsequent Delphi study, taking the novel form of a 'serious leisure' Delphi, in which the participants are not experts in the usual sense, identifies three aspects of fan information behaviour of particular interest beyond the fan context: information gatekeeping; classifying and tagging; and entrepreneurship and economic activity. Â© Chartered Institute of Library and Information Professionals.","This article describes the first two parts of a three-stage study investigating the information behaviour of fans and fan communities, focusing on fans of cult media. A literature analysis shows that information practices are an inherent and major part of fan activities, and that fans are practitioners of new forms of information consumption and production, showing sophisticated activities of information organisation and dissemination. A subsequent Delphi study, taking the novel form of a 'serious leisure' Delphi, in which the participants are not experts in the usual sense, identifies three aspects of fan information behaviour of particular interest beyond the fan context: information gatekeeping; classifying and tagging; and entrepreneurship and economic activity."
A comprehensive study of online event tracking algorithms in social networks,"Recently, social networks have provided an important platform to detect trends of real-world events. The trends of real-world events are detected by analysing flow of massive bulks of data in continuous time steps over various social media platforms. Today, many researchers have been interested in detecting social network trends, in order to analyse the gathered information for enabling users and organisations to satisfy their information need. This article is aimed at complete surveying the recent text-based trend detection approaches, which have been studied from three perspectives (algorithms, dimension and diversity of events). The advantages and disadvantages of the considered approaches have also been paraphrased separately to illustrate a comprehensive view of the previous works and open problems. Â© The Author(s) 2018.","Recently, social networks have provided an important platform to detect trends of real-world events. The trends of real-world events are detected by analysing flow of massive bulks of data in continuous time steps over various social media platforms. Today, many researchers have been interested in detecting social network trends, in order to analyse the gathered information for enabling users and organisations to satisfy their information need. This article is aimed at complete surveying the recent text-based trend detection approaches, which have been studied from three perspectives (algorithms, dimension and diversity of events). The advantages and disadvantages of the considered approaches have also been paraphrased separately to illustrate a comprehensive view of the previous works and open problems."
Representing and integrating bibliographic information into the Semantic Web: A comparison of four conceptual models,"Integration of library data into the Semantic Web environment is a key issue for libraries and is approached on the basis of interoperability between conceptual models. Several data models exist for the representation and publication of library data in the Semantic Web and therefore inter-domain and intra-domain interoperability issues emerge as a growing number of web data are generated. Achieving interoperability for different representations of the same or related entities between the library and other cultural heritage institutions shall enhance rich bibliographic data reusability and support the development of new data-driven information services. This paper aims to investigate common ground and convergences between four conceptual models, namely Functional Requirements for Bibliographic Records (FRBR), FRBR Object-Oriented (FRBRoo), Bibliographic Framework (BIBFRAME) and Europeana Data Model (EDM), enabling semantically-richer interoperability by studying the representation of monographs, as well as of content relationships (derivative and equivalent bibliographic relationships) and of whole-part relationships between them. Â© The Author(s) 2016.","Integration of library data into the Semantic Web environment is a key issue for libraries and is approached on the basis of interoperability between conceptual models. Several data models exist for the representation and publication of library data in the Semantic Web and therefore inter-domain and intra-domain interoperability issues emerge as a growing number of web data are generated. Achieving interoperability for different representations of the same or related entities between the library and other cultural heritage institutions shall enhance rich bibliographic data reusability and support the development of new data-driven information services. This paper aims to investigate common ground and convergences between four conceptual models, namely Functional Requirements for Bibliographic Records (FRBR), FRBR Object-Oriented (FRBRoo), Bibliographic Framework (BIBFRAME) and Europeana Data Model (EDM), enabling semantically-richer interoperability by studying the representation of monographs, as well as of content relationships (derivative and equivalent bibliographic relationships) and of whole-part relationships between them."
Hot and cold spots in the US research: A spatial analysis of bibliometric data on the institutional level,"Spatial bibliometrics addresses the spatial aspects of scientific research activities. In this case study, we use the GetisâOrd Gâ i (d) statistic for bibliometric data on US institutions to identify hot spots of institutions on a map publishing many high-impact papers. The study is based on a dataset with performance data (proportion and number of papers belonging to the 10% most frequently cited papers) and geo-coordinates for all institutions in the United States from the SCImago group (and Scopus). The Getis-Ord Gi* statistic returns, for each institution on a map, a z score. Higher z scores point to intense clustering of institutions, which have published a large proportion or number of highly cited papers (hot spots). The US maps, which we generate as examples in this study, point to four regions. These regions can be labelled as hot spots: around San Francisco, Los Angeles, Boston and Washington, DC. The empirical focus on institutional hot spots in a country using bibliometric data is of specific importance for science policy, because geospatial proximity is shown as an important factor for innovation processes. Â© The Author(s) 2018.","Spatial bibliometrics addresses the spatial aspects of scientific research activities. In this case study, we use the GetisOrd G i statistic for bibliometric data on US institutions to identify hot spots of institutions on a map publishing many high-impact papers. The study is based on a dataset with performance data (proportion and number of papers belonging to the 10% most frequently cited papers) and geo-coordinates for all institutions in the United States from the SCImago group (and Scopus). The Getis-Ord Gi* statistic returns, for each institution on a map, a z score. Higher z scores point to intense clustering of institutions, which have published a large proportion or number of highly cited papers (hot spots). The US maps, which we generate as examples in this study, point to four regions. These regions can be labelled as hot spots: around San Francisco, Los Angeles, Boston and Washington, The empirical focus on institutional hot spots in a country using bibliometric data is of specific importance for science policy, because geospatial proximity is shown as an important factor for innovation processes."
Analysis of user-generated content from online social communities to characterise and predict depression degree,"The identification of a mental disorder at its early stages is a challenging task because it requires clinical interventions that may not be feasible in many cases. Social media such as online communities and blog posts have shown some promising features to help detect and characterise mental disorder at an early stage. In this work, we make use of user-generated content to identify depression and further characterise its degree of severity. We used the user-generated post contents and its associated mood tag to understand and differentiate the linguistic style and sentiments of the user content. We applied machine learning and statistical analysis methods to discriminate the depressive posts and communities from non-depressive ones. The depression degree of a depressed post is identified using variations of valence values based on the mood tag. The proposed methodology achieved 90%, 95% and 92% accuracy for the classification of depressive posts, depressive communities and depression degree, respectively. Â© The Author(s) 2017.","The identification of a mental disorder at its early stages is a challenging task because it requires clinical interventions that may not be feasible in many cases. Social media such as online communities and blog posts have shown some promising features to help detect and characterise mental disorder at an early stage. In this work, we make use of user-generated content to identify depression and further characterise its degree of severity. We used the user-generated post contents and its associated mood tag to understand and differentiate the linguistic style and sentiments of the user content. We applied machine learning and statistical analysis methods to discriminate the depressive posts and communities from non-depressive ones. The depression degree of a depressed post is identified using variations of valence values based on the mood tag. The proposed methodology achieved 90%, 95% and 92% accuracy for the classification of depressive posts, depressive communities and depression degree, respectively."
Fast prediction of web user browsing behaviours using most interesting patterns,"The prediction of usersâ browsing behaviours is essential for putting appropriate information on the web. The browsing behaviours are stored as navigational patterns in web server logs. These weblogs are used to predict the frequently accessed patterns of web users, which can be used to predict user behaviour and to collect business intelligence. However, owing to the exponentially increasing weblog size, existing implementations of frequent-pattern-mining algorithms often take too much time and generate too many redundant patterns. This article introduces the most interesting pattern-based parallel FP-growth (MIP-PFP) algorithm. MIP-PFP is an improved implementation of the parallel FP-growth algorithm and implemented on the Apache Spark platform for extracting frequent patterns from huge weblogs. Experiments were performed on openly available National Aeronautics and Space Administration (NASA) weblog data to test the effectiveness of the MIP-PFP algorithm. The results were compared with existing implementation of PFP algorithms. The results suggest that the MIP-PFP algorithm running on Apache Spark reduced the execution time by a factor of more than 10 times. The effect of sequence length that has been used as input to the MIP-PFP algorithm was also evaluated with different interestingness parameters including support, confidence, lift, leverage, cosine, and conviction. It is observed from experimental results that only sequences of length greater than three produced a very low value of support for these interestingness measures. Â© 2016, Â© The Author(s) 2016.","The prediction of users browsing behaviours is essential for putting appropriate information on the web. The browsing behaviours are stored as navigational patterns in web server logs. These weblogs are used to predict the frequently accessed patterns of web users, which can be used to predict user behaviour and to collect business intelligence. However, owing to the exponentially increasing weblog size, existing implementations of frequent-pattern-mining algorithms often take too much time and generate too many redundant patterns. This article introduces the most interesting pattern-based parallel FP-growth (MIP-PFP) algorithm. MIP-PFP is an improved implementation of the parallel FP-growth algorithm and implemented on the Apache Spark platform for extracting frequent patterns from huge weblogs. Experiments were performed on openly available National Aeronautics and Space Administration (NASA) weblog data to test the effectiveness of the MIP-PFP algorithm. The results were compared with existing implementation of PFP algorithms. The results suggest that the MIP-PFP algorithm running on Apache Spark reduced the execution time by a factor of more than 10 times. The effect of sequence length that has been used as input to the MIP-PFP algorithm was also evaluated with different interestingness parameters including support, confidence, lift, leverage, cosine, and conviction. It is observed from experimental results that only sequences of length greater than three produced a very low value of support for these interestingness measures."
CredSaT: Credibility ranking of users in big social data incorporating semantic analysis and temporal factor,"The widespread use of big social data has influenced the research community in several significant ways. In particular, the notion of social trust has attracted a great deal of attention from information processors and computer scientists as well as information consumers and formal organisations. This attention is embodied in the various shapes social trust has taken, such as its use in recommendation systems, viral marketing and expertise retrieval. Hence, it is essential to implement frameworks that are able to temporally measure a userâs credibility in all categories of big social data. To this end, this article suggests the CredSaT (Credibility incorporating Semantic analysis and Temporal factor), which is a fine-grained credibility analysis framework for use in big social data. A novel metric that includes both new and current features, as well as the temporal factor, is harnessed to establish the credibility ranking of users. Experiments on real-world datasets demonstrate the efficacy and applicability of our model in determining highly domain-based trustworthy users. Furthermore, CredSaT may also be used to identify spammers and other anomalous users. Â© The Author(s) 2018.","The widespread use of big social data has influenced the research community in several significant ways. In particular, the notion of social trust has attracted a great deal of attention from information processors and computer scientists as well as information consumers and formal organisations. This attention is embodied in the various shapes social trust has taken, such as its use in recommendation systems, viral marketing and expertise retrieval. Hence, it is essential to implement frameworks that are able to temporally measure a users credibility in all categories of big social data. To this end, this article suggests the CredSaT (Credibility incorporating Semantic analysis and Temporal factor), which is a fine-grained credibility analysis framework for use in big social data. A novel metric that includes both new and current features, as well as the temporal factor, is harnessed to establish the credibility ranking of users. Experiments on real-world datasets demonstrate the efficacy and applicability of our model in determining highly domain-based trustworthy users. Furthermore, CredSaT may also be used to identify spammers and other anomalous users."
Community detection in dynamic social networks: A local evolutionary approach,"Communities in social networks are groups of individuals who are connected with specific goals. Discovering information on the structure, members and types of changes of communities have always been of great interest. Despite the extensive global researches conducted on these, discovery has not been confirmed yet and researchers try to find methods and improve estimated techniques by using Data Mining tools, Graph Mining tools and artificial intelligence techniques. This paper proposes a novel two-phase approach based on global and local information to detect communities in social network. It explores the global information in the first phase and then exploits the local information in the second phase to discover communities more accurately. It also proposes a novel algorithm which exploits the local information and mines deeply for the second phase. Experimental results show that the proposed method has better performance and achieves more accurate results compared with the previous ones. Â© Chartered Institute of Library and Information Professionals.","Communities in social networks are groups of individuals who are connected with specific goals. Discovering information on the structure, members and types of changes of communities have always been of great interest. Despite the extensive global researches conducted on these, discovery has not been confirmed yet and researchers try to find methods and improve estimated techniques by using Data Mining tools, Graph Mining tools and artificial intelligence techniques. This paper proposes a novel two-phase approach based on global and local information to detect communities in social network. It explores the global information in the first phase and then exploits the local information in the second phase to discover communities more accurately. It also proposes a novel algorithm which exploits the local information and mines deeply for the second phase. Experimental results show that the proposed method has better performance and achieves more accurate results compared with the previous ones."
Profile-based recommendation: A case study in a parliamentary context,"In the context of e-government and more specifically that of parliament, this paper tackles the problem of finding Members of Parliament (MPs) according to their profiles which have been built from their speeches in plenary or committee sessions. The paper presents a common solution for two problems: firstly, a member of the public who is concerned about a certain issue might want to know who the best MP is for dealing with their problem (recommending task); and secondly, each new piece of textual information that reaches the house must be correctly allocated to the appropriate MP according to its content (filtering task). This paper explores both these ways of searching for relevant people conceptually by encapsulating them into a single problem: that of searching for the relevant MP's profile given an information need. Our research work proposes various profile construction methods (by selecting and weighting appropriate terms) and compares these using different retrieval models to evaluate their quality and suitability for different types of information needs in order to simulate real and common situations. Â© Chartered Institute of Library and Information Professionals.","In the context of e-government and more specifically that of parliament, this paper tackles the problem of finding Members of Parliament (MPs) according to their profiles which have been built from their speeches in plenary or committee sessions. The paper presents a common solution for two problems: firstly, a member of the public who is concerned about a certain issue might want to know who the best MP is for dealing with their problem (recommending task); and secondly, each new piece of textual information that reaches the house must be correctly allocated to the appropriate MP according to its content (filtering task). This paper explores both these ways of searching for relevant people conceptually by encapsulating them into a single problem: that of searching for the relevant MP's profile given an information need. Our research work proposes various profile construction methods (by selecting and weighting appropriate terms) and compares these using different retrieval models to evaluate their quality and suitability for different types of information needs in order to simulate real and common situations."
Topic-based hierarchical Bayesian linear regression models for niche items recommendation,"A vital research concern for a personalised recommender system is to target items in the long tail. Studies have shown that sales of the e-commerce platform possess a long-tail character, and niche items in the long tail are challenging to be involved in the recommendation list. Since niche items are defined by the niche market, which is a small market segment, traditional recommendation algorithms focused more on popular items promotion and they do not apply to the niche market. In this article, we aim to find the best users for each niche item and proposed a topic-based hierarchical Bayesian linear regression model for niche item recommendation. We first identify niche items and build niche item subgroups based on descriptive information of items. Moreover, we learn a hierarchical Bayesian linear regression model for each niche item subgroup. Finally, we predict the relevance between users and niche items to provide recommendations. We perform a series of validation experiments on Yahoo Movies dataset and compare the performance of our approach with a set of representative baseline recommender algorithms. The result demonstrates the superior performance of our recommendation approach for niche items. Â© The Author(s) 2018.","A vital research concern for a personalised recommender system is to target items in the long tail. Studies have shown that sales of the e-commerce platform possess a long-tail character, and niche items in the long tail are challenging to be involved in the recommendation list. Since niche items are defined by the niche market, which is a small market segment, traditional recommendation algorithms focused more on popular items promotion and they do not apply to the niche market. In this article, we aim to find the best users for each niche item and proposed a topic-based hierarchical Bayesian linear regression model for niche item recommendation. We first identify niche items and build niche item subgroups based on descriptive information of items. Moreover, we learn a hierarchical Bayesian linear regression model for each niche item subgroup. Finally, we predict the relevance between users and niche items to provide recommendations. We perform a series of validation experiments on Yahoo Movies dataset and compare the performance of our approach with a set of representative baseline recommender algorithms. The result demonstrates the superior performance of our recommendation approach for niche items."
Semantic concept model using Wikipedia semantic features,"Wikipedia has become a high coverage knowledge source which has been used in many research areas such as natural language processing, text mining and information retrieval. Several methods have been introduced for extracting explicit or implicit relations from Wikipedia to represent semantics of concepts/words. However, the main challenge in semantic representation is how to incorporate different types of semantic relations to capture more semantic evidences of the associations of concepts. In this article, we propose a semantic concept model that incorporates different types of semantic features extracting from Wikipedia. For each concept that corresponds to an article, four semantic features are introduced: template links, categories, salient concepts and topics. The proposed model is based on the probability distributions that are defined for these semantic features of a Wikipedia concept. The template links and categories are the document-level features which are directly extracted from the structured information included in the article. On the other hand, the salient concepts and topics are corpus-level features which are extracted to capture implicit relations among concepts. For the salient concepts feature, the distributional-based method is utilised on the hypertext corpus to extract this feature for each Wikipedia concept. Then, the probability product kernel is used to improve the weight of each concept in this feature. For the topic feature, the Labelled latent Dirichlet allocation is adapted on the supervised multi-label of Wikipedia to train the probabilistic model of this feature. Finally, we used the linear interpolation for incorporating these semantic features into the probabilistic model to estimate the semantic relation probability of the specific concept over Wikipedia articles. The proposed model is evaluated on 12 benchmark datasets in three natural language processing tasks: measuring the semantic relatedness of concepts/words in general and in the biomedical domain, semantic textual relatedness measurement and measuring the semantic compositionality of noun compounds. The model is also compared with five methods that depends on separate semantic features in Wikipedia. Experimental results show that the proposed model achieves promising results in three tasks and outperforms the baseline methods in most of the evaluation datasets. This implies that incorporation of explicit and implicit semantic features is useful for representing semantics of concepts in Wikipedia. Â© The Author(s) 2017.","Wikipedia has become a high coverage knowledge source which has been used in many research areas such as natural language processing, text mining and information retrieval. Several methods have been introduced for extracting explicit or implicit relations from Wikipedia to represent semantics of concepts/words. However, the main challenge in semantic representation is how to incorporate different types of semantic relations to capture more semantic evidences of the associations of concepts. In this article, we propose a semantic concept model that incorporates different types of semantic features extracting from Wikipedia. For each concept that corresponds to an article, four semantic features are introduced: template links, categories, salient concepts and topics. The proposed model is based on the probability distributions that are defined for these semantic features of a Wikipedia concept. The template links and categories are the document-level features which are directly extracted from the structured information included in the article. On the other hand, the salient concepts and topics are corpus-level features which are extracted to capture implicit relations among concepts. For the salient concepts feature, the distributional-based method is utilised on the hypertext corpus to extract this feature for each Wikipedia concept. Then, the probability product kernel is used to improve the weight of each concept in this feature. For the topic feature, the Labelled latent Dirichlet allocation is adapted on the supervised multi-label of Wikipedia to train the probabilistic model of this feature. Finally, we used the linear interpolation for incorporating these semantic features into the probabilistic model to estimate the semantic relation probability of the specific concept over Wikipedia articles. The proposed model is evaluated on 12 benchmark datasets in three natural language processing tasks: measuring the semantic relatedness of concepts/words in general and in the biomedical domain, semantic textual relatedness measurement and measuring the semantic compositionality of noun compounds. The model is also compared with five methods that depends on separate semantic features in Wikipedia. Experimental results show that the proposed model achieves promising results in three tasks and outperforms the baseline methods in most of the evaluation datasets. This implies that incorporation of explicit and implicit semantic features is useful for representing semantics of concepts in Wikipedia."
Visualising and mapping the intellectual structure of medical big data,"In this study, we sought to apply recent advances in informetrics to the analysis of literature related to big data in the field of medicine. Our aim was to elucidate research trends, identify knowledge clusters and decipher the links between them. We also sought to ascertain the theories most commonly applied in the processing of medical data and identify potential research gaps. The most important keywords over the last 10 years have been âbig dataâ, âdata miningâ, âhealthcareâ, âcloud computingâ, âmachine learningâ and âelectronic health record systemâ. These could be viewed as the core issues of research associated with big data in the field of medicine. We also identified a number of keywords that are expected to play a pivotal role in this field in the near future. These terms include the âinternet of thingsâ, âe-healthâ, âsensorsâ, âpredictive modelingâ, âquantified selfâ, âsmart cityâ, âwearable deviceâ and âm-healthâ. Finally, we compiled co-word networks indicating the degree of connectivity between keywords, for use in locating knowledge gaps by revealing the overall context of issues commonly encountered when investigating big data. Our findings form a solid academic foundation on which to develop medical technologies, managerial strategies and theory related to big data. Â© The Author(s) 2018.","In this study, we sought to apply recent advances in informetrics to the analysis of literature related to big data in the field of medicine. Our aim was to elucidate research trends, identify knowledge clusters and decipher the links between them. We also sought to ascertain the theories most commonly applied in the processing of medical data and identify potential research gaps. The most important keywords over the last 10 years have been big data, data mining, healthcare, cloud computing, machine learning and electronic health record system. These could be viewed as the core issues of research associated with big data in the field of medicine. We also identified a number of keywords that are expected to play a pivotal role in this field in the near future. These terms include the internet of things, e-health, sensors, predictive modeling, quantified self, smart city, wearable device and m-health. Finally, we compiled co-word networks indicating the degree of connectivity between keywords, for use in locating knowledge gaps by revealing the overall context of issues commonly encountered when investigating big data. Our findings form a solid academic foundation on which to develop medical technologies, managerial strategies and theory related to big data."
Berrypicking and information foraging: Comparison of two theoretical frameworks for studying exploratory search,"Exploratory search is a specialised form of information searching behaviour that is open-ended, dynamic and multi-faceted in nature. The study elaborates the picture of exploratory search by concentrating on two theoretical frameworks for studying searching of this type, that is, the berrypicking model and the information foraging theory. The study focuses on two major issues: the motivators for exploratory search and the nature of search processes. The findings draw on the conceptual analysis of 30 key studies on this topic. The berrypicking model posits changing information needs as the main motivator for information searching, while the information foraging theory emphasises the role need for performing a task as a main trigger of information seeking. The berrypicking model devotes more attention to focused searching as a key constituent of exploratory search, while the information foraging theory concentrates on the characterisation of exploratory browsing. Â© The Author(s) 2017.","Exploratory search is a specialised form of information searching behaviour that is open-ended, dynamic and multi-faceted in nature. The study elaborates the picture of exploratory search by concentrating on two theoretical frameworks for studying searching of this type, that is, the berrypicking model and the information foraging theory. The study focuses on two major issues: the motivators for exploratory search and the nature of search processes. The findings draw on the conceptual analysis of 30 key studies on this topic. The berrypicking model posits changing information needs as the main motivator for information searching, while the information foraging theory emphasises the role need for performing a task as a main trigger of information seeking. The berrypicking model devotes more attention to focused searching as a key constituent of exploratory search, while the information foraging theory concentrates on the characterisation of exploratory browsing."
"Corrigendum to: Linking and using social media data for enhancing public health analytics (Journal of Information Science, (2017), 43, 2, (221-245), 10.1177/0165551515625029)","The authors regret that non-anonymised patient data was used from a social medical network without prior permission. With permission from the social medical network, the authors have anonymised the data and corrected the article. The online version of the article has been corrected. Â© The Author(s) 2018.","The authors regret that non-anonymised patient data was used from a social medical network without prior permission. With permission from the social medical network, the authors have anonymised the data and corrected the article. The online version of the article has been corrected."
Influence maximisation in social networks: A target-oriented estimation,"Influence maximisation (IM) is the problem of finding a set of k-seed nodes that could maximize the amount of influence spread in a social network. In this article, we point out that the existing methods are taking the source-oriented estimation (SOE), which is the main reason of their failure in accurately estimating the amount of potential influence spread of an individual node. We propose a novel target-oriented estimation (TOE) that understands information diffusion more accurately as well as remedies the drawback of the existing methods. Our extensive experiments on four real-world datasets demonstrate that our proposed method outperforms the existing methods consistently with respect to the quality of the selected seed set. Â© The Author(s) 2018.","Influence maximisation is the problem of finding a set of k-seed nodes that could maximize the amount of influence spread in a social network. In this article, we point out that the existing methods are taking the source-oriented estimation (SOE), which is the main reason of their failure in accurately estimating the amount of potential influence spread of an individual node. We propose a novel target-oriented estimation (TOE) that understands information diffusion more accurately as well as remedies the drawback of the existing methods. Our extensive experiments on four real-world datasets demonstrate that our proposed method outperforms the existing methods consistently with respect to the quality of the selected seed set."
The openness of religious beliefs to the influence of external information,"Religious beliefs have important and wide-reaching impacts on society. They also tend to be viewed as impervious to the influence of information external to a religious setting. Eight focus groups were held with attendees of two United Church of Christ congregations. Participants were asked about their core religious beliefs, and transcripts were qualitatively coded for the interplay of belief and information. Analysis found that beliefs that were focused on people, processes and events external to the congregation showed the characteristics of being more open to external information. Specifically, the breadth of these external beliefs allowed for a wider set of external information to be considered relevant; these beliefs were less biased, allowing participants to be more open to disconfirming information from outside the congregation; and these beliefs were held with less certainty, making it more likely that this disconfirming information would be attended to. This study provides suggestions for religious practitioners wishing to make the information behaviour of their organisations more open. Â© The Author(s) 2017.","Religious beliefs have important and wide-reaching impacts on society. They also tend to be viewed as impervious to the influence of information external to a religious setting. Eight focus groups were held with attendees of two United Church of Christ congregations. Participants were asked about their core religious beliefs, and transcripts were qualitatively coded for the interplay of belief and information. Analysis found that beliefs that were focused on people, processes and events external to the congregation showed the characteristics of being more open to external information. Specifically, the breadth of these external beliefs allowed for a wider set of external information to be considered relevant; these beliefs were less biased, allowing participants to be more open to disconfirming information from outside the congregation; and these beliefs were held with less certainty, making it more likely that this disconfirming information would be attended to. This study provides suggestions for religious practitioners wishing to make the information behaviour of their organisations more open."
Detecting the association of health problems in consumer-level medical text,"Consumers usually do not know the complicated links between related health problems. This fact may cause troubles when they wish to seek complete information regarding such problems. This study detects the associations among health problems by extending the meaning of health terms with methods based on the latent Dirichlet allocation (LDA) probability topic model, the Medical Subject Headings (MeSH) thesaurus structure and the Wikipedia concept mapping. The terms represented health problems are selected from and extended by the consumer-level medical text. The vocabulary is different between the consumer-level and the professional-level medical text. Thus, the findings can be easily understood by the general public and be suitable to consumer-oriented applications. The methods were evaluated in two ways: (1) correlation analysis with expert rating to show the overall performance and (2) P@N to reflect the ability of detecting strong associations. The LDA topic-model-based method outperforms the other two types. The judgment incongruence between the best method and the expert ratings has been examined, and the evidence shows that the automatic method sometimes detects real associations beyond those identified by human experts. Â© 2016, Â© The Author(s) 2016.","Consumers usually do not know the complicated links between related health problems. This fact may cause troubles when they wish to seek complete information regarding such problems. This study detects the associations among health problems by extending the meaning of health terms with methods based on the latent Dirichlet allocation (LDA) probability topic model, the Medical Subject Headings (MeSH) thesaurus structure and the Wikipedia concept mapping. The terms represented health problems are selected from and extended by the consumer-level medical text. The vocabulary is different between the consumer-level and the professional-level medical text. Thus, the findings can be easily understood by the general public and be suitable to consumer-oriented applications. The methods were evaluated in two ways: correlation analysis with expert rating to show the overall performance and P@N to reflect the ability of detecting strong associations. The LDA topic-model-based method outperforms the other two types. The judgment incongruence between the best method and the expert ratings has been examined, and the evidence shows that the automatic method sometimes detects real associations beyond those identified by human experts."
TREMO: A dataset for emotion analysis in Turkish,"This study presents a new dataset to be used in emotion extraction studies in Turkish text. We consider emotion extraction as a supervised text classification problem, which thereby requires a dataset for the training process. To satisfy this requirement, we aim to create a new dataset containing data for the six emotion categories: happiness, fear, anger, sadness, disgust and surprise. To gather this dataset, we conducted a survey and collected 27,350 entries from 4709 individuals. In the next step, we performed a validation process in which annotators validated each entry one by one by assigning a related emotion category. As a result of this process, we obtained two datasets, one raw and the other validated. Subsequently, we generated four versions of these two datasets using two different stemming methods and then modelled them using a vector space model. Then, we ran machine learning algorithms, including complement naive Bayes (CNB), random forest (RF), decision tree C4.5 (J48) and an updated version of support vector machines (SVMs), on the models to calculate the accuracy, precision, recall and F-measure values. Based on the results we obtained, we concluded that the SVM classifier yielded the highest performance value and that the models trained with a validated dataset provide more accurate results than the models trained with a non-validated dataset. Â© The Author(s) 2018.","This study presents a new dataset to be used in emotion extraction studies in Turkish text. We consider emotion extraction as a supervised text classification problem, which thereby requires a dataset for the training process. To satisfy this requirement, we aim to create a new dataset containing data for the six emotion categories: happiness, fear, anger, sadness, disgust and surprise. To gather this dataset, we conducted a survey and collected 27,350 entries from 4709 individuals. In the next step, we performed a validation process in which annotators validated each entry one by one by assigning a related emotion category. As a result of this process, we obtained two datasets, one raw and the other validated. Subsequently, we generated four versions of these two datasets using two different stemming methods and then modelled them using a vector space model. Then, we ran machine learning algorithms, including complement naive Bayes (CNB), random forest (RF), decision tree C4.5 (J48) and an updated version of support vector machines (SVMs), on the models to calculate the accuracy, precision, recall and F-measure values. Based on the results we obtained, we concluded that the SVM classifier yielded the highest performance value and that the models trained with a validated dataset provide more accurate results than the models trained with a non-validated dataset."
A study of big data evolution and research challenges,"The world is already into the information age. The huge growth of digital data has overwhelmed the traditional systems and approaches. Big data is touching almost all aspects of our life and the data-driven discovery approach is an emerging paradigm for computing. The ever-growing data provides a tidal wave of opportunities and challenges in terms of data capture, storage, manipulation, management, analysis, knowledge extraction, security, privacy and visualisation. Though the promise of big data seems to be genuine, still a wide gap exists between its potential and realisation. In last few years, there is a huge surge in research efforts in academia as well as industry to have a better understanding of big data. This article discusses the following: (1) big data evolution including a bibliometric study of academic and industry publications pertaining to big data during the period 2000â2017, (2) popular open-source big data stream processing frameworks and (3) prevalent research challenges which must be addressed to realise the true potential of big data. Â© The Author(s) 2018.","The world is already into the information age. The huge growth of digital data has overwhelmed the traditional systems and approaches. Big data is touching almost all aspects of our life and the data-driven discovery approach is an emerging paradigm for computing. The ever-growing data provides a tidal wave of opportunities and challenges in terms of data capture, storage, manipulation, management, analysis, knowledge extraction, security, privacy and visualisation. Though the promise of big data seems to be genuine, still a wide gap exists between its potential and realisation. In last few years, there is a huge surge in research efforts in academia as well as industry to have a better understanding of big data. This article discusses the following: big data evolution including a bibliometric study of academic and industry publications pertaining to big data during the period 20002017, popular open-source big data stream processing frameworks and prevalent research challenges which must be addressed to realise the true potential of big data."
An adaptive plan-based approach to integrating semantic streams with remote RDF data,"To satisfy a userâs complex requirements, Resource Description Framework (RDF) Stream Processing (RSP) systems envision the fusion of remote RDF data with semantic streams, using common data models to query semantic streams continuously. While streaming data are changing at a high rate and are pushed into RSP systems, the remote RDF data are retrieved from different remote sources. With the growth of SPARQL endpoints that provide access to remote RDF data, RSP systems can easily integrate the remote data with streams. Such integration provides new opportunities for mixing static (or quasi-static) data with streams on a large scale. However, the current RSP systems do not offer any optimisation for the integration. In this article, we present an adaptive plan-based approach to efficiently integrate sematic streams with the static data from a remote source. We create a query execution plan based on temporal constraints among constituent services for the timely acquisition of remote data. To predict the change of remote sources in real time, we propose an adaptive process of detecting a source update, forecasting the update in the future, deciding a new plan to obtain remote data and reacting to a new plan. We extend a SPARQL query with operators for describing the multiple strategies of the proposed adaptive process. Experimental results show that our approach is more efficient than the conventional RSP systems in distributed settings. Â© 2016, Â© The Author(s) 2016.","To satisfy a users complex requirements, Resource Description Framework (RDF) Stream Processing (RSP) systems envision the fusion of remote RDF data with semantic streams, using common data models to query semantic streams continuously. While streaming data are changing at a high rate and are pushed into RSP systems, the remote RDF data are retrieved from different remote sources. With the growth of SPARQL endpoints that provide access to remote RDF data, RSP systems can easily integrate the remote data with streams. Such integration provides new opportunities for mixing static (or quasi-static) data with streams on a large scale. However, the current RSP systems do not offer any optimisation for the integration. In this article, we present an adaptive plan-based approach to efficiently integrate sematic streams with the static data from a remote source. We create a query execution plan based on temporal constraints among constituent services for the timely acquisition of remote data. To predict the change of remote sources in real time, we propose an adaptive process of detecting a source update, forecasting the update in the future, deciding a new plan to obtain remote data and reacting to a new plan. We extend a SPARQL query with operators for describing the multiple strategies of the proposed adaptive process. Experimental results show that our approach is more efficient than the conventional RSP systems in distributed settings."
SKOS concepts and natural language concepts: An analysis of latent relationships in KOSs,"The vehicle to represent Knowledge Organisation Systems (KOSs) in the environment of the Semantic Web and linked data is the Simple Knowledge Organisation System (SKOS). SKOS provides a way to assign a Uniform Resource Identifier (URI) to each concept, and this URI functions as a surrogate for the concept. This fact makes of main concern the need to clarify the URIs' ontological meaning. The aim of this study is to investigate the relationship between the ontological substance of KOS concepts and concepts revealed through the grammatical and syntactic formalisms of natural language. For this purpose, we examined the dividableness of concepts in specific KOSs (i.e. a thesaurus, a subject headings system and a classification scheme) by applying Natural Language Processing (NLP) techniques (i.e. morphosyntactic analysis) to the lexical representations (i.e. RDF literals) of SKOS concepts. The results of the comparative analysis reveal that, despite the use of multi-word units, thesauri tend to represent concepts in a way that can hardly be further divided conceptually, while subject headings and classification schemes - to a certain extent - comprise terms that can be decomposed into more conceptual constituents. Consequently, SKOS concepts deriving from thesauri are more likely to represent atomic conceptual units and thus be more appropriate tools for inference and reasoning. Since identifiers represent the meaning of a concept, complex concepts are neither the most appropriate nor the most efficient way of modelling a KOS for the Semantic Web. Â© The Author(s) 2016.","The vehicle to represent Knowledge Organisation Systems (KOSs) in the environment of the Semantic Web and linked data is the Simple Knowledge Organisation System (SKOS). SKOS provides a way to assign a Uniform Resource Identifier (URI) to each concept, and this URI functions as a surrogate for the concept. This fact makes of main concern the need to clarify the URIs' ontological meaning. The aim of this study is to investigate the relationship between the ontological substance of KOS concepts and concepts revealed through the grammatical and syntactic formalisms of natural language. For this purpose, we examined the dividableness of concepts in specific KOSs ( a thesaurus, a subject headings system and a classification scheme) by applying Natural Language Processing (NLP) techniques ( morphosyntactic analysis) to the lexical representations ( RDF literals) of SKOS concepts. The results of the comparative analysis reveal that, despite the use of multi-word units, thesauri tend to represent concepts in a way that can hardly be further divided conceptually, while subject headings and classification schemes - to a certain extent - comprise terms that can be decomposed into more conceptual constituents. Consequently, SKOS concepts deriving from thesauri are more likely to represent atomic conceptual units and thus be more appropriate tools for inference and reasoning. Since identifiers represent the meaning of a concept, complex concepts are neither the most appropriate nor the most efficient way of modelling a KOS for the Semantic Web."
Characterising RDF data sets,"The publication of semantic web data, commonly represented in Resource Description Framework (RDF), has experienced outstanding growth over the last few years. Data from all fields of knowledge are shared publicly and interconnected in active initiatives such as Linked Open Data. However, despite the increasing availability of applications managing large-scale RDF information such as RDF stores and reasoning tools, little attention has been given to the structural features emerging in real-world RDF data. Our work addresses this issue by proposing specific metrics to characterise RDF data. We specifically focus on revealing the redundancy of each data set, as well as common structural patterns. We evaluate the proposed metrics on several data sets, which cover a wide range of designs and models. Our findings provide a basis for more efficient RDF data structures, indexes and compressors. Â© 2017, Â© The Author(s) 2017.","The publication of semantic web data, commonly represented in Resource Description Framework (RDF), has experienced outstanding growth over the last few years. Data from all fields of knowledge are shared publicly and interconnected in active initiatives such as Linked Open Data. However, despite the increasing availability of applications managing large-scale RDF information such as RDF stores and reasoning tools, little attention has been given to the structural features emerging in real-world RDF data. Our work addresses this issue by proposing specific metrics to characterise RDF data. We specifically focus on revealing the redundancy of each data set, as well as common structural patterns. We evaluate the proposed metrics on several data sets, which cover a wide range of designs and models. Our findings provide a basis for more efficient RDF data structures, indexes and compressors."
Developing information quality assessment framework of presentation slides,"Computerized presentation slides have become essential for many occasions such as business meetings, classroom discussions, multipurpose talks and public events. Given the tremendous increases in online resources and materials, locating high-quality slides relevant to a given task is often a formidable challenge, particularly when a user looks for superior quality slides. This study proposes a new, comprehensive framework for information quality (IQ) developed specifically for computerized presentation slides and explores the possibility of automatically detecting the IQ of slides. To determine slide-specific IQ criteria as well as their relative importances, we carried out a user study, involving 60 participants from two universities, and conducted extensive coding analysis. Further, we subsequently conducted a series of multiple experiments to examine the validity of the IQ features developed on the basis of the selected criteria from the user study. The study findings contribute to identifying key dimensions and related features that can improve effective IQ assessments of computerized presentation slides. Â© 2016, Â© The Author(s) 2016.","Computerized presentation slides have become essential for many occasions such as business meetings, classroom discussions, multipurpose talks and public events. Given the tremendous increases in online resources and materials, locating high-quality slides relevant to a given task is often a formidable challenge, particularly when a user looks for superior quality slides. This study proposes a new, comprehensive framework for information quality (IQ) developed specifically for computerized presentation slides and explores the possibility of automatically detecting the IQ of slides. To determine slide-specific IQ criteria as well as their relative importances, we carried out a user study, involving 60 participants from two universities, and conducted extensive coding analysis. Further, we subsequently conducted a series of multiple experiments to examine the validity of the IQ features developed on the basis of the selected criteria from the user study. The study findings contribute to identifying key dimensions and related features that can improve effective IQ assessments of computerized presentation slides."
An empirical test of an Antecedents - Privacy Concerns - Outcomes model,"This study extends privacy concerns research by providing a test of a model inspired by the 'Antecedents - Privacy Concerns - Outcomes' (APCO) framework. Focusing at the individual level of analysis, the study examines the influences of privacy awareness (PA) and demographic variables (age, gender) on concern for information privacy (CFIP). It also considers CFIP's relationship to privacy-protecting behaviours and incorporates trust and risk into the model. These relationships are tested in a specific, Facebook-related context. Results strongly support the overall model. PA and gender are important explanators for CFIP, which in turn explains privacy-protecting behaviours. We also find that perceived risk affects trust, which in turn affects behaviours in the studied context. The results yield several recommendations for future research as well as some implications for management. Â© Chartered Institute of Library and Information Professionals.","This study extends privacy concerns research by providing a test of a model inspired by the 'Antecedents - Privacy Concerns - Outcomes' (APCO) framework. Focusing at the individual level of analysis, the study examines the influences of privacy awareness (PA) and demographic variables (age, gender) on concern for information privacy (CFIP). It also considers CFIP's relationship to privacy-protecting behaviours and incorporates trust and risk into the model. These relationships are tested in a specific, Facebook-related context. Results strongly support the overall model. PA and gender are important explanators for CFIP, which in turn explains privacy-protecting behaviours. We also find that perceived risk affects trust, which in turn affects behaviours in the studied context. The results yield several recommendations for future research as well as some implications for management."
Absorptive capacity: A process and structure approach,"The aim of this study is to understand and explain different dimensions of absorptive capacity and the strategies used in practice to realise it. The theoretical and conceptual contribution of the study lies in the adoption of a dual process and structure approach, aiming at identifying, respectively, how learning takes place and what learning takes place. The study is based upon a case study research design to explore the manifestation of absorptive capacity processes and its relationship to knowledge structures in four company case studies. The findings of the research indicate that knowledge acquisition, transformation and integration involve successive iterations of codification, abstraction and diffusion of knowledge in relation to product concepts, process requirements and problem-solving approaches. This relates to the capability to adopt, simultaneously, different trajectories in terms of knowledge exploration and knowledge exploitation strategies. The combined adoption of these different strategies enables both strategic and operational flexibility and underlies the successful realisation of absorptive capacity. Â© The Author(s) 2018.","The aim of this study is to understand and explain different dimensions of absorptive capacity and the strategies used in practice to realise it. The theoretical and conceptual contribution of the study lies in the adoption of a dual process and structure approach, aiming at identifying, respectively, how learning takes place and what learning takes place. The study is based upon a case study research design to explore the manifestation of absorptive capacity processes and its relationship to knowledge structures in four company case studies. The findings of the research indicate that knowledge acquisition, transformation and integration involve successive iterations of codification, abstraction and diffusion of knowledge in relation to product concepts, process requirements and problem-solving approaches. This relates to the capability to adopt, simultaneously, different trajectories in terms of knowledge exploration and knowledge exploitation strategies. The combined adoption of these different strategies enables both strategic and operational flexibility and underlies the successful realisation of absorptive capacity."
Predicting event mentions based on a semantic analysis of microblogs for inter-region relationships,"An ability to predict peopleâs interests in different regions would be valuable to many applications including marketing and policymaking. We posit that social media plays an important role in capturing collective user interests in different regions and their dynamics over time and across regions. Event mentions in microblogs of social media like Twitter not only reflect the peopleâs interests in different regions but also affect the posting of future messages as the content of microblogs propagates to others through an online social network. Differentiating from the various network analysis techniques that have been developed to capture peopleâs interests and their propagation patterns, we propose an event mention prediction method that utilises an analysis of inter-region relationships. We first obtain regional user interests for each topic by applying Latent Dirichlet Allocation (LDA) to region-specific collections of tweets and then compute pairwise similarities among regions. The resulting similarity-based region network becomes the basis for constructing region groups through Markov Cluster Algorithm, which helps removing noise relationships among regions. We then propose a relatively simple regression technique to predict future event mentions in different regions. We demonstrate that the proposed method outperforms the state-of-the-art event prediction method, confirming that the novel method of constructing groups from region-based sub-topic interests indeed contributes to the increase in the prediction accuracy. Â© The Author(s) 2018.","An ability to predict peoples interests in different regions would be valuable to many applications including marketing and policymaking. We posit that social media plays an important role in capturing collective user interests in different regions and their dynamics over time and across regions. Event mentions in microblogs of social media like Twitter not only reflect the peoples interests in different regions but also affect the posting of future messages as the content of microblogs propagates to others through an online social network. Differentiating from the various network analysis techniques that have been developed to capture peoples interests and their propagation patterns, we propose an event mention prediction method that utilises an analysis of inter-region relationships. We first obtain regional user interests for each topic by applying Latent Dirichlet Allocation (LDA) to region-specific collections of tweets and then compute pairwise similarities among regions. The resulting similarity-based region network becomes the basis for constructing region groups through Markov Cluster Algorithm, which helps removing noise relationships among regions. We then propose a relatively simple regression technique to predict future event mentions in different regions. We demonstrate that the proposed method outperforms the state-of-the-art event prediction method, confirming that the novel method of constructing groups from region-based sub-topic interests indeed contributes to the increase in the prediction accuracy."
Developing a hybrid collaborative filtering recommendation system with opinion mining on purchase review,"The most commonly used algorithm in recommendation systems is collaborative filtering. However, despite its wide use, the prediction accuracy of this algorithm is unexceptional. Furthermore, whether quantitative data such as product rating or purchase history reflect usersâ actual taste is questionable. In this article, we propose a method to utilise user review data extracted with opinion mining for product recommendation systems. To evaluate the proposed method, we perform product recommendation test on Amazon product data, with and without the additional opinion mining result on Amazon purchase review data. The performances of these two variants are compared by means of precision, recall, true positive recommendation (TPR) and false positive recommendation (FPR). In this comparison, a large improvement in prediction accuracy was observed when the opinion mining data were taken into account. Based on these results, we answer two main questions: âWhy is collaborative filtering algorithm not effective?â and âDo quantitative data such as product rating or purchase history reflect usersâ actual tastes?â. Â© The Author(s) 2017.","The most commonly used algorithm in recommendation systems is collaborative filtering. However, despite its wide use, the prediction accuracy of this algorithm is unexceptional. Furthermore, whether quantitative data such as product rating or purchase history reflect users actual taste is questionable. In this article, we propose a method to utilise user review data extracted with opinion mining for product recommendation systems. To evaluate the proposed method, we perform product recommendation test on Amazon product data, with and without the additional opinion mining result on Amazon purchase review data. The performances of these two variants are compared by means of precision, recall, true positive recommendation (TPR) and false positive recommendation (FPR). In this comparison, a large improvement in prediction accuracy was observed when the opinion mining data were taken into account. Based on these results, we answer two main questions: Why is collaborative filtering algorithm not effective? and Do quantitative data such as product rating or purchase history reflect users actual tastes?."
Using four different online media sources to forecast the crude oil price,"This study looks for signals of economic awareness on online social media and tests their significance in economic predictions. The study analyses, over a period of 2 years, the relationship between the West Texas Intermediate daily crude oil price and multiple predictors extracted from Twitter; Google Trends;Wikipedia; and the Global Data on Events, Location and Tone (GDELT) database. Semantic analysis is applied to study the sentiment, emotionality and complexity of the language used. Autoregressive Integrated Moving Average with Explanatory Variable (ARIMAX) models are used to make predictions and to confirm the value of the study variables. Results show that the combined analysis of the four media platforms carries valuable information in making financial forecasting. Twitter language complexity, GDELT number of articles andWikipedia page reads have the highest predictive power. This study also allows a comparison of the different fore-sighting abilities of each platform, in terms of how many days ahead a platform can predict a price movement before it happens. In comparison with previous work, more media sources and more dimensions of the interaction and of the language used are combined in a joint analysis. Â© The Author(s) 2017.","This study looks for signals of economic awareness on online social media and tests their significance in economic predictions. The study analyses, over a period of 2 years, the relationship between the West Texas Intermediate daily crude oil price and multiple predictors extracted from Twitter; Google Trends;Wikipedia; and the Global Data on Events, Location and Tone (GDELT) database. Semantic analysis is applied to study the sentiment, emotionality and complexity of the language used. Autoregressive Integrated Moving Average with Explanatory Variable (ARIMAX) models are used to make predictions and to confirm the value of the study variables. Results show that the combined analysis of the four media platforms carries valuable information in making financial forecasting. Twitter language complexity, GDELT number of articles andWikipedia page reads have the highest predictive power. This study also allows a comparison of the different fore-sighting abilities of each platform, in terms of how many days ahead a platform can predict a price movement before it happens. In comparison with previous work, more media sources and more dimensions of the interaction and of the language used are combined in a joint analysis."
Mobile recommender systems: Identifying the major concepts,"This article identifies the factors that have an impact on mobile recommender systems. Recommender systems have become a technology that has been widely used by various online applications in situations where there is an information overload problem. Numerous applications such as e-Commerce, video platforms and social networks provide personalised recommendations to their users and this has improved the user experience and vendor revenues. The development of recommender systems has been focused mostly on the proposal of new algorithms that provide more accurate recommendations. However, the use of mobile devices and the rapid growth of the Internet and networking infrastructure have brought the necessity of using mobile recommender systems. The links between web and mobile recommender systems are described along with how the recommendations in mobile environments can be improved. This work is focused on identifying the links between web and mobile recommender systems and to provide solid future directions that aim to lead in a more integrated mobile recommendation domain. Â© The Author(s) 2018.","This article identifies the factors that have an impact on mobile recommender systems. Recommender systems have become a technology that has been widely used by various online applications in situations where there is an information overload problem. Numerous applications such as e-Commerce, video platforms and social networks provide personalised recommendations to their users and this has improved the user experience and vendor revenues. The development of recommender systems has been focused mostly on the proposal of new algorithms that provide more accurate recommendations. However, the use of mobile devices and the rapid growth of the Internet and networking infrastructure have brought the necessity of using mobile recommender systems. The links between web and mobile recommender systems are described along with how the recommendations in mobile environments can be improved. This work is focused on identifying the links between web and mobile recommender systems and to provide solid future directions that aim to lead in a more integrated mobile recommendation domain."
Collaborative filtering using non-negative matrix factorisation,"Collaborative filtering is a popular strategy in recommender systems area. This approach gathers users' ratings and then predicts what users will rate based on their similarity to other users. However, most of the collaborative filtering methods have faced problems such as sparseness and scalability. This paper presents a non-negative matrix factorisation method to alleviate these problems via decomposing rating matrix into user matrix and item matrix. This method tries to find two non-negative user matrix and item matrix whose product can well estimate the rating matrix. This approach proposes updated rules to learn the latent factors for factorising the rating matrix. The proposed method can estimate all the unknown ratings and its computational complexity is very low. Empirical studies on benchmark datasets show that the proposed method is more tolerant of the sparseness and scalability problems. Â© The Author(s) 2016.","Collaborative filtering is a popular strategy in recommender systems area. This approach gathers users' ratings and then predicts what users will rate based on their similarity to other users. However, most of the collaborative filtering methods have faced problems such as sparseness and scalability. This paper presents a non-negative matrix factorisation method to alleviate these problems via decomposing rating matrix into user matrix and item matrix. This method tries to find two non-negative user matrix and item matrix whose product can well estimate the rating matrix. This approach proposes updated rules to learn the latent factors for factorising the rating matrix. The proposed method can estimate all the unknown ratings and its computational complexity is very low. Empirical studies on benchmark datasets show that the proposed method is more tolerant of the sparseness and scalability problems."
Semantic tag recommendation based on associated words exploiting the interwiki links of Wikipedia,"The volumes of multimedia content and users have increased on social multimedia sites due to the prevalence of smart mobile devices and digital cameras. It is common for users to take pictures and upload them to image-sharing websites using their smartphones. However, the tag characteristics deteriorate the quality of tag-based image retrieval and decrease the reliability of social multimedia sites. In this article, we propose a semantic tag recommendation technique exploiting associated words that are semantically similar or related to each other using the interwiki links of Wikipedia. First, we generate a word relationship graph after extracting meaningful words from each article in Wikipedia. The candidate words are then rearranged according to importance by applying a link-based ranking algorithm and then the top-k words are defined as the associated words for the article. When a user uploads an image, we collect visually similar images from a social image database. After propagating the proper tags from the collected images, we recommend associated words related to the candidate tags. Our experimental results show that the proposed method can improve the accuracy by up to 14% compared with other works and that exploiting associated words makes it possible to perform semantic tag recommendation. Â© The Author(s) 2017.","The volumes of multimedia content and users have increased on social multimedia sites due to the prevalence of smart mobile devices and digital cameras. It is common for users to take pictures and upload them to image-sharing websites using their smartphones. However, the tag characteristics deteriorate the quality of tag-based image retrieval and decrease the reliability of social multimedia sites. In this article, we propose a semantic tag recommendation technique exploiting associated words that are semantically similar or related to each other using the interwiki links of Wikipedia. First, we generate a word relationship graph after extracting meaningful words from each article in Wikipedia. The candidate words are then rearranged according to importance by applying a link-based ranking algorithm and then the top-k words are defined as the associated words for the article. When a user uploads an image, we collect visually similar images from a social image database. After propagating the proper tags from the collected images, we recommend associated words related to the candidate tags. Our experimental results show that the proposed method can improve the accuracy by up to 14% compared with other works and that exploiting associated words makes it possible to perform semantic tag recommendation."
Feature-based opinion mining in financial news: An ontology-driven approach,"Financial news plays a significant role with regard to predicting the behaviour of financial markets. However, the exponential growth of financial news on the Web has led to a need for new technologies that automatically collect and categorise large volumes of information in a fast and easy manner. Sentiment analysis, or opinion mining, is the field of study that analyses people's opinions, moods and evaluations using written text on Web platforms. In recent research, a substantial effort has been made to develop sophisticated methods with which to classify sentiments in the financial domain. However, there is a lack of approaches that analyse the positive or negative orientation of each aspect contained in a document. In this respect, we propose a new sentiment analysis method for feature and news polarity classification. The method presented is based on an ontology-driven approach that makes it possible to semantically describe relations between concepts in the financial news domain. The polarity of the features in each document is also calculated by taking into account the words from around the linguistic expression of the feature. These words are obtained by using the 'N-GRAM After', 'N-GRAM Before', 'N-GRAM Around' and 'All-Phrase' methods. The effectiveness of our method has been proved by carrying out a set of experiments on a corpus of 1000 financial news items. Our proposal obtained encouraging results with an accuracy of 66.7% and an F-measure of 64.9% for feature polarity classification and an accuracy of 89.8% and an F-measure of 89.7% for news polarity classification. The experimental results additionally show that the N-GRAM Around method provides the best average results. Â© The Author(s) 2016.","Financial news plays a significant role with regard to predicting the behaviour of financial markets. However, the exponential growth of financial news on the Web has led to a need for new technologies that automatically collect and categorise large volumes of information in a fast and easy manner. Sentiment analysis, or opinion mining, is the field of study that analyses people's opinions, moods and evaluations using written text on Web platforms. In recent research, a substantial effort has been made to develop sophisticated methods with which to classify sentiments in the financial domain. However, there is a lack of approaches that analyse the positive or negative orientation of each aspect contained in a document. In this respect, we propose a new sentiment analysis method for feature and news polarity classification. The method presented is based on an ontology-driven approach that makes it possible to semantically describe relations between concepts in the financial news domain. The polarity of the features in each document is also calculated by taking into account the words from around the linguistic expression of the feature. These words are obtained by using the 'N-GRAM After', 'N-GRAM Before', 'N-GRAM Around' and 'All-Phrase' methods. The effectiveness of our method has been proved by carrying out a set of experiments on a corpus of 1000 financial news items. Our proposal obtained encouraging results with an accuracy of 66.7% and an F-measure of 64.9% for feature polarity classification and an accuracy of 89.8% and an F-measure of 89.7% for news polarity classification. The experimental results additionally show that the N-GRAM Around method provides the best average results."
A survey on real-time event detection from the Twitter data stream,"The proliferation of social networking services has resulted in a rapid growth of their user base, spanning across the world. The collective information generated from these online platforms is overwhelming, in terms of both the amount of content produced every moment and the diversity of topics discussed. The real-time nature of the information produced by users has prompted researchers to analyse this content, in order to gain timely insight into the current state of affairs. Specifically, the microblogging service Twitter has been a recent focus of researchers to gather information on events occurring in real time. This article presents a survey of a wide variety of event detection methods applied to streaming Twitter data, classifying them according to shared common traits, and then discusses different aspects of the subtasks and challenges involved in event detection. We believe this survey will act as a guide and starting point for aspiring researchers to gain a structured view on state-of-the-art real-time event detection and spur further research in this direction. Â© The Author(s) 2017.","The proliferation of social networking services has resulted in a rapid growth of their user base, spanning across the world. The collective information generated from these online platforms is overwhelming, in terms of both the amount of content produced every moment and the diversity of topics discussed. The real-time nature of the information produced by users has prompted researchers to analyse this content, in order to gain timely insight into the current state of affairs. Specifically, the microblogging service Twitter has been a recent focus of researchers to gather information on events occurring in real time. This article presents a survey of a wide variety of event detection methods applied to streaming Twitter data, classifying them according to shared common traits, and then discusses different aspects of the subtasks and challenges involved in event detection. We believe this survey will act as a guide and starting point for aspiring researchers to gain a structured view on state-of-the-art real-time event detection and spur further research in this direction."
Mutual information and sensitivity analysis for feature selection in customer targeting: A comparative study,"Feature selection is a highly relevant task in any data-driven knowledge discovery project. The present research focuses on analysing the advantages and disadvantages of using mutual information (MI) and data-based sensitivity analysis (DSA) for feature selection in classification problems, by applying both to a bank telemarketing case. A logistic regression model is built on the tuned set of features identified by each of the two techniques as the most influencing set of features on the success of a telemarketing contact, in a total of 13 features for MI and 9 for DSA. The latter performs better for lower values of false positives while the former is slightly better for a higher false-positive ratio. Thus, MI becomes a better choice if the intention is reducing slightly the cost of contacts without risking losing a high number of successes. However, DSA achieved good prediction results with less features. Â© The Author(s) 2018.","Feature selection is a highly relevant task in any data-driven knowledge discovery project. The present research focuses on analysing the advantages and disadvantages of using mutual information and data-based sensitivity analysis (DSA) for feature selection in classification problems, by applying both to a bank telemarketing case. A logistic regression model is built on the tuned set of features identified by each of the two techniques as the most influencing set of features on the success of a telemarketing contact, in a total of 13 features for MI and 9 for DSA. The latter performs better for lower values of false positives while the former is slightly better for a higher false-positive ratio. Thus, MI becomes a better choice if the intention is reducing slightly the cost of contacts without risking losing a high number of successes. However, DSA achieved good prediction results with less features."
Detecting new Chinese words from massive domain texts with word embedding,"Textual information retrieval (TIR) is based on the relationship between word units. Traditional word segmentation techniques attempt to discern the word units accurately from texts; however, they are unable to appropriately and efficiently identify all new words. Identification of new words, especially in languages such as Chinese, remains a challenge. In recent years, word embedding methods have used numerical word vectors to retain the semantic and correlated information between words in a corpus. In this article, we propose the word-embedding-based method (WEBM), a novel method that combines word embedding and frequent n-gram string mining for discovering new words from domain corpora. First, we mapped all word units in a domain corpus to a high-dimension word vector space. Second, we used a frequent n-gram word string mining method to identify a set of candidates for new words. We designed a pruning strategy based on the word vectors to quantify the possibility of a word string being a new word, thereby allowing the evaluation of candidates based on the similarity of word units in the same string. In a comparative study, our experimental results revealed that WEBM had a great advantage in detecting new words from massive Chinese corpora. Â© The Author(s) 2018.","Textual information retrieval (TIR) is based on the relationship between word units. Traditional word segmentation techniques attempt to discern the word units accurately from texts; however, they are unable to appropriately and efficiently identify all new words. Identification of new words, especially in languages such as Chinese, remains a challenge. In recent years, word embedding methods have used numerical word vectors to retain the semantic and correlated information between words in a corpus. In this article, we propose the word-embedding-based method (WEBM), a novel method that combines word embedding and frequent n-gram string mining for discovering new words from domain corpora. First, we mapped all word units in a domain corpus to a high-dimension word vector space. Second, we used a frequent n-gram word string mining method to identify a set of candidates for new words. We designed a pruning strategy based on the word vectors to quantify the possibility of a word string being a new word, thereby allowing the evaluation of candidates based on the similarity of word units in the same string. In a comparative study, our experimental results revealed that WEBM had a great advantage in detecting new words from massive Chinese corpora."
Arabic senti-lexicon: Constructing publicly available language resources for Arabic sentiment analysis,"Sentiment analysis is held to be one of the highly dynamic recent research fields in Natural Language Processing, facilitated by the quickly growing volume of Web opinion data. Most of the approaches in this field are focused on English due to the lack of sentiment resources in other languages such as the Arabic language and its large variety of dialects. In most sentiment analysis applications, good sentiment resources play a critical role. Based on that, in this article, several publicly available sentiment analysis resources for Arabic are introduced. This article introduces the Arabic senti-lexicon, a list of 3880 positive and negative synsets annotated with their part of speech, polarity scores, dialects synsets and inflected forms. This article also presents a Multi-domain Arabic Sentiment Corpus (MASC) with a size of 8860 positive and negative reviews from different domains. In this article, an in-depth study has been conducted on five types of feature sets for exploiting effective features and investigating their effect on performance of Arabic sentiment analysis. The aim is to assess the quality of the developed language resources and to integrate different feature sets and classification algorithms to synthesise a more accurate sentiment analysis method. The Arabic senti-lexicon is used for generating feature vectors. Five well-known machine learning algorithms: naÃ¯ve Bayes, k-nearest neighbours, support vector machines (SVMs), logistic linear regression and neural network are employed as base-classifiers for each of the feature sets. A wide range of comparative experiments on standard Arabic data sets were conducted, discussion is presented and conclusions are drawn. The experimental results show that the Arabic senti-lexicon is a very useful resource for Arabic sentiment analysis. Moreover, results show that classifiers which are trained on feature vectors derived from the corpus using the Arabic sentiment lexicon are more accurate than classifiers trained using the raw corpus. Â© The Author(s) 2017.","Sentiment analysis is held to be one of the highly dynamic recent research fields in Natural Language Processing, facilitated by the quickly growing volume of Web opinion data. Most of the approaches in this field are focused on English due to the lack of sentiment resources in other languages such as the Arabic language and its large variety of dialects. In most sentiment analysis applications, good sentiment resources play a critical role. Based on that, in this article, several publicly available sentiment analysis resources for Arabic are introduced. This article introduces the Arabic senti-lexicon, a list of 3880 positive and negative synsets annotated with their part of speech, polarity scores, dialects synsets and inflected forms. This article also presents a Multi-domain Arabic Sentiment Corpus (MASC) with a size of 8860 positive and negative reviews from different domains. In this article, an in-depth study has been conducted on five types of feature sets for exploiting effective features and investigating their effect on performance of Arabic sentiment analysis. The aim is to assess the quality of the developed language resources and to integrate different feature sets and classification algorithms to synthesise a more accurate sentiment analysis method. The Arabic senti-lexicon is used for generating feature vectors. Five well-known machine learning algorithms: nave Bayes, k-nearest neighbours, support vector machines (SVMs), logistic linear regression and neural network are employed as base-classifiers for each of the feature sets. A wide range of comparative experiments on standard Arabic data sets were conducted, discussion is presented and conclusions are drawn. The experimental results show that the Arabic senti-lexicon is a very useful resource for Arabic sentiment analysis. Moreover, results show that classifiers which are trained on feature vectors derived from the corpus using the Arabic sentiment lexicon are more accurate than classifiers trained using the raw corpus."
An ontology-based approach for supply-chain quality control: From a principalâagent perspective,"The efficacy of the principalâagent contract in supply-chain quality control depends not only on contract parameters but also such noncontract parameters as cost of a high-quality effort and the diagnostic error of the inspection policy. The noncontract parameters usually fluctuate and are unobservable during contract execution, which may hinder suppliersâ high-quality effort, or, in other words, result in a lower efficacy for the contract. This article proposes an ontology-based approach to facilitating a principalâagent contract by monitoring the contractâs loss of efficacy. The approach consists of ontology-based models and data-centric algorithms. The ontology-based models not only formally represent concepts and relations between concepts involved in predicting whether a contract is efficient, but also organise multichannel data such as news, marketplace reports and industry databases containing information of factors impacting the unobservable noncontract parametersâ fluctuations. Based on the ontology-based models and multichannel data, the data-centric algorithms are developed to predict whether a contract will lose efficacy. We evaluate our approach through case study, simulation and comparison against related approaches to supply-chain quality control. The case study proves that our approach is appropriate. In the simulation evaluation, a combination of our approach and principalâagent contract is more efficient than just a principalâagent contract. The comparison results against related approaches show that our approach is a novel, inexpensive and directly applicable tool for reducing both asymmetric information and moral hazard in supply-chain quality control. Â© The Author(s) 2018.","The efficacy of the principalagent contract in supply-chain quality control depends not only on contract parameters but also such noncontract parameters as cost of a high-quality effort and the diagnostic error of the inspection policy. The noncontract parameters usually fluctuate and are unobservable during contract execution, which may hinder suppliers high-quality effort, or, in other words, result in a lower efficacy for the contract. This article proposes an ontology-based approach to facilitating a principalagent contract by monitoring the contracts loss of efficacy. The approach consists of ontology-based models and data-centric algorithms. The ontology-based models not only formally represent concepts and relations between concepts involved in predicting whether a contract is efficient, but also organise multichannel data such as news, marketplace reports and industry databases containing information of factors impacting the unobservable noncontract parameters fluctuations. Based on the ontology-based models and multichannel data, the data-centric algorithms are developed to predict whether a contract will lose efficacy. We evaluate our approach through case study, simulation and comparison against related approaches to supply-chain quality control. The case study proves that our approach is appropriate. In the simulation evaluation, a combination of our approach and principalagent contract is more efficient than just a principalagent contract. The comparison results against related approaches show that our approach is a novel, inexpensive and directly applicable tool for reducing both asymmetric information and moral hazard in supply-chain quality control."
Information security: Listening to the perspective of organisational insiders,"Aligned with the strategy-as-practice research tradition, this article investigates how organisational insiders understand and perceive their surrounding information security practices, how they interpret them, and how they turn such interpretations into strategic actions. The study takes a qualitative case study approach, and participants are employees at the Research & Development department of a multinational original brand manufacturer. The article makes an important contribution to organisational information security management. It addresses the behaviour of organisational insiders â a group whose role in the prevention, response and mitigation of information security incidents is critical. The article identifies a set of organisational insidersâ perceived components of effective information security practices (organisational mission statement; common understanding of information security; awareness of threats; knowledge of information security incidents, routines and policy; relationships between employees; circulation of stories; role of punishment provisions; and training), based on which more successful information security strategies can be developed. Â© The Author(s) 2018.","Aligned with the strategy-as-practice research tradition, this article investigates how organisational insiders understand and perceive their surrounding information security practices, how they interpret them, and how they turn such interpretations into strategic actions. The study takes a qualitative case study approach, and participants are employees at the Research & Development department of a multinational original brand manufacturer. The article makes an important contribution to organisational information security management. It addresses the behaviour of organisational insiders a group whose role in the prevention, response and mitigation of information security incidents is critical. The article identifies a set of organisational insiders perceived components of effective information security practices (organisational mission statement; common understanding of information security; awareness of threats; knowledge of information security incidents, routines and policy; relationships between employees; circulation of stories; role of punishment provisions; and training), based on which more successful information security strategies can be developed."
Knowing and learning in everyday spaces (KALiEds): Mapping the information landscape of refugee youth learning in everyday spaces,"Refugee youth are faced with complex information needs that require them to identify and map the everyday spaces that can contribute to their learning outside the formal schooling system. The use of everyday spaces by refugee youth aged 16-25 was investigated using photovoice and interview data collection methods. The findings of the study suggest that the information needs and information literacy practices of this cohort arise from the desire to connect with a new community, to learn new social rules and to become established, while at the same time supporting the information needs of other family members and dealing with the social challenges that arise from cultural expectations. These challenges require them to connect with a wide range of everyday spaces to support their learning needs. Â© The Author(s) 2016.","Refugee youth are faced with complex information needs that require them to identify and map the everyday spaces that can contribute to their learning outside the formal schooling system. The use of everyday spaces by refugee youth aged 16-25 was investigated using photovoice and interview data collection methods. The findings of the study suggest that the information needs and information literacy practices of this cohort arise from the desire to connect with a new community, to learn new social rules and to become established, while at the same time supporting the information needs of other family members and dealing with the social challenges that arise from cultural expectations. These challenges require them to connect with a wide range of everyday spaces to support their learning needs."
Discovering aspects of online consumer reviews,"In this paper we propose a fully unsupervised approach for product aspect discovery in on-line consumer reviews. We apply a two-step hierarchical clustering process in which we first cluster words representing aspects based on the semantic similarity of their contexts and then on the similarity of the hypernyms of the cluster members. Our approach also includes a method for assigning class labels to each of the clusters. We evaluated our methods on large datasets of restaurant and camera reviews and found that the two-step clustering process performed better than a single-step clustering process at identifying aspects and words refering to aspects. Finally, we compare our method to a state-of-the-art topic modelling approach by Titov and McDonald, and demonstrate better results on both datasets. Â© The Author(s) 2015.","In this paper we propose a fully unsupervised approach for product aspect discovery in on-line consumer reviews. We apply a two-step hierarchical clustering process in which we first cluster words representing aspects based on the semantic similarity of their contexts and then on the similarity of the hypernyms of the cluster members. Our approach also includes a method for assigning class labels to each of the clusters. We evaluated our methods on large datasets of restaurant and camera reviews and found that the two-step clustering process performed better than a single-step clustering process at identifying aspects and words refering to aspects. Finally, we compare our method to a state-of-the-art topic modelling approach by Titov and McDonald, and demonstrate better results on both datasets."
A flexible aggregation framework on large-scale heterogeneous information networks,"OLAP (On-line Analytical Processing) can provide users with aggregate results from different perspectives and granularities. With the advent of heterogeneous information networks that consist of multi-type, interconnected nodes, such as bibliographic networks and knowledge graphs, it is important to study flexible aggregation in such networks. The aggregation results by existing work are limited to one type of node, which cannot be applied to aggregation on multi-type nodes, and relations in large-scale heterogeneous information networks. In this paper, we investigate the flexible aggregation problem on large-scale heterogeneous information networks, which is defined on multi-type nodes and relations. Moreover, by considering both attributes and structures, we propose a novel function based on graph entropy to measure the similarities of nodes. Further, we prove that the aggregation problem based on the function is NP-hard. Therefore, we develop an efficient heuristic algorithm for aggregation in two phases: informational aggregation and structural aggregation. The algorithm has linear time and space complexity. Extensive experiments on real-world datasets demonstrate the effectiveness and efficiency of the proposed algorithm. Â© Chartered Institute of Library and Information Professionals.","OLAP (On-line Analytical Processing) can provide users with aggregate results from different perspectives and granularities. With the advent of heterogeneous information networks that consist of multi-type, interconnected nodes, such as bibliographic networks and knowledge graphs, it is important to study flexible aggregation in such networks. The aggregation results by existing work are limited to one type of node, which cannot be applied to aggregation on multi-type nodes, and relations in large-scale heterogeneous information networks. In this paper, we investigate the flexible aggregation problem on large-scale heterogeneous information networks, which is defined on multi-type nodes and relations. Moreover, by considering both attributes and structures, we propose a novel function based on graph entropy to measure the similarities of nodes. Further, we prove that the aggregation problem based on the function is NP-hard. Therefore, we develop an efficient heuristic algorithm for aggregation in two phases: informational aggregation and structural aggregation. The algorithm has linear time and space complexity. Extensive experiments on real-world datasets demonstrate the effectiveness and efficiency of the proposed algorithm."
(Re)presenting heritage: Laser scanning and 3D visualisations for cultural resilience and community engagement,"Cultural heritage is increasingly being viewed as an economic asset for geographic areas who aim to capitalise in the surge in interest in local history and heritage tourism from members of the public. Digital technologies have developed that facilitate new forms of engagement with heritage and allow local areas to showcase their history, potentially broadening interest to a wider audience, thus acting as a driver for cultural and economic resilience. The research presented in this paper explores this through interdisciplinary research utilising laser scanning and visualisation in combination with social research in Elgin. 3D data capture technologies were used to develop and test 3D data visualisations and protocols through which the urban built heritage can be digitally recorded. The main focus of this paper surrounds the application and perceptions of these technologies. Findings suggest that the primary driver for cultural heritage developments was economic (with an emphasis on tourism) but further benefits and key factors of community engagement, social learning and cultural resilience were also reported. Stakeholder engagement and partnership working, in particular, were identified as critical factors of success. The findings from the community engagement events demonstrate that laser scanning and visualisation provide a novel and engaging mechanism for co-producing heritage assets. There is a high level of public interest in such technologies and users who engaged with these models reported that they gained new perspectives (including spatial and temporal perspectives) on the built heritage of the area. Â© The Author(s) 2016.","Cultural heritage is increasingly being viewed as an economic asset for geographic areas who aim to capitalise in the surge in interest in local history and heritage tourism from members of the public. Digital technologies have developed that facilitate new forms of engagement with heritage and allow local areas to showcase their history, potentially broadening interest to a wider audience, thus acting as a driver for cultural and economic resilience. The research presented in this paper explores this through interdisciplinary research utilising laser scanning and visualisation in combination with social research in Elgin. 3D data capture technologies were used to develop and test 3D data visualisations and protocols through which the urban built heritage can be digitally recorded. The main focus of this paper surrounds the application and perceptions of these technologies. Findings suggest that the primary driver for cultural heritage developments was economic (with an emphasis on tourism) but further benefits and key factors of community engagement, social learning and cultural resilience were also reported. Stakeholder engagement and partnership working, in particular, were identified as critical factors of success. The findings from the community engagement events demonstrate that laser scanning and visualisation provide a novel and engaging mechanism for co-producing heritage assets. There is a high level of public interest in such technologies and users who engaged with these models reported that they gained new perspectives (including spatial and temporal perspectives) on the built heritage of the area."
Semantic community detection using label propagation algorithm,"The issue of detecting large communities in online social networks is the subject of a wide range of studies in order to explore the network sub-structure. Most of the existing studies are concerned with network topology with no emphasis on active communities among the large online social networks and social portals, which are not based on network topology like forums. Here, new semantic community detection is proposed by focusing on user attributes instead of network topology. In the proposed approach, a network of user activities is established and weighted through semantic data. Furthermore, consistent extended label propagation algorithm is presented. Doing so, semantic representations of active communities are refined and labelled with user-generated tags that are available in web.2. The results show that the proposed semantic algorithm is able to significantly improve the modularity compared with three previously proposed algorithms. Â© Chartered Institute of Library and Information Professionals.","The issue of detecting large communities in online social networks is the subject of a wide range of studies in order to explore the network sub-structure. Most of the existing studies are concerned with network topology with no emphasis on active communities among the large online social networks and social portals, which are not based on network topology like forums. Here, new semantic community detection is proposed by focusing on user attributes instead of network topology. In the proposed approach, a network of user activities is established and weighted through semantic data. Furthermore, consistent extended label propagation algorithm is presented. Doing so, semantic representations of active communities are refined and labelled with user-generated tags that are available in web.2. The results show that the proposed semantic algorithm is able to significantly improve the modularity compared with three previously proposed algorithms."
Everyday health information literacy among young men compared with adults with high risk for metabolic syndrome - A cross-sectional population-based study,"This cross-sectional population-based study aims at identifying differences in the aspects of everyday health information literacy among young healthy men and adults with an increased risk for metabolic syndrome. Data were collected with a self-assessment-based 10-item screening tool administered at the Finnish Defence Force's call-ups (n=2507, response rate 59%) and at health intervention study (n=571, response rate 98%). Adults with increased risk for metabolic syndrome seemed to value health information but had more difficulty in knowing who to believe in health issues and understanding the terminology used. The difficulties applied especially to respondents 35 years old or over. Men, and especially young men, had lower motivation than women to seek health information. Although the results are indicative, the everyday health information literacy screening tool seems to be useful in revealing areas that health communication should be focused on among different populations. Â© The Author(s) 2016.","This cross-sectional population-based study aims at identifying differences in the aspects of everyday health information literacy among young healthy men and adults with an increased risk for metabolic syndrome. Data were collected with a self-assessment-based 10-item screening tool administered at the Finnish Defence Force's call-ups (n=2507, response rate 59%) and at health intervention study (n=571, response rate 98%). Adults with increased risk for metabolic syndrome seemed to value health information but had more difficulty in knowing who to believe in health issues and understanding the terminology used. The difficulties applied especially to respondents 35 years old or over. Men, and especially young men, had lower motivation than women to seek health information. Although the results are indicative, the everyday health information literacy screening tool seems to be useful in revealing areas that health communication should be focused on among different populations."
Performance of computational cognitive models of web-navigation on real websites,"Computational cognitive models of web-navigation developed so far have largely been tested only on mock-up websites. In this paper, for the first time, we compare and contrast the performance of two models, CoLiDeS and CoLiDeS+, on two real websites from the domains of technology and health, under two conditions of task difficulty, simple and difficult. We found that CoLiDeS+ predicted more hyperlinks on the correct path and had a higher path completion ratio than CoLiDeS. CoLiDeS+ found the target page more often than CoLiDeS, took more steps to reach the target page and was more 'disoriented' than CoLiDeS for difficult tasks. Difficult tasks in general for both models had less task success and lower path completion ratio, predicted less hyperlinks on the correct path, visited pages with lower mean LSA and took more steps to complete compared with simple tasks. Overall, inclusion of context from previously visited pages and implementation of backtracking strategies (which are both part of CoLiDeS+) led to better modelling performance. Suggestions to further improve the performance of these computational cognitive models on real websites are discussed. Â© 2016 Chartered Institute of Library and Information Professionals.","Computational cognitive models of web-navigation developed so far have largely been tested only on mock-up websites. In this paper, for the first time, we compare and contrast the performance of two models, CoLiDeS and CoLiDeS+, on two real websites from the domains of technology and health, under two conditions of task difficulty, simple and difficult. We found that CoLiDeS+ predicted more hyperlinks on the correct path and had a higher path completion ratio than CoLiDeS. CoLiDeS+ found the target page more often than CoLiDeS, took more steps to reach the target page and was more 'disoriented' than CoLiDeS for difficult tasks. Difficult tasks in general for both models had less task success and lower path completion ratio, predicted less hyperlinks on the correct path, visited pages with lower mean LSA and took more steps to complete compared with simple tasks. Overall, inclusion of context from previously visited pages and implementation of backtracking strategies (which are both part of CoLiDeS+) led to better modelling performance. Suggestions to further improve the performance of these computational cognitive models on real websites are discussed."
"Confluence of social network, social question and answering community, and user reputation model for information seeking and experts generation","Social question and answering (Q&A) is one of the most effective approaches to knowledge acquisition using information seeking and collaboration. Most modern social Q&A systems use a static points-based user reputation model, which has the effect of diminishing the value of experts. In order to overcome this issue, we have developed a dynamic points-based user reputation model that takes user rating and social network analysis as input. The impact weight of each relation and user ratings are not static but are dependent on the current level of asker and answerer and on the difficulty level of the question. We propose a novel social Q&A platform that is the confluence of different features of social network, social Q&A, and the dynamic points-based user reputation model. The beta version of the system was evaluated by conducting a clinical study for 4 months in different academic environments. The results show that the proposed social Q&A outperforms the available static points-based social Q&A systems in representing the actual user reputation with an increased user satisfaction. Â© Chartered Institute of Library and Information Professionals.","Social question and answering (Q&A) is one of the most effective approaches to knowledge acquisition using information seeking and collaboration. Most modern social Q&A systems use a static points-based user reputation model, which has the effect of diminishing the value of experts. In order to overcome this issue, we have developed a dynamic points-based user reputation model that takes user rating and social network analysis as input. The impact weight of each relation and user ratings are not static but are dependent on the current level of asker and answerer and on the difficulty level of the question. We propose a novel social Q&A platform that is the confluence of different features of social network, social Q&A, and the dynamic points-based user reputation model. The beta version of the system was evaluated by conducting a clinical study for 4 months in different academic environments. The results show that the proposed social Q&A outperforms the available static points-based social Q&A systems in representing the actual user reputation with an increased user satisfaction."
An exploration of search session patterns in an image-based digital library,"Three months of server transaction logs containing complete clickstream data for an image collection digital library were analysed for usage patterns to better understand user searching and browsing behaviour in this environment. Eleven types of user actions were identified from the log content. The study is novel in its combined analytical techniques and use of clickstream data from an image-based digital library. Three analytical techniques were used to analyse the data: (a) network analysis to better understand the relationship between sequential actions; (b) sequential pattern mining to identify frequent action sequences; and (c) k-means cluster analysis to identify groups of session patterns. The analysis revealed strong ties between several pairs of actions, relatively short pattern sequences that frequently duplicate previous actions and largely uniform session behaviour with little individual item browsing within sessions, indicating users are primarily engaged in purposeful and directed searching. Developers of image-based digital libraries should consider design features that support rapid browsing. Â© The Author(s) 2015.","Three months of server transaction logs containing complete clickstream data for an image collection digital library were analysed for usage patterns to better understand user searching and browsing behaviour in this environment. Eleven types of user actions were identified from the log content. The study is novel in its combined analytical techniques and use of clickstream data from an image-based digital library. Three analytical techniques were used to analyse the data: (a) network analysis to better understand the relationship between sequential actions; (b) sequential pattern mining to identify frequent action sequences; and k-means cluster analysis to identify groups of session patterns. The analysis revealed strong ties between several pairs of actions, relatively short pattern sequences that frequently duplicate previous actions and largely uniform session behaviour with little individual item browsing within sessions, indicating users are primarily engaged in purposeful and directed searching. Developers of image-based digital libraries should consider design features that support rapid browsing."
Identification of multi-spreader users in social networks for viral marketing,"Identifying high spreading power nodes is an interesting problem in social networks. Finding super spreader nodes becomes an arduous task when the nodes appear in large numbers, and the number of existing links becomes enormous among them. One of the methods that is used for identifying the nodes is to rank them based on k-shell decomposition. Nevertheless, one of the disadvantages of this method is that it assigns the same rank to the nodes of a shell. Another disadvantage of this method is that only one indicator is fairly used to rank the nodes. k-Shell is an approach that is used for ranking separate spreaders, yet it does not have enough efficiency when a group of nodes with maximum spreading needs to be selected; therefore, this method, alone, does not have enough efficiency. Accordingly, in this study a hybrid method is presented to identify the super spreaders based on k-shell measure. Afterwards, a suitable method is presented to select a group of superior nodes in order to maximize the spread of influence. Experimental results on seven complex networks show that our proposed methods outperforms other well-known measures and represents comparatively more accurate performance in identifying the super spreader nodes. Â© Chartered Institute of Library and Information Professionals.","Identifying high spreading power nodes is an interesting problem in social networks. Finding super spreader nodes becomes an arduous task when the nodes appear in large numbers, and the number of existing links becomes enormous among them. One of the methods that is used for identifying the nodes is to rank them based on k-shell decomposition. Nevertheless, one of the disadvantages of this method is that it assigns the same rank to the nodes of a shell. Another disadvantage of this method is that only one indicator is fairly used to rank the nodes. k-Shell is an approach that is used for ranking separate spreaders, yet it does not have enough efficiency when a group of nodes with maximum spreading needs to be selected; therefore, this method, alone, does not have enough efficiency. Accordingly, in this study a hybrid method is presented to identify the super spreaders based on k-shell measure. Afterwards, a suitable method is presented to select a group of superior nodes in order to maximize the spread of influence. Experimental results on seven complex networks show that our proposed methods outperforms other well-known measures and represents comparatively more accurate performance in identifying the super spreader nodes."
Exploring collaborative work among graduate students through the C5 model of collaboration: A diary study,"Collaborative work among students, while an important topic of inquiry, needs further treatment as we still lack the knowledge regarding obstacles that students face, the strategies they apply, and the relations among personal and group aspects. This article presents a diary study of 54 master's students conducting group projects across four semesters. A total of 332 diary entries were analysed using the C5 model of collaboration that incorporates elements of communication, contribution, coordination, cooperation and collaboration. Quantitative and qualitative analyses show how these elements relate to one another for students working on collaborative projects. It was found that face-to-face communication related positively with satisfaction and group dynamics, whereas online chat correlated positively with feedback and closing the gap. Managing scope was perceived to be the most common challenge. The findings suggest the varying affordances and drawbacks of different methods of communication, collaborative work styles and the strategies of group members. Â© Chartered Institute of Library and Information Professionals.","Collaborative work among students, while an important topic of inquiry, needs further treatment as we still lack the knowledge regarding obstacles that students face, the strategies they apply, and the relations among personal and group aspects. This article presents a diary study of 54 master's students conducting group projects across four semesters. A total of 332 diary entries were analysed using the C5 model of collaboration that incorporates elements of communication, contribution, coordination, cooperation and collaboration. Quantitative and qualitative analyses show how these elements relate to one another for students working on collaborative projects. It was found that face-to-face communication related positively with satisfaction and group dynamics, whereas online chat correlated positively with feedback and closing the gap. Managing scope was perceived to be the most common challenge. The findings suggest the varying affordances and drawbacks of different methods of communication, collaborative work styles and the strategies of group members."
Erratum: A language-model-based approach for subjectivity detection (Journal of Information Science DOI: 10.1177/0165551516641818),"Owing to an error made by SAGE, the corresponding author's affiliation is incorrect as an institution was missed. The author should appear as: Azadeh Shakery School of Electrical and Computer Engineering, College of Engineering, University of Tehran, Iran School of Computer Science, Institute for Research in Fundamental Sciences (IPM), Iran SAGE would like to apologise to the authors for this error. Â© The Author(s) 2016.","Owing to an error made by SAGE, the corresponding author's affiliation is incorrect as an institution was missed. The author should appear as: Azadeh Shakery School of Electrical and Computer Engineering, College of Engineering, University of Tehran, Iran School of Computer Science, Institute for Research in Fundamental Sciences (IPM), Iran SAGE would like to apologise to the authors for this error."
Beyond REF 2014: The impact of impact assessment on the future of information research,"The importance of demonstrating value for money in terms of academic research beyond the walls of institutions grows stronger as demonstrated by the inclusion of impact assessment in the 2014 REF (Research Excellence Framework) exercise for UK higher education institutions (HEIs). To understand if such focus is influencing the library and information science (LIS) discipline, this paper reports a critical examination of impact case studies submitted to REF 2014 under the Communication, Cultural and Media Studies, Library and Information Management Unit of Assessment. Content analysis was conducted on 25 case studies submitted by 14 institutions, establishing the methodologies, impacts, beneficiaries, published outputs and corroborative evidence reported. The implications of impact assessment on future LIS researcher behaviour, in terms of research conceptualisation and design, were explored through nine qualitative telephone interviews. While individual researchers did not anticipate their behaviour to change due to the introduction of impact assessments, there are anticipated changes across the discipline including a greater focus on engaging with stakeholders and research beneficiaries at early stages of research design and an emphasis on mixed methodologies to maximise the power and consequences of research results. Â© The Author(s) 2016.","The importance of demonstrating value for money in terms of academic research beyond the walls of institutions grows stronger as demonstrated by the inclusion of impact assessment in the 2014 REF (Research Excellence Framework) exercise for UK higher education institutions (HEIs). To understand if such focus is influencing the library and information science (LIS) discipline, this paper reports a critical examination of impact case studies submitted to REF 2014 under the Communication, Cultural and Media Studies, Library and Information Management Unit of Assessment. Content analysis was conducted on 25 case studies submitted by 14 institutions, establishing the methodologies, impacts, beneficiaries, published outputs and corroborative evidence reported. The implications of impact assessment on future LIS researcher behaviour, in terms of research conceptualisation and design, were explored through nine qualitative telephone interviews. While individual researchers did not anticipate their behaviour to change due to the introduction of impact assessments, there are anticipated changes across the discipline including a greater focus on engaging with stakeholders and research beneficiaries at early stages of research design and an emphasis on mixed methodologies to maximise the power and consequences of research results."
"Norms of data sharing in biological sciences: The roles of metadata, data repository, and journal and funding requirements","Institutional environments, comprising regulative pressures by funding agencies and journal publishers, and institutional resources, including the availabilities of data repositories and standards for metadata, function as important determinants in scientists' data-sharing norms, attitudes and behaviours. This research investigates how these functions influence biological scientists' data-sharing norms and how the data-sharing norms influence their data-sharing behaviours mediated by attitudes towards data sharing. The research model was developed based on the integration of institutional theory and theory of planned behaviour. The proposed research model was validated based on a total of 608 responses from a national survey conducted in the USA. The Partial Least Squares (PLS) was employed to analyse the survey data. Results show how institutional pressures by funding agencies and journals and the availabilities of data repository and metadata standards all have significant influences on data-sharing norms, which have significant influences on data-sharing behaviours, as mediated by attitudes towards data sharing. Â© Chartered Institute of Library and Information Professionals.","Institutional environments, comprising regulative pressures by funding agencies and journal publishers, and institutional resources, including the availabilities of data repositories and standards for metadata, function as important determinants in scientists' data-sharing norms, attitudes and behaviours. This research investigates how these functions influence biological scientists' data-sharing norms and how the data-sharing norms influence their data-sharing behaviours mediated by attitudes towards data sharing. The research model was developed based on the integration of institutional theory and theory of planned behaviour. The proposed research model was validated based on a total of 608 responses from a national survey conducted in the USA. The Partial Least Squares (PLS) was employed to analyse the survey data. Results show how institutional pressures by funding agencies and journals and the availabilities of data repository and metadata standards all have significant influences on data-sharing norms, which have significant influences on data-sharing behaviours, as mediated by attitudes towards data sharing."
Exploring performance of clustering methods on document sentiment analysis,"Clustering is a powerful unsupervised tool for sentiment analysis from text. However, the clustering results may be affected by any step of the clustering process, such as data pre-processing strategy, term weighting method in Vector Space Model and clustering algorithm. This paper presents the results of an experimental study of some common clustering techniques with respect to the task of sentiment analysis. Different from previous studies, in particular, we investigate the combination effects of these factors with a series of comprehensive experimental studies. The experimental results indicate that, first, the K-means-type clustering algorithms show clear advantages on balanced review datasets, while performing rather poorly on unbalanced datasets by considering clustering accuracy. Second, the comparatively newly designed weighting models are better than the traditional weighting models for sentiment clustering on both balanced and unbalanced datasets. Furthermore, adjective and adverb words extraction strategy can offer obvious improvements on clustering performance, while strategies of adopting stemming and stopword removal will bring negative influences on sentiment clustering. The experimental results would be valuable for both the study and usage of clustering methods in online review sentiment analysis. Â© The Author(s) 2015.","Clustering is a powerful unsupervised tool for sentiment analysis from text. However, the clustering results may be affected by any step of the clustering process, such as data pre-processing strategy, term weighting method in Vector Space Model and clustering algorithm. This paper presents the results of an experimental study of some common clustering techniques with respect to the task of sentiment analysis. Different from previous studies, in particular, we investigate the combination effects of these factors with a series of comprehensive experimental studies. The experimental results indicate that, first, the K-means-type clustering algorithms show clear advantages on balanced review datasets, while performing rather poorly on unbalanced datasets by considering clustering accuracy. Second, the comparatively newly designed weighting models are better than the traditional weighting models for sentiment clustering on both balanced and unbalanced datasets. Furthermore, adjective and adverb words extraction strategy can offer obvious improvements on clustering performance, while strategies of adopting stemming and stopword removal will bring negative influences on sentiment clustering. The experimental results would be valuable for both the study and usage of clustering methods in online review sentiment analysis."
Incorporating social media comments in affective video retrieval,"Affective video retrieval systems aim at finding video contents matching the desires and needs of users. Existing systems typically use the information contained in the video itself to specify its affect category. These systems either extract low-level features or build up higher-level attributes to train classification algorithms. However, using low-level features ignores global relations in data and constructing high-level features is time consuming and problem dependent. To overcome these drawbacks, an external source of information may be helpful. With the explosive growth and availability of social media, users' comments could be such a valuable source of information. In this study, a new method for incorporating social media comments with the audio-visual contents of videos is proposed. Furthermore, for the combination stage a decision-level fusion method based on the Dempster-Shafer theory of evidence is presented. Experiments are carried out on the video clips of the DEAP (Database for Emotion Analysis using Physiological signals) dataset and their associated users' comments on YouTube. Results show that the proposed system significantly outperforms the baseline method of using only the audio-visual contents for affective video retrieval. Â© The Author(s) 2015.","Affective video retrieval systems aim at finding video contents matching the desires and needs of users. Existing systems typically use the information contained in the video itself to specify its affect category. These systems either extract low-level features or build up higher-level attributes to train classification algorithms. However, using low-level features ignores global relations in data and constructing high-level features is time consuming and problem dependent. To overcome these drawbacks, an external source of information may be helpful. With the explosive growth and availability of social media, users' comments could be such a valuable source of information. In this study, a new method for incorporating social media comments with the audio-visual contents of videos is proposed. Furthermore, for the combination stage a decision-level fusion method based on the Dempster-Shafer theory of evidence is presented. Experiments are carried out on the video clips of the DEAP (Database for Emotion Analysis using Physiological signals) dataset and their associated users' comments on YouTube. Results show that the proposed system significantly outperforms the baseline method of using only the audio-visual contents for affective video retrieval."
Summary generation approaches based on semantic analysis for news documents,"With the exponential growth of the internet, a lot of online news reports are produced on the web every day. The news stream flows so rapidly that no one has the time to look at each and every item of information. In this situation, a person would naturally prefer to read updated information at certain time intervals. Document updating technique is very helpful for individuals to acquire new information or knowledge by eliminating out-of-date or redundant information. Existing summarization systems involve identifying the most relevant sentences from the text and putting them together to create a concise initial summary. In the process of identifying the important sentences, features influencing the relevance of sentences are determined. Based on these features the salience of the sentence is calculated and an initial summary is generated from highly important sentences at different compression rates. These types of initial summaries work on a batch of documents and do not consider the documents that may arrive at later time, so that corresponding summaries need to get updated. The update summarization system addresses this issue by taking into account the documents read by the user in the past and seeks to present only fresh or different information. The first step is to create an initial summary based on basic and additional features. The next step is to create an update summary based on the basic, additional and update features. In this paper, two approaches are proposed for generating initial and update summary from multiple documents about given news. The first approach performs semantic analysis by modifying the vector space model with dependency parse relations and applying latent semantic analysis on it to create a summary. The second approach applies sentence annotation based on aspects, prepositions and named entities to generate summary. Experimental results show that the proposed approaches generate better initial and update summaries compared with the existing systems. Â© The Author(s) 2015.","With the exponential growth of the internet, a lot of online news reports are produced on the web every day. The news stream flows so rapidly that no one has the time to look at each and every item of information. In this situation, a person would naturally prefer to read updated information at certain time intervals. Document updating technique is very helpful for individuals to acquire new information or knowledge by eliminating out-of-date or redundant information. Existing summarization systems involve identifying the most relevant sentences from the text and putting them together to create a concise initial summary. In the process of identifying the important sentences, features influencing the relevance of sentences are determined. Based on these features the salience of the sentence is calculated and an initial summary is generated from highly important sentences at different compression rates. These types of initial summaries work on a batch of documents and do not consider the documents that may arrive at later time, so that corresponding summaries need to get updated. The update summarization system addresses this issue by taking into account the documents read by the user in the past and seeks to present only fresh or different information. The first step is to create an initial summary based on basic and additional features. The next step is to create an update summary based on the basic, additional and update features. In this paper, two approaches are proposed for generating initial and update summary from multiple documents about given news. The first approach performs semantic analysis by modifying the vector space model with dependency parse relations and applying latent semantic analysis on it to create a summary. The second approach applies sentence annotation based on aspects, prepositions and named entities to generate summary. Experimental results show that the proposed approaches generate better initial and update summaries compared with the existing systems."
"A qualitative investigation of users' discovery, access, and organization of video games as information objects","Video games are popular consumer products as well as research subjects, yet little exists about how players and other stakeholders find video games and what information they need to select, acquire and play video games. With the aim of better understanding people's game-related information needs and behaviour, we conducted 56 semi-structured interviews with users who find, play, purchase, collect and recommend video games. Participants included gamers, parents, collectors, industry professionals, librarians, educators and scholars. From this user data, we derive and discuss key design implications for video game information systems: designing for target user populations, enabling recommendations based on appeals, offering multiple automatic organization options and providing relationship-based, user-generated, subject and visual metadata. We anticipate this work will contribute to building future video game information systems with new and improved access to games. Â© The Author(s) 2015.","Video games are popular consumer products as well as research subjects, yet little exists about how players and other stakeholders find video games and what information they need to select, acquire and play video games. With the aim of better understanding people's game-related information needs and behaviour, we conducted 56 semi-structured interviews with users who find, play, purchase, collect and recommend video games. Participants included gamers, parents, collectors, industry professionals, librarians, educators and scholars. From this user data, we derive and discuss key design implications for video game information systems: designing for target user populations, enabling recommendations based on appeals, offering multiple automatic organization options and providing relationship-based, user-generated, subject and visual metadata. We anticipate this work will contribute to building future video game information systems with new and improved access to games."
Linking and using social media data for enhancing public health analytics,"There is a large amount of health information available for any patient to address his/her health concerns. The freely available health datasets include community health data at the national, state, and community level, readily accessible and downloadable. These datasets can help to assess and improve healthcare performance, as well as help to modify health-related policies. There are also patient-generated datasets, accessible through social media, on the conditions, treatments, or side effects that individual patients experience. Clinicians and healthcare providers may benefit from being aware of national health trends and individual healthcare experiences that are relevant to their current patients. The available open health datasets vary from structured to highly unstructured. Due to this variability, an information seeker has to spend time visiting many, possibly irrelevant, Websites, and has to select information from each and integrate it into a coherent mental model. In this paper, we discuss an approach to integrating these openly available health data sources and presenting them to be easily understandable by physicians, healthcare staff, and patients. Through linked data principles and Semantic Web technologies we construct a generic model that integrates diverse open health data sources. The integration model is then used as the basis for developing a set of analytics as part of a system called 'Social InfoButtons', providing awareness of both community and patient health issues as well as healthcare trends that may shed light on a specific patient care situation. The prototype system provides patients, public health officials, and healthcare specialists with a unified view of health-related information from both official scientific sources and social networks, and provides the capability of exploring the current data along multiple dimensions, such as time and geographical location. Â© Chartered Institute of Library and Information Professionals.","There is a large amount of health information available for any patient to address his/her health concerns. The freely available health datasets include community health data at the national, state, and community level, readily accessible and downloadable. These datasets can help to assess and improve healthcare performance, as well as help to modify health-related policies. There are also patient-generated datasets, accessible through social media, on the conditions, treatments, or side effects that individual patients experience. Clinicians and healthcare providers may benefit from being aware of national health trends and individual healthcare experiences that are relevant to their current patients. The available open health datasets vary from structured to highly unstructured. Due to this variability, an information seeker has to spend time visiting many, possibly irrelevant, Websites, and has to select information from each and integrate it into a coherent mental model. In this paper, we discuss an approach to integrating these openly available health data sources and presenting them to be easily understandable by physicians, healthcare staff, and patients. Through linked data principles and Semantic Web technologies we construct a generic model that integrates diverse open health data sources. The integration model is then used as the basis for developing a set of analytics as part of a system called 'Social InfoButtons', providing awareness of both community and patient health issues as well as healthcare trends that may shed light on a specific patient care situation. The prototype system provides patients, public health officials, and healthcare specialists with a unified view of health-related information from both official scientific sources and social networks, and provides the capability of exploring the current data along multiple dimensions, such as time and geographical location."
Classification of news-related tweets,"It is important to obtain public opinion about a news article. Microblogs such as Twitter are popular and an important medium for people to share ideas. An important portion of tweets are related to news or events. Our aim is to find tweets about newspaper reports and measure the popularity of these reports on Twitter. However, it is a challenging task to match informal and very short tweets with formal news reports. In this study, we formulate this problem as a supervised classification task. We propose to form a training set using tweets containing a link to the news and the content of the same news article. We preprocess tweets by removing unnecessary words and symbols and apply stemming by means of morphological analysers. We apply binary classifiers and anomaly detection to this task. We also propose a textual similarity-based approach. We observed that preprocessing of tweets increases accuracy. The textual similarity method obtains results with the highest recognition rate. Success increases in some cases when report text is used with tweets containing a link to the news report within the training set of classification studies. We propose that this study, which is made directly in consideration of tweet texts that measure the trends of national newspaper reports on social media, has a higher significance when compared to Twitter analyses made by using a hashtag. Given the limited number of scientific studies on Turkish tweets, this study makes a contribution to the literature. Â© The Author(s) 2016.","It is important to obtain public opinion about a news article. Microblogs such as Twitter are popular and an important medium for people to share ideas. An important portion of tweets are related to news or events. Our aim is to find tweets about newspaper reports and measure the popularity of these reports on Twitter. However, it is a challenging task to match informal and very short tweets with formal news reports. In this study, we formulate this problem as a supervised classification task. We propose to form a training set using tweets containing a link to the news and the content of the same news article. We preprocess tweets by removing unnecessary words and symbols and apply stemming by means of morphological analysers. We apply binary classifiers and anomaly detection to this task. We also propose a textual similarity-based approach. We observed that preprocessing of tweets increases accuracy. The textual similarity method obtains results with the highest recognition rate. Success increases in some cases when report text is used with tweets containing a link to the news report within the training set of classification studies. We propose that this study, which is made directly in consideration of tweet texts that measure the trends of national newspaper reports on social media, has a higher significance when compared to Twitter analyses made by using a hashtag. Given the limited number of scientific studies on Turkish tweets, this study makes a contribution to the literature."
Query-specific signature selection for efficient k -nearest neighbour approximation,"Finding k-nearest neighbours (k-NN) is one of the most important primitives of many applications such as search engines and recommendation systems. However, its computational cost is extremely high when searching for k-NN points in a huge collection of high-dimensional points. Locality-sensitive hashing (LSH) has been introduced for an efficient k-NN approximation, but none of the existing LSH approaches clearly outperforms others. We propose a novel LSH approach, Signature Selection LSH (S2LSH), which finds approximate k-NN points very efficiently in various datasets. It first constructs a large pool of highly diversified signature regions with various sizes. Given a query point, it dynamically generates a query-specific signature region by merging highly effective signature regions selected from the signature pool. We also suggest S2LSH-M, a variant of S2LSH, which processes multiple queries more efficiently by using query-specific features and optimization techniques. Extensive experiments show the performance superiority of our approaches in diverse settings. Â© The Author(s) 2016.","Finding k-nearest neighbours (k-NN) is one of the most important primitives of many applications such as search engines and recommendation systems. However, its computational cost is extremely high when searching for k-NN points in a huge collection of high-dimensional points. Locality-sensitive hashing (LSH) has been introduced for an efficient k-NN approximation, but none of the existing LSH approaches clearly outperforms others. We propose a novel LSH approach, Signature Selection LSH (S2LSH), which finds approximate k-NN points very efficiently in various datasets. It first constructs a large pool of highly diversified signature regions with various sizes. Given a query point, it dynamically generates a query-specific signature region by merging highly effective signature regions selected from the signature pool. We also suggest S2LSH-M, a variant of S2LSH, which processes multiple queries more efficiently by using query-specific features and optimization techniques. Extensive experiments show the performance superiority of our approaches in diverse settings."
A community-based approach to identify the most influential nodes in social networks,"One of the important issues concerning the spreading process in social networks is the influence maximization. This is the problem of identifying the set of the most influential nodes in order to begin the spreading process based on an information diffusion model in the social networks. In this study, two new methods considering the community structure of the social networks and influence-based closeness centrality measure of the nodes are presented to maximize the spread of influence on the multiplication threshold, minimum threshold and linear threshold information diffusion models. The main objective of this study is to improve the efficiency with respect to the run time while maintaining the accuracy of the final influence spread. Efficiency improvement is obtained by reducing the number of candidate nodes subject to evaluation in order to find the most influential. Experiments consist of two parts: first, the effectiveness of the proposed influence-based closeness centrality measure is established by comparing it with available centrality measures; second, the evaluations are conducted to compare the two proposed community-based methods with well-known benchmarks in the literature on the real datasets, leading to the results demonstrate the efficiency and effectiveness of these methods in maximizing the influence spread in social networks. Â© Chartered Institute of Library and Information Professionals.","One of the important issues concerning the spreading process in social networks is the influence maximization. This is the problem of identifying the set of the most influential nodes in order to begin the spreading process based on an information diffusion model in the social networks. In this study, two new methods considering the community structure of the social networks and influence-based closeness centrality measure of the nodes are presented to maximize the spread of influence on the multiplication threshold, minimum threshold and linear threshold information diffusion models. The main objective of this study is to improve the efficiency with respect to the run time while maintaining the accuracy of the final influence spread. Efficiency improvement is obtained by reducing the number of candidate nodes subject to evaluation in order to find the most influential. Experiments consist of two parts: first, the effectiveness of the proposed influence-based closeness centrality measure is established by comparing it with available centrality measures; second, the evaluations are conducted to compare the two proposed community-based methods with well-known benchmarks in the literature on the real datasets, leading to the results demonstrate the efficiency and effectiveness of these methods in maximizing the influence spread in social networks."
Topic segmentation using word-level semantic relatedness functions,"Semantic relatedness deals with the problem of measuring how much two words are related to each other. While there is a large body of research for developing new measures, the use of semantic relatedness (SR) measures in topic segmentation has not been explored. In this research the performance of different SR measures is evaluated in the topic segmentation problem. To this end, two topic segmentation algorithms that use the difference in SR of words are introduced. Our results indicate that using an SR measure trained with a general domain corpora achieves better results than topic segmentation algorithms using Wordnet or simple word repetition. Furthermore, when compared with computationally more complex algorithms performing global analysis, our local analysis, enhanced with general domain lexical semantic information, achieves comparable results. Â© Chartered Institute of Library and Information Professionals.","Semantic relatedness deals with the problem of measuring how much two words are related to each other. While there is a large body of research for developing new measures, the use of semantic relatedness (SR) measures in topic segmentation has not been explored. In this research the performance of different SR measures is evaluated in the topic segmentation problem. To this end, two topic segmentation algorithms that use the difference in SR of words are introduced. Our results indicate that using an SR measure trained with a general domain corpora achieves better results than topic segmentation algorithms using Wordnet or simple word repetition. Furthermore, when compared with computationally more complex algorithms performing global analysis, our local analysis, enhanced with general domain lexical semantic information, achieves comparable results."
Combining resources to improve unsupervised sentiment analysis at aspect-level,"Every day more companies are interested in users' opinions about their products or services. Also, every day there are more users that search for reviews on the web before purchasing a product. These users and companies are not satisfied with knowing the overall sentiment of a product, they want a finer knowledge of users' opinions. Owing to this fact, more and more researchers are working on sentiment analysis at aspect-level. This paper describes an unsupervised approach for aspect-based sentiment analysis, which aims to identify the aspects of given target entities and the sentiment expressed for each aspect. We have evaluated several tasks, although perhaps the major novelty is in the classification of the aspects. We employ a lexicon-based method combining different linguistic resources and we conclude that the combination of several classifiers improves the classification significantly. In addition, a comparison with a supervised system is performed in order to determine the strengths and weakness of each of them. Â© Chartered Institute of Library and Information Professionals.","Every day more companies are interested in users' opinions about their products or services. Also, every day there are more users that search for reviews on the web before purchasing a product. These users and companies are not satisfied with knowing the overall sentiment of a product, they want a finer knowledge of users' opinions. Owing to this fact, more and more researchers are working on sentiment analysis at aspect-level. This paper describes an unsupervised approach for aspect-based sentiment analysis, which aims to identify the aspects of given target entities and the sentiment expressed for each aspect. We have evaluated several tasks, although perhaps the major novelty is in the classification of the aspects. We employ a lexicon-based method combining different linguistic resources and we conclude that the combination of several classifiers improves the classification significantly. In addition, a comparison with a supervised system is performed in order to determine the strengths and weakness of each of them."
Time sensitive blog retrieval using temporal properties of queries,"Blogs are one of the main user-generated contents on the web and are growing in number rapidly. The characteristics of blogs require the development of specialized search methods which are tuned for the blogosphere. In this paper, we focus on blog retrieval, which aims at ranking blogs with respect to their recurrent relevance to a user's topic. Although different blog retrieval algorithms have already been proposed, few of them have considered temporal properties of the input queries. Therefore, we propose an efficient approach to improving relevant blog retrieval using temporal property of queries. First, time sensitivity of each query is automatically computed for different time intervals based on an initially retrieved set of relevant posts. Then a temporal score is calculated for each blog and finally all blogs are ranked based on their temporal and content relevancy with regard to the input query. Experimental analysis and comparison of the proposed method are carried out using a standard dataset with 45 diverse queries. Our experimental results demonstrate that, using different measurement criteria, our proposed method outperforms other blog retrieval methods. Â© The Author(s) 2015.","Blogs are one of the main user-generated contents on the web and are growing in number rapidly. The characteristics of blogs require the development of specialized search methods which are tuned for the blogosphere. In this paper, we focus on blog retrieval, which aims at ranking blogs with respect to their recurrent relevance to a user's topic. Although different blog retrieval algorithms have already been proposed, few of them have considered temporal properties of the input queries. Therefore, we propose an efficient approach to improving relevant blog retrieval using temporal property of queries. First, time sensitivity of each query is automatically computed for different time intervals based on an initially retrieved set of relevant posts. Then a temporal score is calculated for each blog and finally all blogs are ranked based on their temporal and content relevancy with regard to the input query. Experimental analysis and comparison of the proposed method are carried out using a standard dataset with 45 diverse queries. Our experimental results demonstrate that, using different measurement criteria, our proposed method outperforms other blog retrieval methods."
WaPUPS: Web access pattern extraction under user-defined pattern scoring,"Extracting patterns from web usage data helps to facilitate better web personalization and web structure readjustment. The classical frequency-based sequence mining techniques consider only the binary occurrences of web pages in sessions that result in the extraction of many patterns that are not informative for users. To handle this problem, utility-based mining technique has emerged, which assigns non-binary values, called utilities, to web pages and calculates pattern utilities accordingly. However, the utility of a pattern cannot always be determined from distinct web page utilities. For instance, the number of distinct users that traverse an extracted pattern or some demographic data about those users may affect the value of the extracted patterns. However, such information cannot be calculated directly from web page utilities. In this paper, we present a new approach based on a user-defined scoring mechanism so as to extract patterns from web log data. The proposed approach can limit the size of the search space; therefore it has the ability to extract patterns even for large and sparse datasets. The framework is hybrid in the sense that it combines clustering with a heuristic-based pattern extraction algorithm. Substantial experiments on real datasets show that the proposed solution effectively discovers patterns under user-defined evaluation. Â© Chartered Institute of Library and Information Professionals.","Extracting patterns from web usage data helps to facilitate better web personalization and web structure readjustment. The classical frequency-based sequence mining techniques consider only the binary occurrences of web pages in sessions that result in the extraction of many patterns that are not informative for users. To handle this problem, utility-based mining technique has emerged, which assigns non-binary values, called utilities, to web pages and calculates pattern utilities accordingly. However, the utility of a pattern cannot always be determined from distinct web page utilities. For instance, the number of distinct users that traverse an extracted pattern or some demographic data about those users may affect the value of the extracted patterns. However, such information cannot be calculated directly from web page utilities. In this paper, we present a new approach based on a user-defined scoring mechanism so as to extract patterns from web log data. The proposed approach can limit the size of the search space; therefore it has the ability to extract patterns even for large and sparse datasets. The framework is hybrid in the sense that it combines clustering with a heuristic-based pattern extraction algorithm. Substantial experiments on real datasets show that the proposed solution effectively discovers patterns under user-defined evaluation."
Current state of Linked Data in digital libraries,"The Semantic Web encourages institutions, including libraries, to collect, link and share their data across the Web in order to ease its processing by machines to get better queries and results. Linked Data technologies enable us to connect related data on the Web using the principles outlined by Tim Berners-Lee in 2006. Digital libraries have great potential to exchange and disseminate data linked to external resources using Linked Data. In this paper, a study about the current uses of Linked Data in digital libraries, including the most important implementations around the world, is presented. The study focuses on selected vocabularies and ontologies, benefits and problems encountered in implementing Linked Data in digital libraries. In addition, it also identifies and discusses specific challenges that digital libraries face, offering suggestions for ways in which libraries can contribute to the Semantic Web. The study uses an adapted methodology for literature review, to find data available to answer research questions. It is based on the information found in the library websites recommended by W3C Library Linked Data Incubator Group in 2011, and scientific publications from Google Scholar, Scopus, ACM and Springer from the last 5 years. The selected libraries for the study are the National Library of France, the Europeana Library, the Library of Congress of the USA, the British Library and the National Library of Spain. In this paper, we outline the best practices found in each experience and identify gaps and future trends. Â© Chartered Institute of Library and Information Professionals.","The Semantic Web encourages institutions, including libraries, to collect, link and share their data across the Web in order to ease its processing by machines to get better queries and results. Linked Data technologies enable us to connect related data on the Web using the principles outlined by Tim Berners-Lee in 2006. Digital libraries have great potential to exchange and disseminate data linked to external resources using Linked Data. In this paper, a study about the current uses of Linked Data in digital libraries, including the most important implementations around the world, is presented. The study focuses on selected vocabularies and ontologies, benefits and problems encountered in implementing Linked Data in digital libraries. In addition, it also identifies and discusses specific challenges that digital libraries face, offering suggestions for ways in which libraries can contribute to the Semantic Web. The study uses an adapted methodology for literature review, to find data available to answer research questions. It is based on the information found in the library websites recommended by W3C Library Linked Data Incubator Group in 2011, and scientific publications from Google Scholar, Scopus, ACM and Springer from the last 5 years. The selected libraries for the study are the National Library of France, the Europeana Library, the Library of Congress of the USA, the British Library and the National Library of Spain. In this paper, we outline the best practices found in each experience and identify gaps and future trends."
Asking for more than an answer: What do askers expect in online Q&A services?,"Q&A services allow one to express an information need in the form of a natural language question and seek information from users of those services. Despite a recent rise in the research related to various issues of online Q&A, there is still a lack of consideration for how the situational context behind asking a question affects quality judgements. By focusing on users' expectations when asking a question, the work reported here builds on a framework of understanding how people assess information. Mixed method analysis - employing sequentially the Internet-based survey, diary and interviews - was used in a study to investigate this issue. A total of 226 online Q&A users participated in the study, and it was found that looking for quick responses, looking for additional or alternative information, and looking for accurate or complete information were the primary expectations of the askers. Findings can help identify why and how users engage in information seeking within an online Q&A context, and may help develop more comprehensive personalised approaches to deriving information relevance and satisfaction that include user expectations. Â© Chartered Institute of Library and Information Professionals.","Q&A services allow one to express an information need in the form of a natural language question and seek information from users of those services. Despite a recent rise in the research related to various issues of online Q&A, there is still a lack of consideration for how the situational context behind asking a question affects quality judgements. By focusing on users' expectations when asking a question, the work reported here builds on a framework of understanding how people assess information. Mixed method analysis - employing sequentially the Internet-based survey, diary and interviews - was used in a study to investigate this issue. A total of 226 online Q&A users participated in the study, and it was found that looking for quick responses, looking for additional or alternative information, and looking for accurate or complete information were the primary expectations of the askers. Findings can help identify why and how users engage in information seeking within an online Q&A context, and may help develop more comprehensive personalised approaches to deriving information relevance and satisfaction that include user expectations."
A feature selection model based on genetic rank aggregation for text sentiment classification,"Sentiment analysis is an important research direction of natural language processing, text mining and web mining which aims to extract subjective information in source materials. The main challenge encountered in machine learning method-based sentiment classification is the abundant amount of data available. This amount makes it difficult to train the learning algorithms in a feasible time and degrades the classification accuracy of the built model. Hence, feature selection becomes an essential task in developing robust and efficient classification models whilst reducing the training time. In text mining applications, individual filter-based feature selection methods have been widely utilized owing to their simplicity and relatively high performance. This paper presents an ensemble approach for feature selection, which aggregates the several individual feature lists obtained by the different feature selection methods so that a more robust and efficient feature subset can be obtained. In order to aggregate the individual feature lists, a genetic algorithm has been utilized. Experimental evaluations indicated that the proposed aggregation model is an efficient method and it outperforms individual filter-based feature selection methods on sentiment classification. Â© The Author(s) 2015.","Sentiment analysis is an important research direction of natural language processing, text mining and web mining which aims to extract subjective information in source materials. The main challenge encountered in machine learning method-based sentiment classification is the abundant amount of data available. This amount makes it difficult to train the learning algorithms in a feasible time and degrades the classification accuracy of the built model. Hence, feature selection becomes an essential task in developing robust and efficient classification models whilst reducing the training time. In text mining applications, individual filter-based feature selection methods have been widely utilized owing to their simplicity and relatively high performance. This paper presents an ensemble approach for feature selection, which aggregates the several individual feature lists obtained by the different feature selection methods so that a more robust and efficient feature subset can be obtained. In order to aggregate the individual feature lists, a genetic algorithm has been utilized. Experimental evaluations indicated that the proposed aggregation model is an efficient method and it outperforms individual filter-based feature selection methods on sentiment classification."
It takes a community to build a framework: Information literacy within intercultural settings,"Information literacy practice plays a key role in the transitional processes of individuals within new intercultural settings. While this ability to adjust to new cultural contexts is increasingly important within today's multicultural societies, campuses and workplaces, typical approaches to information literacy education struggle to scaffold the newcomer's disrupted information landscapes. In focusing on prescriptive skills, information literacy standards position linguistic and cultural difference as a learning deficiency. Yet when alternative information literacy frameworks centre upon personal habits of mind, they fail to account for contextual dynamics. In this conceptual paper, the authors use research into the health practices of resettling refugees as an example to argue that a move away from behaviourist approaches to information literacy refocuses our attention on questions of adjustment and engagement with cultural understandings of information, and forms a more inclusive way to consider the diversity of today's information societies. Â© The Author(s) 2016.","Information literacy practice plays a key role in the transitional processes of individuals within new intercultural settings. While this ability to adjust to new cultural contexts is increasingly important within today's multicultural societies, campuses and workplaces, typical approaches to information literacy education struggle to scaffold the newcomer's disrupted information landscapes. In focusing on prescriptive skills, information literacy standards position linguistic and cultural difference as a learning deficiency. Yet when alternative information literacy frameworks centre upon personal habits of mind, they fail to account for contextual dynamics. In this conceptual paper, the authors use research into the health practices of resettling refugees as an example to argue that a move away from behaviourist approaches to information literacy refocuses our attention on questions of adjustment and engagement with cultural understandings of information, and forms a more inclusive way to consider the diversity of today's information societies."
Classifier and feature set ensembles for web page classification,"Web page classification is an important research direction on web mining. The abundant amount of data available on the web makes it essential to develop efficient and robust models for web mining tasks. Web page classification is the process of assigning a web page to a particular predefined category based on labelled data. It serves for several other web mining tasks, such as focused web crawling, web link analysis and contextual advertising. Machine learning and data mining methods have been successfully applied for several web mining tasks, including web page classification. Multiple classifier systems are a promising research direction in machine learning, which aims to combine several classifiers by differentiating base classifiers and/or dataset distributions so that more robust classification models can be built. This paper presents a comparative analysis of four different feature selections (correlation, consistency, information gain and chi-square-based feature selection) and four different ensemble learning methods (Boosting, Bagging, Dagging and Random Subspace) based on four different base learners (naive Bayes, K-nearest neighbour algorithm, C4.5 algorithm and FURIA algorithm). The article examines the predictive performance of ensemble methods for web page classification. The experimental results indicate that feature selection and ensemble learning can enhance the predictive performance of classifiers in web page classification. For the DMOZ-50 dataset, the highest average predictive performance (88.1%) is obtained with the combination of consistency-based feature selection with AdaBoost and naive Bayes algorithms, which is a promising result for web page classification. Experimental results indicate that Bagging and Random Subspace ensemble methods and correlation-based and consistency-based feature selection methods obtain better results in terms of accuracy rates. Â© Chartered Institute of Library and Information Professionals.","Web page classification is an important research direction on web mining. The abundant amount of data available on the web makes it essential to develop efficient and robust models for web mining tasks. Web page classification is the process of assigning a web page to a particular predefined category based on labelled data. It serves for several other web mining tasks, such as focused web crawling, web link analysis and contextual advertising. Machine learning and data mining methods have been successfully applied for several web mining tasks, including web page classification. Multiple classifier systems are a promising research direction in machine learning, which aims to combine several classifiers by differentiating base classifiers and/or dataset distributions so that more robust classification models can be built. This paper presents a comparative analysis of four different feature selections (correlation, consistency, information gain and chi-square-based feature selection) and four different ensemble learning methods (Boosting, Bagging, Dagging and Random Subspace) based on four different base learners (naive Bayes, K-nearest neighbour algorithm, C4.5 algorithm and FURIA algorithm). The article examines the predictive performance of ensemble methods for web page classification. The experimental results indicate that feature selection and ensemble learning can enhance the predictive performance of classifiers in web page classification. For the DMOZ-50 dataset, the highest average predictive performance (88.1%) is obtained with the combination of consistency-based feature selection with AdaBoost and naive Bayes algorithms, which is a promising result for web page classification. Experimental results indicate that Bagging and Random Subspace ensemble methods and correlation-based and consistency-based feature selection methods obtain better results in terms of accuracy rates."
An accurate toponym-matching measure based on approximate string matching,"Approximate string matching (ASM) is a challenging problem, which aims to match different string expressions representing the same object. In this paper, detailed experimental studies were conducted on the subject of toponym matching, which is a new domain where ASM can be performed, and the creation of a single string-matching measure that can perform toponym matching process regardless of the language was attempted. For this purpose, an ASM measure called DAS, which comprises name similarity, word similarity and sentence similarity phases, was created. Considering the experimental results, the retrieval performance and system accuracy of DAS were much better than those of other well-known five measures that were compared on toponym test datasets. In addition, DAS had the best metric values of mean average precision in six languages, and precision/recall graphs confirm this result. Â© Chartered Institute of Library and Information Professionals.","Approximate string matching (ASM) is a challenging problem, which aims to match different string expressions representing the same object. In this paper, detailed experimental studies were conducted on the subject of toponym matching, which is a new domain where ASM can be performed, and the creation of a single string-matching measure that can perform toponym matching process regardless of the language was attempted. For this purpose, an ASM measure called DAS, which comprises name similarity, word similarity and sentence similarity phases, was created. Considering the experimental results, the retrieval performance and system accuracy of DAS were much better than those of other well-known five measures that were compared on toponym test datasets. In addition, DAS had the best metric values of mean average precision in six languages, and precision/recall graphs confirm this result."
Incorporating social information to perform diverse replier recommendation in question and answer communities,"Social information is contextual information that has made significant contributions to intelligent information systems. However, social information has not been fully used, especially in question and answer (Q&A) systems. This study describes a contextual recommendation method in which diverse repliers are recommended for new questions using incorporated social information in Q&A communities. We have mined multiple kinds of social information by analysing social behaviours and relations found in a Q&A community and proposed an algorithm to incorporate different social information in various social contexts to perform diverse repliers' recommendations. Recommendation diversity and social contexts have been considered and the properly used social information has been emphasized in this study. We conducted experiments using a dataset collected from the Stack Overflow website. The results demonstrate that different social information makes different contributions in promoting question answering, and incorporating social information properly could improve recommendation diversity and performance, which would then result in the promotion of satisfactory question solving. Â© The Author(s) 2015.","Social information is contextual information that has made significant contributions to intelligent information systems. However, social information has not been fully used, especially in question and answer (Q&A) systems. This study describes a contextual recommendation method in which diverse repliers are recommended for new questions using incorporated social information in Q&A communities. We have mined multiple kinds of social information by analysing social behaviours and relations found in a Q&A community and proposed an algorithm to incorporate different social information in various social contexts to perform diverse repliers' recommendations. Recommendation diversity and social contexts have been considered and the properly used social information has been emphasized in this study. We conducted experiments using a dataset collected from the Stack Overflow website. The results demonstrate that different social information makes different contributions in promoting question answering, and incorporating social information properly could improve recommendation diversity and performance, which would then result in the promotion of satisfactory question solving."
"Authentic versus fictitious online reviews: A textual analysis across luxury, budget, and mid-range hotels","Extant literature suggests that authentic and fictitious online reviews could be distinguished by leveraging on their textual characteristics. However, nuances in textual differences between authentic and fictitious reviews across different categories of hotels remain largely unknown. Therefore, this paper analyzes textual differences between authentic and fictitious reviews across three hotel categories, namely, luxury, budget, and mid-range. It leverages on four possible textual characteristics - comprehensibility, specificity, exaggeration, and negligence - that could offer clues to ascertain review authenticity. Using a dataset of 1800 reviews (900 authentic + 900 fictitious), the results suggest that differences between authentic and fictitious reviews are largely inconsistent across hotel categories. This generally points to the difficulties in ascertaining review authenticity, which in turn offer implications for both research and practice. Â© The Author(s) 2015.","Extant literature suggests that authentic and fictitious online reviews could be distinguished by leveraging on their textual characteristics. However, nuances in textual differences between authentic and fictitious reviews across different categories of hotels remain largely unknown. Therefore, this paper analyzes textual differences between authentic and fictitious reviews across three hotel categories, namely, luxury, budget, and mid-range. It leverages on four possible textual characteristics - comprehensibility, specificity, exaggeration, and negligence - that could offer clues to ascertain review authenticity. Using a dataset of 1800 reviews (900 authentic + 900 fictitious), the results suggest that differences between authentic and fictitious reviews are largely inconsistent across hotel categories. This generally points to the difficulties in ascertaining review authenticity, which in turn offer implications for both research and practice."
Seeking information in circles: The application of Chatman's life in the round theory to the information small world of Catholic clergy in northern Nigeria,This study explores Chatman's proposition of the theory of life in the round that members of a small world who live in the round will not cross the boundaries of their world to seek information. The study tests Chatman's proposition to find out whether it is applicable to the special population of Catholic clergy. The study was conducted with Catholic clergy from northern Nigeria. Findings show that these clergy are not likely to cross boundaries of their small worlds to seek information about their ministry or private lives. They prefer to seek such information within their circle of clergy. The findings align with Chatman's conclusion that life lived in the round has a negative influence on information seeking. This study advances the understanding of Chatman's theory of life in the round and positions religious status as a factor that is capable of influencing the information-seeking process. Â© Chartered Institute of Library and Information Professionals.,This study explores Chatman's proposition of the theory of life in the round that members of a small world who live in the round will not cross the boundaries of their world to seek information. The study tests Chatman's proposition to find out whether it is applicable to the special population of Catholic clergy. The study was conducted with Catholic clergy from northern Nigeria. Findings show that these clergy are not likely to cross boundaries of their small worlds to seek information about their ministry or private lives. They prefer to seek such information within their circle of clergy. The findings align with Chatman's conclusion that life lived in the round has a negative influence on information seeking. This study advances the understanding of Chatman's theory of life in the round and positions religious status as a factor that is capable of influencing the information-seeking process.
The census as an information source in public policy-making,"This paper provides an assessment of the value of national population censuses as information sources with specific reference to UK census data and its use in policy-making. Mixed methods were adopted to collect quantitative and qualitative data from two sources: (a) a content analysis of policy documents; and (b) interviews with policy-makers in Scotland. The findings highlight that, although the general value of the census is recognized, policy-makers are not necessarily closely engaged with the census as a tool for directing the development and implementation of policy. This is evident, for example, in a lack of awareness of proposed changes to the census, and infrequent deployment of available data. The opportunity to change perceptions among policy-makers, and to expand the application of census data in public policy, is identified. With a novel focus on the deployment of censuses as sources of evidence for policy-making that includes the views of policy-makers from both within and beyond government, this work contributes to an established body of global research on international censuses. Â© The Author(s) 2016.","This paper provides an assessment of the value of national population censuses as information sources with specific reference to UK census data and its use in policy-making. Mixed methods were adopted to collect quantitative and qualitative data from two sources: (a) a content analysis of policy documents; and (b) interviews with policy-makers in Scotland. The findings highlight that, although the general value of the census is recognized, policy-makers are not necessarily closely engaged with the census as a tool for directing the development and implementation of policy. This is evident, for example, in a lack of awareness of proposed changes to the census, and infrequent deployment of available data. The opportunity to change perceptions among policy-makers, and to expand the application of census data in public policy, is identified. With a novel focus on the deployment of censuses as sources of evidence for policy-making that includes the views of policy-makers from both within and beyond government, this work contributes to an established body of global research on international censuses."
A semantic-based approach for querying linked data using natural language,"The semantic Web aims to provide to Web information with a well-defined meaning and make it understandable not only by humans but also by computers, thus allowing the automation, integration and reuse of high-quality information across different applications. However, current information retrieval mechanisms for semantic knowledge bases are intended to be only used by expert users. In this work, we propose a natural language interface that allows non-expert users the access to this kind of information through formulating queries in natural language. The present approach uses a domain-independent ontology model to represent the question's structure and context. Also, this model allows determination of the answer type expected by the user based on a proposed question classification. To prove the effectiveness of our approach, we have conducted an evaluation in the music domain using LinkedBrainz, an effort to provide the MusicBrainz information as structured data on the Web by means of Semantic Web technologies. Our proposal obtained encouraging results based on the F-measure metric, ranging from 0.74 to 0.82 for a corpus of questions generated by a group of real-world end users. Â© The Author(s) 2015.","The semantic Web aims to provide to Web information with a well-defined meaning and make it understandable not only by humans but also by computers, thus allowing the automation, integration and reuse of high-quality information across different applications. However, current information retrieval mechanisms for semantic knowledge bases are intended to be only used by expert users. In this work, we propose a natural language interface that allows non-expert users the access to this kind of information through formulating queries in natural language. The present approach uses a domain-independent ontology model to represent the question's structure and context. Also, this model allows determination of the answer type expected by the user based on a proposed question classification. To prove the effectiveness of our approach, we have conducted an evaluation in the music domain using LinkedBrainz, an effort to provide the MusicBrainz information as structured data on the Web by means of Semantic Web technologies. Our proposal obtained encouraging results based on the F-measure metric, ranging from 0.74 to 0.82 for a corpus of questions generated by a group of real-world end users."
Profiling users with tag networks in diffusion-based personalized recommendation,"This study explores new ways of tag-based personalized recommendation by relieving the assumption that tags assigned by a user occur independently of each other. The new methods profile users using tag co-occurrence networks, upon which link-based node weighting methods (e.g. PageRank and HITS) are applied to refine the weights of tags. A diffusion process is then performed upon the tag-item bipartite graph to transform the weights of tags into recommendation scores for items. Experiments on three datasets showed improvements of the proposed method over the tag-based collaborative filtering in terms of precision and recall in the datasets with dense user-tag networks and in terms of inter-diversity in all datasets. In addition, the user-tag network is found to be a necessary instrument for the improvements. The findings of this study contribute to more accurate user profiling and personalized recommendations using network theory and have practical implications for tag-based recommendation systems. Â© Chartered Institute of Library and Information Professionals.","This study explores new ways of tag-based personalized recommendation by relieving the assumption that tags assigned by a user occur independently of each other. The new methods profile users using tag co-occurrence networks, upon which link-based node weighting methods ( PageRank and HITS) are applied to refine the weights of tags. A diffusion process is then performed upon the tag-item bipartite graph to transform the weights of tags into recommendation scores for items. Experiments on three datasets showed improvements of the proposed method over the tag-based collaborative filtering in terms of precision and recall in the datasets with dense user-tag networks and in terms of inter-diversity in all datasets. In addition, the user-tag network is found to be a necessary instrument for the improvements. The findings of this study contribute to more accurate user profiling and personalized recommendations using network theory and have practical implications for tag-based recommendation systems."
Modelling multi-topic information propagation in online social networks based on resource competition,"Understanding information propagation in online social networks is important in many practical applications and is of great interest to many researchers. The challenge with the existing propagation models lies in the requirement of complete network structure, topic-dependent model parameters and topic isolated spread assumption, etc. In this paper, we study the characteristics of multi-topic information propagation based on the data collected from Sina Weibo, one of the most popular microblogging services in China. We find that the daily total amount of user resources is finite and users' attention transfers from one topic to another. This shows evidence on the competitions between multiple dynamical topics. According to these empirical observations, we develop a competition-based multi-topic information propagation model without social network structure. This model is built based on general mechanisms of resource competitions, i.e. attracting and distracting users' attention, and considers the interactions of multiple topics. Simulation results show that the model can effectively produce topics with temporal popularity similar to the real data. The impact of model parameters is also analysed. It is found that topic arrival rate reflects the strength of competitions, and topic fitness is significant in modelling the small scale topic propagation. Â© Chartered Institute of Library and Information Professionals.","Understanding information propagation in online social networks is important in many practical applications and is of great interest to many researchers. The challenge with the existing propagation models lies in the requirement of complete network structure, topic-dependent model parameters and topic isolated spread assumption, etc. In this paper, we study the characteristics of multi-topic information propagation based on the data collected from Sina Weibo, one of the most popular microblogging services in China. We find that the daily total amount of user resources is finite and users' attention transfers from one topic to another. This shows evidence on the competitions between multiple dynamical topics. According to these empirical observations, we develop a competition-based multi-topic information propagation model without social network structure. This model is built based on general mechanisms of resource competitions, attracting and distracting users' attention, and considers the interactions of multiple topics. Simulation results show that the model can effectively produce topics with temporal popularity similar to the real data. The impact of model parameters is also analysed. It is found that topic arrival rate reflects the strength of competitions, and topic fitness is significant in modelling the small scale topic propagation."
A search index-enhanced feature model for news recommendation,"General news recommendations are important but have received limited attention because of the difficulties of measuring public interest. In public search engines, the objects of search terms reflect the issues that interest or concern search engine users. Because of the popularity of search engines, search indexes have become a new measure for describing public interest trends. With the help of a public search index provided by search engines, we construct a news topic search feature and a news object search feature. These features measure the public attention on key elements of the news. In the experiment, we compare various feature models with machine learning algorithms with respect to financial news recommendations. The results demonstrate that the topic search features perform best compared with other feature models. This research contributes to both the feature generation and news recommendation domains. Â© Chartered Institute of Library and Information Professionals.","General news recommendations are important but have received limited attention because of the difficulties of measuring public interest. In public search engines, the objects of search terms reflect the issues that interest or concern search engine users. Because of the popularity of search engines, search indexes have become a new measure for describing public interest trends. With the help of a public search index provided by search engines, we construct a news topic search feature and a news object search feature. These features measure the public attention on key elements of the news. In the experiment, we compare various feature models with machine learning algorithms with respect to financial news recommendations. The results demonstrate that the topic search features perform best compared with other feature models. This research contributes to both the feature generation and news recommendation domains."
A hybrid ontology-based information extraction system,"Information Extraction is the process of automatically obtaining knowledge from plain text. Because of the ambiguity of written natural language, Information Extraction is a difficult task. Ontology-based Information Extraction (OBIE) reduces this complexity by including contextual information in the form of a domain ontology. The ontology provides guidance to the extraction process by providing concepts and relationships about the domain. However, OBIE systems have not been widely adopted because of the difficulties in deployment and maintenance. The Ontology-based Components for Information Extraction (OBCIE) architecture has been proposed as a form to encourage the adoption of OBIE by promoting reusability through modularity. In this paper, we propose two orthogonal extensions to OBCIE that allow the construction of hybrid OBIE systems with higher extraction accuracy and a new functionality. The first extension utilizes OBCIE modularity to integrate different types of implementation into one extraction system, producing a more accurate extraction. For each concept or relationship in the ontology, we can select the best implementation for extraction, or we can combine both implementations under an ensemble learning schema. The second extension is a novel ontology-based error detection mechanism. Following a heuristic approach, we can identify sentences that are logically inconsistent with the domain ontology. Because the implementation strategy for the extraction of a concept is independent of the functionality of the extraction, we can design a hybrid OBIE system with concepts utilizing different implementation strategies for extracting correct or incorrect sentences. Our evaluation shows that, in the implementation extension, our proposed method is more accurate in terms of correctness and completeness of the extraction. Moreover, our error detection method can identify incorrect statements with a high accuracy. Â© The Author(s) 2015.","Information Extraction is the process of automatically obtaining knowledge from plain text. Because of the ambiguity of written natural language, Information Extraction is a difficult task. Ontology-based Information Extraction (OBIE) reduces this complexity by including contextual information in the form of a domain ontology. The ontology provides guidance to the extraction process by providing concepts and relationships about the domain. However, OBIE systems have not been widely adopted because of the difficulties in deployment and maintenance. The Ontology-based Components for Information Extraction (OBCIE) architecture has been proposed as a form to encourage the adoption of OBIE by promoting reusability through modularity. In this paper, we propose two orthogonal extensions to OBCIE that allow the construction of hybrid OBIE systems with higher extraction accuracy and a new functionality. The first extension utilizes OBCIE modularity to integrate different types of implementation into one extraction system, producing a more accurate extraction. For each concept or relationship in the ontology, we can select the best implementation for extraction, or we can combine both implementations under an ensemble learning schema. The second extension is a novel ontology-based error detection mechanism. Following a heuristic approach, we can identify sentences that are logically inconsistent with the domain ontology. Because the implementation strategy for the extraction of a concept is independent of the functionality of the extraction, we can design a hybrid OBIE system with concepts utilizing different implementation strategies for extracting correct or incorrect sentences. Our evaluation shows that, in the implementation extension, our proposed method is more accurate in terms of correctness and completeness of the extraction. Moreover, our error detection method can identify incorrect statements with a high accuracy."
The adoption process in management innovation: A Knowledge Management case study,"This paper draws on findings from a longitudinal study of the adoption of a management innovation within an organisational setting. It is based on the findings of a case study that explores and discusses in depth a Knowledge Management programme that was introduced within a large distributed public sector agency in Europe. The aim of this research was to provide insight into the adoption process associated with management innovation. A qualitative case study strategy generates an account of the process of adoption through three phases (initiation; implementation; and outcomes), the episodes within each phase, and decision-making across the entire process. The findings contribute to the development of an extended and refined model of the process of adoption of management innovation through the consideration of the labelling, sequence and transition of phases and episodes, and decision-making. In this extended and refined model there are three phases with nine episodes, two of which are recursive; the phases occur in a linear sequence but may overlap, while the episodes occur in a non-linear sequence; and decision-making occurs within episodes, between phases and between episodes. The study makes three primary contributions to knowledge. First, it considers the process of adoption (as opposed to the more commonly examined process of generation) of management innovation. Second, it identifies decision-making related to the changes required for adoption of a management innovation. Finally, it develops a model of the process of adoption of management innovation that includes decision-making. In addition, the output of the study can be used as a tool for project management by identifying the questions to be addressed, and the decisions to be made, at particular points of the management innovation process, taking into account local contexts. Â© The Author(s) 2016.","This paper draws on findings from a longitudinal study of the adoption of a management innovation within an organisational setting. It is based on the findings of a case study that explores and discusses in depth a Knowledge Management programme that was introduced within a large distributed public sector agency in Europe. The aim of this research was to provide insight into the adoption process associated with management innovation. A qualitative case study strategy generates an account of the process of adoption through three phases (initiation; implementation; and outcomes), the episodes within each phase, and decision-making across the entire process. The findings contribute to the development of an extended and refined model of the process of adoption of management innovation through the consideration of the labelling, sequence and transition of phases and episodes, and decision-making. In this extended and refined model there are three phases with nine episodes, two of which are recursive; the phases occur in a linear sequence but may overlap, while the episodes occur in a non-linear sequence; and decision-making occurs within episodes, between phases and between episodes. The study makes three primary contributions to knowledge. First, it considers the process of adoption (as opposed to the more commonly examined process of generation) of management innovation. Second, it identifies decision-making related to the changes required for adoption of a management innovation. Finally, it develops a model of the process of adoption of management innovation that includes decision-making. In addition, the output of the study can be used as a tool for project management by identifying the questions to be addressed, and the decisions to be made, at particular points of the management innovation process, taking into account local contexts."
Improving the geospatial consistency of digital libraries metadata,"Consistency is an essential aspect of the quality of metadata. Inconsistent metadata records are harmful: given a themed query, the set of retrieved metadata records would contain descriptions of unrelated or irrelevant resources, and may even not contain some resources considered obvious. This is even worse when the description of the location is inconsistent. Inconsistent spatial descriptions may yield invisible or hidden geographical resources that cannot be retrieved by means of spatially themed queries. Therefore, ensuring spatial consistency should be a primary goal when reusing, sharing and developing georeferenced digital collections. We present a methodology able to detect geospatial inconsistencies in metadata collections based on the combination of spatial ranking, reverse geocoding, geographic knowledge organization systems and information-retrieval techniques. This methodology has been applied to a collection of metadata records describing maps and atlases belonging to the Library of Congress. The proposed approach was able to automatically identify inconsistent metadata records (870 out of 10,575) and propose fixes to most of them (91.5%) These results support the ability of the proposed methodology to assess the impact of spatial inconsistency in the retrievability and visibility of metadata records and improve their spatial consistency. Â© The Author(s) 2015.","Consistency is an essential aspect of the quality of metadata. Inconsistent metadata records are harmful: given a themed query, the set of retrieved metadata records would contain descriptions of unrelated or irrelevant resources, and may even not contain some resources considered obvious. This is even worse when the description of the location is inconsistent. Inconsistent spatial descriptions may yield invisible or hidden geographical resources that cannot be retrieved by means of spatially themed queries. Therefore, ensuring spatial consistency should be a primary goal when reusing, sharing and developing georeferenced digital collections. We present a methodology able to detect geospatial inconsistencies in metadata collections based on the combination of spatial ranking, reverse geocoding, geographic knowledge organization systems and information-retrieval techniques. This methodology has been applied to a collection of metadata records describing maps and atlases belonging to the Library of Congress. The proposed approach was able to automatically identify inconsistent metadata records (870 out of 10,575) and propose fixes to most of them (91.5%) These results support the ability of the proposed methodology to assess the impact of spatial inconsistency in the retrievability and visibility of metadata records and improve their spatial consistency."
Searching as learning: A systematization based on literature,"The paper surveys empirical studies on the relations between information searching and learning, and presents some reflections about learning in a search process based on the findings. First, the meaning of the concepts 'learning' and 'searching' is briefly defined. Learning is conceptualized as changes in one's knowledge structures. Then it is described more in detail how learning occurs in the search process. The point of departure is to focus on tasks that require the restructuring of knowledge structures and to analyse how gradual stabilization of those structures is related to accessing and interacting with information sources. After that, empirical studies on searching and learning are categorized by identifying independent and dependent variables in those studies. In conclusion, some general remarks on the topic are presented. Â© 2016 Chartered Institute of Library and Information Professionals.","The paper surveys empirical studies on the relations between information searching and learning, and presents some reflections about learning in a search process based on the findings. First, the meaning of the concepts 'learning' and 'searching' is briefly defined. Learning is conceptualized as changes in one's knowledge structures. Then it is described more in detail how learning occurs in the search process. The point of departure is to focus on tasks that require the restructuring of knowledge structures and to analyse how gradual stabilization of those structures is related to accessing and interacting with information sources. After that, empirical studies on searching and learning are categorized by identifying independent and dependent variables in those studies. In conclusion, some general remarks on the topic are presented."
Making sense of the past: The embodied information practices of field archaeologists,"This paper reports the findings of a study of the information practices of archaeologists, students and volunteers undertaking an excavation in the field. Conceptually, the study was guided by a social constructionist and practice-theoretical epistemological standpoint. Methodologically, the study employed a multi-faceted approach incorporating both ethnographic observation of archaeologists working in the field and in-depth interviews. The findings show that participants' practices were both social and embodied in nature. Â© The Author(s) 2016.","This paper reports the findings of a study of the information practices of archaeologists, students and volunteers undertaking an excavation in the field. Conceptually, the study was guided by a social constructionist and practice-theoretical epistemological standpoint. Methodologically, the study employed a multi-faceted approach incorporating both ethnographic observation of archaeologists working in the field and in-depth interviews. The findings show that participants' practices were both social and embodied in nature."
Generating query suggestions by exploiting latent semantics in query logs,"Search engines assist users in expressing their information needs more accurately by reformulating the issued queries automatically and suggesting the generated formulations to the users. Many approaches to query suggestion draw on the information stored in query logs, recommending recorded queries that are textually similar to the current user's query or that frequently co-occurred with it in the past. In this paper, we propose an approach that concentrates on deducing the actual information need from the user's query. The challenge therein lies not only in processing keyword queries, which are often short and possibly ambiguous, but especially in handling the complexity of natural language that allows users to express the same or similar information needs in various differing ways. We expect a higher-level semantic representation of a user's query to more accurately reflect the information need than the explicit query terms alone can. To this aim, we employ latent Dirichlet allocation as a probabilistic topic model to reveal latent semantics in the query log. Our evaluations show that, whereas purely topic-based query suggestion performs the worst, the interpolation of our proposed topic-based model with the baseline word-based model that generates suggestions based on matching query terms achieves significant improvements in suggestion quality over the already well performing purely word-based approach. Â© The Author(s) 2015.","Search engines assist users in expressing their information needs more accurately by reformulating the issued queries automatically and suggesting the generated formulations to the users. Many approaches to query suggestion draw on the information stored in query logs, recommending recorded queries that are textually similar to the current user's query or that frequently co-occurred with it in the past. In this paper, we propose an approach that concentrates on deducing the actual information need from the user's query. The challenge therein lies not only in processing keyword queries, which are often short and possibly ambiguous, but especially in handling the complexity of natural language that allows users to express the same or similar information needs in various differing ways. We expect a higher-level semantic representation of a user's query to more accurately reflect the information need than the explicit query terms alone can. To this aim, we employ latent Dirichlet allocation as a probabilistic topic model to reveal latent semantics in the query log. Our evaluations show that, whereas purely topic-based query suggestion performs the worst, the interpolation of our proposed topic-based model with the baseline word-based model that generates suggestions based on matching query terms achieves significant improvements in suggestion quality over the already well performing purely word-based approach."
An improved ant algorithm with LDA-based representation for text document clustering,"Document clustering can be applied in document organisation and browsing, document summarisation and classification. The identification of an appropriate representation for textual documents is extremely important for the performance of clustering or classification algorithms. Textual documents suffer from the high dimensionality and irrelevancy of text features. Besides, conventional clustering algorithms suffer from several shortcomings, such as slow convergence and sensitivity to the initial value. To tackle the problems of conventional clustering algorithms, metaheuristic algorithms are frequently applied to clustering. In this paper, an improved ant clustering algorithm is presented, where two novel heuristic methods are proposed to enhance the clustering quality of ant-based clustering. In addition, the latent Dirichlet allocation (LDA) is used to represent textual documents in a compact and efficient way. The clustering quality of the proposed ant clustering algorithm is compared to the conventional clustering algorithms using 25 text benchmarks in terms of F-measure values. The experimental results indicate that the proposed clustering scheme outperforms the compared conventional and metaheuristic clustering methods for textual documents. Â© Chartered Institute of Library and Information Professionals.","Document clustering can be applied in document organisation and browsing, document summarisation and classification. The identification of an appropriate representation for textual documents is extremely important for the performance of clustering or classification algorithms. Textual documents suffer from the high dimensionality and irrelevancy of text features. Besides, conventional clustering algorithms suffer from several shortcomings, such as slow convergence and sensitivity to the initial value. To tackle the problems of conventional clustering algorithms, metaheuristic algorithms are frequently applied to clustering. In this paper, an improved ant clustering algorithm is presented, where two novel heuristic methods are proposed to enhance the clustering quality of ant-based clustering. In addition, the latent Dirichlet allocation (LDA) is used to represent textual documents in a compact and efficient way. The clustering quality of the proposed ant clustering algorithm is compared to the conventional clustering algorithms using 25 text benchmarks in terms of F-measure values. The experimental results indicate that the proposed clustering scheme outperforms the compared conventional and metaheuristic clustering methods for textual documents."
Framing of different types of information needs within simulated work task situations: An empirical study in the school context,"This paper reports a meta-evaluation of how to frame different types of information needs within simulated work task situations. This is done via an empirical study of teenagers and their teachers' Internet information searching. Two sets of simulated work task situations were carefully designed to reflect verificative, conscious topical and muddled topical information needs of each group of test participants. The study shows that it is challenging to formulate verificative simulated work task situations and to incorporate curiosity in the muddled topical simulated work task situations. The results also show that the search behaviour of the two groups differs across the information needs, as expected, but also between the two groups, owing to the search strategy and attitude of the teenagers. This is seen by how fast they were at searching and assessing relevance, often using Google's 'picture search function', and saving the reading in detail for later. Â© The Author(s) 2016.","This paper reports a meta-evaluation of how to frame different types of information needs within simulated work task situations. This is done via an empirical study of teenagers and their teachers' Internet information searching. Two sets of simulated work task situations were carefully designed to reflect verificative, conscious topical and muddled topical information needs of each group of test participants. The study shows that it is challenging to formulate verificative simulated work task situations and to incorporate curiosity in the muddled topical simulated work task situations. The results also show that the search behaviour of the two groups differs across the information needs, as expected, but also between the two groups, owing to the search strategy and attitude of the teenagers. This is seen by how fast they were at searching and assessing relevance, often using Google's 'picture search function', and saving the reading in detail for later."
"Relationships among tasks, collaborative inquiry processes, inquiry resolutions, and knowledge outcomes in adolescents during guided discovery-based game design in school","This research study investigates US middle school students' collaborative information-seeking, sense-making and knowledge-building practices in a guided discovery-based programme of game design learning in which students and their teachers participate in a formal, in-school class daily, for credit and a grade for an entire year. The learning is supported by information affordances including a wiki learning management system (LMS) housing the curriculum, organized design activities, social media features, tutorials and informational assignments. Students engage in a Constructionist blended learning setting in their classroom, and work collaboratively in teams on game design. The study draws on qualitative video data from six team cases using a coding scheme of categories for the concepts of task, collaborative information seeking (CIS) Modality, and inquiry resolution outcomes. The study also considers linkages between processes and learning outcomes. Variation in engagement across the categories among students was charted, and certain patterns emerged. Findings indicate that some categories of task appear related to some categories of students' chosen CIS Modality for solving problems. Further, CIS processes in support of tasks appear related to inquiry incident resolution (resolved/unresolved). For student completion of advanced programming tasks in particular, we observe more frequent uses of the wiki-based LMS resources, and greater levels of challenge in fulfilling tasks. Results support existing work on these theoretical constructs in the information sciences, and lead to questions on how naturalistic emergence of CIS practices result from greater task knowledge, and whether learned CIS practices (as tasks in and of themselves) can yield project task knowledge gains. Findings of the study and the ongoing questions the work invites hold instructional design implications, and show how social constructivist educational contexts involving collaborative and information seeking and knowledge building among youth game designers can contribute to scholarly understanding of these processes more broadly in related project-based work contexts occurring among both youth and adults. Â© 2016 Chartered Institute of Library and Information Professionals.","This research study investigates US middle school students' collaborative information-seeking, sense-making and knowledge-building practices in a guided discovery-based programme of game design learning in which students and their teachers participate in a formal, in-school class daily, for credit and a grade for an entire year. The learning is supported by information affordances including a wiki learning management system (LMS) housing the curriculum, organized design activities, social media features, tutorials and informational assignments. Students engage in a Constructionist blended learning setting in their classroom, and work collaboratively in teams on game design. The study draws on qualitative video data from six team cases using a coding scheme of categories for the concepts of task, collaborative information seeking (CIS) Modality, and inquiry resolution outcomes. The study also considers linkages between processes and learning outcomes. Variation in engagement across the categories among students was charted, and certain patterns emerged. Findings indicate that some categories of task appear related to some categories of students' chosen CIS Modality for solving problems. Further, CIS processes in support of tasks appear related to inquiry incident resolution (resolved/unresolved). For student completion of advanced programming tasks in particular, we observe more frequent uses of the wiki-based LMS resources, and greater levels of challenge in fulfilling tasks. Results support existing work on these theoretical constructs in the information sciences, and lead to questions on how naturalistic emergence of CIS practices result from greater task knowledge, and whether learned CIS practices (as tasks in and of themselves) can yield project task knowledge gains. Findings of the study and the ongoing questions the work invites hold instructional design implications, and show how social constructivist educational contexts involving collaborative and information seeking and knowledge building among youth game designers can contribute to scholarly understanding of these processes more broadly in related project-based work contexts occurring among both youth and adults."
Implications of augmented reality in the management of television audiovisual information,"This document analyses the possibilities offered by augmented reality for exploiting the audiovisual collections of television archives, thereby presenting the idea, which has not been developed by any network, of providing viewers with the material issued and submitted synchronously for broadcasting. By using external elements other than TV sets (tablets, iPads, Smartphones), users can access images from the archive, which have been used to generate the information or programme, contributing also with additional elements that may be of interest. Other contents related to the information provided may be similarly accessed, thus facilitating a real conceptual map of the audiovisual contents of any event. Access may be granted free of charge or by paying a fee. Commercial exploitation is achieved in the form of viewer loyalty by gaining access to additional content and providing greater bi-directionality to the communication between viewers and the media. Â© Chartered Institute of Library and Information Professionals.","This document analyses the possibilities offered by augmented reality for exploiting the audiovisual collections of television archives, thereby presenting the idea, which has not been developed by any network, of providing viewers with the material issued and submitted synchronously for broadcasting. By using external elements other than TV sets (tablets, iPads, Smartphones), users can access images from the archive, which have been used to generate the information or programme, contributing also with additional elements that may be of interest. Other contents related to the information provided may be similarly accessed, thus facilitating a real conceptual map of the audiovisual contents of any event. Access may be granted free of charge or by paying a fee. Commercial exploitation is achieved in the form of viewer loyalty by gaining access to additional content and providing greater bi-directionality to the communication between viewers and the media."
Investigating the precision of Web image search engines for popular and less popular entities,"Image search is the second most frequently used search service on the Web. However, there are very few studies investigating any aspect of it. In this study, we investigate the precision of Web image search engines of Google and Bing for popular and less popular entities using text-based queries. Furthermore, we investigate four additional aspects of Web image search engines that have not been studied before. We used 60 different queries in total from three different domains for popular and less popular categories. We examined the relevancy of the top 100 images for each query. Our results indicate that image search is a solved problem for popular entities. They deliver 97% precision on the average for popular entities. However, precision values are much lower for less popular entities. For the top 100 results, average precision is 48% for Google and 33% for Bing. The most important problem seems to be the worst cases in which the precision can be less than 10%. The results show that significant improvement is needed to better identify relevant images for less popular entities. One of the main issues is the association problem. When a Web page has query words and multiple images, both Google and Bing are having difficulty determining the relevant images. Â© Chartered Institute of Library and Information Professionals.","Image search is the second most frequently used search service on the Web. However, there are very few studies investigating any aspect of it. In this study, we investigate the precision of Web image search engines of Google and Bing for popular and less popular entities using text-based queries. Furthermore, we investigate four additional aspects of Web image search engines that have not been studied before. We used 60 different queries in total from three different domains for popular and less popular categories. We examined the relevancy of the top 100 images for each query. Our results indicate that image search is a solved problem for popular entities. They deliver 97% precision on the average for popular entities. However, precision values are much lower for less popular entities. For the top 100 results, average precision is 48% for Google and 33% for Bing. The most important problem seems to be the worst cases in which the precision can be less than 10%. The results show that significant improvement is needed to better identify relevant images for less popular entities. One of the main issues is the association problem. When a Web page has query words and multiple images, both Google and Bing are having difficulty determining the relevant images."
Applying the semantic web to represent an individual's academic and professional background,"The Lattes Platform is a web-based system that brings together the academic, professional and scientific histories of students, teachers, researchers and other professionals linked to scientific and technological careers. The data are entered by users themselves and are the subject of much research and forecasting in relation to how educational resources are directed in Brazil. In this paper, we report our experience in applying the Linked Data principles to this system. We have also demonstrated the potential of federated queries using data from DBPedia. Â© Chartered Institute of Library and Information Professionals.","The Lattes Platform is a web-based system that brings together the academic, professional and scientific histories of students, teachers, researchers and other professionals linked to scientific and technological careers. The data are entered by users themselves and are the subject of much research and forecasting in relation to how educational resources are directed in Brazil. In this paper, we report our experience in applying the Linked Data principles to this system. We have also demonstrated the potential of federated queries using data from DBPedia."
Towards searching as a learning process: A review of current perspectives and future directions,"We critically review literature on the association between searching and learning and contribute to the formulation of a research agenda for searching as learning. The paper begins by reviewing current literature that tends to characterize search systems as tools for learning. We then present a perspective on searching as learning that focuses on the learning that occurs during the search process, as well as search outputs and learning outcomes. The concept of 'comprehensive search' is proposed to describe iterative, reflective and integrative search sessions that facilitate critical and creative learning beyond receptive learning. We also discuss how search interaction data can provide a rich source of implicit and explicit features through which to assess search-related learning. In conclusion, we summarize opportunities and challenges for future research with respect to four agendas: developing a search system that supports sense-making and enhances learning; supporting effective user interaction for searching as learning; providing an inquiry-based literacy tool within a search system; and assessing learning from online searching behaviour. Â© 2016 Chartered Institute of Library and Information Professionals.","We critically review literature on the association between searching and learning and contribute to the formulation of a research agenda for searching as learning. The paper begins by reviewing current literature that tends to characterize search systems as tools for learning. We then present a perspective on searching as learning that focuses on the learning that occurs during the search process, as well as search outputs and learning outcomes. The concept of 'comprehensive search' is proposed to describe iterative, reflective and integrative search sessions that facilitate critical and creative learning beyond receptive learning. We also discuss how search interaction data can provide a rich source of implicit and explicit features through which to assess search-related learning. In conclusion, we summarize opportunities and challenges for future research with respect to four agendas: developing a search system that supports sense-making and enhances learning; supporting effective user interaction for searching as learning; providing an inquiry-based literacy tool within a search system; and assessing learning from online searching behaviour."
Impression management through people tagging in the enterprise: Implications for social media sampling and design,"People tagging allows a person to tag one's self or others; it is reciprocal and therefore has social implications. The main uses of corporate people tagging systems are for building internal social networks, solving problems, and seeking expertise. We explored the statistical and terminological relation between self-presentation and perception by others as reflected by the use of tags in a people tagging system within a large enterprise. Due to the features of the power law distribution of the data, two different samples were analyzed. Using content analysis, we found that when there are few self or social tags, users prefer to use tags from the Environment and Technology categories, providing tags that tend to be objective or factual. When tagging approaches saturation, it becomes more subjective and social, using tags from the Individual category. Self-tags tend to be more factual describing technology expertise while social tags augment the individual tags by adding a personal dimension. The more people tag and get tagged, the more terminological overlap develops. We conclude by providing practical advice on how to create a sustainable system by balancing originality and duplication using interactivity and feedback. Â© Chartered Institute of Library and Information Professionals.","People tagging allows a person to tag one's self or others; it is reciprocal and therefore has social implications. The main uses of corporate people tagging systems are for building internal social networks, solving problems, and seeking expertise. We explored the statistical and terminological relation between self-presentation and perception by others as reflected by the use of tags in a people tagging system within a large enterprise. Due to the features of the power law distribution of the data, two different samples were analyzed. Using content analysis, we found that when there are few self or social tags, users prefer to use tags from the Environment and Technology categories, providing tags that tend to be objective or factual. When tagging approaches saturation, it becomes more subjective and social, using tags from the Individual category. Self-tags tend to be more factual describing technology expertise while social tags augment the individual tags by adding a personal dimension. The more people tag and get tagged, the more terminological overlap develops. We conclude by providing practical advice on how to create a sustainable system by balancing originality and duplication using interactivity and feedback."
Arabic tweets sentiment analysis - A hybrid scheme,"The fact that people freely express their opinions and ideas in no more than 140 characters makes Twitter one of the most prevalent social networking websites in the world. Being popular in Saudi Arabia, we believe that tweets are a good source to capture the public's sentiment, especially since the country is in a fractious region. Going over the challenges and the difficulties that the Arabic tweets present - using Saudi Arabia as a basis - we propose our solution. A typical problem is the practice of tweeting in dialectical Arabic. Based on our observation we recommend a hybrid approach that combines semantic orientation and machine learning techniques. Through this approach, the lexical-based classifier will label the training data, a time-consuming task often prepared manually. The output of the lexical classifier will be used as training data for the SVM machine learning classifier. The experiments show that our hybrid approach improved the F-measure of the lexical classifier by 5.76% while the accuracy jumped by 16.41%, achieving an overall F-measure and accuracy of 84 and 84.01% respectively. Â© The Author(s) 2015.","The fact that people freely express their opinions and ideas in no more than 140 characters makes Twitter one of the most prevalent social networking websites in the world. Being popular in Saudi Arabia, we believe that tweets are a good source to capture the public's sentiment, especially since the country is in a fractious region. Going over the challenges and the difficulties that the Arabic tweets present - using Saudi Arabia as a basis - we propose our solution. A typical problem is the practice of tweeting in dialectical Arabic. Based on our observation we recommend a hybrid approach that combines semantic orientation and machine learning techniques. Through this approach, the lexical-based classifier will label the training data, a time-consuming task often prepared manually. The output of the lexical classifier will be used as training data for the SVM machine learning classifier. The experiments show that our hybrid approach improved the F-measure of the lexical classifier by 5.76% while the accuracy jumped by 16.41%, achieving an overall F-measure and accuracy of 84 and 84.01% respectively."
Information encountering on social media and tacit knowledge sharing,"The purpose of this paper is to investigate how social media may support information encountering (i.e. where individuals encounter useful and interesting information while seeking or browsing for some other information) and how this may lead to the facilitation of tacit knowledge creation and sharing. The study employed a qualitative survey design that interviewed 24 physicians who were active users of social media to better understand the phenomenon of information encountering on social media. The data was analysed using the thematic analysis approach. The study found six main ways through which social media supports information encountering. Furthermore, drawing upon knowledge creation theories, the study concluded that information encountering on social media facilitates tacit knowledge creation and sharing among individuals. The study provides new directions for further empirical investigations to examine whether information encountering on social media actually leads to tacit knowledge creation and sharing. The findings of the study may also provide opportunities for users to adopt social media effectively or gain greater value from social media use. Â© The Author(s) 2015.","The purpose of this paper is to investigate how social media may support information encountering ( where individuals encounter useful and interesting information while seeking or browsing for some other information) and how this may lead to the facilitation of tacit knowledge creation and sharing. The study employed a qualitative survey design that interviewed 24 physicians who were active users of social media to better understand the phenomenon of information encountering on social media. The data was analysed using the thematic analysis approach. The study found six main ways through which social media supports information encountering. Furthermore, drawing upon knowledge creation theories, the study concluded that information encountering on social media facilitates tacit knowledge creation and sharing among individuals. The study provides new directions for further empirical investigations to examine whether information encountering on social media actually leads to tacit knowledge creation and sharing. The findings of the study may also provide opportunities for users to adopt social media effectively or gain greater value from social media use."
A novel algorithm for scalable k -nearest neighbour graph construction,"Finding the k-nearest neighbours of every node in a dataset is one of the most important data operations with wide application in various areas such as recommendation and information retrieval. However, a major challenge is that the execution time of existing approaches grows rapidly as the number of nodes or dimensions increases. In this paper, we present greedy filtering, an efficient and scalable algorithm for finding an approximate k-nearest neighbour graph. It selects a fixed number of nodes as candidates for every node by filtering out node pairs that do not have any matching dimensions with large values. Greedy filtering achieves consistent approximation accuracy across nodes in linear execution time. We also present a faster version of greedy filtering that uses inverted indices on the node prefixes. Through theoretical analysis, we show that greedy filtering is effective for datasets whose features have Zipfian distribution, a characteristic observed in majority of large datasets. We also conduct extensive comparative experiments against (a) three state-of-the-art algorithms, and (b) three algorithms in related research domains. Our experimental results show that greedy filtering consistently outperforms other algorithms in various types of high-dimensional datasets. Â© Chartered Institute of Library and Information Professionals.","Finding the k-nearest neighbours of every node in a dataset is one of the most important data operations with wide application in various areas such as recommendation and information retrieval. However, a major challenge is that the execution time of existing approaches grows rapidly as the number of nodes or dimensions increases. In this paper, we present greedy filtering, an efficient and scalable algorithm for finding an approximate k-nearest neighbour graph. It selects a fixed number of nodes as candidates for every node by filtering out node pairs that do not have any matching dimensions with large values. Greedy filtering achieves consistent approximation accuracy across nodes in linear execution time. We also present a faster version of greedy filtering that uses inverted indices on the node prefixes. Through theoretical analysis, we show that greedy filtering is effective for datasets whose features have Zipfian distribution, a characteristic observed in majority of large datasets. We also conduct extensive comparative experiments against (a) three state-of-the-art algorithms, and (b) three algorithms in related research domains. Our experimental results show that greedy filtering consistently outperforms other algorithms in various types of high-dimensional datasets."
Topic-based content and sentiment analysis of Ebola virus on Twitter and in the news,"The present study investigates topic coverage and sentiment dynamics of two different media sources, Twitter and news publications, on the hot health issue of Ebola. We conduct content and sentiment analysis by: (1) applying vocabulary control to collected datasets; (2) employing the n-gram LDA topic modeling technique; (3) adopting entity extraction and entity network; and (4) introducing the concept of topic-based sentiment scores. With the query term 'Ebola' or 'Ebola virus', we collected 16,189 news articles from 1006 different publications and 7,106,297 tweets with the Twitter stream API. The experiments indicate that topic coverage of Twitter is narrower and more blurry than that of the news media. In terms of sentiment dynamics, the life span and variance of sentiment on Twitter is shorter and smaller than in the news. In addition, we observe that news articles focus more on event-related entities such as person, organization and location, whereas Twitter covers more time-oriented entities. Based on the results, we report on the characteristics of Twitter and news media as two distinct news outlets in terms of content coverage and sentiment dynamics. Â© The Author(s) 2015.","The present study investigates topic coverage and sentiment dynamics of two different media sources, Twitter and news publications, on the hot health issue of Ebola. We conduct content and sentiment analysis by: applying vocabulary control to collected datasets; employing the n-gram LDA topic modeling technique; adopting entity extraction and entity network; and introducing the concept of topic-based sentiment scores. With the query term 'Ebola' or 'Ebola virus', we collected 16,189 news articles from 1006 different publications and 7,106,297 tweets with the Twitter stream API. The experiments indicate that topic coverage of Twitter is narrower and more blurry than that of the news media. In terms of sentiment dynamics, the life span and variance of sentiment on Twitter is shorter and smaller than in the news. In addition, we observe that news articles focus more on event-related entities such as person, organization and location, whereas Twitter covers more time-oriented entities. Based on the results, we report on the characteristics of Twitter and news media as two distinct news outlets in terms of content coverage and sentiment dynamics."
Exploiting semantics for searching agricultural bibliographic data,"Filtering and search mechanisms which permit to identify key bibliographic references are fundamental for researchers. In this paper we propose a fully automatic and semantic method for filtering/searching bibliographic data, which allows users to look for information by specifying simple keyword queries or document queries, i.e. by simply submitting existing documents to the system. The limitations of standard techniques, based on either syntactical text search and on manually assigned descriptors, are overcome by considering the semantics intrinsically associated to the document/query terms; to this aim, we exploit different kinds of external knowledge sources (both general and specific domain dictionaries or thesauri). The proposed techniques have been developed and successfully tested for agricultural bibliographic data, which play a central role to enable researchers and policy makers to retrieve related agricultural and scientific information by using the AGROVOC thesaurus. Â© The Author(s) 2015.","Filtering and search mechanisms which permit to identify key bibliographic references are fundamental for researchers. In this paper we propose a fully automatic and semantic method for filtering/searching bibliographic data, which allows users to look for information by specifying simple keyword queries or document queries, by simply submitting existing documents to the system. The limitations of standard techniques, based on either syntactical text search and on manually assigned descriptors, are overcome by considering the semantics intrinsically associated to the document/query terms; to this aim, we exploit different kinds of external knowledge sources (both general and specific domain dictionaries or thesauri). The proposed techniques have been developed and successfully tested for agricultural bibliographic data, which play a central role to enable researchers and policy makers to retrieve related agricultural and scientific information by using the AGROVOC thesaurus."
Proposal reviewer recommendation system based on big data for a national research management institute,"National research management organizations need to ensure that research proposals are reviewed fairly and efficiently, which requires the selection of suitable reviewers. In particular, reviewing research proposals in a particular area necessitates the selection of a group with the most reasonable standard for recommending an expert in that area. In this study, we develop an automatic matching system that matches a research proposal with a reviewer who can evaluate it most effectively, using keywords with fuzzy weights based on databases in the corresponding field of research. All functions that we developed were based on the MapReduce framework created by Hadoop, which was verified to enhance matching performance and ensure expandability. This enabled us to select suitable researchers from existing research projects, papers and research reviewer databases. Our system can influence the operation of the national research management system and contribute to academic development. Â© Chartered Institute of Library and Information Professionals.","National research management organizations need to ensure that research proposals are reviewed fairly and efficiently, which requires the selection of suitable reviewers. In particular, reviewing research proposals in a particular area necessitates the selection of a group with the most reasonable standard for recommending an expert in that area. In this study, we develop an automatic matching system that matches a research proposal with a reviewer who can evaluate it most effectively, using keywords with fuzzy weights based on databases in the corresponding field of research. All functions that we developed were based on the MapReduce framework created by Hadoop, which was verified to enhance matching performance and ensure expandability. This enabled us to select suitable researchers from existing research projects, papers and research reviewer databases. Our system can influence the operation of the national research management system and contribute to academic development."
A refined twig-join swift query algorithm for diversification issues of XML,"Compiling documents in extensible markup language (XML) plays an important role in accessing data services. An efficient query service should be based on a skillful representation that can support query diversification and solve ambiguity in order to improve high-precision search capabilities. However, to the best of our knowledge, research on query diversification, target hierarchical level and the problem of ambiguity is insufficient. In this study we aimed to solve these problems so that the results are able not only to satisfy query diversification, but also to offer better precision compared with the existing twig join algorithms. An extended twig join Swift (TJSwift) associated with adjacent linked lists for the provision of efficient XML query services is also proposed, whereby queries can be versatile in terms of predicates. It can completely preserve hierarchical information; in addition, the new index generated from XML is used to save semantic information. Â© The Author(s) 2015.","Compiling documents in extensible markup language plays an important role in accessing data services. An efficient query service should be based on a skillful representation that can support query diversification and solve ambiguity in order to improve high-precision search capabilities. However, to the best of our knowledge, research on query diversification, target hierarchical level and the problem of ambiguity is insufficient. In this study we aimed to solve these problems so that the results are able not only to satisfy query diversification, but also to offer better precision compared with the existing twig join algorithms. An extended twig join Swift (TJSwift) associated with adjacent linked lists for the provision of efficient XML query services is also proposed, whereby queries can be versatile in terms of predicates. It can completely preserve hierarchical information; in addition, the new index generated from XML is used to save semantic information."
The effects of textual environment on reading comprehension: Implications for searching as learning,"This paper reports on a study of digital reading that investigates the effects of different textual environments on information interaction and comprehension outcomes. While there is a large body of literature that compares print and digital reading, research that compares differently designed digital reading environments is limited. Such work can inform the design of information and search systems intended to support learning. This study investigated the effects of two design dimensions: Text Presentation (Plain Text vs In-Context) and Interactivity (availability of Reading Tools). Results show that the simplest textual environment (Plain Text presentation with no Interactivity) was associated with the highest comprehension outcomes, but that Interactivity mitigated the negative effects of texts presented In-Context. Both time spent reading and certain reading behaviours varied to some extent by condition and may be associated with comprehension; however, personal characteristics of the readers played little to no role in determining outcomes. Â© 2016 Chartered Institute of Library and Information Professionals.","This paper reports on a study of digital reading that investigates the effects of different textual environments on information interaction and comprehension outcomes. While there is a large body of literature that compares print and digital reading, research that compares differently designed digital reading environments is limited. Such work can inform the design of information and search systems intended to support learning. This study investigated the effects of two design dimensions: Text Presentation (Plain Text vs In-Context) and Interactivity (availability of Reading Tools). Results show that the simplest textual environment (Plain Text presentation with no Interactivity) was associated with the highest comprehension outcomes, but that Interactivity mitigated the negative effects of texts presented In-Context. Both time spent reading and certain reading behaviours varied to some extent by condition and may be associated with comprehension; however, personal characteristics of the readers played little to no role in determining outcomes."
Modelling trust networks using resistive circuits for trust-aware recommender systems,"Recommender systems have been widely used for predicting unknown ratings. Collaborative filtering as a recommendation technique uses known ratings for predicting user preferences in the item selection. However, current collaborative filtering methods cannot distinguish malicious users from unknown users. Also, they have serious drawbacks in generating ratings for cold-start users. Trust networks among recommender systems have been proved beneficial to improve the quality and number of predictions. This paper proposes an improved trust-aware recommender system that uses resistive circuits for trust inference. This method uses trust information to produce personalized recommendations. The result of evaluating the proposed method on Epinions dataset shows that this method can significantly improve the accuracy of recommender systems while not reducing the coverage of recommender systems. Â© The Author(s) 2015.","Recommender systems have been widely used for predicting unknown ratings. Collaborative filtering as a recommendation technique uses known ratings for predicting user preferences in the item selection. However, current collaborative filtering methods cannot distinguish malicious users from unknown users. Also, they have serious drawbacks in generating ratings for cold-start users. Trust networks among recommender systems have been proved beneficial to improve the quality and number of predictions. This paper proposes an improved trust-aware recommender system that uses resistive circuits for trust inference. This method uses trust information to produce personalized recommendations. The result of evaluating the proposed method on Epinions dataset shows that this method can significantly improve the accuracy of recommender systems while not reducing the coverage of recommender systems."
The impact of indexing approaches on Arabic text classification,"This paper investigates the impact of using different indexing approaches (full-word, stem, and root) when classifying Arabic text. In this study, the naÃ¯ve Bayes classifier is used to construct the multinomial classification models and is evaluated using stratified k-fold cross-validation (k ranges from 2 to 10). It is also uses a corpus that consists of 1000 normalized Arabic documents. The results of one experiment in this study show that significant accuracy improvements have occurred when the full-word form is used in most k-folds. Further experiments show that the classifier has achieved the highest accuracy in the eight-fold by using 7/8-1/8 train-test ratio, despite the indexing approach being used. The overall results of this study show that the classifier has achieved the maximum micro-average accuracy 99.36%, either by using the full-word form or the stem form. This proves that the stem is a better choice to use when classifying Arabic text, because it makes the corpus dataset smaller and this will enhance both the processing time and storage utilization, and achieve the highest level of accuracy. Â© Chartered Institute of Library and Information Professionals.","This paper investigates the impact of using different indexing approaches (full-word, stem, and root) when classifying Arabic text. In this study, the nave Bayes classifier is used to construct the multinomial classification models and is evaluated using stratified k-fold cross-validation (k ranges from 2 to 10). It is also uses a corpus that consists of 1000 normalized Arabic documents. The results of one experiment in this study show that significant accuracy improvements have occurred when the full-word form is used in most k-folds. Further experiments show that the classifier has achieved the highest accuracy in the eight-fold by using 7/8-1/8 train-test ratio, despite the indexing approach being used. The overall results of this study show that the classifier has achieved the maximum micro-average accuracy 99.36%, either by using the full-word form or the stem form. This proves that the stem is a better choice to use when classifying Arabic text, because it makes the corpus dataset smaller and this will enhance both the processing time and storage utilization, and achieve the highest level of accuracy."
Common neighbour similarity-based approach to support intimacy measurement in social networks,"A large amount of social data is being generated every day, as the Internet becomes more pervasive and mobile devices more ubiquitous. Accordingly, Internet users often experience difficulty finding the content they want, resulting in the popularity of personalized services that provide user-customized content. Intimacy between users of social network services can be utilized as a foundational technology for personalized services. In this paper, an intimacy measurement method for social networking services based on common neighbour similarity is proposed. The proposed method uses the link relationship between users for intimacy measurements and can be applied to general users. Further, it promotes easy data collection using publicly available data. To evaluate the proposed intimacy measurement method experimentally, a significant amount of user data was collected from Twitter. In addition, various statistical datasets were presented, and regression analyses conducted on graphs extracted from user data were collected to interpret the meaning of the intimacy index measured using the proposed method with existing social networking services. Â© Chartered Institute of Library and Information Professionals.","A large amount of social data is being generated every day, as the Internet becomes more pervasive and mobile devices more ubiquitous. Accordingly, Internet users often experience difficulty finding the content they want, resulting in the popularity of personalized services that provide user-customized content. Intimacy between users of social network services can be utilized as a foundational technology for personalized services. In this paper, an intimacy measurement method for social networking services based on common neighbour similarity is proposed. The proposed method uses the link relationship between users for intimacy measurements and can be applied to general users. Further, it promotes easy data collection using publicly available data. To evaluate the proposed intimacy measurement method experimentally, a significant amount of user data was collected from Twitter. In addition, various statistical datasets were presented, and regression analyses conducted on graphs extracted from user data were collected to interpret the meaning of the intimacy index measured using the proposed method with existing social networking services."
Email thread identification using latent Dirichlet allocation and non-negative matrix factorization based clustering techniques,"Emails are the most popular and effective way of communicating over the internet. A number of applications are available today for computers and mobile devices for email messaging. Email messaging is constantly getting more popular and, as a result, numbers of sent and received emails are also increasing. It is very difficult for a user to remember emails and relate newer incoming emails to previous communications made on similar topics. Email threads provide a mechanism using which a user can obtain sequences of emails for a particular set of communication in a time frame and provides a number of benefits to users. In this work two email thread identification algorithms based on a nested textual clustering approach are presented. The work is planned in two stages; in the first stage two popular text clustering approaches, latent Dirichlet allocation and non-negative matrix factorization, are applied over the email messages to form the email clusters. Then in the second stage, clustering is again performed over the created email clusters to identify the email threads using threading features. Performance parameters like accuracy, precision, recall and F-measure are evaluated for the presented thread identification algorithms. Â© Chartered Institute of Library and Information Professionals.","Emails are the most popular and effective way of communicating over the internet. A number of applications are available today for computers and mobile devices for email messaging. Email messaging is constantly getting more popular and, as a result, numbers of sent and received emails are also increasing. It is very difficult for a user to remember emails and relate newer incoming emails to previous communications made on similar topics. Email threads provide a mechanism using which a user can obtain sequences of emails for a particular set of communication in a time frame and provides a number of benefits to users. In this work two email thread identification algorithms based on a nested textual clustering approach are presented. The work is planned in two stages; in the first stage two popular text clustering approaches, latent Dirichlet allocation and non-negative matrix factorization, are applied over the email messages to form the email clusters. Then in the second stage, clustering is again performed over the created email clusters to identify the email threads using threading features. Performance parameters like accuracy, precision, recall and F-measure are evaluated for the presented thread identification algorithms."
QSem: A novel question representation framework for question matching over accumulated question-answer data,"This paper proposes a novel question representation framework to assist automated question answering through reusing accumulated question-answer data. The framework, named QSem, defines three types of question words - question-target words, user-oriented words and irrelevant words, along with semantic patterns, for representing a question. The question word types are semantically labelled by a pre-defined ontology to enrich the semantic representation of questions. The semantic patterns through equivalent pattern linking enhance normal structure matching aiming at improving question matching performance. We trained QSem on 400 randomly selected questions with semantic patterns and obtained optimized parameters. After that, 5000 questions from our system were tested and the precision of question matching was between 0.71 and 0.93 with respect to various generators, indicating the stability of the approach. We further compared our approach with Cosine similarity, WordNet-based semantic similarity and IBM translation model on a standard TREC dataset containing 5536 questions. The results presented that our approach achieved best performance with mean reciprocal rank increased by 7.2% and accuracy increased by 7.5% on average, demonstrating the effectiveness of the approach. Â© Chartered Institute of Library and Information Professionals.","This paper proposes a novel question representation framework to assist automated question answering through reusing accumulated question-answer data. The framework, named QSem, defines three types of question words - question-target words, user-oriented words and irrelevant words, along with semantic patterns, for representing a question. The question word types are semantically labelled by a pre-defined ontology to enrich the semantic representation of questions. The semantic patterns through equivalent pattern linking enhance normal structure matching aiming at improving question matching performance. We trained QSem on 400 randomly selected questions with semantic patterns and obtained optimized parameters. After that, 5000 questions from our system were tested and the precision of question matching was between 0.71 and 0.93 with respect to various generators, indicating the stability of the approach. We further compared our approach with Cosine similarity, WordNet-based semantic similarity and IBM translation model on a standard TREC dataset containing 5536 questions. The results presented that our approach achieved best performance with mean reciprocal rank increased by 7.2% and accuracy increased by 7.5% on average, demonstrating the effectiveness of the approach."
OLFinder: Finding opinion leaders in online social networks,"Opinion leaders are the influential people who are able to shape the minds and thoughts of other people in their society. Finding opinion leaders is an important task in various domains ranging from marketing to politics. In this paper, a new effective algorithm for finding opinion leaders in a given domain in online social networks is introduced. The proposed algorithm, named OLFinder, detects the main topics of discussion in a given domain, calculates a competency and a popularity score for each user in the given domain, then calculates a probability for being an opinion leader in that domain by using the competency and the popularity scores and finally ranks the users of the social network based on their probability of being an opinion leader. Our experimental results show that OLFinder outperforms other methods based on precision-recall, average precision and P@N measures. Â© Chartered Institute of Library and Information Professionals.","Opinion leaders are the influential people who are able to shape the minds and thoughts of other people in their society. Finding opinion leaders is an important task in various domains ranging from marketing to politics. In this paper, a new effective algorithm for finding opinion leaders in a given domain in online social networks is introduced. The proposed algorithm, named OLFinder, detects the main topics of discussion in a given domain, calculates a competency and a popularity score for each user in the given domain, then calculates a probability for being an opinion leader in that domain by using the competency and the popularity scores and finally ranks the users of the social network based on their probability of being an opinion leader. Our experimental results show that OLFinder outperforms other methods based on precision-recall, average precision and P@N measures."
'An intensity around information': The changing face of chemical information literacy,"The changing nature of chemical information literacy over 50 years is examined by a comparison of a number of guides to chemical literature and information. It is concluded that: an understanding of the world of information is the sole aspect to have remained important and essentially unchanged over time; that knowledge of sources, ability to access information and ability to organize information have been of importance throughout, but have changed their nature dramatically; and that evaluation of information has gained in importance since the advent of the World Wide Web. The link between chemical structure and corresponding substance information is the most significant threshold concept. Information literacy in chemistry is strongly subject-specific. Â© The Author(s) 2015.","The changing nature of chemical information literacy over 50 years is examined by a comparison of a number of guides to chemical literature and information. It is concluded that: an understanding of the world of information is the sole aspect to have remained important and essentially unchanged over time; that knowledge of sources, ability to access information and ability to organize information have been of importance throughout, but have changed their nature dramatically; and that evaluation of information has gained in importance since the advent of the World Wide Web. The link between chemical structure and corresponding substance information is the most significant threshold concept. Information literacy in chemistry is strongly subject-specific."
How well does Google work with Persian documents?,"The performance evaluation of an information retrieval system is a decisive aspect of the measure of the improvements in search technology. The Google search engine, as a tool for retrieving information on the Web, is used by almost 92% of Iranian users. The purpose of this paper is to study Google's performance in retrieving relevant information from Persian documents. The information retrieval effectiveness is based on the precision measures of the search results done to a website that we have built with the documents of a TREC standard corpus. We asked Google for 100 topics available on the corpus and we compared the retrieved webpages with the relevant documents. The obtained results indicated that the morphological analysis of the Persian language is not fully taken into account by the Google search engine. The incorrect text tokenisation, considering the stop words as the content keywords of a document and the wrong 'variants encountered' of words found by Google are the main reasons that affect the relevance of the Persian information retrieval on the Web for this search engine. Â© Chartered Institute of Library and Information Professionals.","The performance evaluation of an information retrieval system is a decisive aspect of the measure of the improvements in search technology. The Google search engine, as a tool for retrieving information on the Web, is used by almost 92% of Iranian users. The purpose of this paper is to study Google's performance in retrieving relevant information from Persian documents. The information retrieval effectiveness is based on the precision measures of the search results done to a website that we have built with the documents of a TREC standard corpus. We asked Google for 100 topics available on the corpus and we compared the retrieved webpages with the relevant documents. The obtained results indicated that the morphological analysis of the Persian language is not fully taken into account by the Google search engine. The incorrect text tokenisation, considering the stop words as the content keywords of a document and the wrong 'variants encountered' of words found by Google are the main reasons that affect the relevance of the Persian information retrieval on the Web for this search engine."
TTC-3600: A new benchmark dataset for Turkish text categorization,"Owing to the rapid growth of the World Wide Web, the number of documents that can be accessed via the Internet explosively increases with each passing day. Considering news portals in particular, sometimes documents related to categories such as technology, sports and politics seem to be in the wrong category or documents are located in a generic category called others. At this point, text categorization (TC), which is generally addressed as a supervised learning task is needed. Although there are substantial number of studies conducted on TC in other languages, the number of studies conducted in Turkish is very limited owing to the lack of accessibility and usability of datasets created. In this paper, a new dataset named TTC-3600, which can be widely used in studies of TC of Turkish news and articles, is created. TTC-3600 is a well-documented dataset and its file formats are compatible with well-known text mining tools. Five widely used classifiers within the field of TC and two feature selection methods are evaluated on TTC-3600. The experimental results indicate that the best accuracy criterion value 91.03% is obtained with the combination of Random Forest classifier and attribute ranking-based feature selection method in all comparisons performed after pre-processing and feature selection steps. The publicly available TTC-3600 dataset and the experimental results of this study can be utilized in comparative experiments by other researchers. Â© Chartered Institute of Library and Information Professionals.","Owing to the rapid growth of the World Wide Web, the number of documents that can be accessed via the Internet explosively increases with each passing day. Considering news portals in particular, sometimes documents related to categories such as technology, sports and politics seem to be in the wrong category or documents are located in a generic category called others. At this point, text categorization (TC), which is generally addressed as a supervised learning task is needed. Although there are substantial number of studies conducted on TC in other languages, the number of studies conducted in Turkish is very limited owing to the lack of accessibility and usability of datasets created. In this paper, a new dataset named TTC-3600, which can be widely used in studies of TC of Turkish news and articles, is created. TTC-3600 is a well-documented dataset and its file formats are compatible with well-known text mining tools. Five widely used classifiers within the field of TC and two feature selection methods are evaluated on TTC-3600. The experimental results indicate that the best accuracy criterion value 91.03% is obtained with the combination of Random Forest classifier and attribute ranking-based feature selection method in all comparisons performed after pre-processing and feature selection steps. The publicly available TTC-3600 dataset and the experimental results of this study can be utilized in comparative experiments by other researchers."
Information as causality: An approach to a general theory of information,"Although various approaches have been proposed throughout history, information, as one of the most fundamental elements in the world, does not have a general definition or theory that is acceptable to all disciplines. The biggest challenge is the unification of objective and subjective views, because they represent very different characteristics of information which are difficult to integrate into a single framework. We argue that the key to bridging the gap between objective and subjective views of information is a proper understanding of intelligence, because it gives rise to subjective experiences and assigns meaning to things. The purpose of this research is to explore possibilities and implications of applying neuroscience theory in the discussion of information. By incorporating the memory-prediction framework of intelligence developed by Jeff Hawkins, we propose causality to be the general definition of information, and the combination of 'Physical Representations of Mental Patterns' and 'Physical Representations of Physical Patterns' to be the restricted definition in social contexts. With both general and restricted definitions clarified, we then discuss a few cases of information use and the implications of our approach. Â© The Author(s) 2015.","Although various approaches have been proposed throughout history, information, as one of the most fundamental elements in the world, does not have a general definition or theory that is acceptable to all disciplines. The biggest challenge is the unification of objective and subjective views, because they represent very different characteristics of information which are difficult to integrate into a single framework. We argue that the key to bridging the gap between objective and subjective views of information is a proper understanding of intelligence, because it gives rise to subjective experiences and assigns meaning to things. The purpose of this research is to explore possibilities and implications of applying neuroscience theory in the discussion of information. By incorporating the memory-prediction framework of intelligence developed by Jeff Hawkins, we propose causality to be the general definition of information, and the combination of 'Physical Representations of Mental Patterns' and 'Physical Representations of Physical Patterns' to be the restricted definition in social contexts. With both general and restricted definitions clarified, we then discuss a few cases of information use and the implications of our approach."
The information environment and information behaviour of the Offshore Installation Manager (OIM) in the context of safety and emergency response: An exploratory study,"The offshore installation manager (OIM) is a unique role in the oil and gas industry with the legal responsibility for the health and safety of individuals on an offshore installation, as well as holding commercial responsibilities. Using exploratory, qualitative data based on 10 interviews conducted with OIMs, the information environment and behaviour of the OIM are described and areas for further research are explored. The OIM's information environment is one that is complex and relies heavily on both formal and informal sources of information. Two modes of OIM information behaviour are identified: everyday information need, in which the OIM seeks, uses and shares information to maintain safe operations; and emergency information need, in which there is both reliance on information that must be known in order to react to an emergency situation and a need for accessible information about the status of a rapidly changing environment. The OIM is both the user of information and a source of information for others and as such must be trusted, reliable and automotive. Â© The Author(s) 2015.","The offshore installation manager (OIM) is a unique role in the oil and gas industry with the legal responsibility for the health and safety of individuals on an offshore installation, as well as holding commercial responsibilities. Using exploratory, qualitative data based on 10 interviews conducted with OIMs, the information environment and behaviour of the OIM are described and areas for further research are explored. The OIM's information environment is one that is complex and relies heavily on both formal and informal sources of information. Two modes of OIM information behaviour are identified: everyday information need, in which the OIM seeks, uses and shares information to maintain safe operations; and emergency information need, in which there is both reliance on information that must be known in order to react to an emergency situation and a need for accessible information about the status of a rapidly changing environment. The OIM is both the user of information and a source of information for others and as such must be trusted, reliable and automotive."
Topic modelling for qualitative studies,"Qualitative studies, such as sociological research, opinion analysis and media studies, can benefit greatly from automated topic mining provided by topic models such as latent Dirichlet allocation (LDA). However, examples of qualitative studies that employ topic modelling as a tool are currently few and far between. In this work, we identify two important problems along the way to using topic models in qualitative studies: lack of a good quality metric that closely matches human judgement in understanding topics and the need to indicate specific subtopics that a specific qualitative study may be most interested in mining. For the first problem, we propose a new quality metric, tf-idf coherence, that reflects human judgement more accurately than regular coherence, and conduct an experiment to verify this claim. For the second problem, we propose an interval semi-supervised approach (ISLDA) where certain predefined sets of keywords (that define the topics researchers are interested in) are restricted to specific intervals of topic assignments. Our experiments show that ISLDA is better for topic extraction than LDA in terms of tf-idf coherence, number of topics identified to predefined keywords and topic stability. We also present a case study on a Russian LiveJournal dataset aimed at ethnicity discourse analysis. Â© The Author(s) 2015.","Qualitative studies, such as sociological research, opinion analysis and media studies, can benefit greatly from automated topic mining provided by topic models such as latent Dirichlet allocation (LDA). However, examples of qualitative studies that employ topic modelling as a tool are currently few and far between. In this work, we identify two important problems along the way to using topic models in qualitative studies: lack of a good quality metric that closely matches human judgement in understanding topics and the need to indicate specific subtopics that a specific qualitative study may be most interested in mining. For the first problem, we propose a new quality metric, tf-idf coherence, that reflects human judgement more accurately than regular coherence, and conduct an experiment to verify this claim. For the second problem, we propose an interval semi-supervised approach (ISLDA) where certain predefined sets of keywords (that define the topics researchers are interested in) are restricted to specific intervals of topic assignments. Our experiments show that ISLDA is better for topic extraction than LDA in terms of tf-idf coherence, number of topics identified to predefined keywords and topic stability. We also present a case study on a Russian LiveJournal dataset aimed at ethnicity discourse analysis."
A language-model-based approach for subjectivity detection,"The rapid growth of opinionated text on the Web increases the demand for efficient methods for detecting subjective texts. In this paper, a subjectivity detection method is proposed which utilizes a language-model-based structure to define a subjectivity score for each document where the topic relevance of documents does not affect the subjectivity scores. In order to overcome the limited content in short documents, we further propose an expansion method to better estimate the language models. Since the lack of linguistic resources in resource-lean languages like Persian makes subjectivity detection difficult in these languages, the method is proposed in two versions: a semi-supervised version for resource-lean languages and a supervised version. Experimental evaluations on five datasets in two languages, English and Persian, demonstrate that the method performs well in distinguishing subjective documents from objective ones in both languages. Â© Chartered Institute of Library and Information Professionals.","The rapid growth of opinionated text on the Web increases the demand for efficient methods for detecting subjective texts. In this paper, a subjectivity detection method is proposed which utilizes a language-model-based structure to define a subjectivity score for each document where the topic relevance of documents does not affect the subjectivity scores. In order to overcome the limited content in short documents, we further propose an expansion method to better estimate the language models. Since the lack of linguistic resources in resource-lean languages like Persian makes subjectivity detection difficult in these languages, the method is proposed in two versions: a semi-supervised version for resource-lean languages and a supervised version. Experimental evaluations on five datasets in two languages, English and Persian, demonstrate that the method performs well in distinguishing subjective documents from objective ones in both languages."
DSont: DSpace to ontology transformation,"Semantic web facilitates the effective sharing and reuse of existing information. Institutional repositories (IRs) are built to organize and manage the intellectual output of an institute. They generally use relational databases for maintaining metadata of digital documents. The focus of this research is to share the information of an existing IR with other information systems for discovering common interests. To process the data in semantic context, a relational database needs to be transformed into an ontology. The existing relation to ontology transformation systems produces odd results if they are applied on an IR database because its schema is meta-schema. The proposed system first creates an intermediate database, having a normalized schema for the data model of an institute preserved in an IR database and then transforms it into an ontology. Finally semantic correspondence is established between entities of source and target ontologies in order to integrate them. The system has been implemented and evaluated for its correct and lossless transformation. The results demonstrate that the transformation is correct and the information is preserved. Â© Chartered Institute of Library and Information Professionals.","Semantic web facilitates the effective sharing and reuse of existing information. Institutional repositories (IRs) are built to organize and manage the intellectual output of an institute. They generally use relational databases for maintaining metadata of digital documents. The focus of this research is to share the information of an existing IR with other information systems for discovering common interests. To process the data in semantic context, a relational database needs to be transformed into an ontology. The existing relation to ontology transformation systems produces odd results if they are applied on an IR database because its schema is meta-schema. The proposed system first creates an intermediate database, having a normalized schema for the data model of an institute preserved in an IR database and then transforms it into an ontology. Finally semantic correspondence is established between entities of source and target ontologies in order to integrate them. The system has been implemented and evaluated for its correct and lossless transformation. The results demonstrate that the transformation is correct and the information is preserved."
Linking for influence: Twitter linked content in the Scottish Referendum televised debates,"Twitter, the micro-blogging social media tool, has established a critical role in facilitating social engagement. Its low technical and economic barriers to uptake provide a readily accessible forum for public engagement with events such as televised political debates, and in this context provides a 'backchannel' to mainstream media, allowing users to comment on and engage in debates. Most recently during the 2014 Scottish Referendum, Twitter was used extensively by both 'Better Together' (pro-Unionist) and 'Yes' (pro-independence) campaigners. The aim of this research was to develop an understanding of the linked content present in tweets sent during three televised debates on the issue of Scottish Independence. Analysis of the linked content shows a broad subject proximity to the topics under discussion during the debates, but highlights the lack of specificity in relation to the peaks and troughs of Twitter traffic during the debates. The paper also highlights the use made of links to a variety of resources such as the mainstream media as well as more informal sources including user-generated image and video content to support political viewpoints, and argues that, while the use of such content is beneficial in terms of unifying perspectives, supporter activism and the gratification of the social need for connectivity, it does not act to convert political opinion. Â© The Author(s) 2016.","Twitter, the micro-blogging social media tool, has established a critical role in facilitating social engagement. Its low technical and economic barriers to uptake provide a readily accessible forum for public engagement with events such as televised political debates, and in this context provides a 'backchannel' to mainstream media, allowing users to comment on and engage in debates. Most recently during the 2014 Scottish Referendum, Twitter was used extensively by both 'Better Together' (pro-Unionist) and 'Yes' (pro-independence) campaigners. The aim of this research was to develop an understanding of the linked content present in tweets sent during three televised debates on the issue of Scottish Independence. Analysis of the linked content shows a broad subject proximity to the topics under discussion during the debates, but highlights the lack of specificity in relation to the peaks and troughs of Twitter traffic during the debates. The paper also highlights the use made of links to a variety of resources such as the mainstream media as well as more informal sources including user-generated image and video content to support political viewpoints, and argues that, while the use of such content is beneficial in terms of unifying perspectives, supporter activism and the gratification of the social need for connectivity, it does not act to convert political opinion."
Information and the gaining of understanding,"It is suggested that, in addition to data, information and knowledge, the information sciences should focus on understanding, understood as a higher-order knowledge, with coherent and explanatory potential. The limited ways in which understanding has been addressed in the design of information systems, in studies of information behaviour, in formulations of information literacy and in impact studies are briefly reviewed, and future prospects considered. The paper is an extended version of a keynote presentation given at the i3 conference in June 2015. Â© The Author(s) 2016.","It is suggested that, in addition to data, information and knowledge, the information sciences should focus on understanding, understood as a higher-order knowledge, with coherent and explanatory potential. The limited ways in which understanding has been addressed in the design of information systems, in studies of information behaviour, in formulations of information literacy and in impact studies are briefly reviewed, and future prospects considered. The paper is an extended version of a keynote presentation given at the i3 conference in June 2015."
A stream-based method to detect differences between XML documents,"Detecting differences between XML documents is one of most important research topics for XML. Since XML documents are generally considered to be organized in a tree structure, most previous research has attempted to detect differences using tree-matching algorithms. However, most tree-matching algorithms have inadequate performance owing to limitations in terms of the execution time, optimality and scalability. This study proposes a stream-based difference detection method in which an XML binary encoding algorithm is used to provide improved performance relative to that of previous tree-matching algorithms. A tree-structured analysis of XML is not essential in order to detect differences. We use a D-Path algorithm that has an optimal result quality for difference detection between two streams and has a lower time complexity than tree-based methods. We then modify the existing XML binary encoding method to tokenize the stream and the algorithm in order to support more operations than D-Path algorithm does. The experimental results reveal greater efficiency for the proposed method relative to tree-based methods. The execution time is at least 4 times faster than state-of-the-art tree-based methods. In addition, the scalability is much more efficient. Â© The Author(s) 2015.","Detecting differences between XML documents is one of most important research topics for Since XML documents are generally considered to be organized in a tree structure, most previous research has attempted to detect differences using tree-matching algorithms. However, most tree-matching algorithms have inadequate performance owing to limitations in terms of the execution time, optimality and scalability. This study proposes a stream-based difference detection method in which an XML binary encoding algorithm is used to provide improved performance relative to that of previous tree-matching algorithms. A tree-structured analysis of XML is not essential in order to detect differences. We use a D-Path algorithm that has an optimal result quality for difference detection between two streams and has a lower time complexity than tree-based methods. We then modify the existing XML binary encoding method to tokenize the stream and the algorithm in order to support more operations than D-Path algorithm does. The experimental results reveal greater efficiency for the proposed method relative to tree-based methods. The execution time is at least 4 times faster than state-of-the-art tree-based methods. In addition, the scalability is much more efficient."
Evaluating open access journals using Semantic Web technologies and scorecards,"This paper describes a process to develop and publish a scorecard from an OAJ (Open Access Journal) on the Semantic Web using Linked Data technologies in such a way that it can be linked to related datasets. Furthermore, methodological guidelines are presented with activities related to each step of the process. The proposed process was applied to a university OAJ, including the definition of the KPIs (Key Performance Indicators) linked to the institutional strategies, the extraction, cleaning and loading of data from the data sources into a data mart, the transformation of data into RDF (Resource Description Framework), and the publication of data by means of a SPARQL endpoint using the Virtuoso software. Additionally, the RDF data cube vocabulary has been used to publish the multidimensional data on the Web. The visualization was made using CubeViz, a faceted browser to present the KPIs in interactive charts. Â© The Author(s) 2015.","This paper describes a process to develop and publish a scorecard from an OAJ (Open Access Journal) on the Semantic Web using Linked Data technologies in such a way that it can be linked to related datasets. Furthermore, methodological guidelines are presented with activities related to each step of the process. The proposed process was applied to a university OAJ, including the definition of the KPIs (Key Performance Indicators) linked to the institutional strategies, the extraction, cleaning and loading of data from the data sources into a data mart, the transformation of data into RDF (Resource Description Framework), and the publication of data by means of a SPARQL endpoint using the Virtuoso software. Additionally, the RDF data cube vocabulary has been used to publish the multidimensional data on the Web. The visualization was made using CubeViz, a faceted browser to present the KPIs in interactive charts."
Semantically enhanced pseudo relevance feedback for Arabic information retrieval,"The conventional information retrieval (IR) framework consists of four primary phases, namely, pre-processing, indexing, querying and retrieving results. Some phases of the current Arabic IR (AIR) framework have several drawbacks. This research aims to enhance an AIR by improving the processes in a conventional IR framework. We introduce an enhanced stop-word list in the pre-processing level and investigate several Arabic stemmers. In addition, an Arabic WordNet was utilized in the corpus and query expansion levels. We also adopted semantic information for the Pseudo Relevance Feedback. The enhanced Arabic IR framework was built and evaluated using TREC 2001 data. The technique of using the Arabic WordNet to build a semantic relationship between query and corpus in two levels, that is, the corpus and query levels, is a new one. The enhanced AIR framework demonstrated an improvement by 49% in terms of mean average precision, with an increase of 7.3% in recall compared with the baseline framework. Â© Chartered Institute of Library and Information Professionals.","The conventional information retrieval (IR) framework consists of four primary phases, namely, pre-processing, indexing, querying and retrieving results. Some phases of the current Arabic IR (AIR) framework have several drawbacks. This research aims to enhance an AIR by improving the processes in a conventional IR framework. We introduce an enhanced stop-word list in the pre-processing level and investigate several Arabic stemmers. In addition, an Arabic WordNet was utilized in the corpus and query expansion levels. We also adopted semantic information for the Pseudo Relevance Feedback. The enhanced Arabic IR framework was built and evaluated using TREC 2001 data. The technique of using the Arabic WordNet to build a semantic relationship between query and corpus in two levels, that is, the corpus and query levels, is a new one. The enhanced AIR framework demonstrated an improvement by 49% in terms of mean average precision, with an increase of 7.3% in recall compared with the baseline framework."
"When time meets information retrieval: Past proposals, current plans and future trends","With the advent of Web search and the large amount of data published on the Web sphere, a tremendous amount of documents become strongly time-dependent. In this respect, the time dimension has been extensively exploited as a highly important relevance criterion to improve the retrieval effectiveness of document ranking models. Thus, a compelling research interest is going on the temporal information retrieval realm, which gives rise to several temporal search applications. In this article, we intend to provide a scrutinizing overview of time-aware information retrieval models. We specifically put the focus on the use of timeliness and its impact on the global value of relevance as well as on the retrieval effectiveness. First, we attempt to motivate the importance of temporal signals, whenever combined with other relevance features, in accounting for document relevance. Then, we review the relevant studies standing at the crossroads of both information retrieval and time according to three common information retrieval aspects: the query level, the document content level and the document ranking model level. We organize the related temporal-based approaches around specific information retrieval tasks and regarding the task at hand, we emphasize the importance of results presentation and particularly timelines to the end user. We also report a set of relevant research trends and avenues that can be explored in the future. Â© The Author(s) 2015.","With the advent of Web search and the large amount of data published on the Web sphere, a tremendous amount of documents become strongly time-dependent. In this respect, the time dimension has been extensively exploited as a highly important relevance criterion to improve the retrieval effectiveness of document ranking models. Thus, a compelling research interest is going on the temporal information retrieval realm, which gives rise to several temporal search applications. In this article, we intend to provide a scrutinizing overview of time-aware information retrieval models. We specifically put the focus on the use of timeliness and its impact on the global value of relevance as well as on the retrieval effectiveness. First, we attempt to motivate the importance of temporal signals, whenever combined with other relevance features, in accounting for document relevance. Then, we review the relevant studies standing at the crossroads of both information retrieval and time according to three common information retrieval aspects: the query level, the document content level and the document ranking model level. We organize the related temporal-based approaches around specific information retrieval tasks and regarding the task at hand, we emphasize the importance of results presentation and particularly timelines to the end user. We also report a set of relevant research trends and avenues that can be explored in the future."
Process patterns and conceptual changes in knowledge representations during information seeking and sensemaking: A qualitative user study,"The construction of knowledge representations during sensemaking resembles meaningful learning in which conceptual changes to knowledge structure take place in various forms. Guided by a cognitive process model of sensemaking expanding prior models with ideas from learning and cognitive psychology, we conducted a qualitative user study of 15 participants with news writing and business analysis tasks to investigate the evolvement of their knowledge structures. We collected and analysed think-aloud protocols along with recorded screen activities, intermediate work products including notes and concept maps, and the final reports. Findings suggested that: (a) the sensemaking process can be viewed as composed of several iterations that fall into nine slightly varied common patterns, which make up the components of sensemaking; (b) conceptual changes fall into three broad classes - accretion, tuning and restructuring; and (c) changes in forms of representation seem to assist in sensemaking. These findings provide insights for system design that assists in sensemaking and intelligent use of information. Â© 2016 Chartered Institute of Library and Information Professionals.","The construction of knowledge representations during sensemaking resembles meaningful learning in which conceptual changes to knowledge structure take place in various forms. Guided by a cognitive process model of sensemaking expanding prior models with ideas from learning and cognitive psychology, we conducted a qualitative user study of 15 participants with news writing and business analysis tasks to investigate the evolvement of their knowledge structures. We collected and analysed think-aloud protocols along with recorded screen activities, intermediate work products including notes and concept maps, and the final reports. Findings suggested that: (a) the sensemaking process can be viewed as composed of several iterations that fall into nine slightly varied common patterns, which make up the components of sensemaking; (b) conceptual changes fall into three broad classes - accretion, tuning and restructuring; and changes in forms of representation seem to assist in sensemaking. These findings provide insights for system design that assists in sensemaking and intelligent use of information."
Students' collaborative inquiry - Relation to approaches to studying and instructional intervention,"In order to develop suitable pedagogical methods for inquiry based learning we need an increased understanding of factors that influence students' work in inquiry assignments. The aim of this study was to investigate how high school students' ways to work in a collaborative source-based writing assignment was influenced by their individual approach to studying and the teacher's instructions and guidance in the class. The respondents were 53 high school students who filled out a questionnaire regarding their work on the source based assignment and the OPPI test of their approaches to studying. A factor analysis revealed three work patterns: a collaborative, a labour intense and a subject oriented. The results showed that the collaborative pattern was related to instructional differences, while a subject oriented work pattern was typical for students with a deep approach regardless of instruction. Instructional differences and study approaches also influenced degree of challenges in the project and, to a certain extent, learning experiences. The findings show a complex interplay between personal preferences and instructional interventions in forming students' paths through source based writing assignments. Â© The Author(s) 2016.","In order to develop suitable pedagogical methods for inquiry based learning we need an increased understanding of factors that influence students' work in inquiry assignments. The aim of this study was to investigate how high school students' ways to work in a collaborative source-based writing assignment was influenced by their individual approach to studying and the teacher's instructions and guidance in the class. The respondents were 53 high school students who filled out a questionnaire regarding their work on the source based assignment and the OPPI test of their approaches to studying. A factor analysis revealed three work patterns: a collaborative, a labour intense and a subject oriented. The results showed that the collaborative pattern was related to instructional differences, while a subject oriented work pattern was typical for students with a deep approach regardless of instruction. Instructional differences and study approaches also influenced degree of challenges in the project and, to a certain extent, learning experiences. The findings show a complex interplay between personal preferences and instructional interventions in forming students' paths through source based writing assignments."
Approximate pattern matching with gap constraints,"Pattern matching is a key issue in sequential pattern mining. Many researchers now focus on pattern matching with gap constraints. However, most of these studies involve exact pattern matching problems, a special case of approximate pattern matching and a more challenging task. In this study, we introduce an approximate pattern matching problem with Hamming distance. Its objective is to compute the number of approximate occurrences of pattern P with gap constraints in sequence S under similarity constraint d. We propose an efficient algorithm named Single-rOot Nettree for approximate pattern matchinG with gap constraints (SONG) based on a new non-linear data structure Single-root Nettree to effectively solve the problem. Theoretical analysis and experiments demonstrate an interesting law that the ratio M(P,S,d)/N(P,S,m) approximately follows a binomial distribution, where M(P,S,d) and N(P,S,m) are the numbers of the approximate occurrences whose distances to pattern P are d (0â¤dâ¤m) and no more than m (the length of pattern P), respectively. Experimental results for real biological data validate the efficiency and effectiveness of SONG. Â© Chartered Institute of Library and Information Professionals.","Pattern matching is a key issue in sequential pattern mining. Many researchers now focus on pattern matching with gap constraints. However, most of these studies involve exact pattern matching problems, a special case of approximate pattern matching and a more challenging task. In this study, we introduce an approximate pattern matching problem with Hamming distance. Its objective is to compute the number of approximate occurrences of pattern P with gap constraints in sequence S under similarity constraint We propose an efficient algorithm named Single-rOot Nettree for approximate pattern matchinG with gap constraints (SONG) based on a new non-linear data structure Single-root Nettree to effectively solve the problem. Theoretical analysis and experiments demonstrate an interesting law that the ratio M(P,S,d)/N(P,S,m) approximately follows a binomial distribution, where M(P,S,d) and N(P,S,m) are the numbers of the approximate occurrences whose distances to pattern P are d (0dm) and no more than m (the length of pattern P), respectively. Experimental results for real biological data validate the efficiency and effectiveness of SONG."
A user-oriented semantic annotation approach to knowledge acquisition and conversion,"Semantic annotation on natural language texts labels the meaning of an annotated element in specific contexts, and thus is an essential procedure for domain knowledge acquisition. An extensible and coherent annotation method is crucial for knowledge engineers to reduce human efforts to keep annotations consistent. This article proposes a comprehensive semantic annotation approach supported by a user-oriented markup language named UOML to enhance annotation efficiency with the aim of building a high quality knowledge base. UOML is operable by human annotators and convertible to formal knowledge representation languages. A pattern-based annotation conversion method named PAC is further proposed for knowledge exchange by utilizing automatic pattern learning. We designed and implemented a semantic annotation platform Annotation Assistant to test the effectiveness of the approach. By applying this platform in a long-term international research project for more than three years aiming at high quality knowledge acquisition from a classical Chinese poetry corpus containing 52,621 Chinese characters, we effectively acquired 150,624 qualified annotations. Our test shows that the approach has improved operational efficiency by 56.8%, on average, compared with text-based manual annotation. By using UOML, PAC achieved a conversion error ratio of 0.2% on average, significantly improving the annotation consistency compared with baseline annotations. The results indicate the approach is feasible for practical use in knowledge acquisition and conversion. Â© Chartered Institute of Library and Information Professionals.","Semantic annotation on natural language texts labels the meaning of an annotated element in specific contexts, and thus is an essential procedure for domain knowledge acquisition. An extensible and coherent annotation method is crucial for knowledge engineers to reduce human efforts to keep annotations consistent. This article proposes a comprehensive semantic annotation approach supported by a user-oriented markup language named UOML to enhance annotation efficiency with the aim of building a high quality knowledge base. UOML is operable by human annotators and convertible to formal knowledge representation languages. A pattern-based annotation conversion method named PAC is further proposed for knowledge exchange by utilizing automatic pattern learning. We designed and implemented a semantic annotation platform Annotation Assistant to test the effectiveness of the approach. By applying this platform in a long-term international research project for more than three years aiming at high quality knowledge acquisition from a classical Chinese poetry corpus containing 52,621 Chinese characters, we effectively acquired 150,624 qualified annotations. Our test shows that the approach has improved operational efficiency by 56.8%, on average, compared with text-based manual annotation. By using UOML, PAC achieved a conversion error ratio of 0.2% on average, significantly improving the annotation consistency compared with baseline annotations. The results indicate the approach is feasible for practical use in knowledge acquisition and conversion."
SMS spam filtering and thread identification using bi-level text classification and clustering techniques,"SMS spam detection is an important task where spam SMS messages are identified and filtered. As greater numbers of SMS messages are communicated every day, it is very difficult for a user to remember and correlate the newer SMS messages received in context to previously received SMS. SMS threads provide a solution to this problem. In this work the problem of SMS spam detection and thread identification is discussed and a state of the art clustering-based algorithm is presented. The work is planned in two stages. In the first stage the binary classification technique is applied to categorize SMS messages into two categories namely, spam and non-spam SMS; then, in the second stage, SMS clusters are created for non-spam SMS messages using non-negative matrix factorization and K-means clustering techniques. A threading-based similarity feature, that is, time between consecutive communications, is described for the identification of SMS threads, and the impact of the time threshold in thread identification is also analysed experimentally. Performance parameters like accuracy, precision, recall and F-measure are also evaluated. The SMS threads identified in this proposed work can be used in applications like SMS thread summarization, SMS folder classification and other SMS management-related tasks. Â© The Author(s) 2015.","SMS spam detection is an important task where spam SMS messages are identified and filtered. As greater numbers of SMS messages are communicated every day, it is very difficult for a user to remember and correlate the newer SMS messages received in context to previously received SMS. SMS threads provide a solution to this problem. In this work the problem of SMS spam detection and thread identification is discussed and a state of the art clustering-based algorithm is presented. The work is planned in two stages. In the first stage the binary classification technique is applied to categorize SMS messages into two categories namely, spam and non-spam SMS; then, in the second stage, SMS clusters are created for non-spam SMS messages using non-negative matrix factorization and K-means clustering techniques. A threading-based similarity feature, that is, time between consecutive communications, is described for the identification of SMS threads, and the impact of the time threshold in thread identification is also analysed experimentally. Performance parameters like accuracy, precision, recall and F-measure are also evaluated. The SMS threads identified in this proposed work can be used in applications like SMS thread summarization, SMS folder classification and other SMS management-related tasks."
Social informatics as a concept: Widening the discourse,"This contribution examines the different concepts known as social informatics that have historically been separate. The paradigm that is preferred worldwide (based on Kling) is well described and often promoted, with a strong base both in the USA and Europe. This article, however, introduces lesser-known paradigms (based on Sokolov and later Ursul) that originated in the era of the USSR and have so far been employed chiefly in post-Soviet countries, including Russia. These paradigms have been neglected in English-written scientific literature, mainly because of the limited number of articles available in English. Other approaches are also introduced and related, which were historically named or classified as social informatics (American, British, Norwegian, Slovenian, German and Japanese). The present article introduces and further discusses the origin, historical development and basic methodological grounding of these approaches. All the approaches are then discussed and their differences as well as their similarities are pointed out. The aim is to create connections across the current generation of researchers, which includes the formation and conceptualization of different approaches and an exploration of possible areas for future cooperation. Â© Chartered Institute of Library and Information Professionals.","This contribution examines the different concepts known as social informatics that have historically been separate. The paradigm that is preferred worldwide (based on Kling) is well described and often promoted, with a strong base both in the USA and Europe. This article, however, introduces lesser-known paradigms (based on Sokolov and later Ursul) that originated in the era of the USSR and have so far been employed chiefly in post-Soviet countries, including Russia. These paradigms have been neglected in English-written scientific literature, mainly because of the limited number of articles available in English. Other approaches are also introduced and related, which were historically named or classified as social informatics (American, British, Norwegian, Slovenian, German and Japanese). The present article introduces and further discusses the origin, historical development and basic methodological grounding of these approaches. All the approaches are then discussed and their differences as well as their similarities are pointed out. The aim is to create connections across the current generation of researchers, which includes the formation and conceptualization of different approaches and an exploration of possible areas for future cooperation."
